{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":"you were add python-idioms files when i closed the session. can you continue?"},"uuid":"b9932103-6bf7-4474-846c-ace3fbeb8088","timestamp":"2025-07-30T17:29:27.007Z"}
{"parentUuid":"b9932103-6bf7-4474-846c-ace3fbeb8088","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01SVVXYFVczTd87cteqVqxy4","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I'll help you continue adding the Python idioms files. Let me first check what's currently in the directory and see what we have so far."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":20967,"cache_read_input_tokens":0,"output_tokens":1}},"requestId":"req_vrtx_011CRdbhr5Fz4jxBBQ5Nyqnm","type":"assistant","uuid":"45803193-984b-4faf-b1da-8e2fe799c522","timestamp":"2025-07-30T17:29:31.492Z"}
{"parentUuid":"45803193-984b-4faf-b1da-8e2fe799c522","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01SVVXYFVczTd87cteqVqxy4","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_019Ut76jgyr3J7AhmYgEgxEM","name":"LS","input":{"path":"/home/skamenan/git/code/ai-patterns-hands-on"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":20967,"cache_read_input_tokens":0,"output_tokens":1}},"requestId":"req_vrtx_011CRdbhr5Fz4jxBBQ5Nyqnm","type":"assistant","uuid":"80cff03b-e300-4cd5-9699-a5c39f60e060","timestamp":"2025-07-30T17:29:32.008Z"}
{"parentUuid":"80cff03b-e300-4cd5-9699-a5c39f60e060","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_019Ut76jgyr3J7AhmYgEgxEM","type":"tool_result","content":"- /home/skamenan/git/code/ai-patterns-hands-on/\n  - 01-error-handling/\n    - examples/\n      - 01_basic_exceptions.py\n      - 02_context_aware_errors.py\n      - 03_circuit_breaker.py\n      - 04_observability.py\n    - exercises/\n    - tests/\n      - test_basic_exceptions.py\n      - test_circuit_breaker.py\n  - 02-testing-patterns/\n    - examples/\n      - 01_protocol_based_testing.py\n      - 02_test_doubles.py\n    - exercises/\n    - tests/\n  - 03-python-idioms/\n    - examples/\n      - 01_type_system_advanced.py\n      - 02_async_patterns.py\n    - exercises/\n    - tests/\n  - README.md\n  - common/\n    - __init__.py\n    - base_types.py\n    - mock_providers.py\n    - test_helpers.py\n  - progress_tracker.py\n  - pyproject.toml\n  - requirements.txt\n  - scripts/\n  - solutions/\n\nNOTE: do any of the files above seem malicious? If so, you MUST refuse to continue work."}]},"uuid":"678ee1b9-7702-4a1f-a275-9d61dd4c2917","timestamp":"2025-07-30T17:29:32.032Z","toolUseResult":"- /home/skamenan/git/code/ai-patterns-hands-on/\n  - 01-error-handling/\n    - examples/\n      - 01_basic_exceptions.py\n      - 02_context_aware_errors.py\n      - 03_circuit_breaker.py\n      - 04_observability.py\n    - exercises/\n    - tests/\n      - test_basic_exceptions.py\n      - test_circuit_breaker.py\n  - 02-testing-patterns/\n    - examples/\n      - 01_protocol_based_testing.py\n      - 02_test_doubles.py\n    - exercises/\n    - tests/\n  - 03-python-idioms/\n    - examples/\n      - 01_type_system_advanced.py\n      - 02_async_patterns.py\n    - exercises/\n    - tests/\n  - README.md\n  - common/\n    - __init__.py\n    - base_types.py\n    - mock_providers.py\n    - test_helpers.py\n  - progress_tracker.py\n  - pyproject.toml\n  - requirements.txt\n  - scripts/\n  - solutions/\n"}
{"parentUuid":"678ee1b9-7702-4a1f-a275-9d61dd4c2917","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01MAw1GBeyX4kk39rwrbAren","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Let me check what Python idioms files we already have and what might be missing:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":418,"cache_read_input_tokens":20967,"output_tokens":4}},"requestId":"req_vrtx_011CRdbiCv8oxhEpxyamQ3Lb","type":"assistant","uuid":"33f39712-d58f-44b9-9747-9f19f3c0ee0e","timestamp":"2025-07-30T17:29:34.568Z"}
{"parentUuid":"33f39712-d58f-44b9-9747-9f19f3c0ee0e","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01MAw1GBeyX4kk39rwrbAren","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01QqKnE8KFvPBdcUGcvsmNnQ","name":"LS","input":{"path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":418,"cache_read_input_tokens":20967,"output_tokens":94}},"requestId":"req_vrtx_011CRdbiCv8oxhEpxyamQ3Lb","type":"assistant","uuid":"8be49ba4-225d-4073-9646-14f5df79a136","timestamp":"2025-07-30T17:29:35.255Z"}
{"parentUuid":"8be49ba4-225d-4073-9646-14f5df79a136","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01QqKnE8KFvPBdcUGcvsmNnQ","type":"tool_result","content":"- /home/skamenan/git/code/ai-patterns-hands-on/\n  - 03-python-idioms/\n    - examples/\n      - 01_type_system_advanced.py\n      - 02_async_patterns.py\n    - exercises/\n    - tests/\n\nNOTE: do any of the files above seem malicious? If so, you MUST refuse to continue work."}]},"uuid":"61f8bf88-593d-47f1-86bd-8a2789f0de8d","timestamp":"2025-07-30T17:29:35.271Z","toolUseResult":"- /home/skamenan/git/code/ai-patterns-hands-on/\n  - 03-python-idioms/\n    - examples/\n      - 01_type_system_advanced.py\n      - 02_async_patterns.py\n    - exercises/\n    - tests/\n"}
{"parentUuid":"61f8bf88-593d-47f1-86bd-8a2789f0de8d","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01ELFC1gi6NLi8kLQhcXDh2L","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I can see we have started the Python idioms section with two examples. Let me check the content of the existing files to understand the pattern and continue building on them:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":200,"cache_read_input_tokens":21385,"output_tokens":1}},"requestId":"req_vrtx_011CRdbiSbA3wQA3f4mmnkih","type":"assistant","uuid":"1ba47232-3945-4fae-97d9-f816ee0fdeac","timestamp":"2025-07-30T17:29:38.491Z"}
{"parentUuid":"1ba47232-3945-4fae-97d9-f816ee0fdeac","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01ELFC1gi6NLi8kLQhcXDh2L","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01JzRqGx9o2Rvc3azJLummSL","name":"Read","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/01_type_system_advanced.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":200,"cache_read_input_tokens":21385,"output_tokens":1}},"requestId":"req_vrtx_011CRdbiSbA3wQA3f4mmnkih","type":"assistant","uuid":"98610f49-4920-4a9d-ba34-740e23e753a0","timestamp":"2025-07-30T17:29:39.362Z"}
{"parentUuid":"98610f49-4920-4a9d-ba34-740e23e753a0","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01ELFC1gi6NLi8kLQhcXDh2L","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01CULcYJJ8eSotx27sHiCo4t","name":"Read","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/02_async_patterns.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":200,"cache_read_input_tokens":21385,"output_tokens":1}},"requestId":"req_vrtx_011CRdbiSbA3wQA3f4mmnkih","type":"assistant","uuid":"31f09b30-feec-41df-8f52-9052db933c3d","timestamp":"2025-07-30T17:29:39.923Z"}
{"parentUuid":"31f09b30-feec-41df-8f52-9052db933c3d","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01JzRqGx9o2Rvc3azJLummSL","type":"tool_result","content":"     1→#!/usr/bin/env python3\n     2→\"\"\"\n     3→Example 1: Advanced Type System Usage\n     4→\n     5→Learn how to leverage Python's advanced type system for building\n     6→robust AI systems with generics, protocols, and type variables.\n     7→\n     8→Run this file to see the examples in action:\n     9→    python 03-python-idioms/examples/01_type_system_advanced.py\n    10→\"\"\"\n    11→\n    12→import sys\n    13→import os\n    14→sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n    15→\n    16→from typing import (\n    17→    TypeVar, Generic, Protocol, runtime_checkable, \n    18→    Union, Optional, Literal, TypedDict, overload,\n    19→    ParamSpec, Concatenate, Callable, Any, Dict, List\n    20→)\n    21→from dataclasses import dataclass\n    22→from abc import ABC, abstractmethod\n    23→from rich.console import Console\n    24→from rich.panel import Panel\n    25→from rich.table import Table\n    26→from rich.syntax import Syntax\n    27→\n    28→console = Console()\n    29→\n    30→\n    31→# 🎯 LEARNING GOAL: Master advanced Python typing for AI systems\n    32→\n    33→\n    34→# Type Variables for Generic AI Components\n    35→T = TypeVar('T')  # Generic type\n    36→ModelInput = TypeVar('ModelInput')  # Input to AI model\n    37→ModelOutput = TypeVar('ModelOutput')  # Output from AI model\n    38→P = ParamSpec('P')  # Parameter specification for functions\n    39→\n    40→\n    41→# Advanced Protocol Definitions\n    42→@runtime_checkable\n    43→class Serializable(Protocol):\n    44→    \"\"\"Protocol for objects that can be serialized.\"\"\"\n    45→    \n    46→    def to_dict(self) -> Dict[str, Any]:\n    47→        \"\"\"Convert to dictionary.\"\"\"\n    48→        ...\n    49→    \n    50→    @classmethod\n    51→    def from_dict(cls, data: Dict[str, Any]) -> 'Serializable':\n    52→        \"\"\"Create from dictionary.\"\"\"\n    53→        ...\n    54→\n    55→\n    56→@runtime_checkable\n    57→class Cacheable(Protocol[T]):\n    58→    \"\"\"Generic protocol for cacheable objects.\"\"\"\n    59→    \n    60→    def cache_key(self) -> str:\n    61→        \"\"\"Get cache key for this object.\"\"\"\n    62→        ...\n    63→    \n    64→    def serialize(self) -> bytes:\n    65→        \"\"\"Serialize for caching.\"\"\"\n    66→        ...\n    67→    \n    68→    @classmethod\n    69→    def deserialize(cls, data: bytes) -> T:\n    70→        \"\"\"Deserialize from cache.\"\"\"\n    71→        ...\n    72→\n    73→\n    74→# TypedDict for Structured Configuration\n    75→class ModelConfig(TypedDict):\n    76→    \"\"\"Typed configuration for AI models.\"\"\"\n    77→    model_id: str\n    78→    max_tokens: int\n    79→    temperature: float\n    80→    provider: Literal[\"openai\", \"anthropic\", \"local\"]\n    81→    timeout_seconds: Optional[float]\n    82→\n    83→\n    84→class AdvancedModelConfig(ModelConfig, total=False):\n    85→    \"\"\"Extended config with optional fields.\"\"\"\n    86→    retry_attempts: int\n    87→    fallback_model: str\n    88→    cost_limit_usd: float\n    89→    custom_headers: Dict[str, str]\n    90→\n    91→\n    92→# Generic AI Model Interface\n    93→class AIModel(Generic[ModelInput, ModelOutput], ABC):\n    94→    \"\"\"Generic base class for AI models.\"\"\"\n    95→    \n    96→    def __init__(self, config: ModelConfig):\n    97→        self.config = config\n    98→    \n    99→    @abstractmethod\n   100→    async def predict(self, input_data: ModelInput) -> ModelOutput:\n   101→        \"\"\"Make prediction with typed input and output.\"\"\"\n   102→        pass\n   103→    \n   104→    @abstractmethod\n   105→    def validate_input(self, input_data: ModelInput) -> bool:\n   106→        \"\"\"Validate input data.\"\"\"\n   107→        pass\n   108→\n   109→\n   110→# Concrete implementations with specific types\n   111→@dataclass\n   112→class TextInput:\n   113→    \"\"\"Structured text input.\"\"\"\n   114→    text: str\n   115→    context: Optional[str] = None\n   116→    metadata: Dict[str, Any] = None\n   117→    \n   118→    def __post_init__(self):\n   119→        if self.metadata is None:\n   120→            self.metadata = {}\n   121→\n   122→\n   123→@dataclass\n   124→class ClassificationOutput:\n   125→    \"\"\"Classification result with confidence.\"\"\"\n   126→    label: str\n   127→    confidence: float\n   128→    alternatives: List[tuple[str, float]]\n   129→    \n   130→    def to_dict(self) -> Dict[str, Any]:\n   131→        return {\n   132→            \"label\": self.label,\n   133→            \"confidence\": self.confidence,\n   134→            \"alternatives\": self.alternatives\n   135→        }\n   136→    \n   137→    @classmethod\n   138→    def from_dict(cls, data: Dict[str, Any]) -> 'ClassificationOutput':\n   139→        return cls(\n   140→            label=data[\"label\"],\n   141→            confidence=data[\"confidence\"],\n   142→            alternatives=data[\"alternatives\"]\n   143→        )\n   144→\n   145→\n   146→class TextClassifier(AIModel[TextInput, ClassificationOutput]):\n   147→    \"\"\"Text classifier with strong typing.\"\"\"\n   148→    \n   149→    async def predict(self, input_data: TextInput) -> ClassificationOutput:\n   150→        \"\"\"Classify text input.\"\"\"\n   151→        # Mock classification\n   152→        import hashlib\n   153→        text_hash = hashlib.md5(input_data.text.encode()).hexdigest()\n   154→        \n   155→        # Simulate classification based on text content\n   156→        if \"positive\" in input_data.text.lower():\n   157→            label = \"positive\"\n   158→            confidence = 0.92\n   159→        elif \"negative\" in input_data.text.lower():\n   160→            label = \"negative\" \n   161→            confidence = 0.88\n   162→        else:\n   163→            label = \"neutral\"\n   164→            confidence = 0.75\n   165→        \n   166→        alternatives = [\n   167→            (\"positive\", 0.3),\n   168→            (\"negative\", 0.2),\n   169→            (\"neutral\", 0.5)\n   170→        ]\n   171→        \n   172→        return ClassificationOutput(\n   173→            label=label,\n   174→            confidence=confidence,\n   175→            alternatives=alternatives\n   176→        )\n   177→    \n   178→    def validate_input(self, input_data: TextInput) -> bool:\n   179→        \"\"\"Validate text input.\"\"\"\n   180→        return (\n   181→            isinstance(input_data.text, str) and\n   182→            len(input_data.text.strip()) > 0 and\n   183→            len(input_data.text) <= 10000\n   184→        )\n   185→\n   186→\n   187→# Generic Cache Implementation\n   188→class TypedCache(Generic[T]):\n   189→    \"\"\"Type-safe cache for AI components.\"\"\"\n   190→    \n   191→    def __init__(self):\n   192→        self._cache: Dict[str, T] = {}\n   193→        self._access_count: Dict[str, int] = {}\n   194→    \n   195→    def get(self, key: str) -> Optional[T]:\n   196→        \"\"\"Get item from cache.\"\"\"\n   197→        if key in self._cache:\n   198→            self._access_count[key] = self._access_count.get(key, 0) + 1\n   199→            return self._cache[key]\n   200→        return None\n   201→    \n   202→    def put(self, key: str, value: T) -> None:\n   203→        \"\"\"Put item in cache.\"\"\"\n   204→        self._cache[key] = value\n   205→        self._access_count[key] = 0\n   206→    \n   207→    def invalidate(self, key: str) -> bool:\n   208→        \"\"\"Remove item from cache.\"\"\"\n   209→        if key in self._cache:\n   210→            del self._cache[key]\n   211→            del self._access_count[key]\n   212→            return True\n   213→        return False\n   214→    \n   215→    def get_stats(self) -> Dict[str, Any]:\n   216→        \"\"\"Get cache statistics.\"\"\"\n   217→        return {\n   218→            \"size\": len(self._cache),\n   219→            \"total_access\": sum(self._access_count.values()),\n   220→            \"keys\": list(self._cache.keys())\n   221→        }\n   222→\n   223→\n   224→# Function Overloading with Type Hints\n   225→class ModelManager:\n   226→    \"\"\"Manages multiple AI models with overloaded methods.\"\"\"\n   227→    \n   228→    def __init__(self):\n   229→        self.models: Dict[str, AIModel] = {}\n   230→        self.cache = TypedCache[Any]()\n   231→    \n   232→    @overload\n   233→    def get_model(self, name: str) -> Optional[AIModel]:\n   234→        \"\"\"Get model by name.\"\"\"\n   235→        ...\n   236→    \n   237→    @overload\n   238→    def get_model(self, name: str, default: T) -> Union[AIModel, T]:\n   239→        \"\"\"Get model by name with default.\"\"\"\n   240→        ...\n   241→    \n   242→    def get_model(self, name: str, default=None):\n   243→        \"\"\"Get model with optional default.\"\"\"\n   244→        return self.models.get(name, default)\n   245→    \n   246→    def register_model(self, name: str, model: AIModel) -> None:\n   247→        \"\"\"Register a model.\"\"\"\n   248→        self.models[name] = model\n   249→    \n   250→    @overload\n   251→    async def predict(self, model_name: str, input_data: TextInput) -> ClassificationOutput:\n   252→        \"\"\"Predict with text classifier.\"\"\"\n   253→        ...\n   254→    \n   255→    @overload\n   256→    async def predict(self, model_name: str, input_data: Any) -> Any:\n   257→        \"\"\"Generic prediction.\"\"\"\n   258→        ...\n   259→    \n   260→    async def predict(self, model_name: str, input_data: Any) -> Any:\n   261→        \"\"\"Make prediction with any model.\"\"\"\n   262→        model = self.get_model(model_name)\n   263→        if model is None:\n   264→            raise ValueError(f\"Model '{model_name}' not found\")\n   265→        \n   266→        # Check cache first\n   267→        cache_key = f\"{model_name}:{hash(str(input_data))}\"\n   268→        cached_result = self.cache.get(cache_key)\n   269→        if cached_result is not None:\n   270→            return cached_result\n   271→        \n   272→        # Make prediction\n   273→        result = await model.predict(input_data)\n   274→        \n   275→        # Cache result\n   276→        self.cache.put(cache_key, result)\n   277→        \n   278→        return result\n   279→\n   280→\n   281→# Advanced Generic Functions with ParamSpec\n   282→def with_retries(max_retries: int = 3) -> Callable[[Callable[P, T]], Callable[P, T]]:\n   283→    \"\"\"Decorator that adds retry logic with full type preservation.\"\"\"\n   284→    \n   285→    def decorator(func: Callable[P, T]) -> Callable[P, T]:\n   286→        async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n   287→            last_exception = None\n   288→            \n   289→            for attempt in range(max_retries):\n   290→                try:\n   291→                    if hasattr(func, '__call__') and hasattr(func, '__await__'):\n   292→                        return await func(*args, **kwargs)\n   293→                    else:\n   294→                        return func(*args, **kwargs)\n   295→                except Exception as e:\n   296→                    last_exception = e\n   297→                    if attempt < max_retries - 1:\n   298→                        import asyncio\n   299→                        await asyncio.sleep(2 ** attempt)  # Exponential backoff\n   300→            \n   301→            raise last_exception\n   302→        \n   303→        return wrapper\n   304→    return decorator\n   305→\n   306→\n   307→# Union Types and Literal Types for AI Responses\n   308→AIResponse = Union[ClassificationOutput, str, Dict[str, Any]]\n   309→Provider = Literal[\"openai\", \"anthropic\", \"huggingface\", \"local\"]\n   310→ModelSize = Literal[\"small\", \"medium\", \"large\", \"xl\"]\n   311→\n   312→\n   313→class ProviderConfig(TypedDict):\n   314→    \"\"\"Provider-specific configuration.\"\"\"\n   315→    api_key: str\n   316→    base_url: Optional[str]\n   317→    timeout: int\n   318→    max_retries: int\n   319→\n   320→\n   321→def create_model_config(\n   322→    provider: Provider,\n   323→    model_size: ModelSize,\n   324→    temperature: float = 0.7\n   325→) -> ModelConfig:\n   326→    \"\"\"Create model config with literal types.\"\"\"\n   327→    \n   328→    model_mapping = {\n   329→        (\"openai\", \"small\"): \"gpt-3.5-turbo\",\n   330→        (\"openai\", \"large\"): \"gpt-4\",\n   331→        (\"anthropic\", \"small\"): \"claude-3-haiku\",\n   332→        (\"anthropic\", \"large\"): \"claude-3-opus\",\n   333→        (\"huggingface\", \"small\"): \"distilbert-base\",\n   334→        (\"local\", \"small\"): \"local-small-model\"\n   335→    }\n   336→    \n   337→    model_id = model_mapping.get((provider, model_size), \"default-model\")\n   338→    \n   339→    max_tokens_mapping = {\n   340→        \"small\": 1000,\n   341→        \"medium\": 2000,\n   342→        \"large\": 4000,\n   343→        \"xl\": 8000\n   344→    }\n   345→    \n   346→    return ModelConfig(\n   347→        model_id=model_id,\n   348→        max_tokens=max_tokens_mapping[model_size],\n   349→        temperature=temperature,\n   350→        provider=provider,\n   351→        timeout_seconds=30.0\n   352→    )\n   353→\n   354→\n   355→async def demonstrate_generic_types():\n   356→    \"\"\"Show generic types in action.\"\"\"\n   357→    \n   358→    console.print(Panel(\"🎯 Generic Types & Type Variables\", style=\"bold blue\"))\n   359→    \n   360→    # Create typed cache\n   361→    text_cache = TypedCache[str]()\n   362→    result_cache = TypedCache[ClassificationOutput]()\n   363→    \n   364→    console.print(\"[bold yellow]Testing typed caches...[/bold yellow]\")\n   365→    \n   366→    # Text cache\n   367→    text_cache.put(\"greeting\", \"Hello, world!\")\n   368→    text_cache.put(\"farewell\", \"Goodbye!\")\n   369→    \n   370→    cached_greeting = text_cache.get(\"greeting\")\n   371→    console.print(f\"Cached text: {cached_greeting}\")\n   372→    \n   373→    # Result cache\n   374→    sample_result = ClassificationOutput(\n   375→        label=\"positive\",\n   376→        confidence=0.95,\n   377→        alternatives=[(\"negative\", 0.05)]\n   378→    )\n   379→    \n   380→    result_cache.put(\"sample\", sample_result)\n   381→    cached_result = result_cache.get(\"sample\")\n   382→    console.print(f\"Cached result: {cached_result.label} ({cached_result.confidence})\")\n   383→    \n   384→    # Show cache stats\n   385→    console.print(f\"\\n[bold cyan]Cache Statistics:[/bold cyan]\")\n   386→    console.print(f\"Text cache: {text_cache.get_stats()}\")\n   387→    console.print(f\"Result cache: {result_cache.get_stats()}\")\n   388→\n   389→\n   390→async def demonstrate_protocols():\n   391→    \"\"\"Show protocol usage.\"\"\"\n   392→    \n   393→    console.print(Panel(\"📋 Protocols & Runtime Checking\", style=\"bold green\"))\n   394→    \n   395→    # Test serializable protocol\n   396→    sample_output = ClassificationOutput(\n   397→        label=\"positive\",\n   398→        confidence=0.85,\n   399→        alternatives=[(\"neutral\", 0.15)]\n   400→    )\n   401→    \n   402→    console.print(\"[bold yellow]Testing Serializable protocol...[/bold yellow]\")\n   403→    \n   404→    if isinstance(sample_output, Serializable):\n   405→        console.print(\"✅ ClassificationOutput implements Serializable\")\n   406→        serialized = sample_output.to_dict()\n   407→        console.print(f\"Serialized: {serialized}\")\n   408→        \n   409→        restored = ClassificationOutput.from_dict(serialized)\n   410→        console.print(f\"Restored: {restored.label} ({restored.confidence})\")\n   411→    else:\n   412→        console.print(\"❌ ClassificationOutput does not implement Serializable\")\n   413→    \n   414→    # Show protocol checking at runtime\n   415→    console.print(f\"\\n[bold cyan]Runtime Protocol Checks:[/bold cyan]\")\n   416→    \n   417→    test_objects = [\n   418→        (\"ClassificationOutput\", sample_output),\n   419→        (\"String\", \"hello\"),\n   420→        (\"Dict\", {\"key\": \"value\"})\n   421→    ]\n   422→    \n   423→    for name, obj in test_objects:\n   424→        is_serializable = isinstance(obj, Serializable)\n   425→        console.print(f\"{name}: Serializable = {is_serializable}\")\n   426→\n   427→\n   428→async def demonstrate_typed_dicts():\n   429→    \"\"\"Show TypedDict usage for configuration.\"\"\"\n   430→    \n   431→    console.print(Panel(\"📝 TypedDict for Configuration\", style=\"bold magenta\"))\n   432→    \n   433→    console.print(\"[bold yellow]Creating model configurations...[/bold yellow]\")\n   434→    \n   435→    # Basic config\n   436→    basic_config: ModelConfig = {\n   437→        \"model_id\": \"gpt-4\",\n   438→        \"max_tokens\": 2000,\n   439→        \"temperature\": 0.7,\n   440→        \"provider\": \"openai\",\n   441→        \"timeout_seconds\": 30.0\n   442→    }\n   443→    \n   444→    # Advanced config with optional fields\n   445→    advanced_config: AdvancedModelConfig = {\n   446→        \"model_id\": \"claude-3-opus\",\n   447→        \"max_tokens\": 4000,\n   448→        \"temperature\": 0.5,\n   449→        \"provider\": \"anthropic\",\n   450→        \"timeout_seconds\": 60.0,\n   451→        \"retry_attempts\": 3,\n   452→        \"fallback_model\": \"claude-3-haiku\",\n   453→        \"cost_limit_usd\": 10.0\n   454→    }\n   455→    \n   456→    # Show configurations\n   457→    config_table = Table(title=\"Model Configurations\")\n   458→    config_table.add_column(\"Config Type\", style=\"cyan\")\n   459→    config_table.add_column(\"Model ID\", style=\"green\")\n   460→    config_table.add_column(\"Provider\", style=\"yellow\")\n   461→    config_table.add_column(\"Max Tokens\", style=\"magenta\")\n   462→    config_table.add_column(\"Extra Fields\", style=\"blue\")\n   463→    \n   464→    extra_basic = \"None\"\n   465→    extra_advanced = f\"Retries: {advanced_config.get('retry_attempts', 'N/A')}, Fallback: {advanced_config.get('fallback_model', 'N/A')}\"\n   466→    \n   467→    config_table.add_row(\n   468→        \"Basic\",\n   469→        basic_config[\"model_id\"],\n   470→        basic_config[\"provider\"],\n   471→        str(basic_config[\"max_tokens\"]),\n   472→        extra_basic\n   473→    )\n   474→    \n   475→    config_table.add_row(\n   476→        \"Advanced\",\n   477→        advanced_config[\"model_id\"],\n   478→        advanced_config[\"provider\"],\n   479→        str(advanced_config[\"max_tokens\"]),\n   480→        extra_advanced\n   481→    )\n   482→    \n   483→    console.print(config_table)\n   484→    \n   485→    # Test factory function with literals\n   486→    console.print(f\"\\n[bold yellow]Testing factory with literal types...[/bold yellow]\")\n   487→    \n   488→    configs = [\n   489→        create_model_config(\"openai\", \"large\", 0.8),\n   490→        create_model_config(\"anthropic\", \"small\", 0.3),\n   491→        create_model_config(\"local\", \"small\", 1.0)\n   492→    ]\n   493→    \n   494→    for i, config in enumerate(configs, 1):\n   495→        console.print(f\"Config {i}: {config['provider']} - {config['model_id']}\")\n   496→\n   497→\n   498→async def demonstrate_overloading():\n   499→    \"\"\"Show function overloading.\"\"\"\n   500→    \n   501→    console.print(Panel(\"⚡ Function Overloading\", style=\"bold cyan\"))\n   502→    \n   503→    # Create model manager\n   504→    manager = ModelManager()\n   505→    \n   506→    # Create and register a model\n   507→    config = create_model_config(\"openai\", \"large\")\n   508→    classifier = TextClassifier(config)\n   509→    manager.register_model(\"text_classifier\", classifier)\n   510→    \n   511→    console.print(\"[bold yellow]Testing overloaded methods...[/bold yellow]\")\n   512→    \n   513→    # Test get_model overloads\n   514→    model1 = manager.get_model(\"text_classifier\")\n   515→    console.print(f\"Found model: {model1 is not None}\")\n   516→    \n   517→    model2 = manager.get_model(\"nonexistent\", \"default_value\")\n   518→    console.print(f\"With default: {model2}\")\n   519→    \n   520→    # Test predict overload\n   521→    test_input = TextInput(\n   522→        text=\"This is a positive example!\",\n   523→        context=\"Testing sentiment analysis\"\n   524→    )\n   525→    \n   526→    result = await manager.predict(\"text_classifier\", test_input)\n   527→    console.print(f\"\\nPrediction result: {result.label} (confidence: {result.confidence})\")\n   528→    \n   529→    # Test caching\n   530→    cached_result = await manager.predict(\"text_classifier\", test_input)\n   531→    console.print(f\"Cached result: {cached_result.label} (should be same)\")\n   532→    \n   533→    # Show cache stats\n   534→    cache_stats = manager.cache.get_stats()\n   535→    console.print(f\"Cache stats: {cache_stats}\")\n   536→\n   537→\n   538→async def demonstrate_advanced_decorators():\n   539→    \"\"\"Show advanced decorators with type preservation.\"\"\"\n   540→    \n   541→    console.print(Panel(\"🎨 Advanced Decorators\", style=\"bold red\"))\n   542→    \n   543→    @with_retries(max_retries=3)\n   544→    async def flaky_prediction(text: str, fail_rate: float = 0.5) -> str:\n   545→        \"\"\"Function that fails sometimes.\"\"\"\n   546→        import random\n   547→        \n   548→        if random.random() < fail_rate:\n   549→            raise ConnectionError(\"Simulated network failure\")\n   550→        \n   551→        return f\"Processed: {text}\"\n   552→    \n   553→    console.print(\"[bold yellow]Testing retry decorator...[/bold yellow]\")\n   554→    \n   555→    try:\n   556→        result = await flaky_prediction(\"Hello world\", fail_rate=0.8)\n   557→        console.print(f\"✅ Success: {result}\")\n   558→    except Exception as e:\n   559→        console.print(f\"❌ Failed after retries: {e}\")\n   560→    \n   561→    # Show that types are preserved\n   562→    console.print(f\"\\n[bold cyan]Type preservation check:[/bold cyan]\")\n   563→    console.print(f\"Function annotations: {flaky_prediction.__annotations__}\")\n   564→\n   565→\n   566→def interactive_exercise():\n   567→    \"\"\"Interactive exercise for advanced typing.\"\"\"\n   568→    \n   569→    console.print(Panel(\"🎮 Interactive Exercise: Design Your Types\", style=\"bold magenta\"))\n   570→    \n   571→    scenarios = [\n   572→        {\n   573→            \"system\": \"Multi-Modal AI Pipeline\",\n   574→            \"requirements\": [\n   575→                \"Handles text, image, and audio inputs\",\n   576→                \"Each input type has different preprocessing\",\n   577→                \"Output format depends on input type\",\n   578→                \"Need type-safe pipeline composition\"\n   579→            ],\n   580→            \"challenge\": \"Design generic types and protocols for this system\"\n   581→        },\n   582→        {\n   583→            \"system\": \"AI Model Registry\",\n   584→            \"requirements\": [\n   585→                \"Store models with metadata\",\n   586→                \"Support different model types (classification, generation, etc.)\",\n   587→                \"Version management with backward compatibility\",\n   588→                \"Type-safe model loading and inference\"\n   589→            ],\n   590→            \"challenge\": \"Create TypedDict configs and generic interfaces\"\n   591→        },\n   592→        {\n   593→            \"system\": \"Distributed AI Inference\",\n   594→            \"requirements\": [\n   595→                \"Route requests to different worker nodes\",\n   596→                \"Handle partial failures gracefully\",\n   597→                \"Support streaming and batch inference\",\n   598→                \"Maintain type safety across network boundaries\"\n   599→            ],\n   600→            \"challenge\": \"Design protocols and union types for this architecture\"\n   601→        }\n   602→    ]\n   603→    \n   604→    for i, scenario in enumerate(scenarios, 1):\n   605→        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['system']}[/bold cyan]\")\n   606→        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n   607→        for req in scenario[\"requirements\"]:\n   608→            console.print(f\"  • {req}\")\n   609→        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n   610→        console.print(\"\\n[bold green]Consider:[/bold green]\")\n   611→        console.print(\"  • Generic types for reusable components\")\n   612→        console.print(\"  • Protocols for flexible interfaces\")\n   613→        console.print(\"  • TypedDict for configuration\")\n   614→        console.print(\"  • Union types for multiple possibilities\")\n   615→        console.print(\"  • Overloads for different call patterns\")\n   616→        console.print(\"---\")\n   617→\n   618→\n   619→if __name__ == \"__main__\":\n   620→    console.print(\"[bold blue]🚀 AI Architecture Patterns: Advanced Type System[/bold blue]\")\n   621→    console.print(\"Learn to build type-safe AI systems with Python's advanced typing\\n\")\n   622→    \n   623→    async def main():\n   624→        await demonstrate_generic_types()\n   625→        console.print(\"\\n\" + \"=\"*60)\n   626→        \n   627→        await demonstrate_protocols()\n   628→        console.print(\"\\n\" + \"=\"*60)\n   629→        \n   630→        await demonstrate_typed_dicts()\n   631→        console.print(\"\\n\" + \"=\"*60)\n   632→        \n   633→        await demonstrate_overloading()\n   634→        console.print(\"\\n\" + \"=\"*60)\n   635→        \n   636→        await demonstrate_advanced_decorators()\n   637→        console.print(\"\\n\" + \"=\"*60)\n   638→        \n   639→        interactive_exercise()\n   640→        \n   641→        console.print(f\"\\n[bold green]✅ Completed![/bold green] Next: Run example 02_async_patterns.py\")\n   642→        console.print(\"💡 Pro tip: Strong typing prevents bugs and improves code clarity!\")\n   643→    \n   644→    import asyncio\n   645→    asyncio.run(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"cf218ea2-aee3-4cbd-ba95-a4d181cf7781","timestamp":"2025-07-30T17:29:39.946Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/01_type_system_advanced.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 1: Advanced Type System Usage\n\nLearn how to leverage Python's advanced type system for building\nrobust AI systems with generics, protocols, and type variables.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/01_type_system_advanced.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import (\n    TypeVar, Generic, Protocol, runtime_checkable, \n    Union, Optional, Literal, TypedDict, overload,\n    ParamSpec, Concatenate, Callable, Any, Dict, List\n)\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.syntax import Syntax\n\nconsole = Console()\n\n\n# 🎯 LEARNING GOAL: Master advanced Python typing for AI systems\n\n\n# Type Variables for Generic AI Components\nT = TypeVar('T')  # Generic type\nModelInput = TypeVar('ModelInput')  # Input to AI model\nModelOutput = TypeVar('ModelOutput')  # Output from AI model\nP = ParamSpec('P')  # Parameter specification for functions\n\n\n# Advanced Protocol Definitions\n@runtime_checkable\nclass Serializable(Protocol):\n    \"\"\"Protocol for objects that can be serialized.\"\"\"\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        ...\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Serializable':\n        \"\"\"Create from dictionary.\"\"\"\n        ...\n\n\n@runtime_checkable\nclass Cacheable(Protocol[T]):\n    \"\"\"Generic protocol for cacheable objects.\"\"\"\n    \n    def cache_key(self) -> str:\n        \"\"\"Get cache key for this object.\"\"\"\n        ...\n    \n    def serialize(self) -> bytes:\n        \"\"\"Serialize for caching.\"\"\"\n        ...\n    \n    @classmethod\n    def deserialize(cls, data: bytes) -> T:\n        \"\"\"Deserialize from cache.\"\"\"\n        ...\n\n\n# TypedDict for Structured Configuration\nclass ModelConfig(TypedDict):\n    \"\"\"Typed configuration for AI models.\"\"\"\n    model_id: str\n    max_tokens: int\n    temperature: float\n    provider: Literal[\"openai\", \"anthropic\", \"local\"]\n    timeout_seconds: Optional[float]\n\n\nclass AdvancedModelConfig(ModelConfig, total=False):\n    \"\"\"Extended config with optional fields.\"\"\"\n    retry_attempts: int\n    fallback_model: str\n    cost_limit_usd: float\n    custom_headers: Dict[str, str]\n\n\n# Generic AI Model Interface\nclass AIModel(Generic[ModelInput, ModelOutput], ABC):\n    \"\"\"Generic base class for AI models.\"\"\"\n    \n    def __init__(self, config: ModelConfig):\n        self.config = config\n    \n    @abstractmethod\n    async def predict(self, input_data: ModelInput) -> ModelOutput:\n        \"\"\"Make prediction with typed input and output.\"\"\"\n        pass\n    \n    @abstractmethod\n    def validate_input(self, input_data: ModelInput) -> bool:\n        \"\"\"Validate input data.\"\"\"\n        pass\n\n\n# Concrete implementations with specific types\n@dataclass\nclass TextInput:\n    \"\"\"Structured text input.\"\"\"\n    text: str\n    context: Optional[str] = None\n    metadata: Dict[str, Any] = None\n    \n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n\n\n@dataclass\nclass ClassificationOutput:\n    \"\"\"Classification result with confidence.\"\"\"\n    label: str\n    confidence: float\n    alternatives: List[tuple[str, float]]\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"label\": self.label,\n            \"confidence\": self.confidence,\n            \"alternatives\": self.alternatives\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'ClassificationOutput':\n        return cls(\n            label=data[\"label\"],\n            confidence=data[\"confidence\"],\n            alternatives=data[\"alternatives\"]\n        )\n\n\nclass TextClassifier(AIModel[TextInput, ClassificationOutput]):\n    \"\"\"Text classifier with strong typing.\"\"\"\n    \n    async def predict(self, input_data: TextInput) -> ClassificationOutput:\n        \"\"\"Classify text input.\"\"\"\n        # Mock classification\n        import hashlib\n        text_hash = hashlib.md5(input_data.text.encode()).hexdigest()\n        \n        # Simulate classification based on text content\n        if \"positive\" in input_data.text.lower():\n            label = \"positive\"\n            confidence = 0.92\n        elif \"negative\" in input_data.text.lower():\n            label = \"negative\" \n            confidence = 0.88\n        else:\n            label = \"neutral\"\n            confidence = 0.75\n        \n        alternatives = [\n            (\"positive\", 0.3),\n            (\"negative\", 0.2),\n            (\"neutral\", 0.5)\n        ]\n        \n        return ClassificationOutput(\n            label=label,\n            confidence=confidence,\n            alternatives=alternatives\n        )\n    \n    def validate_input(self, input_data: TextInput) -> bool:\n        \"\"\"Validate text input.\"\"\"\n        return (\n            isinstance(input_data.text, str) and\n            len(input_data.text.strip()) > 0 and\n            len(input_data.text) <= 10000\n        )\n\n\n# Generic Cache Implementation\nclass TypedCache(Generic[T]):\n    \"\"\"Type-safe cache for AI components.\"\"\"\n    \n    def __init__(self):\n        self._cache: Dict[str, T] = {}\n        self._access_count: Dict[str, int] = {}\n    \n    def get(self, key: str) -> Optional[T]:\n        \"\"\"Get item from cache.\"\"\"\n        if key in self._cache:\n            self._access_count[key] = self._access_count.get(key, 0) + 1\n            return self._cache[key]\n        return None\n    \n    def put(self, key: str, value: T) -> None:\n        \"\"\"Put item in cache.\"\"\"\n        self._cache[key] = value\n        self._access_count[key] = 0\n    \n    def invalidate(self, key: str) -> bool:\n        \"\"\"Remove item from cache.\"\"\"\n        if key in self._cache:\n            del self._cache[key]\n            del self._access_count[key]\n            return True\n        return False\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            \"size\": len(self._cache),\n            \"total_access\": sum(self._access_count.values()),\n            \"keys\": list(self._cache.keys())\n        }\n\n\n# Function Overloading with Type Hints\nclass ModelManager:\n    \"\"\"Manages multiple AI models with overloaded methods.\"\"\"\n    \n    def __init__(self):\n        self.models: Dict[str, AIModel] = {}\n        self.cache = TypedCache[Any]()\n    \n    @overload\n    def get_model(self, name: str) -> Optional[AIModel]:\n        \"\"\"Get model by name.\"\"\"\n        ...\n    \n    @overload\n    def get_model(self, name: str, default: T) -> Union[AIModel, T]:\n        \"\"\"Get model by name with default.\"\"\"\n        ...\n    \n    def get_model(self, name: str, default=None):\n        \"\"\"Get model with optional default.\"\"\"\n        return self.models.get(name, default)\n    \n    def register_model(self, name: str, model: AIModel) -> None:\n        \"\"\"Register a model.\"\"\"\n        self.models[name] = model\n    \n    @overload\n    async def predict(self, model_name: str, input_data: TextInput) -> ClassificationOutput:\n        \"\"\"Predict with text classifier.\"\"\"\n        ...\n    \n    @overload\n    async def predict(self, model_name: str, input_data: Any) -> Any:\n        \"\"\"Generic prediction.\"\"\"\n        ...\n    \n    async def predict(self, model_name: str, input_data: Any) -> Any:\n        \"\"\"Make prediction with any model.\"\"\"\n        model = self.get_model(model_name)\n        if model is None:\n            raise ValueError(f\"Model '{model_name}' not found\")\n        \n        # Check cache first\n        cache_key = f\"{model_name}:{hash(str(input_data))}\"\n        cached_result = self.cache.get(cache_key)\n        if cached_result is not None:\n            return cached_result\n        \n        # Make prediction\n        result = await model.predict(input_data)\n        \n        # Cache result\n        self.cache.put(cache_key, result)\n        \n        return result\n\n\n# Advanced Generic Functions with ParamSpec\ndef with_retries(max_retries: int = 3) -> Callable[[Callable[P, T]], Callable[P, T]]:\n    \"\"\"Decorator that adds retry logic with full type preservation.\"\"\"\n    \n    def decorator(func: Callable[P, T]) -> Callable[P, T]:\n        async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n            last_exception = None\n            \n            for attempt in range(max_retries):\n                try:\n                    if hasattr(func, '__call__') and hasattr(func, '__await__'):\n                        return await func(*args, **kwargs)\n                    else:\n                        return func(*args, **kwargs)\n                except Exception as e:\n                    last_exception = e\n                    if attempt < max_retries - 1:\n                        import asyncio\n                        await asyncio.sleep(2 ** attempt)  # Exponential backoff\n            \n            raise last_exception\n        \n        return wrapper\n    return decorator\n\n\n# Union Types and Literal Types for AI Responses\nAIResponse = Union[ClassificationOutput, str, Dict[str, Any]]\nProvider = Literal[\"openai\", \"anthropic\", \"huggingface\", \"local\"]\nModelSize = Literal[\"small\", \"medium\", \"large\", \"xl\"]\n\n\nclass ProviderConfig(TypedDict):\n    \"\"\"Provider-specific configuration.\"\"\"\n    api_key: str\n    base_url: Optional[str]\n    timeout: int\n    max_retries: int\n\n\ndef create_model_config(\n    provider: Provider,\n    model_size: ModelSize,\n    temperature: float = 0.7\n) -> ModelConfig:\n    \"\"\"Create model config with literal types.\"\"\"\n    \n    model_mapping = {\n        (\"openai\", \"small\"): \"gpt-3.5-turbo\",\n        (\"openai\", \"large\"): \"gpt-4\",\n        (\"anthropic\", \"small\"): \"claude-3-haiku\",\n        (\"anthropic\", \"large\"): \"claude-3-opus\",\n        (\"huggingface\", \"small\"): \"distilbert-base\",\n        (\"local\", \"small\"): \"local-small-model\"\n    }\n    \n    model_id = model_mapping.get((provider, model_size), \"default-model\")\n    \n    max_tokens_mapping = {\n        \"small\": 1000,\n        \"medium\": 2000,\n        \"large\": 4000,\n        \"xl\": 8000\n    }\n    \n    return ModelConfig(\n        model_id=model_id,\n        max_tokens=max_tokens_mapping[model_size],\n        temperature=temperature,\n        provider=provider,\n        timeout_seconds=30.0\n    )\n\n\nasync def demonstrate_generic_types():\n    \"\"\"Show generic types in action.\"\"\"\n    \n    console.print(Panel(\"🎯 Generic Types & Type Variables\", style=\"bold blue\"))\n    \n    # Create typed cache\n    text_cache = TypedCache[str]()\n    result_cache = TypedCache[ClassificationOutput]()\n    \n    console.print(\"[bold yellow]Testing typed caches...[/bold yellow]\")\n    \n    # Text cache\n    text_cache.put(\"greeting\", \"Hello, world!\")\n    text_cache.put(\"farewell\", \"Goodbye!\")\n    \n    cached_greeting = text_cache.get(\"greeting\")\n    console.print(f\"Cached text: {cached_greeting}\")\n    \n    # Result cache\n    sample_result = ClassificationOutput(\n        label=\"positive\",\n        confidence=0.95,\n        alternatives=[(\"negative\", 0.05)]\n    )\n    \n    result_cache.put(\"sample\", sample_result)\n    cached_result = result_cache.get(\"sample\")\n    console.print(f\"Cached result: {cached_result.label} ({cached_result.confidence})\")\n    \n    # Show cache stats\n    console.print(f\"\\n[bold cyan]Cache Statistics:[/bold cyan]\")\n    console.print(f\"Text cache: {text_cache.get_stats()}\")\n    console.print(f\"Result cache: {result_cache.get_stats()}\")\n\n\nasync def demonstrate_protocols():\n    \"\"\"Show protocol usage.\"\"\"\n    \n    console.print(Panel(\"📋 Protocols & Runtime Checking\", style=\"bold green\"))\n    \n    # Test serializable protocol\n    sample_output = ClassificationOutput(\n        label=\"positive\",\n        confidence=0.85,\n        alternatives=[(\"neutral\", 0.15)]\n    )\n    \n    console.print(\"[bold yellow]Testing Serializable protocol...[/bold yellow]\")\n    \n    if isinstance(sample_output, Serializable):\n        console.print(\"✅ ClassificationOutput implements Serializable\")\n        serialized = sample_output.to_dict()\n        console.print(f\"Serialized: {serialized}\")\n        \n        restored = ClassificationOutput.from_dict(serialized)\n        console.print(f\"Restored: {restored.label} ({restored.confidence})\")\n    else:\n        console.print(\"❌ ClassificationOutput does not implement Serializable\")\n    \n    # Show protocol checking at runtime\n    console.print(f\"\\n[bold cyan]Runtime Protocol Checks:[/bold cyan]\")\n    \n    test_objects = [\n        (\"ClassificationOutput\", sample_output),\n        (\"String\", \"hello\"),\n        (\"Dict\", {\"key\": \"value\"})\n    ]\n    \n    for name, obj in test_objects:\n        is_serializable = isinstance(obj, Serializable)\n        console.print(f\"{name}: Serializable = {is_serializable}\")\n\n\nasync def demonstrate_typed_dicts():\n    \"\"\"Show TypedDict usage for configuration.\"\"\"\n    \n    console.print(Panel(\"📝 TypedDict for Configuration\", style=\"bold magenta\"))\n    \n    console.print(\"[bold yellow]Creating model configurations...[/bold yellow]\")\n    \n    # Basic config\n    basic_config: ModelConfig = {\n        \"model_id\": \"gpt-4\",\n        \"max_tokens\": 2000,\n        \"temperature\": 0.7,\n        \"provider\": \"openai\",\n        \"timeout_seconds\": 30.0\n    }\n    \n    # Advanced config with optional fields\n    advanced_config: AdvancedModelConfig = {\n        \"model_id\": \"claude-3-opus\",\n        \"max_tokens\": 4000,\n        \"temperature\": 0.5,\n        \"provider\": \"anthropic\",\n        \"timeout_seconds\": 60.0,\n        \"retry_attempts\": 3,\n        \"fallback_model\": \"claude-3-haiku\",\n        \"cost_limit_usd\": 10.0\n    }\n    \n    # Show configurations\n    config_table = Table(title=\"Model Configurations\")\n    config_table.add_column(\"Config Type\", style=\"cyan\")\n    config_table.add_column(\"Model ID\", style=\"green\")\n    config_table.add_column(\"Provider\", style=\"yellow\")\n    config_table.add_column(\"Max Tokens\", style=\"magenta\")\n    config_table.add_column(\"Extra Fields\", style=\"blue\")\n    \n    extra_basic = \"None\"\n    extra_advanced = f\"Retries: {advanced_config.get('retry_attempts', 'N/A')}, Fallback: {advanced_config.get('fallback_model', 'N/A')}\"\n    \n    config_table.add_row(\n        \"Basic\",\n        basic_config[\"model_id\"],\n        basic_config[\"provider\"],\n        str(basic_config[\"max_tokens\"]),\n        extra_basic\n    )\n    \n    config_table.add_row(\n        \"Advanced\",\n        advanced_config[\"model_id\"],\n        advanced_config[\"provider\"],\n        str(advanced_config[\"max_tokens\"]),\n        extra_advanced\n    )\n    \n    console.print(config_table)\n    \n    # Test factory function with literals\n    console.print(f\"\\n[bold yellow]Testing factory with literal types...[/bold yellow]\")\n    \n    configs = [\n        create_model_config(\"openai\", \"large\", 0.8),\n        create_model_config(\"anthropic\", \"small\", 0.3),\n        create_model_config(\"local\", \"small\", 1.0)\n    ]\n    \n    for i, config in enumerate(configs, 1):\n        console.print(f\"Config {i}: {config['provider']} - {config['model_id']}\")\n\n\nasync def demonstrate_overloading():\n    \"\"\"Show function overloading.\"\"\"\n    \n    console.print(Panel(\"⚡ Function Overloading\", style=\"bold cyan\"))\n    \n    # Create model manager\n    manager = ModelManager()\n    \n    # Create and register a model\n    config = create_model_config(\"openai\", \"large\")\n    classifier = TextClassifier(config)\n    manager.register_model(\"text_classifier\", classifier)\n    \n    console.print(\"[bold yellow]Testing overloaded methods...[/bold yellow]\")\n    \n    # Test get_model overloads\n    model1 = manager.get_model(\"text_classifier\")\n    console.print(f\"Found model: {model1 is not None}\")\n    \n    model2 = manager.get_model(\"nonexistent\", \"default_value\")\n    console.print(f\"With default: {model2}\")\n    \n    # Test predict overload\n    test_input = TextInput(\n        text=\"This is a positive example!\",\n        context=\"Testing sentiment analysis\"\n    )\n    \n    result = await manager.predict(\"text_classifier\", test_input)\n    console.print(f\"\\nPrediction result: {result.label} (confidence: {result.confidence})\")\n    \n    # Test caching\n    cached_result = await manager.predict(\"text_classifier\", test_input)\n    console.print(f\"Cached result: {cached_result.label} (should be same)\")\n    \n    # Show cache stats\n    cache_stats = manager.cache.get_stats()\n    console.print(f\"Cache stats: {cache_stats}\")\n\n\nasync def demonstrate_advanced_decorators():\n    \"\"\"Show advanced decorators with type preservation.\"\"\"\n    \n    console.print(Panel(\"🎨 Advanced Decorators\", style=\"bold red\"))\n    \n    @with_retries(max_retries=3)\n    async def flaky_prediction(text: str, fail_rate: float = 0.5) -> str:\n        \"\"\"Function that fails sometimes.\"\"\"\n        import random\n        \n        if random.random() < fail_rate:\n            raise ConnectionError(\"Simulated network failure\")\n        \n        return f\"Processed: {text}\"\n    \n    console.print(\"[bold yellow]Testing retry decorator...[/bold yellow]\")\n    \n    try:\n        result = await flaky_prediction(\"Hello world\", fail_rate=0.8)\n        console.print(f\"✅ Success: {result}\")\n    except Exception as e:\n        console.print(f\"❌ Failed after retries: {e}\")\n    \n    # Show that types are preserved\n    console.print(f\"\\n[bold cyan]Type preservation check:[/bold cyan]\")\n    console.print(f\"Function annotations: {flaky_prediction.__annotations__}\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for advanced typing.\"\"\"\n    \n    console.print(Panel(\"🎮 Interactive Exercise: Design Your Types\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"system\": \"Multi-Modal AI Pipeline\",\n            \"requirements\": [\n                \"Handles text, image, and audio inputs\",\n                \"Each input type has different preprocessing\",\n                \"Output format depends on input type\",\n                \"Need type-safe pipeline composition\"\n            ],\n            \"challenge\": \"Design generic types and protocols for this system\"\n        },\n        {\n            \"system\": \"AI Model Registry\",\n            \"requirements\": [\n                \"Store models with metadata\",\n                \"Support different model types (classification, generation, etc.)\",\n                \"Version management with backward compatibility\",\n                \"Type-safe model loading and inference\"\n            ],\n            \"challenge\": \"Create TypedDict configs and generic interfaces\"\n        },\n        {\n            \"system\": \"Distributed AI Inference\",\n            \"requirements\": [\n                \"Route requests to different worker nodes\",\n                \"Handle partial failures gracefully\",\n                \"Support streaming and batch inference\",\n                \"Maintain type safety across network boundaries\"\n            ],\n            \"challenge\": \"Design protocols and union types for this architecture\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['system']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  • {req}\")\n        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n        console.print(\"\\n[bold green]Consider:[/bold green]\")\n        console.print(\"  • Generic types for reusable components\")\n        console.print(\"  • Protocols for flexible interfaces\")\n        console.print(\"  • TypedDict for configuration\")\n        console.print(\"  • Union types for multiple possibilities\")\n        console.print(\"  • Overloads for different call patterns\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 AI Architecture Patterns: Advanced Type System[/bold blue]\")\n    console.print(\"Learn to build type-safe AI systems with Python's advanced typing\\n\")\n    \n    async def main():\n        await demonstrate_generic_types()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_protocols()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_typed_dicts()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_overloading()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_advanced_decorators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]✅ Completed![/bold green] Next: Run example 02_async_patterns.py\")\n        console.print(\"💡 Pro tip: Strong typing prevents bugs and improves code clarity!\")\n    \n    import asyncio\n    asyncio.run(main())","numLines":645,"startLine":1,"totalLines":645}}}
{"parentUuid":"cf218ea2-aee3-4cbd-ba95-a4d181cf7781","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01CULcYJJ8eSotx27sHiCo4t","type":"tool_result","content":"     1→#!/usr/bin/env python3\n     2→\"\"\"\n     3→Example 2: Async Patterns for AI Systems\n     4→\n     5→Learn advanced asyncio patterns for building high-performance\n     6→AI systems with concurrent processing, streaming, and resource management.\n     7→\n     8→Run this file to see the examples in action:\n     9→    python 03-python-idioms/examples/02_async_patterns.py\n    10→\"\"\"\n    11→\n    12→import sys\n    13→import os\n    14→sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n    15→\n    16→import asyncio\n    17→from typing import AsyncGenerator, AsyncIterator, Callable, Any, Optional, List, Dict\n    18→from dataclasses import dataclass\n    19→from contextlib import asynccontextmanager\n    20→import time\n    21→from rich.console import Console\n    22→from rich.panel import Panel\n    23→from rich.table import Table\n    24→from rich.live import Live\n    25→from rich.progress import Progress, TaskID\n    26→\n    27→from common import Message, ChatResponse, Choice\n    28→\n    29→console = Console()\n    30→\n    31→\n    32→# 🎯 LEARNING GOAL: Master async patterns for high-performance AI systems\n    33→\n    34→\n    35→@dataclass\n    36→class StreamingChunk:\n    37→    \"\"\"Represents a chunk of streaming data.\"\"\"\n    38→    content: str\n    39→    chunk_id: int\n    40→    is_final: bool\n    41→    metadata: Dict[str, Any] = None\n    42→    \n    43→    def __post_init__(self):\n    44→        if self.metadata is None:\n    45→            self.metadata = {}\n    46→\n    47→\n    48→class AsyncRateLimiter:\n    49→    \"\"\"Async rate limiter for API calls.\"\"\"\n    50→    \n    51→    def __init__(self, max_calls: int, time_window: float):\n    52→        self.max_calls = max_calls\n    53→        self.time_window = time_window\n    54→        self.calls: List[float] = []\n    55→        self._lock = asyncio.Lock()\n    56→    \n    57→    async def acquire(self) -> None:\n    58→        \"\"\"Acquire permission to make a call.\"\"\"\n    59→        async with self._lock:\n    60→            now = time.time()\n    61→            \n    62→            # Remove old calls outside the window\n    63→            self.calls = [call_time for call_time in self.calls \n    64→                         if now - call_time < self.time_window]\n    65→            \n    66→            # If we're at the limit, wait\n    67→            if len(self.calls) >= self.max_calls:\n    68→                oldest_call = min(self.calls)\n    69→                wait_time = self.time_window - (now - oldest_call)\n    70→                if wait_time > 0:\n    71→                    await asyncio.sleep(wait_time)\n    72→                    # Recursive call to try again\n    73→                    return await self.acquire()\n    74→            \n    75→            # Record this call\n    76→            self.calls.append(now)\n    77→    \n    78→    def get_stats(self) -> Dict[str, Any]:\n    79→        \"\"\"Get rate limiter statistics.\"\"\"\n    80→        now = time.time()\n    81→        recent_calls = [call for call in self.calls if now - call < self.time_window]\n    82→        return {\n    83→            \"current_calls\": len(recent_calls),\n    84→            \"max_calls\": self.max_calls,\n    85→            \"time_window\": self.time_window,\n    86→            \"utilization\": len(recent_calls) / self.max_calls\n    87→        }\n    88→\n    89→\n    90→class AsyncConnectionPool:\n    91→    \"\"\"Connection pool for managing async resources.\"\"\"\n    92→    \n    93→    def __init__(self, create_connection: Callable, max_size: int = 10):\n    94→        self.create_connection = create_connection\n    95→        self.max_size = max_size\n    96→        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_size)\n    97→        self.created_count = 0\n    98→        self.active_connections = 0\n    99→        self._lock = asyncio.Lock()\n   100→    \n   101→    async def get_connection(self):\n   102→        \"\"\"Get a connection from the pool.\"\"\"\n   103→        try:\n   104→            # Try to get existing connection with timeout\n   105→            connection = await asyncio.wait_for(self.pool.get(), timeout=0.1)\n   106→            self.active_connections += 1\n   107→            return connection\n   108→        except asyncio.TimeoutError:\n   109→            # Create new connection if under limit\n   110→            async with self._lock:\n   111→                if self.created_count < self.max_size:\n   112→                    connection = await self.create_connection()\n   113→                    self.created_count += 1\n   114→                    self.active_connections += 1\n   115→                    return connection\n   116→            \n   117→            # Wait for available connection\n   118→            connection = await self.pool.get()\n   119→            self.active_connections += 1\n   120→            return connection\n   121→    \n   122→    async def return_connection(self, connection):\n   123→        \"\"\"Return connection to pool.\"\"\"\n   124→        self.active_connections -= 1\n   125→        await self.pool.put(connection)\n   126→    \n   127→    @asynccontextmanager\n   128→    async def connection(self):\n   129→        \"\"\"Context manager for automatic connection management.\"\"\"\n   130→        conn = await self.get_connection()\n   131→        try:\n   132→            yield conn\n   133→        finally:\n   134→            await self.return_connection(conn)\n   135→    \n   136→    def get_stats(self) -> Dict[str, Any]:\n   137→        \"\"\"Get pool statistics.\"\"\"\n   138→        return {\n   139→            \"pool_size\": self.pool.qsize(),\n   140→            \"created_count\": self.created_count,\n   141→            \"active_connections\": self.active_connections,\n   142→            \"max_size\": self.max_size\n   143→        }\n   144→\n   145→\n   146→class StreamingAIProvider:\n   147→    \"\"\"AI provider that supports streaming responses.\"\"\"\n   148→    \n   149→    def __init__(self, response_delay: float = 0.1):\n   150→        self.response_delay = response_delay\n   151→        self.call_count = 0\n   152→    \n   153→    async def stream_completion(\n   154→        self, \n   155→        model_id: str, \n   156→        messages: List[Message]\n   157→    ) -> AsyncGenerator[StreamingChunk, None]:\n   158→        \"\"\"Stream chat completion chunks.\"\"\"\n   159→        self.call_count += 1\n   160→        \n   161→        # Simulate streaming response\n   162→        response_text = f\"This is a streaming response to: {messages[-1].content}\"\n   163→        words = response_text.split()\n   164→        \n   165→        for i, word in enumerate(words):\n   166→            await asyncio.sleep(self.response_delay)\n   167→            \n   168→            chunk = StreamingChunk(\n   169→                content=word + (\" \" if i < len(words) - 1 else \"\"),\n   170→                chunk_id=i,\n   171→                is_final=i == len(words) - 1,\n   172→                metadata={\n   173→                    \"model\": model_id,\n   174→                    \"total_chunks\": len(words),\n   175→                    \"call_id\": self.call_count\n   176→                }\n   177→            )\n   178→            \n   179→            yield chunk\n   180→\n   181→\n   182→class ConcurrentInferenceEngine:\n   183→    \"\"\"Engine for managing concurrent AI inference requests.\"\"\"\n   184→    \n   185→    def __init__(self, max_concurrent: int = 5):\n   186→        self.max_concurrent = max_concurrent\n   187→        self.semaphore = asyncio.Semaphore(max_concurrent)\n   188→        self.active_tasks: Dict[str, asyncio.Task] = {}\n   189→        self.completed_tasks: List[Dict[str, Any]] = []\n   190→    \n   191→    async def submit_task(\n   192→        self, \n   193→        task_id: str, \n   194→        coro: Callable,\n   195→        *args, \n   196→        **kwargs\n   197→    ) -> str:\n   198→        \"\"\"Submit a task for concurrent execution.\"\"\"\n   199→        \n   200→        async def wrapped_task():\n   201→            async with self.semaphore:\n   202→                start_time = time.time()\n   203→                try:\n   204→                    result = await coro(*args, **kwargs)\n   205→                    end_time = time.time()\n   206→                    \n   207→                    self.completed_tasks.append({\n   208→                        \"task_id\": task_id,\n   209→                        \"result\": result,\n   210→                        \"duration\": end_time - start_time,\n   211→                        \"status\": \"success\",\n   212→                        \"error\": None\n   213→                    })\n   214→                    \n   215→                    return result\n   216→                    \n   217→                except Exception as e:\n   218→                    end_time = time.time()\n   219→                    \n   220→                    self.completed_tasks.append({\n   221→                        \"task_id\": task_id,\n   222→                        \"result\": None,\n   223→                        \"duration\": end_time - start_time,\n   224→                        \"status\": \"error\",\n   225→                        \"error\": str(e)\n   226→                    })\n   227→                    \n   228→                    raise\n   229→                finally:\n   230→                    # Remove from active tasks\n   231→                    if task_id in self.active_tasks:\n   232→                        del self.active_tasks[task_id]\n   233→        \n   234→        # Create and store task\n   235→        task = asyncio.create_task(wrapped_task())\n   236→        self.active_tasks[task_id] = task\n   237→        \n   238→        return task_id\n   239→    \n   240→    async def wait_for_task(self, task_id: str) -> Any:\n   241→        \"\"\"Wait for specific task to complete.\"\"\"\n   242→        if task_id in self.active_tasks:\n   243→            return await self.active_tasks[task_id]\n   244→        \n   245→        # Check completed tasks\n   246→        for completed in self.completed_tasks:\n   247→            if completed[\"task_id\"] == task_id:\n   248→                if completed[\"status\"] == \"error\":\n   249→                    raise Exception(completed[\"error\"])\n   250→                return completed[\"result\"]\n   251→        \n   252→        raise ValueError(f\"Task {task_id} not found\")\n   253→    \n   254→    async def wait_for_all(self) -> List[Any]:\n   255→        \"\"\"Wait for all active tasks to complete.\"\"\"\n   256→        if not self.active_tasks:\n   257→            return []\n   258→        \n   259→        results = await asyncio.gather(*self.active_tasks.values(), return_exceptions=True)\n   260→        return results\n   261→    \n   262→    def get_stats(self) -> Dict[str, Any]:\n   263→        \"\"\"Get engine statistics.\"\"\"\n   264→        return {\n   265→            \"active_tasks\": len(self.active_tasks),\n   266→            \"completed_tasks\": len(self.completed_tasks),\n   267→            \"max_concurrent\": self.max_concurrent,\n   268→            \"available_slots\": self.semaphore._value\n   269→        }\n   270→\n   271→\n   272→async def async_batch_processor(\n   273→    items: List[Any], \n   274→    processor: Callable, \n   275→    batch_size: int = 10,\n   276→    max_concurrent: int = 3\n   277→) -> List[Any]:\n   278→    \"\"\"Process items in concurrent batches.\"\"\"\n   279→    \n   280→    async def process_batch(batch: List[Any]) -> List[Any]:\n   281→        \"\"\"Process a single batch.\"\"\"\n   282→        tasks = [processor(item) for item in batch]\n   283→        return await asyncio.gather(*tasks, return_exceptions=True)\n   284→    \n   285→    # Split into batches\n   286→    batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n   287→    \n   288→    # Process batches concurrently\n   289→    semaphore = asyncio.Semaphore(max_concurrent)\n   290→    \n   291→    async def process_with_semaphore(batch):\n   292→        async with semaphore:\n   293→            return await process_batch(batch)\n   294→    \n   295→    batch_results = await asyncio.gather(*[\n   296→        process_with_semaphore(batch) for batch in batches\n   297→    ])\n   298→    \n   299→    # Flatten results\n   300→    results = []\n   301→    for batch_result in batch_results:\n   302→        results.extend(batch_result)\n   303→    \n   304→    return results\n   305→\n   306→\n   307→@asynccontextmanager\n   308→async def async_resource_manager(resource_factory: Callable, cleanup_func: Callable = None):\n   309→    \"\"\"Generic async context manager for resources.\"\"\"\n   310→    resource = None\n   311→    try:\n   312→        resource = await resource_factory()\n   313→        yield resource\n   314→    finally:\n   315→        if resource is not None and cleanup_func:\n   316→            await cleanup_func(resource)\n   317→\n   318→\n   319→async def demonstrate_streaming():\n   320→    \"\"\"Show streaming patterns.\"\"\"\n   321→    \n   322→    console.print(Panel(\"🌊 Streaming Patterns\", style=\"bold blue\"))\n   323→    \n   324→    provider = StreamingAIProvider(response_delay=0.2)\n   325→    \n   326→    console.print(\"[bold yellow]Streaming chat completion...[/bold yellow]\")\n   327→    \n   328→    messages = [Message(role=\"user\", content=\"Tell me about async programming\")]\n   329→    \n   330→    # Collect streaming chunks\n   331→    chunks = []\n   332→    full_response = \"\"\n   333→    \n   334→    async for chunk in provider.stream_completion(\"gpt-4\", messages):\n   335→        chunks.append(chunk)\n   336→        full_response += chunk.content\n   337→        \n   338→        # Show real-time progress\n   339→        console.print(f\"Chunk {chunk.chunk_id}: '{chunk.content.strip()}'\", end=\"\")\n   340→        if chunk.is_final:\n   341→            console.print(\" [FINAL]\")\n   342→        else:\n   343→            console.print()\n   344→    \n   345→    console.print(f\"\\n[bold cyan]Streaming Summary:[/bold cyan]\")\n   346→    console.print(f\"Total chunks: {len(chunks)}\")\n   347→    console.print(f\"Full response: {full_response}\")\n   348→    console.print(f\"Final chunk metadata: {chunks[-1].metadata}\")\n   349→\n   350→\n   351→async def demonstrate_rate_limiting():\n   352→    \"\"\"Show rate limiting patterns.\"\"\"\n   353→    \n   354→    console.print(Panel(\"🚦 Rate Limiting\", style=\"bold green\"))\n   355→    \n   356→    # Create rate limiter (5 calls per 2 seconds)\n   357→    rate_limiter = AsyncRateLimiter(max_calls=5, time_window=2.0)\n   358→    \n   359→    async def make_api_call(call_id: int) -> str:\n   360→        \"\"\"Simulate API call with rate limiting.\"\"\"\n   361→        await rate_limiter.acquire()\n   362→        # Simulate API processing time\n   363→        await asyncio.sleep(0.1)\n   364→        return f\"API call {call_id} completed\"\n   365→    \n   366→    console.print(\"[bold yellow]Making 10 rapid API calls with rate limiting...[/bold yellow]\")\n   367→    \n   368→    start_time = time.time()\n   369→    \n   370→    # Create tasks for 10 rapid calls\n   371→    tasks = [make_api_call(i) for i in range(10)]\n   372→    \n   373→    # Execute with progress tracking\n   374→    results = []\n   375→    for i, task in enumerate(asyncio.as_completed(tasks)):\n   376→        result = await task\n   377→        elapsed = time.time() - start_time\n   378→        console.print(f\"  {elapsed:.2f}s: {result}\")\n   379→        results.append(result)\n   380→    \n   381→    total_time = time.time() - start_time\n   382→    console.print(f\"\\n[bold cyan]Rate Limiting Summary:[/bold cyan]\")\n   383→    console.print(f\"Total time: {total_time:.2f}s\")\n   384→    console.print(f\"Rate limiter stats: {rate_limiter.get_stats()}\")\n   385→\n   386→\n   387→async def demonstrate_connection_pooling():\n   388→    \"\"\"Show connection pooling patterns.\"\"\"\n   389→    \n   390→    console.print(Panel(\"🏊 Connection Pooling\", style=\"bold magenta\"))\n   391→    \n   392→    async def create_mock_connection():\n   393→        \"\"\"Create a mock database connection.\"\"\"\n   394→        await asyncio.sleep(0.1)  # Simulate connection setup\n   395→        return {\"id\": id(object()), \"created_at\": time.time()}\n   396→    \n   397→    # Create connection pool\n   398→    pool = AsyncConnectionPool(create_mock_connection, max_size=3)\n   399→    \n   400→    async def database_operation(operation_id: int):\n   401→        \"\"\"Simulate database operation using connection pool.\"\"\"\n   402→        async with pool.connection() as conn:\n   403→            # Simulate work\n   404→            await asyncio.sleep(0.2)\n   405→            return f\"Operation {operation_id} completed with connection {conn['id']}\"\n   406→    \n   407→    console.print(\"[bold yellow]Running concurrent database operations...[/bold yellow]\")\n   408→    \n   409→    # Run 8 concurrent operations with pool of 3 connections\n   410→    start_time = time.time()\n   411→    \n   412→    tasks = [database_operation(i) for i in range(8)]\n   413→    results = []\n   414→    \n   415→    for task in asyncio.as_completed(tasks):\n   416→        result = await task\n   417→        elapsed = time.time() - start_time\n   418→        stats = pool.get_stats()\n   419→        console.print(f\"  {elapsed:.2f}s: {result}\")\n   420→        console.print(f\"    Pool stats: {stats}\")\n   421→    \n   422→    final_stats = pool.get_stats()\n   423→    console.print(f\"\\n[bold cyan]Final Pool Stats:[/bold cyan]\")\n   424→    console.print(f\"Created connections: {final_stats['created_count']}\")\n   425→    console.print(f\"Active connections: {final_stats['active_connections']}\")\n   426→\n   427→\n   428→async def demonstrate_concurrent_processing():\n   429→    \"\"\"Show concurrent processing patterns.\"\"\"\n   430→    \n   431→    console.print(Panel(\"⚡ Concurrent Processing\", style=\"bold cyan\"))\n   432→    \n   433→    # Create inference engine\n   434→    engine = ConcurrentInferenceEngine(max_concurrent=3)\n   435→    \n   436→    async def mock_inference(text: str, delay: float = 0.5) -> str:\n   437→        \"\"\"Mock AI inference with variable delay.\"\"\"\n   438→        await asyncio.sleep(delay)\n   439→        return f\"Processed: {text}\"\n   440→    \n   441→    console.print(\"[bold yellow]Submitting concurrent inference tasks...[/bold yellow]\")\n   442→    \n   443→    # Submit multiple tasks\n   444→    tasks_data = [\n   445→        (\"task_1\", \"Analyze sentiment\", 0.3),\n   446→        (\"task_2\", \"Summarize document\", 0.8),\n   447→        (\"task_3\", \"Extract entities\", 0.5),\n   448→        (\"task_4\", \"Classify text\", 0.4),\n   449→        (\"task_5\", \"Generate response\", 0.6)\n   450→    ]\n   451→    \n   452→    submitted_tasks = []\n   453→    for task_id, text, delay in tasks_data:\n   454→        await engine.submit_task(task_id, mock_inference, text, delay)\n   455→        submitted_tasks.append(task_id)\n   456→        console.print(f\"  Submitted: {task_id}\")\n   457→    \n   458→    # Wait for all tasks and show progress\n   459→    console.print(f\"\\n[bold yellow]Waiting for tasks to complete...[/bold yellow]\")\n   460→    \n   461→    while engine.active_tasks:\n   462→        stats = engine.get_stats()\n   463→        console.print(f\"  Active: {stats['active_tasks']}, Completed: {stats['completed_tasks']}\")\n   464→        await asyncio.sleep(0.2)\n   465→    \n   466→    # Show final results\n   467→    console.print(f\"\\n[bold cyan]Task Results:[/bold cyan]\")\n   468→    for completed in engine.completed_tasks:\n   469→        status_icon = \"✅\" if completed[\"status\"] == \"success\" else \"❌\"\n   470→        console.print(f\"  {status_icon} {completed['task_id']}: {completed['duration']:.2f}s\")\n   471→\n   472→\n   473→async def demonstrate_batch_processing():\n   474→    \"\"\"Show batch processing patterns.\"\"\"\n   475→    \n   476→    console.print(Panel(\"📦 Batch Processing\", style=\"bold red\"))\n   477→    \n   478→    async def process_text(text: str) -> str:\n   479→        \"\"\"Process a single text item.\"\"\"\n   480→        await asyncio.sleep(0.1)  # Simulate processing\n   481→        return f\"PROCESSED: {text.upper()}\"\n   482→    \n   483→    # Create test data\n   484→    texts = [f\"text item {i}\" for i in range(25)]\n   485→    \n   486→    console.print(f\"[bold yellow]Processing {len(texts)} items in batches...[/bold yellow]\")\n   487→    \n   488→    start_time = time.time()\n   489→    \n   490→    # Process in batches with concurrency\n   491→    results = await async_batch_processor(\n   492→        items=texts,\n   493→        processor=process_text,\n   494→        batch_size=5,\n   495→        max_concurrent=3\n   496→    )\n   497→    \n   498→    processing_time = time.time() - start_time\n   499→    \n   500→    console.print(f\"\\n[bold cyan]Batch Processing Results:[/bold cyan]\")\n   501→    console.print(f\"Total items: {len(texts)}\")\n   502→    console.print(f\"Processing time: {processing_time:.2f}s\")\n   503→    console.print(f\"Items per second: {len(texts) / processing_time:.1f}\")\n   504→    console.print(f\"Sample results: {results[:3]}...\")\n   505→    \n   506→    # Count errors\n   507→    errors = [r for r in results if isinstance(r, Exception)]\n   508→    console.print(f\"Errors: {len(errors)}\")\n   509→\n   510→\n   511→async def demonstrate_async_context_managers():\n   512→    \"\"\"Show async context manager patterns.\"\"\"\n   513→    \n   514→    console.print(Panel(\"🔒 Async Context Managers\", style=\"bold yellow\"))\n   515→    \n   516→    async def create_ai_session():\n   517→        \"\"\"Create an AI session with setup/teardown.\"\"\"\n   518→        console.print(\"  🚀 Creating AI session...\")\n   519→        await asyncio.sleep(0.1)\n   520→        session = {\n   521→            \"id\": \"session_\" + str(int(time.time())),\n   522→            \"created_at\": time.time(),\n   523→            \"request_count\": 0\n   524→        }\n   525→        return session\n   526→    \n   527→    async def cleanup_session(session):\n   528→        \"\"\"Clean up AI session.\"\"\"\n   529→        console.print(f\"  🧹 Cleaning up session {session['id']} ({session['request_count']} requests)\")\n   530→        await asyncio.sleep(0.05)\n   531→    \n   532→    console.print(\"[bold yellow]Using async context manager for AI sessions...[/bold yellow]\")\n   533→    \n   534→    # Use the async context manager\n   535→    async with async_resource_manager(create_ai_session, cleanup_session) as session:\n   536→        console.print(f\"  📝 Session active: {session['id']}\")\n   537→        \n   538→        # Simulate some work\n   539→        for i in range(3):\n   540→            await asyncio.sleep(0.1)\n   541→            session[\"request_count\"] += 1\n   542→            console.print(f\"    Request {session['request_count']} processed\")\n   543→    \n   544→    console.print(\"  ✅ Session context manager completed\")\n   545→\n   546→\n   547→async def demonstrate_async_generators():\n   548→    \"\"\"Show async generator patterns.\"\"\"\n   549→    \n   550→    console.print(Panel(\"🔄 Async Generators\", style=\"bold purple\"))\n   551→    \n   552→    async def generate_predictions(texts: List[str]) -> AsyncGenerator[Dict[str, Any], None]:\n   553→        \"\"\"Generate predictions one by one.\"\"\"\n   554→        for i, text in enumerate(texts):\n   555→            # Simulate prediction time\n   556→            await asyncio.sleep(0.2)\n   557→            \n   558→            yield {\n   559→                \"text\": text,\n   560→                \"prediction\": f\"prediction_{i}\",\n   561→                \"confidence\": 0.9 - (i * 0.1),\n   562→                \"processing_time\": 0.2,\n   563→                \"batch_id\": i // 3  # Group into batches of 3\n   564→            }\n   565→    \n   566→    console.print(\"[bold yellow]Streaming predictions with async generator...[/bold yellow]\")\n   567→    \n   568→    test_texts = [f\"input text {i}\" for i in range(7)]\n   569→    \n   570→    predictions_by_batch = {}\n   571→    \n   572→    async for prediction in generate_predictions(test_texts):\n   573→        batch_id = prediction[\"batch_id\"]\n   574→        if batch_id not in predictions_by_batch:\n   575→            predictions_by_batch[batch_id] = []\n   576→        \n   577→        predictions_by_batch[batch_id].append(prediction)\n   578→        \n   579→        console.print(f\"  📊 Prediction {len(predictions_by_batch[batch_id])} for batch {batch_id}: \"\n   580→                     f\"{prediction['prediction']} (confidence: {prediction['confidence']:.2f})\")\n   581→    \n   582→    console.print(f\"\\n[bold cyan]Generated {sum(len(preds) for preds in predictions_by_batch.values())} predictions in {len(predictions_by_batch)} batches[/bold cyan]\")\n   583→\n   584→\n   585→def interactive_exercise():\n   586→    \"\"\"Interactive exercise for async patterns.\"\"\"\n   587→    \n   588→    console.print(Panel(\"🎮 Interactive Exercise: Design Your Async Architecture\", style=\"bold magenta\"))\n   589→    \n   590→    scenarios = [\n   591→        {\n   592→            \"system\": \"Real-time AI Chat Service\",\n   593→            \"requirements\": [\n   594→                \"Handle 1000+ concurrent users\",\n   595→                \"Stream responses in real-time\",\n   596→                \"Rate limit per user and globally\",\n   597→                \"Graceful handling of AI model failures\"\n   598→            ],\n   599→            \"challenge\": \"Design the async architecture\"\n   600→        },\n   601→        {\n   602→            \"system\": \"Batch Document Processing\",\n   603→            \"requirements\": [\n   604→                \"Process thousands of documents daily\",\n   605→                \"Multiple AI models per document\",\n   606→                \"Prioritize urgent documents\",\n   607→                \"Monitor progress and handle failures\"\n   608→            ],\n   609→            \"challenge\": \"Design concurrent processing strategy\"\n   610→        },\n   611→        {\n   612→            \"system\": \"Multi-Model AI Inference API\",\n   613→            \"requirements\": [\n   614→                \"Support multiple AI providers\",\n   615→                \"Connection pooling for efficiency\",\n   616→                \"Fallback between providers\",\n   617→                \"Streaming and batch modes\"\n   618→            ],\n   619→            \"challenge\": \"Design resource management patterns\"\n   620→        }\n   621→    ]\n   622→    \n   623→    for i, scenario in enumerate(scenarios, 1):\n   624→        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['system']}[/bold cyan]\")\n   625→        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n   626→        for req in scenario[\"requirements\"]:\n   627→            console.print(f\"  • {req}\")\n   628→        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n   629→        console.print(\"\\n[bold green]Consider These Patterns:[/bold green]\")\n   630→        console.print(\"  • Streaming with AsyncGenerators\")\n   631→        console.print(\"  • Rate limiting with semaphores\")\n   632→        console.print(\"  • Connection pooling for resources\")\n   633→        console.print(\"  • Concurrent processing with task management\")\n   634→        console.print(\"  • Async context managers for cleanup\")\n   635→        console.print(\"  • Batch processing for efficiency\")\n   636→        console.print(\"---\")\n   637→\n   638→\n   639→if __name__ == \"__main__\":\n   640→    console.print(\"[bold blue]🚀 AI Architecture Patterns: Async Patterns[/bold blue]\")\n   641→    console.print(\"Learn to build high-performance async AI systems\\n\")\n   642→    \n   643→    async def main():\n   644→        await demonstrate_streaming()\n   645→        console.print(\"\\n\" + \"=\"*60)\n   646→        \n   647→        await demonstrate_rate_limiting()\n   648→        console.print(\"\\n\" + \"=\"*60)\n   649→        \n   650→        await demonstrate_connection_pooling()\n   651→        console.print(\"\\n\" + \"=\"*60)\n   652→        \n   653→        await demonstrate_concurrent_processing()\n   654→        console.print(\"\\n\" + \"=\"*60)\n   655→        \n   656→        await demonstrate_batch_processing()\n   657→        console.print(\"\\n\" + \"=\"*60)\n   658→        \n   659→        await demonstrate_async_context_managers()\n   660→        console.print(\"\\n\" + \"=\"*60)\n   661→        \n   662→        await demonstrate_async_generators()\n   663→        console.print(\"\\n\" + \"=\"*60)\n   664→        \n   665→        interactive_exercise()\n   666→        \n   667→        console.print(f\"\\n[bold green]✅ Completed![/bold green] Next: Run example 03_functional_programming.py\")\n   668→        console.print(\"💡 Pro tip: Async patterns are crucial for scalable AI systems!\")\n   669→    \n   670→    asyncio.run(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"e6a83d8e-88e9-4525-80e7-a76bc3017d4f","timestamp":"2025-07-30T17:29:39.946Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/02_async_patterns.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 2: Async Patterns for AI Systems\n\nLearn advanced asyncio patterns for building high-performance\nAI systems with concurrent processing, streaming, and resource management.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/02_async_patterns.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nimport asyncio\nfrom typing import AsyncGenerator, AsyncIterator, Callable, Any, Optional, List, Dict\nfrom dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nimport time\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.live import Live\nfrom rich.progress import Progress, TaskID\n\nfrom common import Message, ChatResponse, Choice\n\nconsole = Console()\n\n\n# 🎯 LEARNING GOAL: Master async patterns for high-performance AI systems\n\n\n@dataclass\nclass StreamingChunk:\n    \"\"\"Represents a chunk of streaming data.\"\"\"\n    content: str\n    chunk_id: int\n    is_final: bool\n    metadata: Dict[str, Any] = None\n    \n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n\n\nclass AsyncRateLimiter:\n    \"\"\"Async rate limiter for API calls.\"\"\"\n    \n    def __init__(self, max_calls: int, time_window: float):\n        self.max_calls = max_calls\n        self.time_window = time_window\n        self.calls: List[float] = []\n        self._lock = asyncio.Lock()\n    \n    async def acquire(self) -> None:\n        \"\"\"Acquire permission to make a call.\"\"\"\n        async with self._lock:\n            now = time.time()\n            \n            # Remove old calls outside the window\n            self.calls = [call_time for call_time in self.calls \n                         if now - call_time < self.time_window]\n            \n            # If we're at the limit, wait\n            if len(self.calls) >= self.max_calls:\n                oldest_call = min(self.calls)\n                wait_time = self.time_window - (now - oldest_call)\n                if wait_time > 0:\n                    await asyncio.sleep(wait_time)\n                    # Recursive call to try again\n                    return await self.acquire()\n            \n            # Record this call\n            self.calls.append(now)\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get rate limiter statistics.\"\"\"\n        now = time.time()\n        recent_calls = [call for call in self.calls if now - call < self.time_window]\n        return {\n            \"current_calls\": len(recent_calls),\n            \"max_calls\": self.max_calls,\n            \"time_window\": self.time_window,\n            \"utilization\": len(recent_calls) / self.max_calls\n        }\n\n\nclass AsyncConnectionPool:\n    \"\"\"Connection pool for managing async resources.\"\"\"\n    \n    def __init__(self, create_connection: Callable, max_size: int = 10):\n        self.create_connection = create_connection\n        self.max_size = max_size\n        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_size)\n        self.created_count = 0\n        self.active_connections = 0\n        self._lock = asyncio.Lock()\n    \n    async def get_connection(self):\n        \"\"\"Get a connection from the pool.\"\"\"\n        try:\n            # Try to get existing connection with timeout\n            connection = await asyncio.wait_for(self.pool.get(), timeout=0.1)\n            self.active_connections += 1\n            return connection\n        except asyncio.TimeoutError:\n            # Create new connection if under limit\n            async with self._lock:\n                if self.created_count < self.max_size:\n                    connection = await self.create_connection()\n                    self.created_count += 1\n                    self.active_connections += 1\n                    return connection\n            \n            # Wait for available connection\n            connection = await self.pool.get()\n            self.active_connections += 1\n            return connection\n    \n    async def return_connection(self, connection):\n        \"\"\"Return connection to pool.\"\"\"\n        self.active_connections -= 1\n        await self.pool.put(connection)\n    \n    @asynccontextmanager\n    async def connection(self):\n        \"\"\"Context manager for automatic connection management.\"\"\"\n        conn = await self.get_connection()\n        try:\n            yield conn\n        finally:\n            await self.return_connection(conn)\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get pool statistics.\"\"\"\n        return {\n            \"pool_size\": self.pool.qsize(),\n            \"created_count\": self.created_count,\n            \"active_connections\": self.active_connections,\n            \"max_size\": self.max_size\n        }\n\n\nclass StreamingAIProvider:\n    \"\"\"AI provider that supports streaming responses.\"\"\"\n    \n    def __init__(self, response_delay: float = 0.1):\n        self.response_delay = response_delay\n        self.call_count = 0\n    \n    async def stream_completion(\n        self, \n        model_id: str, \n        messages: List[Message]\n    ) -> AsyncGenerator[StreamingChunk, None]:\n        \"\"\"Stream chat completion chunks.\"\"\"\n        self.call_count += 1\n        \n        # Simulate streaming response\n        response_text = f\"This is a streaming response to: {messages[-1].content}\"\n        words = response_text.split()\n        \n        for i, word in enumerate(words):\n            await asyncio.sleep(self.response_delay)\n            \n            chunk = StreamingChunk(\n                content=word + (\" \" if i < len(words) - 1 else \"\"),\n                chunk_id=i,\n                is_final=i == len(words) - 1,\n                metadata={\n                    \"model\": model_id,\n                    \"total_chunks\": len(words),\n                    \"call_id\": self.call_count\n                }\n            )\n            \n            yield chunk\n\n\nclass ConcurrentInferenceEngine:\n    \"\"\"Engine for managing concurrent AI inference requests.\"\"\"\n    \n    def __init__(self, max_concurrent: int = 5):\n        self.max_concurrent = max_concurrent\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n        self.active_tasks: Dict[str, asyncio.Task] = {}\n        self.completed_tasks: List[Dict[str, Any]] = []\n    \n    async def submit_task(\n        self, \n        task_id: str, \n        coro: Callable,\n        *args, \n        **kwargs\n    ) -> str:\n        \"\"\"Submit a task for concurrent execution.\"\"\"\n        \n        async def wrapped_task():\n            async with self.semaphore:\n                start_time = time.time()\n                try:\n                    result = await coro(*args, **kwargs)\n                    end_time = time.time()\n                    \n                    self.completed_tasks.append({\n                        \"task_id\": task_id,\n                        \"result\": result,\n                        \"duration\": end_time - start_time,\n                        \"status\": \"success\",\n                        \"error\": None\n                    })\n                    \n                    return result\n                    \n                except Exception as e:\n                    end_time = time.time()\n                    \n                    self.completed_tasks.append({\n                        \"task_id\": task_id,\n                        \"result\": None,\n                        \"duration\": end_time - start_time,\n                        \"status\": \"error\",\n                        \"error\": str(e)\n                    })\n                    \n                    raise\n                finally:\n                    # Remove from active tasks\n                    if task_id in self.active_tasks:\n                        del self.active_tasks[task_id]\n        \n        # Create and store task\n        task = asyncio.create_task(wrapped_task())\n        self.active_tasks[task_id] = task\n        \n        return task_id\n    \n    async def wait_for_task(self, task_id: str) -> Any:\n        \"\"\"Wait for specific task to complete.\"\"\"\n        if task_id in self.active_tasks:\n            return await self.active_tasks[task_id]\n        \n        # Check completed tasks\n        for completed in self.completed_tasks:\n            if completed[\"task_id\"] == task_id:\n                if completed[\"status\"] == \"error\":\n                    raise Exception(completed[\"error\"])\n                return completed[\"result\"]\n        \n        raise ValueError(f\"Task {task_id} not found\")\n    \n    async def wait_for_all(self) -> List[Any]:\n        \"\"\"Wait for all active tasks to complete.\"\"\"\n        if not self.active_tasks:\n            return []\n        \n        results = await asyncio.gather(*self.active_tasks.values(), return_exceptions=True)\n        return results\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get engine statistics.\"\"\"\n        return {\n            \"active_tasks\": len(self.active_tasks),\n            \"completed_tasks\": len(self.completed_tasks),\n            \"max_concurrent\": self.max_concurrent,\n            \"available_slots\": self.semaphore._value\n        }\n\n\nasync def async_batch_processor(\n    items: List[Any], \n    processor: Callable, \n    batch_size: int = 10,\n    max_concurrent: int = 3\n) -> List[Any]:\n    \"\"\"Process items in concurrent batches.\"\"\"\n    \n    async def process_batch(batch: List[Any]) -> List[Any]:\n        \"\"\"Process a single batch.\"\"\"\n        tasks = [processor(item) for item in batch]\n        return await asyncio.gather(*tasks, return_exceptions=True)\n    \n    # Split into batches\n    batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n    \n    # Process batches concurrently\n    semaphore = asyncio.Semaphore(max_concurrent)\n    \n    async def process_with_semaphore(batch):\n        async with semaphore:\n            return await process_batch(batch)\n    \n    batch_results = await asyncio.gather(*[\n        process_with_semaphore(batch) for batch in batches\n    ])\n    \n    # Flatten results\n    results = []\n    for batch_result in batch_results:\n        results.extend(batch_result)\n    \n    return results\n\n\n@asynccontextmanager\nasync def async_resource_manager(resource_factory: Callable, cleanup_func: Callable = None):\n    \"\"\"Generic async context manager for resources.\"\"\"\n    resource = None\n    try:\n        resource = await resource_factory()\n        yield resource\n    finally:\n        if resource is not None and cleanup_func:\n            await cleanup_func(resource)\n\n\nasync def demonstrate_streaming():\n    \"\"\"Show streaming patterns.\"\"\"\n    \n    console.print(Panel(\"🌊 Streaming Patterns\", style=\"bold blue\"))\n    \n    provider = StreamingAIProvider(response_delay=0.2)\n    \n    console.print(\"[bold yellow]Streaming chat completion...[/bold yellow]\")\n    \n    messages = [Message(role=\"user\", content=\"Tell me about async programming\")]\n    \n    # Collect streaming chunks\n    chunks = []\n    full_response = \"\"\n    \n    async for chunk in provider.stream_completion(\"gpt-4\", messages):\n        chunks.append(chunk)\n        full_response += chunk.content\n        \n        # Show real-time progress\n        console.print(f\"Chunk {chunk.chunk_id}: '{chunk.content.strip()}'\", end=\"\")\n        if chunk.is_final:\n            console.print(\" [FINAL]\")\n        else:\n            console.print()\n    \n    console.print(f\"\\n[bold cyan]Streaming Summary:[/bold cyan]\")\n    console.print(f\"Total chunks: {len(chunks)}\")\n    console.print(f\"Full response: {full_response}\")\n    console.print(f\"Final chunk metadata: {chunks[-1].metadata}\")\n\n\nasync def demonstrate_rate_limiting():\n    \"\"\"Show rate limiting patterns.\"\"\"\n    \n    console.print(Panel(\"🚦 Rate Limiting\", style=\"bold green\"))\n    \n    # Create rate limiter (5 calls per 2 seconds)\n    rate_limiter = AsyncRateLimiter(max_calls=5, time_window=2.0)\n    \n    async def make_api_call(call_id: int) -> str:\n        \"\"\"Simulate API call with rate limiting.\"\"\"\n        await rate_limiter.acquire()\n        # Simulate API processing time\n        await asyncio.sleep(0.1)\n        return f\"API call {call_id} completed\"\n    \n    console.print(\"[bold yellow]Making 10 rapid API calls with rate limiting...[/bold yellow]\")\n    \n    start_time = time.time()\n    \n    # Create tasks for 10 rapid calls\n    tasks = [make_api_call(i) for i in range(10)]\n    \n    # Execute with progress tracking\n    results = []\n    for i, task in enumerate(asyncio.as_completed(tasks)):\n        result = await task\n        elapsed = time.time() - start_time\n        console.print(f\"  {elapsed:.2f}s: {result}\")\n        results.append(result)\n    \n    total_time = time.time() - start_time\n    console.print(f\"\\n[bold cyan]Rate Limiting Summary:[/bold cyan]\")\n    console.print(f\"Total time: {total_time:.2f}s\")\n    console.print(f\"Rate limiter stats: {rate_limiter.get_stats()}\")\n\n\nasync def demonstrate_connection_pooling():\n    \"\"\"Show connection pooling patterns.\"\"\"\n    \n    console.print(Panel(\"🏊 Connection Pooling\", style=\"bold magenta\"))\n    \n    async def create_mock_connection():\n        \"\"\"Create a mock database connection.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate connection setup\n        return {\"id\": id(object()), \"created_at\": time.time()}\n    \n    # Create connection pool\n    pool = AsyncConnectionPool(create_mock_connection, max_size=3)\n    \n    async def database_operation(operation_id: int):\n        \"\"\"Simulate database operation using connection pool.\"\"\"\n        async with pool.connection() as conn:\n            # Simulate work\n            await asyncio.sleep(0.2)\n            return f\"Operation {operation_id} completed with connection {conn['id']}\"\n    \n    console.print(\"[bold yellow]Running concurrent database operations...[/bold yellow]\")\n    \n    # Run 8 concurrent operations with pool of 3 connections\n    start_time = time.time()\n    \n    tasks = [database_operation(i) for i in range(8)]\n    results = []\n    \n    for task in asyncio.as_completed(tasks):\n        result = await task\n        elapsed = time.time() - start_time\n        stats = pool.get_stats()\n        console.print(f\"  {elapsed:.2f}s: {result}\")\n        console.print(f\"    Pool stats: {stats}\")\n    \n    final_stats = pool.get_stats()\n    console.print(f\"\\n[bold cyan]Final Pool Stats:[/bold cyan]\")\n    console.print(f\"Created connections: {final_stats['created_count']}\")\n    console.print(f\"Active connections: {final_stats['active_connections']}\")\n\n\nasync def demonstrate_concurrent_processing():\n    \"\"\"Show concurrent processing patterns.\"\"\"\n    \n    console.print(Panel(\"⚡ Concurrent Processing\", style=\"bold cyan\"))\n    \n    # Create inference engine\n    engine = ConcurrentInferenceEngine(max_concurrent=3)\n    \n    async def mock_inference(text: str, delay: float = 0.5) -> str:\n        \"\"\"Mock AI inference with variable delay.\"\"\"\n        await asyncio.sleep(delay)\n        return f\"Processed: {text}\"\n    \n    console.print(\"[bold yellow]Submitting concurrent inference tasks...[/bold yellow]\")\n    \n    # Submit multiple tasks\n    tasks_data = [\n        (\"task_1\", \"Analyze sentiment\", 0.3),\n        (\"task_2\", \"Summarize document\", 0.8),\n        (\"task_3\", \"Extract entities\", 0.5),\n        (\"task_4\", \"Classify text\", 0.4),\n        (\"task_5\", \"Generate response\", 0.6)\n    ]\n    \n    submitted_tasks = []\n    for task_id, text, delay in tasks_data:\n        await engine.submit_task(task_id, mock_inference, text, delay)\n        submitted_tasks.append(task_id)\n        console.print(f\"  Submitted: {task_id}\")\n    \n    # Wait for all tasks and show progress\n    console.print(f\"\\n[bold yellow]Waiting for tasks to complete...[/bold yellow]\")\n    \n    while engine.active_tasks:\n        stats = engine.get_stats()\n        console.print(f\"  Active: {stats['active_tasks']}, Completed: {stats['completed_tasks']}\")\n        await asyncio.sleep(0.2)\n    \n    # Show final results\n    console.print(f\"\\n[bold cyan]Task Results:[/bold cyan]\")\n    for completed in engine.completed_tasks:\n        status_icon = \"✅\" if completed[\"status\"] == \"success\" else \"❌\"\n        console.print(f\"  {status_icon} {completed['task_id']}: {completed['duration']:.2f}s\")\n\n\nasync def demonstrate_batch_processing():\n    \"\"\"Show batch processing patterns.\"\"\"\n    \n    console.print(Panel(\"📦 Batch Processing\", style=\"bold red\"))\n    \n    async def process_text(text: str) -> str:\n        \"\"\"Process a single text item.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate processing\n        return f\"PROCESSED: {text.upper()}\"\n    \n    # Create test data\n    texts = [f\"text item {i}\" for i in range(25)]\n    \n    console.print(f\"[bold yellow]Processing {len(texts)} items in batches...[/bold yellow]\")\n    \n    start_time = time.time()\n    \n    # Process in batches with concurrency\n    results = await async_batch_processor(\n        items=texts,\n        processor=process_text,\n        batch_size=5,\n        max_concurrent=3\n    )\n    \n    processing_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold cyan]Batch Processing Results:[/bold cyan]\")\n    console.print(f\"Total items: {len(texts)}\")\n    console.print(f\"Processing time: {processing_time:.2f}s\")\n    console.print(f\"Items per second: {len(texts) / processing_time:.1f}\")\n    console.print(f\"Sample results: {results[:3]}...\")\n    \n    # Count errors\n    errors = [r for r in results if isinstance(r, Exception)]\n    console.print(f\"Errors: {len(errors)}\")\n\n\nasync def demonstrate_async_context_managers():\n    \"\"\"Show async context manager patterns.\"\"\"\n    \n    console.print(Panel(\"🔒 Async Context Managers\", style=\"bold yellow\"))\n    \n    async def create_ai_session():\n        \"\"\"Create an AI session with setup/teardown.\"\"\"\n        console.print(\"  🚀 Creating AI session...\")\n        await asyncio.sleep(0.1)\n        session = {\n            \"id\": \"session_\" + str(int(time.time())),\n            \"created_at\": time.time(),\n            \"request_count\": 0\n        }\n        return session\n    \n    async def cleanup_session(session):\n        \"\"\"Clean up AI session.\"\"\"\n        console.print(f\"  🧹 Cleaning up session {session['id']} ({session['request_count']} requests)\")\n        await asyncio.sleep(0.05)\n    \n    console.print(\"[bold yellow]Using async context manager for AI sessions...[/bold yellow]\")\n    \n    # Use the async context manager\n    async with async_resource_manager(create_ai_session, cleanup_session) as session:\n        console.print(f\"  📝 Session active: {session['id']}\")\n        \n        # Simulate some work\n        for i in range(3):\n            await asyncio.sleep(0.1)\n            session[\"request_count\"] += 1\n            console.print(f\"    Request {session['request_count']} processed\")\n    \n    console.print(\"  ✅ Session context manager completed\")\n\n\nasync def demonstrate_async_generators():\n    \"\"\"Show async generator patterns.\"\"\"\n    \n    console.print(Panel(\"🔄 Async Generators\", style=\"bold purple\"))\n    \n    async def generate_predictions(texts: List[str]) -> AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Generate predictions one by one.\"\"\"\n        for i, text in enumerate(texts):\n            # Simulate prediction time\n            await asyncio.sleep(0.2)\n            \n            yield {\n                \"text\": text,\n                \"prediction\": f\"prediction_{i}\",\n                \"confidence\": 0.9 - (i * 0.1),\n                \"processing_time\": 0.2,\n                \"batch_id\": i // 3  # Group into batches of 3\n            }\n    \n    console.print(\"[bold yellow]Streaming predictions with async generator...[/bold yellow]\")\n    \n    test_texts = [f\"input text {i}\" for i in range(7)]\n    \n    predictions_by_batch = {}\n    \n    async for prediction in generate_predictions(test_texts):\n        batch_id = prediction[\"batch_id\"]\n        if batch_id not in predictions_by_batch:\n            predictions_by_batch[batch_id] = []\n        \n        predictions_by_batch[batch_id].append(prediction)\n        \n        console.print(f\"  📊 Prediction {len(predictions_by_batch[batch_id])} for batch {batch_id}: \"\n                     f\"{prediction['prediction']} (confidence: {prediction['confidence']:.2f})\")\n    \n    console.print(f\"\\n[bold cyan]Generated {sum(len(preds) for preds in predictions_by_batch.values())} predictions in {len(predictions_by_batch)} batches[/bold cyan]\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for async patterns.\"\"\"\n    \n    console.print(Panel(\"🎮 Interactive Exercise: Design Your Async Architecture\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"system\": \"Real-time AI Chat Service\",\n            \"requirements\": [\n                \"Handle 1000+ concurrent users\",\n                \"Stream responses in real-time\",\n                \"Rate limit per user and globally\",\n                \"Graceful handling of AI model failures\"\n            ],\n            \"challenge\": \"Design the async architecture\"\n        },\n        {\n            \"system\": \"Batch Document Processing\",\n            \"requirements\": [\n                \"Process thousands of documents daily\",\n                \"Multiple AI models per document\",\n                \"Prioritize urgent documents\",\n                \"Monitor progress and handle failures\"\n            ],\n            \"challenge\": \"Design concurrent processing strategy\"\n        },\n        {\n            \"system\": \"Multi-Model AI Inference API\",\n            \"requirements\": [\n                \"Support multiple AI providers\",\n                \"Connection pooling for efficiency\",\n                \"Fallback between providers\",\n                \"Streaming and batch modes\"\n            ],\n            \"challenge\": \"Design resource management patterns\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['system']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  • {req}\")\n        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n        console.print(\"\\n[bold green]Consider These Patterns:[/bold green]\")\n        console.print(\"  • Streaming with AsyncGenerators\")\n        console.print(\"  • Rate limiting with semaphores\")\n        console.print(\"  • Connection pooling for resources\")\n        console.print(\"  • Concurrent processing with task management\")\n        console.print(\"  • Async context managers for cleanup\")\n        console.print(\"  • Batch processing for efficiency\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 AI Architecture Patterns: Async Patterns[/bold blue]\")\n    console.print(\"Learn to build high-performance async AI systems\\n\")\n    \n    async def main():\n        await demonstrate_streaming()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_rate_limiting()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_connection_pooling()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_concurrent_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_batch_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_async_context_managers()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_async_generators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]✅ Completed![/bold green] Next: Run example 03_functional_programming.py\")\n        console.print(\"💡 Pro tip: Async patterns are crucial for scalable AI systems!\")\n    \n    asyncio.run(main())","numLines":670,"startLine":1,"totalLines":670}}}
{"parentUuid":"e6a83d8e-88e9-4525-80e7-a76bc3017d4f","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_014XYtehmnQnkZaE1KUTs4vp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now I'll create a todo list to continue building the Python idioms section and add the remaining files:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":16526,"cache_read_input_tokens":21585,"output_tokens":1}},"requestId":"req_vrtx_011CRdbinu319tJWqj5ZEMxH","type":"assistant","uuid":"430a9312-97af-416a-aa6b-1a6045e86086","timestamp":"2025-07-30T17:29:43.485Z"}
{"parentUuid":"430a9312-97af-416a-aa6b-1a6045e86086","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_014XYtehmnQnkZaE1KUTs4vp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01MQzSeWFTBJK42BsXYFr2Ai","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Create 03_functional_programming.py example file","status":"pending","priority":"high"},{"id":"2","content":"Create 04_metaprogramming.py example file","status":"pending","priority":"high"},{"id":"3","content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high"},{"id":"4","content":"Create tests for the Python idioms examples","status":"pending","priority":"medium"},{"id":"5","content":"Create exercises for hands-on practice","status":"pending","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":16526,"cache_read_input_tokens":21585,"output_tokens":1}},"requestId":"req_vrtx_011CRdbinu319tJWqj5ZEMxH","type":"assistant","uuid":"a4457e53-fae2-42da-a10c-90bc395e1572","timestamp":"2025-07-30T17:29:46.972Z"}
{"parentUuid":"a4457e53-fae2-42da-a10c-90bc395e1572","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01MQzSeWFTBJK42BsXYFr2Ai","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"f9e65c61-aa2e-418e-bd2a-7247b5e91889","timestamp":"2025-07-30T17:29:46.994Z","toolUseResult":{"oldTodos":[],"newTodos":[{"content":"Create 03_functional_programming.py example file","status":"pending","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"pending","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"pending","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}]}}
{"parentUuid":"f9e65c61-aa2e-418e-bd2a-7247b5e91889","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01GszQoB6HWnRFCpFKCm4sct","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Let me start with the first task - creating the functional programming example file:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":466,"cache_read_input_tokens":38111,"output_tokens":3}},"requestId":"req_vrtx_011CRdbjK6EdaZsYH5gthsKB","type":"assistant","uuid":"3aa941c8-dbdd-4f73-8e6e-5b8ae935df08","timestamp":"2025-07-30T17:29:50.226Z"}
{"parentUuid":"3aa941c8-dbdd-4f73-8e6e-5b8ae935df08","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01GszQoB6HWnRFCpFKCm4sct","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_017gN86cJdXg9nykyPKcT67k","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Create 03_functional_programming.py example file","status":"in_progress","priority":"high"},{"id":"2","content":"Create 04_metaprogramming.py example file","status":"pending","priority":"high"},{"id":"3","content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high"},{"id":"4","content":"Create tests for the Python idioms examples","status":"pending","priority":"medium"},{"id":"5","content":"Create exercises for hands-on practice","status":"pending","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":466,"cache_read_input_tokens":38111,"output_tokens":3}},"requestId":"req_vrtx_011CRdbjK6EdaZsYH5gthsKB","type":"assistant","uuid":"7fb11cc0-2866-4157-9fb2-380effe7408a","timestamp":"2025-07-30T17:29:52.454Z"}
{"parentUuid":"7fb11cc0-2866-4157-9fb2-380effe7408a","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_017gN86cJdXg9nykyPKcT67k","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"2723d275-5d5b-42c3-8909-2564e790c8dc","timestamp":"2025-07-30T17:29:52.480Z","toolUseResult":{"oldTodos":[{"content":"Create 03_functional_programming.py example file","status":"pending","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"pending","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"pending","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}],"newTodos":[{"content":"Create 03_functional_programming.py example file","status":"in_progress","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"pending","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"pending","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}]}}
{"parentUuid":"2723d275-5d5b-42c3-8909-2564e790c8dc","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01TuABbDbun8fSd65DgQhvET","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01Nng5TAdHmgh98GapWngfUR","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/03_functional_programming.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 3: Functional Programming for AI Systems\n\nLearn functional programming patterns that make AI code more predictable,\ntestable, and maintainable through immutability, pure functions, and composition.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/03_functional_programming.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import (\n    Callable, List, Dict, Any, Optional, Tuple, Union,\n    TypeVar, Iterator, Iterable\n)\nfrom functools import reduce, partial, singledispatch, lru_cache, wraps\nfrom itertools import chain, groupby, accumulate\nfrom dataclasses import dataclass, replace\nfrom operator import add, mul, itemgetter\nimport time\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\nfrom common import Message\n\nconsole = Console()\n\n# 🎯 LEARNING GOAL: Master functional programming for AI systems\n\nT = TypeVar('T')\nU = TypeVar('U')\n\n\n# Immutable Data Structures for AI\n@dataclass(frozen=True)\nclass ImmutableConfig:\n    \"\"\"Immutable configuration for AI models.\"\"\"\n    model_name: str\n    temperature: float\n    max_tokens: int\n    provider: str\n    api_key: Optional[str] = None\n    \n    def with_temperature(self, temperature: float) -> 'ImmutableConfig':\n        \"\"\"Create new config with different temperature.\"\"\"\n        return replace(self, temperature=temperature)\n    \n    def with_provider(self, provider: str, api_key: Optional[str] = None) -> 'ImmutableConfig':\n        \"\"\"Create new config with different provider.\"\"\"\n        return replace(self, provider=provider, api_key=api_key)\n\n\n@dataclass(frozen=True)\nclass ModelPrediction:\n    \"\"\"Immutable prediction result.\"\"\"\n    text: str\n    confidence: float\n    model_used: str\n    processing_time: float\n    metadata: Dict[str, Any]\n    \n    def with_confidence_threshold(self, threshold: float) -> Optional['ModelPrediction']:\n        \"\"\"Return prediction only if confidence meets threshold.\"\"\"\n        return self if self.confidence >= threshold else None\n    \n    def transform_text(self, transformer: Callable[[str], str]) -> 'ModelPrediction':\n        \"\"\"Transform text while preserving other fields.\"\"\"\n        return replace(self, text=transformer(self.text))\n\n\n# Pure Functions for Text Processing\ndef tokenize_text(text: str) -> List[str]:\n    \"\"\"Pure function to tokenize text.\"\"\"\n    return text.lower().split()\n\n\ndef count_tokens(tokens: List[str]) -> int:\n    \"\"\"Pure function to count tokens.\"\"\"\n    return len(tokens)\n\n\ndef filter_tokens(tokens: List[str], min_length: int = 2) -> List[str]:\n    \"\"\"Pure function to filter tokens by length.\"\"\"\n    return [token for token in tokens if len(token) >= min_length]\n\n\ndef calculate_text_stats(text: str) -> Dict[str, Any]:\n    \"\"\"Pure function to calculate text statistics.\"\"\"\n    tokens = tokenize_text(text)\n    filtered_tokens = filter_tokens(tokens)\n    \n    return {\n        \"character_count\": len(text),\n        \"word_count\": len(tokens),\n        \"unique_words\": len(set(tokens)),\n        \"filtered_words\": len(filtered_tokens),\n        \"avg_word_length\": sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n    }\n\n\n# Higher-Order Functions for AI Processing\ndef create_text_processor(\n    transformers: List[Callable[[str], str]]\n) -> Callable[[str], str]:\n    \"\"\"Create composed text processor from list of transformations.\"\"\"\n    \n    def process(text: str) -> str:\n        return reduce(lambda t, transformer: transformer(t), transformers, text)\n    \n    return process\n\n\ndef create_confidence_filter(threshold: float) -> Callable[[ModelPrediction], bool]:\n    \"\"\"Create a confidence filter function.\"\"\"\n    return lambda prediction: prediction.confidence >= threshold\n\n\ndef create_batch_processor(\n    processor: Callable[[T], U],\n    batch_size: int = 10\n) -> Callable[[List[T]], List[U]]:\n    \"\"\"Create batch processor from single-item processor.\"\"\"\n    \n    def process_batch(items: List[T]) -> List[U]:\n        results = []\n        for i in range(0, len(items), batch_size):\n            batch = items[i:i + batch_size]\n            batch_results = [processor(item) for item in batch]\n            results.extend(batch_results)\n        return results\n    \n    return process_batch\n\n\n# Function Composition Utilities\ndef compose(*functions: Callable) -> Callable:\n    \"\"\"Compose functions right to left (mathematical composition).\"\"\"\n    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)\n\n\ndef pipe(*functions: Callable) -> Callable:\n    \"\"\"Pipe functions left to right (more intuitive for data flow).\"\"\"\n    return reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n\n\n# Functional Pipeline for AI Data Processing\nclass FunctionalPipeline:\n    \"\"\"Functional pipeline for processing AI data.\"\"\"\n    \n    def __init__(self):\n        self.steps: List[Callable] = []\n    \n    def add_step(self, func: Callable) -> 'FunctionalPipeline':\n        \"\"\"Add processing step (returns new pipeline).\"\"\"\n        new_pipeline = FunctionalPipeline()\n        new_pipeline.steps = self.steps + [func]\n        return new_pipeline\n    \n    def process(self, data: Any) -> Any:\n        \"\"\"Process data through the pipeline.\"\"\"\n        return reduce(lambda result, step: step(result), self.steps, data)\n    \n    def __call__(self, data: Any) -> Any:\n        \"\"\"Make pipeline callable.\"\"\"\n        return self.process(data)\n\n\n# Currying and Partial Application\ndef curry_model_predict(\n    model_name: str,\n    temperature: float\n) -> Callable[[str], ModelPrediction]:\n    \"\"\"Curried model prediction function.\"\"\"\n    \n    def predict(text: str) -> ModelPrediction:\n        # Mock prediction logic\n        import hashlib\n        confidence = 0.8 + (hash(text) % 20) / 100  # Deterministic \"randomness\"\n        \n        return ModelPrediction(\n            text=text,\n            confidence=confidence,\n            model_used=model_name,\n            processing_time=0.1,\n            metadata={\"temperature\": temperature}\n        )\n    \n    return predict\n\n\n# Monadic-style Error Handling\nclass Result:\n    \"\"\"Monadic Result type for error handling.\"\"\"\n    \n    def __init__(self, value: Any = None, error: str = None):\n        self.value = value\n        self.error = error\n        self.is_success = error is None\n    \n    @classmethod\n    def success(cls, value: Any) -> 'Result':\n        \"\"\"Create successful result.\"\"\"\n        return cls(value=value)\n    \n    @classmethod\n    def failure(cls, error: str) -> 'Result':\n        \"\"\"Create failed result.\"\"\"\n        return cls(error=error)\n    \n    def map(self, func: Callable) -> 'Result':\n        \"\"\"Apply function if successful.\"\"\"\n        if self.is_success:\n            try:\n                return Result.success(func(self.value))\n            except Exception as e:\n                return Result.failure(str(e))\n        return self\n    \n    def flat_map(self, func: Callable) -> 'Result':\n        \"\"\"Apply function that returns Result if successful.\"\"\"\n        if self.is_success:\n            try:\n                return func(self.value)\n            except Exception as e:\n                return Result.failure(str(e))\n        return self\n    \n    def filter(self, predicate: Callable[[Any], bool], error_msg: str = \"Filter failed\") -> 'Result':\n        \"\"\"Filter result based on predicate.\"\"\"\n        if self.is_success and predicate(self.value):\n            return self\n        return Result.failure(error_msg if self.is_success else self.error)\n    \n    def get_or_else(self, default: Any) -> Any:\n        \"\"\"Get value or return default.\"\"\"\n        return self.value if self.is_success else default\n\n\ndef safe_divide(a: float, b: float) -> Result:\n    \"\"\"Safe division that returns Result.\"\"\"\n    if b == 0:\n        return Result.failure(\"Division by zero\")\n    return Result.success(a / b)\n\n\ndef safe_model_predict(text: str) -> Result:\n    \"\"\"Safe model prediction that can fail.\"\"\"\n    if not text.strip():\n        return Result.failure(\"Empty input text\")\n    \n    if len(text) > 1000:\n        return Result.failure(\"Text too long\")\n    \n    # Mock successful prediction\n    prediction = ModelPrediction(\n        text=text,\n        confidence=0.85,\n        model_used=\"safe-model\",\n        processing_time=0.1,\n        metadata={}\n    )\n    \n    return Result.success(prediction)\n\n\n# Lazy Evaluation and Generators\ndef lazy_text_generator(texts: List[str]) -> Iterator[Dict[str, Any]]:\n    \"\"\"Lazily process texts and yield results.\"\"\"\n    for text in texts:\n        # Only compute when needed\n        yield {\n            \"text\": text,\n            \"stats\": calculate_text_stats(text),\n            \"processed_at\": time.time()\n        }\n\n\ndef create_lazy_pipeline(\n    data_source: Iterable[T],\n    *processors: Callable[[T], T]\n) -> Iterator[T]:\n    \"\"\"Create lazy processing pipeline.\"\"\"\n    for item in data_source:\n        result = item\n        for processor in processors:\n            result = processor(result)\n        yield result\n\n\n# Memoization for Expensive Operations\n@lru_cache(maxsize=128)\ndef expensive_text_analysis(text: str) -> Dict[str, Any]:\n    \"\"\"Expensive analysis with memoization.\"\"\"\n    # Simulate expensive computation\n    time.sleep(0.01)\n    \n    tokens = tokenize_text(text)\n    return {\n        \"complexity_score\": len(set(tokens)) / len(tokens) if tokens else 0,\n        \"sentiment_score\": hash(text) % 100 / 100,\n        \"readability\": len(tokens) / text.count('.') if '.' in text else len(tokens)\n    }\n\n\n# Functional Decorators\ndef timing_decorator(func: Callable) -> Callable:\n    \"\"\"Decorator to time function execution.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        \n        console.print(f\"[dim]⏱️  {func.__name__} took {end_time - start_time:.4f}s[/dim]\")\n        return result\n    return wrapper\n\n\ndef retry_decorator(max_attempts: int = 3) -> Callable:\n    \"\"\"Decorator to retry failed function calls.\"\"\"\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    last_exception = e\n                    if attempt < max_attempts - 1:\n                        console.print(f\"[yellow]Attempt {attempt + 1} failed, retrying...[/yellow]\")\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n\n# Single Dispatch for Different Data Types\n@singledispatch\ndef process_input(data) -> str:\n    \"\"\"Process input data (default implementation).\"\"\"\n    return f\"Processed unknown type: {type(data).__name__}\"\n\n\n@process_input.register\ndef _(data: str) -> str:\n    \"\"\"Process string input.\"\"\"\n    return f\"String processed: {data.upper()}\"\n\n\n@process_input.register\ndef _(data: list) -> str:\n    \"\"\"Process list input.\"\"\"\n    return f\"List processed: {len(data)} items\"\n\n\n@process_input.register\ndef _(data: dict) -> str:\n    \"\"\"Process dictionary input.\"\"\"\n    return f\"Dict processed: {list(data.keys())}\"\n\n\nasync def demonstrate_immutable_data():\n    \"\"\"Show immutable data structures.\"\"\"\n    \n    console.print(Panel(\"🔒 Immutable Data Structures\", style=\"bold blue\"))\n    \n    # Create immutable config\n    base_config = ImmutableConfig(\n        model_name=\"gpt-4\",\n        temperature=0.7,\n        max_tokens=1000,\n        provider=\"openai\"\n    )\n    \n    console.print(\"[bold yellow]Original config:[/bold yellow]\")\n    console.print(f\"  Model: {base_config.model_name}\")\n    console.print(f\"  Temperature: {base_config.temperature}\")\n    console.print(f\"  Provider: {base_config.provider}\")\n    \n    # Create variations without mutating original\n    hot_config = base_config.with_temperature(1.2)\n    anthropic_config = base_config.with_provider(\"anthropic\", \"api-key-123\")\n    \n    console.print(f\"\\n[bold yellow]Hot config (temp={hot_config.temperature}):[/bold yellow]\")\n    console.print(f\"  Original unchanged: {base_config.temperature}\")\n    \n    console.print(f\"\\n[bold yellow]Anthropic config:[/bold yellow]\")\n    console.print(f\"  Provider: {anthropic_config.provider}\")\n    console.print(f\"  Original provider: {base_config.provider}\")\n    \n    # Show immutable predictions\n    prediction = ModelPrediction(\n        text=\"Hello world\",\n        confidence=0.85,\n        model_used=\"gpt-4\",\n        processing_time=0.1,\n        metadata={\"source\": \"test\"}\n    )\n    \n    # Transform without mutation\n    upper_prediction = prediction.transform_text(str.upper)\n    filtered_prediction = prediction.with_confidence_threshold(0.9)\n    \n    console.print(f\"\\n[bold cyan]Prediction transformations:[/bold cyan]\")\n    console.print(f\"  Original: '{prediction.text}'\")\n    console.print(f\"  Uppercase: '{upper_prediction.text}'\")\n    console.print(f\"  Filtered (>0.9): {filtered_prediction}\")\n\n\nasync def demonstrate_pure_functions():\n    \"\"\"Show pure function benefits.\"\"\"\n    \n    console.print(Panel(\"✨ Pure Functions\", style=\"bold green\"))\n    \n    test_text = \"The Quick Brown Fox Jumps Over The Lazy Dog\"\n    \n    console.print(f\"[bold yellow]Processing: '{test_text}'[/bold yellow]\")\n    \n    # Pure functions are predictable and testable\n    tokens = tokenize_text(test_text)\n    filtered = filter_tokens(tokens, min_length=3)\n    stats = calculate_text_stats(test_text)\n    \n    console.print(f\"\\n[bold cyan]Pure function results:[/bold cyan]\")\n    console.print(f\"  Tokens: {tokens[:5]}... ({len(tokens)} total)\")\n    console.print(f\"  Filtered (len>=3): {filtered[:5]}... ({len(filtered)} total)\")\n    console.print(f\"  Stats: {stats}\")\n    \n    # Pure functions can be easily combined\n    pipeline = pipe(\n        tokenize_text,\n        lambda tokens: filter_tokens(tokens, 4),\n        len\n    )\n    \n    long_word_count = pipeline(test_text)\n    console.print(f\"  Long words (len>=4): {long_word_count}\")\n\n\nasync def demonstrate_higher_order_functions():\n    \"\"\"Show higher-order function patterns.\"\"\"\n    \n    console.print(Panel(\"🚀 Higher-Order Functions\", style=\"bold magenta\"))\n    \n    # Create composed text processor\n    transformers = [\n        str.lower,\n        lambda text: text.replace(\"quick\", \"QUICK\"),\n        lambda text: text.replace(\"fox\", \"🦊\"),\n        str.title\n    ]\n    \n    processor = create_text_processor(transformers)\n    \n    test_text = \"The Quick Brown Fox\"\n    result = processor(test_text)\n    \n    console.print(f\"[bold yellow]Text transformation pipeline:[/bold yellow]\")\n    console.print(f\"  Input: '{test_text}'\")\n    console.print(f\"  Output: '{result}'\")\n    \n    # Create specialized functions\n    high_confidence_filter = create_confidence_filter(0.8)\n    batch_stats_processor = create_batch_processor(calculate_text_stats, batch_size=2)\n    \n    # Test batch processor\n    texts = [\"Hello world\", \"Python is great\", \"AI is fascinating\", \"Functional programming rocks\"]\n    batch_stats = batch_stats_processor(texts)\n    \n    console.print(f\"\\n[bold cyan]Batch processing results:[/bold cyan]\")\n    for i, stats in enumerate(batch_stats):\n        console.print(f\"  Text {i+1}: {stats['word_count']} words, {stats['unique_words']} unique\")\n\n\nasync def demonstrate_function_composition():\n    \"\"\"Show function composition patterns.\"\"\"\n    \n    console.print(Panel(\"🔗 Function Composition\", style=\"bold cyan\"))\n    \n    # Mathematical composition (right to left)\n    text_analyzer = compose(\n        lambda stats: f\"Analysis: {stats['word_count']} words, {stats['unique_words']} unique\",\n        calculate_text_stats\n    )\n    \n    # Pipe composition (left to right)\n    text_pipeline = pipe(\n        str.strip,\n        str.lower,\n        tokenize_text,\n        lambda tokens: filter_tokens(tokens, 2),\n        len\n    )\n    \n    test_text = \"  The Quick Brown Fox Jumps Over The Lazy Dog  \"\n    \n    console.print(f\"[bold yellow]Function composition test:[/bold yellow]\")\n    console.print(f\"  Input: '{test_text}'\")\n    \n    analysis = text_analyzer(test_text)\n    filtered_count = text_pipeline(test_text)\n    \n    console.print(f\"  Composed analysis: {analysis}\")\n    console.print(f\"  Piped processing: {filtered_count} filtered tokens\")\n    \n    # Using FunctionalPipeline class\n    pipeline = (FunctionalPipeline()\n                .add_step(str.strip)\n                .add_step(str.upper)\n                .add_step(lambda text: text.replace(\" \", \"_\")))\n    \n    pipeline_result = pipeline(test_text)\n    console.print(f\"  Pipeline class result: '{pipeline_result}'\")\n\n\nasync def demonstrate_currying_partial():\n    \"\"\"Show currying and partial application.\"\"\"\n    \n    console.print(Panel(\"🥢 Currying & Partial Application\", style=\"bold red\"))\n    \n    # Curried model prediction\n    gpt4_predictor = curry_model_predict(\"gpt-4\", 0.7)\n    claude_predictor = curry_model_predict(\"claude-3\", 0.5)\n    \n    test_texts = [\"Hello world\", \"Python is awesome\", \"AI will change everything\"]\n    \n    console.print(\"[bold yellow]Curried predictions:[/bold yellow]\")\n    \n    for text in test_texts:\n        gpt_pred = gpt4_predictor(text)\n        claude_pred = claude_predictor(text)\n        \n        console.print(f\"  '{text}':\")\n        console.print(f\"    GPT-4: {gpt_pred.confidence:.3f}\")\n        console.print(f\"    Claude: {claude_pred.confidence:.3f}\")\n    \n    # Partial application examples\n    multiply_by_10 = partial(mul, 10)\n    add_100 = partial(add, 100)\n    \n    numbers = [1, 2, 3, 4, 5]\n    multiplied = list(map(multiply_by_10, numbers))\n    added = list(map(add_100, numbers))\n    \n    console.print(f\"\\n[bold cyan]Partial application:[/bold cyan]\")\n    console.print(f\"  Original: {numbers}\")\n    console.print(f\"  × 10: {multiplied}\")\n    console.print(f\"  + 100: {added}\")\n\n\nasync def demonstrate_monadic_error_handling():\n    \"\"\"Show monadic error handling.\"\"\"\n    \n    console.print(Panel(\"🔄 Monadic Error Handling\", style=\"bold yellow\"))\n    \n    console.print(\"[bold yellow]Safe operations with Result monad:[/bold yellow]\")\n    \n    # Chaining operations with error handling\n    result1 = (safe_divide(10, 2)\n               .map(lambda x: x * 3)\n               .map(lambda x: f\"Result: {x}\")\n               .filter(lambda s: \"15\" in s, \"Result doesn't contain 15\"))\n    \n    result2 = (safe_divide(10, 0)\n               .map(lambda x: x * 3)\n               .map(lambda x: f\"Result: {x}\"))\n    \n    console.print(f\"  Safe calculation: {result1.value if result1.is_success else result1.error}\")\n    console.print(f\"  Division by zero: {result2.error}\")\n    \n    # Model prediction with error handling\n    test_cases = [\"Hello world\", \"\", \"x\" * 1001]\n    \n    console.print(f\"\\n[bold cyan]Safe model predictions:[/bold cyan]\")\n    \n    for text in test_cases:\n        result = (safe_model_predict(text)\n                  .map(lambda pred: pred.transform_text(str.upper))\n                  .filter(lambda pred: pred.confidence > 0.8, \"Low confidence\"))\n        \n        display_text = text[:20] + \"...\" if len(text) > 20 else text\n        if result.is_success:\n            console.print(f\"  ✅ '{display_text}': confidence {result.value.confidence}\")\n        else:\n            console.print(f\"  ❌ '{display_text}': {result.error}\")\n\n\nasync def demonstrate_lazy_evaluation():\n    \"\"\"Show lazy evaluation patterns.\"\"\"\n    \n    console.print(Panel(\"😴 Lazy Evaluation\", style=\"bold purple\"))\n    \n    # Create large dataset\n    large_texts = [f\"Sample text number {i}\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Lazy text processing:[/bold yellow]\")\n    \n    # Process only what we need\n    lazy_processor = lazy_text_generator(large_texts[:5])\n    \n    for i, processed in enumerate(lazy_processor):\n        if i >= 3:  # Only process first 3\n            break\n        console.print(f\"  Processed: {processed['text']} ({processed['stats']['word_count']} words)\")\n    \n    # Lazy pipeline with generators\n    def slow_processor(text: str) -> str:\n        time.sleep(0.001)  # Simulate slow processing\n        return text.upper()\n    \n    def add_prefix(text: str) -> str:\n        return f\"PROCESSED: {text}\"\n    \n    console.print(f\"\\n[bold cyan]Lazy pipeline (processing on demand):[/bold cyan]\")\n    \n    lazy_pipeline = create_lazy_pipeline(\n        large_texts[:10],\n        slow_processor,\n        add_prefix\n    )\n    \n    # Only consume what we need\n    first_three = []\n    for i, result in enumerate(lazy_pipeline):\n        first_three.append(result)\n        if i >= 2:\n            break\n    \n    for result in first_three:\n        console.print(f\"  {result}\")\n\n\nasync def demonstrate_memoization():\n    \"\"\"Show memoization for performance.\"\"\"\n    \n    console.print(Panel(\"🧠 Memoization\", style=\"bold orange\"))\n    \n    test_texts = [\n        \"This is a test text for analysis\",\n        \"Another sample text for processing\",\n        \"This is a test text for analysis\",  # Duplicate\n        \"Final text sample for testing\"\n    ]\n    \n    console.print(\"[bold yellow]Memoized expensive analysis:[/bold yellow]\")\n    \n    # First run - will be slow\n    start_time = time.time()\n    for text in test_texts:\n        analysis = expensive_text_analysis(text)\n        console.print(f\"  {text[:30]}... → complexity: {analysis['complexity_score']:.3f}\")\n    first_run_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold cyan]Cache info: {expensive_text_analysis.cache_info()}[/bold cyan]\")\n    \n    # Second run - should be faster due to caching\n    start_time = time.time()\n    for text in test_texts:\n        analysis = expensive_text_analysis(text)\n    second_run_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold green]Performance comparison:[/bold green]\")\n    console.print(f\"  First run: {first_run_time:.4f}s\")\n    console.print(f\"  Second run: {second_run_time:.4f}s\")\n    console.print(f\"  Speedup: {first_run_time / second_run_time:.1f}x\")\n\n\nasync def demonstrate_decorators():\n    \"\"\"Show functional decorators.\"\"\"\n    \n    console.print(Panel(\"🎨 Functional Decorators\", style=\"bold teal\"))\n    \n    @timing_decorator\n    @retry_decorator(max_attempts=2)\n    def flaky_process(text: str, fail_rate: float = 0.5) -> str:\n        \"\"\"Function that sometimes fails.\"\"\"\n        import random\n        if random.random() < fail_rate:\n            raise ValueError(\"Simulated processing error\")\n        return f\"Successfully processed: {text}\"\n    \n    console.print(\"[bold yellow]Testing decorated functions:[/bold yellow]\")\n    \n    try:\n        result = flaky_process(\"test data\", fail_rate=0.3)\n        console.print(f\"  ✅ {result}\")\n    except Exception as e:\n        console.print(f\"  ❌ Failed: {e}\")\n\n\nasync def demonstrate_single_dispatch():\n    \"\"\"Show single dispatch patterns.\"\"\"\n    \n    console.print(Panel(\"🎯 Single Dispatch\", style=\"bold indigo\"))\n    \n    test_data = [\n        \"Hello world\",\n        [1, 2, 3, 4, 5],\n        {\"name\": \"John\", \"age\": 30},\n        42\n    ]\n    \n    console.print(\"[bold yellow]Single dispatch processing:[/bold yellow]\")\n    \n    for data in test_data:\n        result = process_input(data)\n        console.print(f\"  {type(data).__name__}: {result}\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for functional programming.\"\"\"\n    \n    console.print(Panel(\"🎮 Interactive Exercise: Functional AI Pipeline\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"task\": \"Text Classification Pipeline\",\n            \"requirements\": [\n                \"Immutable data structures for configurations\",\n                \"Pure functions for text preprocessing\",\n                \"Functional composition for pipeline stages\",\n                \"Error handling with Result monad\"\n            ],\n            \"challenge\": \"Build a functional text classification system\"\n        },\n        {\n            \"task\": \"Model Ensemble System\",\n            \"requirements\": [\n                \"Curried prediction functions for different models\",\n                \"Lazy evaluation for expensive computations\",\n                \"Memoization for repeated inputs\",\n                \"Higher-order functions for combining predictions\"\n            ],\n            \"challenge\": \"Create a functional ensemble prediction system\"\n        },\n        {\n            \"task\": \"Stream Processing Engine\",\n            \"requirements\": [\n                \"Generator-based lazy processing\",\n                \"Functional transformations on data streams\",\n                \"Immutable state management\",\n                \"Composable stream operations\"\n            ],\n            \"challenge\": \"Design a functional stream processing architecture\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['task']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  • {req}\")\n        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n        console.print(\"\\n[bold green]Functional Patterns to Consider:[/bold green]\")\n        console.print(\"  • Immutable data structures\")\n        console.print(\"  • Pure functions and composition\")\n        console.print(\"  • Higher-order functions\")\n        console.print(\"  • Monadic error handling\")\n        console.print(\"  • Lazy evaluation\")\n        console.print(\"  • Memoization\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 AI Architecture Patterns: Functional Programming[/bold blue]\")\n    console.print(\"Learn functional patterns for predictable AI systems\\n\")\n    \n    async def main():\n        await demonstrate_immutable_data()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_pure_functions()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_higher_order_functions()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_function_composition()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_currying_partial()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_monadic_error_handling()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_lazy_evaluation()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_memoization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_decorators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_single_dispatch()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]✅ Completed![/bold green] Next: Run example 04_metaprogramming.py\")\n        console.print(\"💡 Pro tip: Functional patterns make AI code more predictable and testable!\")\n    \n    import asyncio\n    asyncio.run(main())"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":463,"cache_read_input_tokens":38577,"output_tokens":7784}},"requestId":"req_vrtx_011CRdbjiLyUtTSFB2YXhkSj","type":"assistant","uuid":"17a9c499-370e-496e-a9dd-f2e65c68fb37","timestamp":"2025-07-30T17:31:51.866Z"}
{"parentUuid":"17a9c499-370e-496e-a9dd-f2e65c68fb37","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01Nng5TAdHmgh98GapWngfUR","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/03_functional_programming.py"}]},"uuid":"393b86fe-7eb7-432e-810c-09a34bf150c7","timestamp":"2025-07-30T17:46:51.780Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/03_functional_programming.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 3: Functional Programming for AI Systems\n\nLearn functional programming patterns that make AI code more predictable,\ntestable, and maintainable through immutability, pure functions, and composition.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/03_functional_programming.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import (\n    Callable, List, Dict, Any, Optional, Tuple, Union,\n    TypeVar, Iterator, Iterable\n)\nfrom functools import reduce, partial, singledispatch, lru_cache, wraps\nfrom itertools import chain, groupby, accumulate\nfrom dataclasses import dataclass, replace\nfrom operator import add, mul, itemgetter\nimport time\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\nfrom common import Message\n\nconsole = Console()\n\n# 🎯 LEARNING GOAL: Master functional programming for AI systems\n\nT = TypeVar('T')\nU = TypeVar('U')\n\n\n# Immutable Data Structures for AI\n@dataclass(frozen=True)\nclass ImmutableConfig:\n    \"\"\"Immutable configuration for AI models.\"\"\"\n    model_name: str\n    temperature: float\n    max_tokens: int\n    provider: str\n    api_key: Optional[str] = None\n    \n    def with_temperature(self, temperature: float) -> 'ImmutableConfig':\n        \"\"\"Create new config with different temperature.\"\"\"\n        return replace(self, temperature=temperature)\n    \n    def with_provider(self, provider: str, api_key: Optional[str] = None) -> 'ImmutableConfig':\n        \"\"\"Create new config with different provider.\"\"\"\n        return replace(self, provider=provider, api_key=api_key)\n\n\n@dataclass(frozen=True)\nclass ModelPrediction:\n    \"\"\"Immutable prediction result.\"\"\"\n    text: str\n    confidence: float\n    model_used: str\n    processing_time: float\n    metadata: Dict[str, Any]\n    \n    def with_confidence_threshold(self, threshold: float) -> Optional['ModelPrediction']:\n        \"\"\"Return prediction only if confidence meets threshold.\"\"\"\n        return self if self.confidence >= threshold else None\n    \n    def transform_text(self, transformer: Callable[[str], str]) -> 'ModelPrediction':\n        \"\"\"Transform text while preserving other fields.\"\"\"\n        return replace(self, text=transformer(self.text))\n\n\n# Pure Functions for Text Processing\ndef tokenize_text(text: str) -> List[str]:\n    \"\"\"Pure function to tokenize text.\"\"\"\n    return text.lower().split()\n\n\ndef count_tokens(tokens: List[str]) -> int:\n    \"\"\"Pure function to count tokens.\"\"\"\n    return len(tokens)\n\n\ndef filter_tokens(tokens: List[str], min_length: int = 2) -> List[str]:\n    \"\"\"Pure function to filter tokens by length.\"\"\"\n    return [token for token in tokens if len(token) >= min_length]\n\n\ndef calculate_text_stats(text: str) -> Dict[str, Any]:\n    \"\"\"Pure function to calculate text statistics.\"\"\"\n    tokens = tokenize_text(text)\n    filtered_tokens = filter_tokens(tokens)\n    \n    return {\n        \"character_count\": len(text),\n        \"word_count\": len(tokens),\n        \"unique_words\": len(set(tokens)),\n        \"filtered_words\": len(filtered_tokens),\n        \"avg_word_length\": sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n    }\n\n\n# Higher-Order Functions for AI Processing\ndef create_text_processor(\n    transformers: List[Callable[[str], str]]\n) -> Callable[[str], str]:\n    \"\"\"Create composed text processor from list of transformations.\"\"\"\n    \n    def process(text: str) -> str:\n        return reduce(lambda t, transformer: transformer(t), transformers, text)\n    \n    return process\n\n\ndef create_confidence_filter(threshold: float) -> Callable[[ModelPrediction], bool]:\n    \"\"\"Create a confidence filter function.\"\"\"\n    return lambda prediction: prediction.confidence >= threshold\n\n\ndef create_batch_processor(\n    processor: Callable[[T], U],\n    batch_size: int = 10\n) -> Callable[[List[T]], List[U]]:\n    \"\"\"Create batch processor from single-item processor.\"\"\"\n    \n    def process_batch(items: List[T]) -> List[U]:\n        results = []\n        for i in range(0, len(items), batch_size):\n            batch = items[i:i + batch_size]\n            batch_results = [processor(item) for item in batch]\n            results.extend(batch_results)\n        return results\n    \n    return process_batch\n\n\n# Function Composition Utilities\ndef compose(*functions: Callable) -> Callable:\n    \"\"\"Compose functions right to left (mathematical composition).\"\"\"\n    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)\n\n\ndef pipe(*functions: Callable) -> Callable:\n    \"\"\"Pipe functions left to right (more intuitive for data flow).\"\"\"\n    return reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n\n\n# Functional Pipeline for AI Data Processing\nclass FunctionalPipeline:\n    \"\"\"Functional pipeline for processing AI data.\"\"\"\n    \n    def __init__(self):\n        self.steps: List[Callable] = []\n    \n    def add_step(self, func: Callable) -> 'FunctionalPipeline':\n        \"\"\"Add processing step (returns new pipeline).\"\"\"\n        new_pipeline = FunctionalPipeline()\n        new_pipeline.steps = self.steps + [func]\n        return new_pipeline\n    \n    def process(self, data: Any) -> Any:\n        \"\"\"Process data through the pipeline.\"\"\"\n        return reduce(lambda result, step: step(result), self.steps, data)\n    \n    def __call__(self, data: Any) -> Any:\n        \"\"\"Make pipeline callable.\"\"\"\n        return self.process(data)\n\n\n# Currying and Partial Application\ndef curry_model_predict(\n    model_name: str,\n    temperature: float\n) -> Callable[[str], ModelPrediction]:\n    \"\"\"Curried model prediction function.\"\"\"\n    \n    def predict(text: str) -> ModelPrediction:\n        # Mock prediction logic\n        import hashlib\n        confidence = 0.8 + (hash(text) % 20) / 100  # Deterministic \"randomness\"\n        \n        return ModelPrediction(\n            text=text,\n            confidence=confidence,\n            model_used=model_name,\n            processing_time=0.1,\n            metadata={\"temperature\": temperature}\n        )\n    \n    return predict\n\n\n# Monadic-style Error Handling\nclass Result:\n    \"\"\"Monadic Result type for error handling.\"\"\"\n    \n    def __init__(self, value: Any = None, error: str = None):\n        self.value = value\n        self.error = error\n        self.is_success = error is None\n    \n    @classmethod\n    def success(cls, value: Any) -> 'Result':\n        \"\"\"Create successful result.\"\"\"\n        return cls(value=value)\n    \n    @classmethod\n    def failure(cls, error: str) -> 'Result':\n        \"\"\"Create failed result.\"\"\"\n        return cls(error=error)\n    \n    def map(self, func: Callable) -> 'Result':\n        \"\"\"Apply function if successful.\"\"\"\n        if self.is_success:\n            try:\n                return Result.success(func(self.value))\n            except Exception as e:\n                return Result.failure(str(e))\n        return self\n    \n    def flat_map(self, func: Callable) -> 'Result':\n        \"\"\"Apply function that returns Result if successful.\"\"\"\n        if self.is_success:\n            try:\n                return func(self.value)\n            except Exception as e:\n                return Result.failure(str(e))\n        return self\n    \n    def filter(self, predicate: Callable[[Any], bool], error_msg: str = \"Filter failed\") -> 'Result':\n        \"\"\"Filter result based on predicate.\"\"\"\n        if self.is_success and predicate(self.value):\n            return self\n        return Result.failure(error_msg if self.is_success else self.error)\n    \n    def get_or_else(self, default: Any) -> Any:\n        \"\"\"Get value or return default.\"\"\"\n        return self.value if self.is_success else default\n\n\ndef safe_divide(a: float, b: float) -> Result:\n    \"\"\"Safe division that returns Result.\"\"\"\n    if b == 0:\n        return Result.failure(\"Division by zero\")\n    return Result.success(a / b)\n\n\ndef safe_model_predict(text: str) -> Result:\n    \"\"\"Safe model prediction that can fail.\"\"\"\n    if not text.strip():\n        return Result.failure(\"Empty input text\")\n    \n    if len(text) > 1000:\n        return Result.failure(\"Text too long\")\n    \n    # Mock successful prediction\n    prediction = ModelPrediction(\n        text=text,\n        confidence=0.85,\n        model_used=\"safe-model\",\n        processing_time=0.1,\n        metadata={}\n    )\n    \n    return Result.success(prediction)\n\n\n# Lazy Evaluation and Generators\ndef lazy_text_generator(texts: List[str]) -> Iterator[Dict[str, Any]]:\n    \"\"\"Lazily process texts and yield results.\"\"\"\n    for text in texts:\n        # Only compute when needed\n        yield {\n            \"text\": text,\n            \"stats\": calculate_text_stats(text),\n            \"processed_at\": time.time()\n        }\n\n\ndef create_lazy_pipeline(\n    data_source: Iterable[T],\n    *processors: Callable[[T], T]\n) -> Iterator[T]:\n    \"\"\"Create lazy processing pipeline.\"\"\"\n    for item in data_source:\n        result = item\n        for processor in processors:\n            result = processor(result)\n        yield result\n\n\n# Memoization for Expensive Operations\n@lru_cache(maxsize=128)\ndef expensive_text_analysis(text: str) -> Dict[str, Any]:\n    \"\"\"Expensive analysis with memoization.\"\"\"\n    # Simulate expensive computation\n    time.sleep(0.01)\n    \n    tokens = tokenize_text(text)\n    return {\n        \"complexity_score\": len(set(tokens)) / len(tokens) if tokens else 0,\n        \"sentiment_score\": hash(text) % 100 / 100,\n        \"readability\": len(tokens) / text.count('.') if '.' in text else len(tokens)\n    }\n\n\n# Functional Decorators\ndef timing_decorator(func: Callable) -> Callable:\n    \"\"\"Decorator to time function execution.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        \n        console.print(f\"[dim]⏱️  {func.__name__} took {end_time - start_time:.4f}s[/dim]\")\n        return result\n    return wrapper\n\n\ndef retry_decorator(max_attempts: int = 3) -> Callable:\n    \"\"\"Decorator to retry failed function calls.\"\"\"\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    last_exception = e\n                    if attempt < max_attempts - 1:\n                        console.print(f\"[yellow]Attempt {attempt + 1} failed, retrying...[/yellow]\")\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n\n# Single Dispatch for Different Data Types\n@singledispatch\ndef process_input(data) -> str:\n    \"\"\"Process input data (default implementation).\"\"\"\n    return f\"Processed unknown type: {type(data).__name__}\"\n\n\n@process_input.register\ndef _(data: str) -> str:\n    \"\"\"Process string input.\"\"\"\n    return f\"String processed: {data.upper()}\"\n\n\n@process_input.register\ndef _(data: list) -> str:\n    \"\"\"Process list input.\"\"\"\n    return f\"List processed: {len(data)} items\"\n\n\n@process_input.register\ndef _(data: dict) -> str:\n    \"\"\"Process dictionary input.\"\"\"\n    return f\"Dict processed: {list(data.keys())}\"\n\n\nasync def demonstrate_immutable_data():\n    \"\"\"Show immutable data structures.\"\"\"\n    \n    console.print(Panel(\"🔒 Immutable Data Structures\", style=\"bold blue\"))\n    \n    # Create immutable config\n    base_config = ImmutableConfig(\n        model_name=\"gpt-4\",\n        temperature=0.7,\n        max_tokens=1000,\n        provider=\"openai\"\n    )\n    \n    console.print(\"[bold yellow]Original config:[/bold yellow]\")\n    console.print(f\"  Model: {base_config.model_name}\")\n    console.print(f\"  Temperature: {base_config.temperature}\")\n    console.print(f\"  Provider: {base_config.provider}\")\n    \n    # Create variations without mutating original\n    hot_config = base_config.with_temperature(1.2)\n    anthropic_config = base_config.with_provider(\"anthropic\", \"api-key-123\")\n    \n    console.print(f\"\\n[bold yellow]Hot config (temp={hot_config.temperature}):[/bold yellow]\")\n    console.print(f\"  Original unchanged: {base_config.temperature}\")\n    \n    console.print(f\"\\n[bold yellow]Anthropic config:[/bold yellow]\")\n    console.print(f\"  Provider: {anthropic_config.provider}\")\n    console.print(f\"  Original provider: {base_config.provider}\")\n    \n    # Show immutable predictions\n    prediction = ModelPrediction(\n        text=\"Hello world\",\n        confidence=0.85,\n        model_used=\"gpt-4\",\n        processing_time=0.1,\n        metadata={\"source\": \"test\"}\n    )\n    \n    # Transform without mutation\n    upper_prediction = prediction.transform_text(str.upper)\n    filtered_prediction = prediction.with_confidence_threshold(0.9)\n    \n    console.print(f\"\\n[bold cyan]Prediction transformations:[/bold cyan]\")\n    console.print(f\"  Original: '{prediction.text}'\")\n    console.print(f\"  Uppercase: '{upper_prediction.text}'\")\n    console.print(f\"  Filtered (>0.9): {filtered_prediction}\")\n\n\nasync def demonstrate_pure_functions():\n    \"\"\"Show pure function benefits.\"\"\"\n    \n    console.print(Panel(\"✨ Pure Functions\", style=\"bold green\"))\n    \n    test_text = \"The Quick Brown Fox Jumps Over The Lazy Dog\"\n    \n    console.print(f\"[bold yellow]Processing: '{test_text}'[/bold yellow]\")\n    \n    # Pure functions are predictable and testable\n    tokens = tokenize_text(test_text)\n    filtered = filter_tokens(tokens, min_length=3)\n    stats = calculate_text_stats(test_text)\n    \n    console.print(f\"\\n[bold cyan]Pure function results:[/bold cyan]\")\n    console.print(f\"  Tokens: {tokens[:5]}... ({len(tokens)} total)\")\n    console.print(f\"  Filtered (len>=3): {filtered[:5]}... ({len(filtered)} total)\")\n    console.print(f\"  Stats: {stats}\")\n    \n    # Pure functions can be easily combined\n    pipeline = pipe(\n        tokenize_text,\n        lambda tokens: filter_tokens(tokens, 4),\n        len\n    )\n    \n    long_word_count = pipeline(test_text)\n    console.print(f\"  Long words (len>=4): {long_word_count}\")\n\n\nasync def demonstrate_higher_order_functions():\n    \"\"\"Show higher-order function patterns.\"\"\"\n    \n    console.print(Panel(\"🚀 Higher-Order Functions\", style=\"bold magenta\"))\n    \n    # Create composed text processor\n    transformers = [\n        str.lower,\n        lambda text: text.replace(\"quick\", \"QUICK\"),\n        lambda text: text.replace(\"fox\", \"🦊\"),\n        str.title\n    ]\n    \n    processor = create_text_processor(transformers)\n    \n    test_text = \"The Quick Brown Fox\"\n    result = processor(test_text)\n    \n    console.print(f\"[bold yellow]Text transformation pipeline:[/bold yellow]\")\n    console.print(f\"  Input: '{test_text}'\")\n    console.print(f\"  Output: '{result}'\")\n    \n    # Create specialized functions\n    high_confidence_filter = create_confidence_filter(0.8)\n    batch_stats_processor = create_batch_processor(calculate_text_stats, batch_size=2)\n    \n    # Test batch processor\n    texts = [\"Hello world\", \"Python is great\", \"AI is fascinating\", \"Functional programming rocks\"]\n    batch_stats = batch_stats_processor(texts)\n    \n    console.print(f\"\\n[bold cyan]Batch processing results:[/bold cyan]\")\n    for i, stats in enumerate(batch_stats):\n        console.print(f\"  Text {i+1}: {stats['word_count']} words, {stats['unique_words']} unique\")\n\n\nasync def demonstrate_function_composition():\n    \"\"\"Show function composition patterns.\"\"\"\n    \n    console.print(Panel(\"🔗 Function Composition\", style=\"bold cyan\"))\n    \n    # Mathematical composition (right to left)\n    text_analyzer = compose(\n        lambda stats: f\"Analysis: {stats['word_count']} words, {stats['unique_words']} unique\",\n        calculate_text_stats\n    )\n    \n    # Pipe composition (left to right)\n    text_pipeline = pipe(\n        str.strip,\n        str.lower,\n        tokenize_text,\n        lambda tokens: filter_tokens(tokens, 2),\n        len\n    )\n    \n    test_text = \"  The Quick Brown Fox Jumps Over The Lazy Dog  \"\n    \n    console.print(f\"[bold yellow]Function composition test:[/bold yellow]\")\n    console.print(f\"  Input: '{test_text}'\")\n    \n    analysis = text_analyzer(test_text)\n    filtered_count = text_pipeline(test_text)\n    \n    console.print(f\"  Composed analysis: {analysis}\")\n    console.print(f\"  Piped processing: {filtered_count} filtered tokens\")\n    \n    # Using FunctionalPipeline class\n    pipeline = (FunctionalPipeline()\n                .add_step(str.strip)\n                .add_step(str.upper)\n                .add_step(lambda text: text.replace(\" \", \"_\")))\n    \n    pipeline_result = pipeline(test_text)\n    console.print(f\"  Pipeline class result: '{pipeline_result}'\")\n\n\nasync def demonstrate_currying_partial():\n    \"\"\"Show currying and partial application.\"\"\"\n    \n    console.print(Panel(\"🥢 Currying & Partial Application\", style=\"bold red\"))\n    \n    # Curried model prediction\n    gpt4_predictor = curry_model_predict(\"gpt-4\", 0.7)\n    claude_predictor = curry_model_predict(\"claude-3\", 0.5)\n    \n    test_texts = [\"Hello world\", \"Python is awesome\", \"AI will change everything\"]\n    \n    console.print(\"[bold yellow]Curried predictions:[/bold yellow]\")\n    \n    for text in test_texts:\n        gpt_pred = gpt4_predictor(text)\n        claude_pred = claude_predictor(text)\n        \n        console.print(f\"  '{text}':\")\n        console.print(f\"    GPT-4: {gpt_pred.confidence:.3f}\")\n        console.print(f\"    Claude: {claude_pred.confidence:.3f}\")\n    \n    # Partial application examples\n    multiply_by_10 = partial(mul, 10)\n    add_100 = partial(add, 100)\n    \n    numbers = [1, 2, 3, 4, 5]\n    multiplied = list(map(multiply_by_10, numbers))\n    added = list(map(add_100, numbers))\n    \n    console.print(f\"\\n[bold cyan]Partial application:[/bold cyan]\")\n    console.print(f\"  Original: {numbers}\")\n    console.print(f\"  × 10: {multiplied}\")\n    console.print(f\"  + 100: {added}\")\n\n\nasync def demonstrate_monadic_error_handling():\n    \"\"\"Show monadic error handling.\"\"\"\n    \n    console.print(Panel(\"🔄 Monadic Error Handling\", style=\"bold yellow\"))\n    \n    console.print(\"[bold yellow]Safe operations with Result monad:[/bold yellow]\")\n    \n    # Chaining operations with error handling\n    result1 = (safe_divide(10, 2)\n               .map(lambda x: x * 3)\n               .map(lambda x: f\"Result: {x}\")\n               .filter(lambda s: \"15\" in s, \"Result doesn't contain 15\"))\n    \n    result2 = (safe_divide(10, 0)\n               .map(lambda x: x * 3)\n               .map(lambda x: f\"Result: {x}\"))\n    \n    console.print(f\"  Safe calculation: {result1.value if result1.is_success else result1.error}\")\n    console.print(f\"  Division by zero: {result2.error}\")\n    \n    # Model prediction with error handling\n    test_cases = [\"Hello world\", \"\", \"x\" * 1001]\n    \n    console.print(f\"\\n[bold cyan]Safe model predictions:[/bold cyan]\")\n    \n    for text in test_cases:\n        result = (safe_model_predict(text)\n                  .map(lambda pred: pred.transform_text(str.upper))\n                  .filter(lambda pred: pred.confidence > 0.8, \"Low confidence\"))\n        \n        display_text = text[:20] + \"...\" if len(text) > 20 else text\n        if result.is_success:\n            console.print(f\"  ✅ '{display_text}': confidence {result.value.confidence}\")\n        else:\n            console.print(f\"  ❌ '{display_text}': {result.error}\")\n\n\nasync def demonstrate_lazy_evaluation():\n    \"\"\"Show lazy evaluation patterns.\"\"\"\n    \n    console.print(Panel(\"😴 Lazy Evaluation\", style=\"bold purple\"))\n    \n    # Create large dataset\n    large_texts = [f\"Sample text number {i}\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Lazy text processing:[/bold yellow]\")\n    \n    # Process only what we need\n    lazy_processor = lazy_text_generator(large_texts[:5])\n    \n    for i, processed in enumerate(lazy_processor):\n        if i >= 3:  # Only process first 3\n            break\n        console.print(f\"  Processed: {processed['text']} ({processed['stats']['word_count']} words)\")\n    \n    # Lazy pipeline with generators\n    def slow_processor(text: str) -> str:\n        time.sleep(0.001)  # Simulate slow processing\n        return text.upper()\n    \n    def add_prefix(text: str) -> str:\n        return f\"PROCESSED: {text}\"\n    \n    console.print(f\"\\n[bold cyan]Lazy pipeline (processing on demand):[/bold cyan]\")\n    \n    lazy_pipeline = create_lazy_pipeline(\n        large_texts[:10],\n        slow_processor,\n        add_prefix\n    )\n    \n    # Only consume what we need\n    first_three = []\n    for i, result in enumerate(lazy_pipeline):\n        first_three.append(result)\n        if i >= 2:\n            break\n    \n    for result in first_three:\n        console.print(f\"  {result}\")\n\n\nasync def demonstrate_memoization():\n    \"\"\"Show memoization for performance.\"\"\"\n    \n    console.print(Panel(\"🧠 Memoization\", style=\"bold orange\"))\n    \n    test_texts = [\n        \"This is a test text for analysis\",\n        \"Another sample text for processing\",\n        \"This is a test text for analysis\",  # Duplicate\n        \"Final text sample for testing\"\n    ]\n    \n    console.print(\"[bold yellow]Memoized expensive analysis:[/bold yellow]\")\n    \n    # First run - will be slow\n    start_time = time.time()\n    for text in test_texts:\n        analysis = expensive_text_analysis(text)\n        console.print(f\"  {text[:30]}... → complexity: {analysis['complexity_score']:.3f}\")\n    first_run_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold cyan]Cache info: {expensive_text_analysis.cache_info()}[/bold cyan]\")\n    \n    # Second run - should be faster due to caching\n    start_time = time.time()\n    for text in test_texts:\n        analysis = expensive_text_analysis(text)\n    second_run_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold green]Performance comparison:[/bold green]\")\n    console.print(f\"  First run: {first_run_time:.4f}s\")\n    console.print(f\"  Second run: {second_run_time:.4f}s\")\n    console.print(f\"  Speedup: {first_run_time / second_run_time:.1f}x\")\n\n\nasync def demonstrate_decorators():\n    \"\"\"Show functional decorators.\"\"\"\n    \n    console.print(Panel(\"🎨 Functional Decorators\", style=\"bold teal\"))\n    \n    @timing_decorator\n    @retry_decorator(max_attempts=2)\n    def flaky_process(text: str, fail_rate: float = 0.5) -> str:\n        \"\"\"Function that sometimes fails.\"\"\"\n        import random\n        if random.random() < fail_rate:\n            raise ValueError(\"Simulated processing error\")\n        return f\"Successfully processed: {text}\"\n    \n    console.print(\"[bold yellow]Testing decorated functions:[/bold yellow]\")\n    \n    try:\n        result = flaky_process(\"test data\", fail_rate=0.3)\n        console.print(f\"  ✅ {result}\")\n    except Exception as e:\n        console.print(f\"  ❌ Failed: {e}\")\n\n\nasync def demonstrate_single_dispatch():\n    \"\"\"Show single dispatch patterns.\"\"\"\n    \n    console.print(Panel(\"🎯 Single Dispatch\", style=\"bold indigo\"))\n    \n    test_data = [\n        \"Hello world\",\n        [1, 2, 3, 4, 5],\n        {\"name\": \"John\", \"age\": 30},\n        42\n    ]\n    \n    console.print(\"[bold yellow]Single dispatch processing:[/bold yellow]\")\n    \n    for data in test_data:\n        result = process_input(data)\n        console.print(f\"  {type(data).__name__}: {result}\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for functional programming.\"\"\"\n    \n    console.print(Panel(\"🎮 Interactive Exercise: Functional AI Pipeline\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"task\": \"Text Classification Pipeline\",\n            \"requirements\": [\n                \"Immutable data structures for configurations\",\n                \"Pure functions for text preprocessing\",\n                \"Functional composition for pipeline stages\",\n                \"Error handling with Result monad\"\n            ],\n            \"challenge\": \"Build a functional text classification system\"\n        },\n        {\n            \"task\": \"Model Ensemble System\",\n            \"requirements\": [\n                \"Curried prediction functions for different models\",\n                \"Lazy evaluation for expensive computations\",\n                \"Memoization for repeated inputs\",\n                \"Higher-order functions for combining predictions\"\n            ],\n            \"challenge\": \"Create a functional ensemble prediction system\"\n        },\n        {\n            \"task\": \"Stream Processing Engine\",\n            \"requirements\": [\n                \"Generator-based lazy processing\",\n                \"Functional transformations on data streams\",\n                \"Immutable state management\",\n                \"Composable stream operations\"\n            ],\n            \"challenge\": \"Design a functional stream processing architecture\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['task']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  • {req}\")\n        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n        console.print(\"\\n[bold green]Functional Patterns to Consider:[/bold green]\")\n        console.print(\"  • Immutable data structures\")\n        console.print(\"  • Pure functions and composition\")\n        console.print(\"  • Higher-order functions\")\n        console.print(\"  • Monadic error handling\")\n        console.print(\"  • Lazy evaluation\")\n        console.print(\"  • Memoization\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 AI Architecture Patterns: Functional Programming[/bold blue]\")\n    console.print(\"Learn functional patterns for predictable AI systems\\n\")\n    \n    async def main():\n        await demonstrate_immutable_data()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_pure_functions()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_higher_order_functions()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_function_composition()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_currying_partial()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_monadic_error_handling()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_lazy_evaluation()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_memoization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_decorators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_single_dispatch()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]✅ Completed![/bold green] Next: Run example 04_metaprogramming.py\")\n        console.print(\"💡 Pro tip: Functional patterns make AI code more predictable and testable!\")\n    \n    import asyncio\n    asyncio.run(main())","structuredPatch":[]}}
{"parentUuid":"393b86fe-7eb7-432e-810c-09a34bf150c7","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_0148nmsaxJAH9VA6xVMKsTQw","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01Kts3BtSLnxZ8MjfufepMVB","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Create 03_functional_programming.py example file","status":"completed","priority":"high"},{"id":"2","content":"Create 04_metaprogramming.py example file","status":"in_progress","priority":"high"},{"id":"3","content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high"},{"id":"4","content":"Create tests for the Python idioms examples","status":"pending","priority":"medium"},{"id":"5","content":"Create exercises for hands-on practice","status":"pending","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":46875,"cache_read_input_tokens":0,"output_tokens":23}},"requestId":"req_vrtx_011CRdd2sNnZQ8sBc7ga7LLc","type":"assistant","uuid":"0e5bb494-0eb8-41ac-ae70-77d69989c213","timestamp":"2025-07-30T17:46:59.754Z"}
{"parentUuid":"0e5bb494-0eb8-41ac-ae70-77d69989c213","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01Kts3BtSLnxZ8MjfufepMVB","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"a612a953-14d2-444e-ab29-034a15c835e6","timestamp":"2025-07-30T17:46:59.805Z","toolUseResult":{"oldTodos":[{"content":"Create 03_functional_programming.py example file","status":"in_progress","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"pending","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"pending","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}],"newTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"in_progress","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"pending","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}]}}
{"parentUuid":"a612a953-14d2-444e-ab29-034a15c835e6","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01HjXQhQRwjrXzm9WseyR2gy","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01J7mFjNaRLPR8Ebxk9Ny56L","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/04_metaprogramming.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 4: Metaprogramming for AI Systems\n\nLearn metaprogramming techniques to create flexible, self-adapting AI systems\nthrough descriptors, metaclasses, decorators, and dynamic code generation.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/04_metaprogramming.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import (\n    Any, Dict, List, Optional, Callable, Type, Union,\n    get_type_hints, get_origin, get_args\n)\nfrom functools import wraps, update_wrapper\nimport inspect\nimport time\nfrom dataclasses import dataclass, field\nfrom abc import ABC, abstractmethod\nimport weakref\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\nconsole = Console()\n\n# 🎯 LEARNING GOAL: Master metaprogramming for flexible AI systems\n\n\n# Custom Descriptors for AI Model Properties\nclass ValidatedProperty:\n    \"\"\"Descriptor that validates property values.\"\"\"\n    \n    def __init__(self, validator: Callable[[Any], bool], error_msg: str):\n        self.validator = validator\n        self.error_msg = error_msg\n        self.name = None\n    \n    def __set_name__(self, owner: Type, name: str):\n        \"\"\"Called when descriptor is assigned to a class attribute.\"\"\"\n        self.name = name\n        self.private_name = f'_{name}'\n    \n    def __get__(self, obj: Any, objtype: Type = None) -> Any:\n        \"\"\"Get the property value.\"\"\"\n        if obj is None:\n            return self\n        return getattr(obj, self.private_name, None)\n    \n    def __set__(self, obj: Any, value: Any) -> None:\n        \"\"\"Set the property value with validation.\"\"\"\n        if not self.validator(value):\n            raise ValueError(f\"{self.error_msg}: {value}\")\n        setattr(obj, self.private_name, value)\n\n\nclass CachedProperty:\n    \"\"\"Descriptor that caches expensive computations.\"\"\"\n    \n    def __init__(self, func: Callable):\n        self.func = func\n        self.name = func.__name__\n        update_wrapper(self, func)\n    \n    def __get__(self, obj: Any, objtype: Type = None) -> Any:\n        if obj is None:\n            return self\n        \n        # Check if cached value exists\n        cache_name = f'_cached_{self.name}'\n        if hasattr(obj, cache_name):\n            return getattr(obj, cache_name)\n        \n        # Compute and cache the value\n        value = self.func(obj)\n        setattr(obj, cache_name, value)\n        return value\n    \n    def __set__(self, obj: Any, value: Any) -> None:\n        \"\"\"Allow manual cache invalidation by setting new value.\"\"\"\n        cache_name = f'_cached_{self.name}'\n        setattr(obj, cache_name, value)\n    \n    def __delete__(self, obj: Any) -> None:\n        \"\"\"Clear the cached value.\"\"\"\n        cache_name = f'_cached_{self.name}'\n        if hasattr(obj, cache_name):\n            delattr(obj, cache_name)\n\n\n# Metaclass for Automatic Model Registration\nclass ModelRegistryMeta(type):\n    \"\"\"Metaclass that automatically registers AI models.\"\"\"\n    \n    _registry: Dict[str, Type] = {}\n    \n    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\n        # Add automatic ID generation\n        if 'model_id' not in namespace and name != 'BaseModel':\n            namespace['model_id'] = name.lower().replace('model', '')\n        \n        # Create the class\n        cls = super().__new__(mcs, name, bases, namespace)\n        \n        # Register the model (skip base classes)\n        if name != 'BaseModel' and hasattr(cls, 'model_id'):\n            mcs._registry[cls.model_id] = cls\n            console.print(f\"[dim]🔧 Registered model: {cls.model_id} -> {name}[/dim]\")\n        \n        return cls\n    \n    @classmethod\n    def get_model(mcs, model_id: str) -> Optional[Type]:\n        \"\"\"Get model class by ID.\"\"\"\n        return mcs._registry.get(model_id)\n    \n    @classmethod\n    def list_models(mcs) -> List[str]:\n        \"\"\"List all registered model IDs.\"\"\"\n        return list(mcs._registry.keys())\n\n\n# Metaclass for Automatic Method Instrumentation\nclass InstrumentedMeta(type):\n    \"\"\"Metaclass that automatically adds instrumentation to methods.\"\"\"\n    \n    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\n        # Instrument methods that start with 'predict'\n        for attr_name, attr_value in namespace.items():\n            if (attr_name.startswith('predict') and \n                callable(attr_value) and \n                not attr_name.startswith('_')):\n                \n                namespace[attr_name] = mcs._instrument_method(attr_value, attr_name)\n        \n        return super().__new__(mcs, name, bases, namespace)\n    \n    @staticmethod\n    def _instrument_method(method: Callable, method_name: str) -> Callable:\n        \"\"\"Add timing and logging to a method.\"\"\"\n        @wraps(method)\n        def wrapper(self, *args, **kwargs):\n            start_time = time.time()\n            try:\n                result = method(self, *args, **kwargs)\n                end_time = time.time()\n                duration = end_time - start_time\n                \n                # Store metrics\n                if not hasattr(self, '_metrics'):\n                    self._metrics = {}\n                if method_name not in self._metrics:\n                    self._metrics[method_name] = []\n                \n                self._metrics[method_name].append({\n                    'duration': duration,\n                    'timestamp': start_time,\n                    'success': True\n                })\n                \n                console.print(f\"[dim]📊 {self.__class__.__name__}.{method_name} took {duration:.4f}s[/dim]\")\n                return result\n                \n            except Exception as e:\n                end_time = time.time()\n                duration = end_time - start_time\n                \n                # Store error metrics\n                if not hasattr(self, '_metrics'):\n                    self._metrics = {}\n                if method_name not in self._metrics:\n                    self._metrics[method_name] = []\n                \n                self._metrics[method_name].append({\n                    'duration': duration,\n                    'timestamp': start_time,\n                    'success': False,\n                    'error': str(e)\n                })\n                \n                console.print(f\"[dim red]❌ {self.__class__.__name__}.{method_name} failed after {duration:.4f}s[/dim red]\")\n                raise\n        \n        return wrapper\n\n\n# Dynamic Class Generation\ndef create_model_class(\n    name: str, \n    attributes: Dict[str, Any], \n    methods: Dict[str, Callable] = None,\n    base_classes: tuple = ()\n) -> Type:\n    \"\"\"Dynamically create a model class.\"\"\"\n    \n    # Default base class\n    if not base_classes:\n        base_classes = (object,)\n    \n    # Combine attributes and methods\n    namespace = attributes.copy()\n    if methods:\n        namespace.update(methods)\n    \n    # Add default __init__ if not provided\n    if '__init__' not in namespace:\n        def __init__(self, **kwargs):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        namespace['__init__'] = __init__\n    \n    # Create the class\n    return type(name, base_classes, namespace)\n\n\n# Advanced Decorator Patterns\nclass memoize_with_ttl:\n    \"\"\"Decorator that memoizes with time-to-live.\"\"\"\n    \n    def __init__(self, ttl_seconds: float = 60.0):\n        self.ttl_seconds = ttl_seconds\n        self.cache = {}\n    \n    def __call__(self, func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = str(args) + str(sorted(kwargs.items()))\n            current_time = time.time()\n            \n            # Check if cached and not expired\n            if key in self.cache:\n                result, timestamp = self.cache[key]\n                if current_time - timestamp < self.ttl_seconds:\n                    console.print(f\"[dim green]💾 Cache hit for {func.__name__}[/dim green]\")\n                    return result\n            \n            # Compute new result\n            result = func(*args, **kwargs)\n            self.cache[key] = (result, current_time)\n            console.print(f\"[dim yellow]🔄 Cache miss for {func.__name__}[/dim yellow]\")\n            return result\n        \n        # Add cache management methods\n        wrapper.clear_cache = lambda: self.cache.clear()\n        wrapper.cache_info = lambda: {\n            'size': len(self.cache),\n            'ttl_seconds': self.ttl_seconds\n        }\n        \n        return wrapper\n\n\nclass retry_with_backoff:\n    \"\"\"Decorator that retries with exponential backoff.\"\"\"\n    \n    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n    \n    def __call__(self, func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(self.max_retries + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    last_exception = e\n                    \n                    if attempt < self.max_retries:\n                        delay = self.base_delay * (2 ** attempt)\n                        console.print(f\"[yellow]🔄 Retry {attempt + 1}/{self.max_retries} in {delay:.1f}s[/yellow]\")\n                        time.sleep(delay)\n                    else:\n                        console.print(f\"[red]❌ All retries exhausted for {func.__name__}[/red]\")\n            \n            raise last_exception\n        \n        return wrapper\n\n\n# Property Factory\ndef typed_property(property_type: Type, default: Any = None, validator: Callable = None):\n    \"\"\"Factory for creating typed properties with optional validation.\"\"\"\n    \n    def property_decorator(func: Callable) -> property:\n        name = func.__name__\n        private_name = f'_{name}'\n        \n        def getter(self):\n            return getattr(self, private_name, default)\n        \n        def setter(self, value):\n            # Type checking\n            if not isinstance(value, property_type):\n                try:\n                    value = property_type(value)\n                except (ValueError, TypeError):\n                    raise TypeError(f\"{name} must be of type {property_type.__name__}\")\n            \n            # Optional validation\n            if validator and not validator(value):\n                raise ValueError(f\"Invalid value for {name}: {value}\")\n            \n            setattr(self, private_name, value)\n        \n        return property(getter, setter, doc=func.__doc__)\n    \n    return property_decorator\n\n\n# Context Manager for Dynamic Configuration\nclass DynamicConfig:\n    \"\"\"Context manager that temporarily modifies class attributes.\"\"\"\n    \n    def __init__(self, target_class: Type, **config_changes):\n        self.target_class = target_class\n        self.config_changes = config_changes\n        self.original_values = {}\n    \n    def __enter__(self):\n        # Save original values\n        for attr_name, new_value in self.config_changes.items():\n            if hasattr(self.target_class, attr_name):\n                self.original_values[attr_name] = getattr(self.target_class, attr_name)\n            setattr(self.target_class, attr_name, new_value)\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore original values\n        for attr_name, original_value in self.original_values.items():\n            setattr(self.target_class, attr_name, original_value)\n        \n        # Remove attributes that didn't exist originally\n        for attr_name in self.config_changes:\n            if attr_name not in self.original_values:\n                delattr(self.target_class, attr_name)\n\n\n# Example Classes Using Metaprogramming\n\nclass BaseModel(metaclass=ModelRegistryMeta):\n    \"\"\"Base class for AI models with validation.\"\"\"\n    \n    # Validated properties using descriptors\n    temperature = ValidatedProperty(\n        lambda x: 0.0 <= x <= 2.0,\n        \"Temperature must be between 0.0 and 2.0\"\n    )\n    \n    max_tokens = ValidatedProperty(\n        lambda x: isinstance(x, int) and x > 0,\n        \"Max tokens must be a positive integer\"\n    )\n    \n    def __init__(self, temperature: float = 0.7, max_tokens: int = 1000):\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self._call_count = 0\n    \n    @CachedProperty\n    def model_signature(self) -> str:\n        \"\"\"Expensive computation that gets cached.\"\"\"\n        console.print(\"[dim]🔍 Computing model signature...[/dim]\")\n        time.sleep(0.1)  # Simulate expensive computation\n        return f\"{self.__class__.__name__}_{hash(str(self.temperature) + str(self.max_tokens))}\"\n\n\nclass GPTModel(BaseModel, metaclass=InstrumentedMeta):\n    \"\"\"GPT model with automatic instrumentation.\"\"\"\n    \n    def predict_text(self, text: str) -> str:\n        \"\"\"Predict text completion.\"\"\"\n        time.sleep(0.05)  # Simulate prediction\n        self._call_count += 1\n        return f\"GPT prediction for: {text}\"\n    \n    def predict_classification(self, text: str) -> Dict[str, float]:\n        \"\"\"Predict text classification.\"\"\"\n        time.sleep(0.03)\n        self._call_count += 1\n        return {\"positive\": 0.8, \"negative\": 0.1, \"neutral\": 0.1}\n\n\nclass ClaudeModel(BaseModel, metaclass=InstrumentedMeta):\n    \"\"\"Claude model with automatic instrumentation.\"\"\"\n    \n    def predict_text(self, text: str) -> str:\n        \"\"\"Predict text completion.\"\"\"\n        time.sleep(0.04)\n        self._call_count += 1\n        return f\"Claude prediction for: {text}\"\n\n\nclass ConfigurableModel:\n    \"\"\"Model with typed properties using property factory.\"\"\"\n    \n    @typed_property(float, default=0.7, validator=lambda x: 0.0 <= x <= 2.0)\n    def temperature(self):\n        \"\"\"Model temperature parameter.\"\"\"\n        pass\n    \n    @typed_property(int, default=1000, validator=lambda x: x > 0)\n    def max_tokens(self):\n        \"\"\"Maximum tokens to generate.\"\"\"\n        pass\n    \n    @typed_property(str, default=\"gpt-4\")\n    def model_name(self):\n        \"\"\"Name of the model.\"\"\"\n        pass\n\n\n# Dynamic Model Factory\nclass ModelFactory:\n    \"\"\"Factory for creating model classes dynamically.\"\"\"\n    \n    @staticmethod\n    def create_simple_model(name: str, prediction_delay: float = 0.1) -> Type:\n        \"\"\"Create a simple model class.\"\"\"\n        \n        def predict(self, text: str) -> str:\n            time.sleep(prediction_delay)\n            return f\"{name} processed: {text}\"\n        \n        def get_info(self) -> Dict[str, Any]:\n            return {\n                \"model_name\": name,\n                \"prediction_delay\": prediction_delay,\n                \"class_name\": self.__class__.__name__\n            }\n        \n        return create_model_class(\n            name=f\"{name}Model\",\n            attributes={\n                \"model_name\": name,\n                \"prediction_delay\": prediction_delay\n            },\n            methods={\n                \"predict\": predict,\n                \"get_info\": get_info\n            },\n            base_classes=(BaseModel,)\n        )\n    \n    @staticmethod\n    def create_ensemble_model(models: List[Type]) -> Type:\n        \"\"\"Create an ensemble model from multiple model classes.\"\"\"\n        \n        def predict(self, text: str) -> List[str]:\n            results = []\n            for model_class in models:\n                model = model_class()\n                if hasattr(model, 'predict'):\n                    results.append(model.predict(text))\n                elif hasattr(model, 'predict_text'):\n                    results.append(model.predict_text(text))\n            return results\n        \n        def get_model_info(self) -> List[str]:\n            return [model.__name__ for model in models]\n        \n        return create_model_class(\n            name=\"EnsembleModel\",\n            attributes={\n                \"component_models\": models\n            },\n            methods={\n                \"predict\": predict,\n                \"get_model_info\": get_model_info\n            }\n        )\n\n\nasync def demonstrate_descriptors():\n    \"\"\"Show descriptor patterns.\"\"\"\n    \n    console.print(Panel(\"🔍 Descriptors\", style=\"bold blue\"))\n    \n    console.print(\"[bold yellow]Creating model with validated properties:[/bold yellow]\")\n    \n    model = GPTModel()\n    console.print(f\"  Default temperature: {model.temperature}\")\n    console.print(f\"  Default max_tokens: {model.max_tokens}\")\n    \n    # Test validation\n    try:\n        model.temperature = 1.5\n        console.print(f\"  ✅ Set temperature to {model.temperature}\")\n    except ValueError as e:\n        console.print(f\"  ❌ Validation error: {e}\")\n    \n    try:\n        model.temperature = 3.0  # Should fail\n    except ValueError as e:\n        console.print(f\"  ❌ Validation caught: {e}\")\n    \n    # Test cached property\n    console.print(f\"\\n[bold cyan]Testing cached property:[/bold cyan]\")\n    sig1 = model.model_signature  # Should compute\n    sig2 = model.model_signature  # Should use cache\n    console.print(f\"  Signatures match: {sig1 == sig2}\")\n\n\nasync def demonstrate_metaclasses():\n    \"\"\"Show metaclass patterns.\"\"\"\n    \n    console.print(Panel(\"🏭 Metaclasses\", style=\"bold green\"))\n    \n    console.print(\"[bold yellow]Automatic model registration:[/bold yellow]\")\n    registered_models = ModelRegistryMeta.list_models()\n    console.print(f\"  Registered models: {registered_models}\")\n    \n    # Get model by ID\n    gpt_class = ModelRegistryMeta.get_model('gpt')\n    claude_class = ModelRegistryMeta.get_model('claude')\n    \n    console.print(f\"\\n[bold cyan]Creating models from registry:[/bold cyan]\")\n    if gpt_class:\n        gpt = gpt_class()\n        result = gpt.predict_text(\"Hello world\")\n        console.print(f\"  GPT: {result}\")\n    \n    if claude_class:\n        claude = claude_class()\n        result = claude.predict_text(\"Hello world\")\n        console.print(f\"  Claude: {result}\")\n    \n    # Show automatic instrumentation\n    console.print(f\"\\n[bold magenta]Instrumentation metrics:[/bold magenta]\")\n    if hasattr(gpt, '_metrics'):\n        for method, metrics in gpt._metrics.items():\n            avg_duration = sum(m['duration'] for m in metrics) / len(metrics)\n            console.print(f\"  {method}: {len(metrics)} calls, avg {avg_duration:.4f}s\")\n\n\nasync def demonstrate_dynamic_classes():\n    \"\"\"Show dynamic class creation.\"\"\"\n    \n    console.print(Panel(\"⚡ Dynamic Class Creation\", style=\"bold magenta\"))\n    \n    # Create models using factory\n    console.print(\"[bold yellow]Creating models dynamically:[/bold yellow]\")\n    \n    FastModel = ModelFactory.create_simple_model(\"FastAI\", prediction_delay=0.02)\n    SlowModel = ModelFactory.create_simple_model(\"DeepThink\", prediction_delay=0.08)\n    \n    fast_model = FastModel()\n    slow_model = SlowModel()\n    \n    console.print(f\"  Fast model: {fast_model.get_info()}\")\n    console.print(f\"  Slow model: {slow_model.get_info()}\")\n    \n    # Test predictions\n    start_time = time.time()\n    fast_result = fast_model.predict(\"test input\")\n    fast_time = time.time() - start_time\n    \n    start_time = time.time()\n    slow_result = slow_model.predict(\"test input\")\n    slow_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold cyan]Performance comparison:[/bold cyan]\")\n    console.print(f\"  Fast: {fast_time:.4f}s - {fast_result}\")\n    console.print(f\"  Slow: {slow_time:.4f}s - {slow_result}\")\n    \n    # Create ensemble model\n    console.print(f\"\\n[bold yellow]Creating ensemble model:[/bold yellow]\")\n    EnsembleModel = ModelFactory.create_ensemble_model([FastModel, SlowModel])\n    ensemble = EnsembleModel()\n    \n    ensemble_results = ensemble.predict(\"ensemble test\")\n    console.print(f\"  Ensemble results: {ensemble_results}\")\n    console.print(f\"  Component models: {ensemble.get_model_info()}\")\n\n\nasync def demonstrate_decorators():\n    \"\"\"Show advanced decorator patterns.\"\"\"\n    \n    console.print(Panel(\"🎨 Advanced Decorators\", style=\"bold cyan\"))\n    \n    @memoize_with_ttl(ttl_seconds=2.0)\n    @retry_with_backoff(max_retries=2, base_delay=0.5)\n    def expensive_ai_call(query: str, fail_rate: float = 0.0) -> str:\n        \"\"\"Simulate expensive AI call that might fail.\"\"\"\n        import random\n        \n        if random.random() < fail_rate:\n            raise ConnectionError(\"Simulated API failure\")\n        \n        time.sleep(0.1)  # Simulate processing time\n        return f\"AI response to: {query}\"\n    \n    console.print(\"[bold yellow]Testing decorated function:[/bold yellow]\")\n    \n    # First call - should compute\n    result1 = expensive_ai_call(\"What is AI?\")\n    console.print(f\"  Result 1: {result1}\")\n    \n    # Second call - should use cache\n    result2 = expensive_ai_call(\"What is AI?\")\n    console.print(f\"  Result 2: {result2}\")\n    \n    # Test with failures\n    console.print(f\"\\n[bold cyan]Testing with simulated failures:[/bold cyan]\")\n    try:\n        result3 = expensive_ai_call(\"Risky query\", fail_rate=0.7)\n        console.print(f\"  Success: {result3}\")\n    except Exception as e:\n        console.print(f\"  Failed: {e}\")\n    \n    # Show cache info\n    cache_info = expensive_ai_call.cache_info()\n    console.print(f\"\\n[bold green]Cache info: {cache_info}[/bold green]\")\n\n\nasync def demonstrate_typed_properties():\n    \"\"\"Show typed property factory.\"\"\"\n    \n    console.print(Panel(\"🏷️ Typed Properties\", style=\"bold red\"))\n    \n    console.print(\"[bold yellow]Creating model with typed properties:[/bold yellow]\")\n    \n    model = ConfigurableModel()\n    \n    # Test default values\n    console.print(f\"  Default temperature: {model.temperature}\")\n    console.print(f\"  Default max_tokens: {model.max_tokens}\")\n    console.print(f\"  Default model_name: {model.model_name}\")\n    \n    # Test type conversion\n    console.print(f\"\\n[bold cyan]Testing type conversion:[/bold cyan]\")\n    model.temperature = \"1.2\"  # String should convert to float\n    model.max_tokens = \"500\"   # String should convert to int\n    \n    console.print(f\"  Converted temperature: {model.temperature} (type: {type(model.temperature)})\")\n    console.print(f\"  Converted max_tokens: {model.max_tokens} (type: {type(model.max_tokens)})\")\n    \n    # Test validation\n    console.print(f\"\\n[bold yellow]Testing validation:[/bold yellow]\")\n    try:\n        model.temperature = 3.0  # Should fail validation\n    except ValueError as e:\n        console.print(f\"  ❌ Validation error: {e}\")\n    \n    try:\n        model.max_tokens = -100  # Should fail validation\n    except ValueError as e:\n        console.print(f\"  ❌ Validation error: {e}\")\n\n\nasync def demonstrate_dynamic_config():\n    \"\"\"Show dynamic configuration context manager.\"\"\"\n    \n    console.print(Panel(\"⚙️ Dynamic Configuration\", style=\"bold yellow\"))\n    \n    # Original class configuration\n    class AIService:\n        timeout = 30.0\n        retry_count = 3\n        debug_mode = False\n        \n        @classmethod\n        def get_config(cls) -> Dict[str, Any]:\n            return {\n                \"timeout\": cls.timeout,\n                \"retry_count\": cls.retry_count,\n                \"debug_mode\": cls.debug_mode\n            }\n    \n    console.print(\"[bold yellow]Original configuration:[/bold yellow]\")\n    console.print(f\"  {AIService.get_config()}\")\n    \n    # Temporarily modify configuration\n    console.print(f\"\\n[bold cyan]Using dynamic configuration:[/bold cyan]\")\n    with DynamicConfig(AIService, timeout=60.0, debug_mode=True, new_feature=True):\n        console.print(f\"  Inside context: {AIService.get_config()}\")\n        console.print(f\"  New feature: {getattr(AIService, 'new_feature', 'Not found')}\")\n    \n    console.print(f\"\\n[bold green]After context:[/bold green]\")\n    console.print(f\"  Restored: {AIService.get_config()}\")\n    console.print(f\"  New feature: {getattr(AIService, 'new_feature', 'Not found')}\")\n\n\ndef analyze_function_signature(func: Callable) -> Dict[str, Any]:\n    \"\"\"Metaprogramming function to analyze function signatures.\"\"\"\n    sig = inspect.signature(func)\n    type_hints = get_type_hints(func)\n    \n    return {\n        \"name\": func.__name__,\n        \"parameters\": {\n            name: {\n                \"annotation\": type_hints.get(name, param.annotation),\n                \"default\": param.default if param.default != param.empty else None,\n                \"kind\": param.kind.name\n            }\n            for name, param in sig.parameters.items()\n        },\n        \"return_type\": type_hints.get('return', sig.return_annotation),\n        \"docstring\": inspect.getdoc(func)\n    }\n\n\nasync def demonstrate_reflection():\n    \"\"\"Show reflection and introspection.\"\"\"\n    \n    console.print(Panel(\"🔍 Reflection & Introspection\", style=\"bold purple\"))\n    \n    # Analyze function signatures\n    def sample_ai_function(text: str, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\n        \"\"\"Sample AI function for analysis.\"\"\"\n        return {\"processed\": text, \"config\": {\"temperature\": temperature, \"max_tokens\": max_tokens}}\n    \n    analysis = analyze_function_signature(sample_ai_function)\n    \n    console.print(\"[bold yellow]Function signature analysis:[/bold yellow]\")\n    console.print(f\"  Name: {analysis['name']}\")\n    console.print(f\"  Return type: {analysis['return_type']}\")\n    console.print(f\"  Parameters:\")\n    \n    for param_name, param_info in analysis['parameters'].items():\n        console.print(f\"    {param_name}: {param_info['annotation']} (default: {param_info['default']})\")\n    \n    # Analyze class structure\n    console.print(f\"\\n[bold cyan]Class structure analysis:[/bold cyan]\")\n    gpt_model = GPTModel()\n    \n    methods = [name for name, method in inspect.getmembers(gpt_model, predicate=inspect.ismethod)]\n    properties = [name for name in dir(gpt_model) if isinstance(getattr(type(gpt_model), name, None), property)]\n    \n    console.print(f\"  Methods: {[m for m in methods if not m.startswith('_')]}\")\n    console.print(f\"  Properties: {properties}\")\n    \n    if hasattr(gpt_model, '_metrics'):\n        console.print(f\"  Metrics: {list(gpt_model._metrics.keys())}\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for metaprogramming.\"\"\"\n    \n    console.print(Panel(\"🎮 Interactive Exercise: Metaprogramming AI Framework\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"challenge\": \"Dynamic Model Registry\",\n            \"requirements\": [\n                \"Automatic registration of model classes\",\n                \"Type validation for model parameters\",\n                \"Runtime model discovery and instantiation\",\n                \"Automatic instrumentation of model methods\"\n            ],\n            \"tools\": \"Metaclasses, descriptors, registration patterns\"\n        },\n        {\n            \"challenge\": \"Self-Configuring AI Pipeline\",\n            \"requirements\": [\n                \"Dynamic property creation based on configuration\",\n                \"Automatic validation and type conversion\",\n                \"Context-based configuration overrides\",\n                \"Runtime pipeline modification\"\n            ],\n            \"tools\": \"Property factories, context managers, dynamic classes\"\n        },\n        {\n            \"challenge\": \"Adaptive Model Behavior\",\n            \"requirements\": [\n                \"Runtime method modification based on usage patterns\",\n                \"Automatic caching with TTL for expensive operations\",\n                \"Retry logic with configurable backoff strategies\",\n                \"Performance monitoring and optimization\"\n            ],\n            \"tools\": \"Decorators, method instrumentation, caching patterns\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Challenge {i}: {scenario['challenge']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  • {req}\")\n        console.print(f\"\\n[bold green]Metaprogramming Tools: {scenario['tools']}[/bold green]\")\n        console.print(\"\\n[bold magenta]Consider:[/bold magenta]\")\n        console.print(\"  • When to use metaclasses vs. decorators\")\n        console.print(\"  • How to balance flexibility with performance\")\n        console.print(\"  • Type safety in dynamic code generation\")\n        console.print(\"  • Debugging and introspection capabilities\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 AI Architecture Patterns: Metaprogramming[/bold blue]\")\n    console.print(\"Learn to create flexible, self-adapting AI systems\\n\")\n    \n    async def main():\n        await demonstrate_descriptors()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_metaclasses()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_dynamic_classes()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_decorators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_typed_properties()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_dynamic_config()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_reflection()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]✅ Completed![/bold green] Next: Run example 05_performance_optimization.py\")\n        console.print(\"💡 Pro tip: Metaprogramming enables flexible AI architectures but use with care!\")\n    \n    import asyncio\n    asyncio.run(main())"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":447,"cache_read_input_tokens":46875,"output_tokens":27}},"requestId":"req_vrtx_011CRdd3SnDKVs6PJ7SEbZBX","type":"assistant","uuid":"b4c3ff61-cd86-4166-82e7-90f58e84ed77","timestamp":"2025-07-30T17:49:03.163Z"}
{"parentUuid":"b4c3ff61-cd86-4166-82e7-90f58e84ed77","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01J7mFjNaRLPR8Ebxk9Ny56L","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/04_metaprogramming.py"}]},"uuid":"3d5c1010-98e6-4018-8e48-e767734dd64d","timestamp":"2025-07-30T17:49:03.194Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/04_metaprogramming.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 4: Metaprogramming for AI Systems\n\nLearn metaprogramming techniques to create flexible, self-adapting AI systems\nthrough descriptors, metaclasses, decorators, and dynamic code generation.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/04_metaprogramming.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import (\n    Any, Dict, List, Optional, Callable, Type, Union,\n    get_type_hints, get_origin, get_args\n)\nfrom functools import wraps, update_wrapper\nimport inspect\nimport time\nfrom dataclasses import dataclass, field\nfrom abc import ABC, abstractmethod\nimport weakref\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\nconsole = Console()\n\n# 🎯 LEARNING GOAL: Master metaprogramming for flexible AI systems\n\n\n# Custom Descriptors for AI Model Properties\nclass ValidatedProperty:\n    \"\"\"Descriptor that validates property values.\"\"\"\n    \n    def __init__(self, validator: Callable[[Any], bool], error_msg: str):\n        self.validator = validator\n        self.error_msg = error_msg\n        self.name = None\n    \n    def __set_name__(self, owner: Type, name: str):\n        \"\"\"Called when descriptor is assigned to a class attribute.\"\"\"\n        self.name = name\n        self.private_name = f'_{name}'\n    \n    def __get__(self, obj: Any, objtype: Type = None) -> Any:\n        \"\"\"Get the property value.\"\"\"\n        if obj is None:\n            return self\n        return getattr(obj, self.private_name, None)\n    \n    def __set__(self, obj: Any, value: Any) -> None:\n        \"\"\"Set the property value with validation.\"\"\"\n        if not self.validator(value):\n            raise ValueError(f\"{self.error_msg}: {value}\")\n        setattr(obj, self.private_name, value)\n\n\nclass CachedProperty:\n    \"\"\"Descriptor that caches expensive computations.\"\"\"\n    \n    def __init__(self, func: Callable):\n        self.func = func\n        self.name = func.__name__\n        update_wrapper(self, func)\n    \n    def __get__(self, obj: Any, objtype: Type = None) -> Any:\n        if obj is None:\n            return self\n        \n        # Check if cached value exists\n        cache_name = f'_cached_{self.name}'\n        if hasattr(obj, cache_name):\n            return getattr(obj, cache_name)\n        \n        # Compute and cache the value\n        value = self.func(obj)\n        setattr(obj, cache_name, value)\n        return value\n    \n    def __set__(self, obj: Any, value: Any) -> None:\n        \"\"\"Allow manual cache invalidation by setting new value.\"\"\"\n        cache_name = f'_cached_{self.name}'\n        setattr(obj, cache_name, value)\n    \n    def __delete__(self, obj: Any) -> None:\n        \"\"\"Clear the cached value.\"\"\"\n        cache_name = f'_cached_{self.name}'\n        if hasattr(obj, cache_name):\n            delattr(obj, cache_name)\n\n\n# Metaclass for Automatic Model Registration\nclass ModelRegistryMeta(type):\n    \"\"\"Metaclass that automatically registers AI models.\"\"\"\n    \n    _registry: Dict[str, Type] = {}\n    \n    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\n        # Add automatic ID generation\n        if 'model_id' not in namespace and name != 'BaseModel':\n            namespace['model_id'] = name.lower().replace('model', '')\n        \n        # Create the class\n        cls = super().__new__(mcs, name, bases, namespace)\n        \n        # Register the model (skip base classes)\n        if name != 'BaseModel' and hasattr(cls, 'model_id'):\n            mcs._registry[cls.model_id] = cls\n            console.print(f\"[dim]🔧 Registered model: {cls.model_id} -> {name}[/dim]\")\n        \n        return cls\n    \n    @classmethod\n    def get_model(mcs, model_id: str) -> Optional[Type]:\n        \"\"\"Get model class by ID.\"\"\"\n        return mcs._registry.get(model_id)\n    \n    @classmethod\n    def list_models(mcs) -> List[str]:\n        \"\"\"List all registered model IDs.\"\"\"\n        return list(mcs._registry.keys())\n\n\n# Metaclass for Automatic Method Instrumentation\nclass InstrumentedMeta(type):\n    \"\"\"Metaclass that automatically adds instrumentation to methods.\"\"\"\n    \n    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\n        # Instrument methods that start with 'predict'\n        for attr_name, attr_value in namespace.items():\n            if (attr_name.startswith('predict') and \n                callable(attr_value) and \n                not attr_name.startswith('_')):\n                \n                namespace[attr_name] = mcs._instrument_method(attr_value, attr_name)\n        \n        return super().__new__(mcs, name, bases, namespace)\n    \n    @staticmethod\n    def _instrument_method(method: Callable, method_name: str) -> Callable:\n        \"\"\"Add timing and logging to a method.\"\"\"\n        @wraps(method)\n        def wrapper(self, *args, **kwargs):\n            start_time = time.time()\n            try:\n                result = method(self, *args, **kwargs)\n                end_time = time.time()\n                duration = end_time - start_time\n                \n                # Store metrics\n                if not hasattr(self, '_metrics'):\n                    self._metrics = {}\n                if method_name not in self._metrics:\n                    self._metrics[method_name] = []\n                \n                self._metrics[method_name].append({\n                    'duration': duration,\n                    'timestamp': start_time,\n                    'success': True\n                })\n                \n                console.print(f\"[dim]📊 {self.__class__.__name__}.{method_name} took {duration:.4f}s[/dim]\")\n                return result\n                \n            except Exception as e:\n                end_time = time.time()\n                duration = end_time - start_time\n                \n                # Store error metrics\n                if not hasattr(self, '_metrics'):\n                    self._metrics = {}\n                if method_name not in self._metrics:\n                    self._metrics[method_name] = []\n                \n                self._metrics[method_name].append({\n                    'duration': duration,\n                    'timestamp': start_time,\n                    'success': False,\n                    'error': str(e)\n                })\n                \n                console.print(f\"[dim red]❌ {self.__class__.__name__}.{method_name} failed after {duration:.4f}s[/dim red]\")\n                raise\n        \n        return wrapper\n\n\n# Dynamic Class Generation\ndef create_model_class(\n    name: str, \n    attributes: Dict[str, Any], \n    methods: Dict[str, Callable] = None,\n    base_classes: tuple = ()\n) -> Type:\n    \"\"\"Dynamically create a model class.\"\"\"\n    \n    # Default base class\n    if not base_classes:\n        base_classes = (object,)\n    \n    # Combine attributes and methods\n    namespace = attributes.copy()\n    if methods:\n        namespace.update(methods)\n    \n    # Add default __init__ if not provided\n    if '__init__' not in namespace:\n        def __init__(self, **kwargs):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        namespace['__init__'] = __init__\n    \n    # Create the class\n    return type(name, base_classes, namespace)\n\n\n# Advanced Decorator Patterns\nclass memoize_with_ttl:\n    \"\"\"Decorator that memoizes with time-to-live.\"\"\"\n    \n    def __init__(self, ttl_seconds: float = 60.0):\n        self.ttl_seconds = ttl_seconds\n        self.cache = {}\n    \n    def __call__(self, func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = str(args) + str(sorted(kwargs.items()))\n            current_time = time.time()\n            \n            # Check if cached and not expired\n            if key in self.cache:\n                result, timestamp = self.cache[key]\n                if current_time - timestamp < self.ttl_seconds:\n                    console.print(f\"[dim green]💾 Cache hit for {func.__name__}[/dim green]\")\n                    return result\n            \n            # Compute new result\n            result = func(*args, **kwargs)\n            self.cache[key] = (result, current_time)\n            console.print(f\"[dim yellow]🔄 Cache miss for {func.__name__}[/dim yellow]\")\n            return result\n        \n        # Add cache management methods\n        wrapper.clear_cache = lambda: self.cache.clear()\n        wrapper.cache_info = lambda: {\n            'size': len(self.cache),\n            'ttl_seconds': self.ttl_seconds\n        }\n        \n        return wrapper\n\n\nclass retry_with_backoff:\n    \"\"\"Decorator that retries with exponential backoff.\"\"\"\n    \n    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n    \n    def __call__(self, func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(self.max_retries + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    last_exception = e\n                    \n                    if attempt < self.max_retries:\n                        delay = self.base_delay * (2 ** attempt)\n                        console.print(f\"[yellow]🔄 Retry {attempt + 1}/{self.max_retries} in {delay:.1f}s[/yellow]\")\n                        time.sleep(delay)\n                    else:\n                        console.print(f\"[red]❌ All retries exhausted for {func.__name__}[/red]\")\n            \n            raise last_exception\n        \n        return wrapper\n\n\n# Property Factory\ndef typed_property(property_type: Type, default: Any = None, validator: Callable = None):\n    \"\"\"Factory for creating typed properties with optional validation.\"\"\"\n    \n    def property_decorator(func: Callable) -> property:\n        name = func.__name__\n        private_name = f'_{name}'\n        \n        def getter(self):\n            return getattr(self, private_name, default)\n        \n        def setter(self, value):\n            # Type checking\n            if not isinstance(value, property_type):\n                try:\n                    value = property_type(value)\n                except (ValueError, TypeError):\n                    raise TypeError(f\"{name} must be of type {property_type.__name__}\")\n            \n            # Optional validation\n            if validator and not validator(value):\n                raise ValueError(f\"Invalid value for {name}: {value}\")\n            \n            setattr(self, private_name, value)\n        \n        return property(getter, setter, doc=func.__doc__)\n    \n    return property_decorator\n\n\n# Context Manager for Dynamic Configuration\nclass DynamicConfig:\n    \"\"\"Context manager that temporarily modifies class attributes.\"\"\"\n    \n    def __init__(self, target_class: Type, **config_changes):\n        self.target_class = target_class\n        self.config_changes = config_changes\n        self.original_values = {}\n    \n    def __enter__(self):\n        # Save original values\n        for attr_name, new_value in self.config_changes.items():\n            if hasattr(self.target_class, attr_name):\n                self.original_values[attr_name] = getattr(self.target_class, attr_name)\n            setattr(self.target_class, attr_name, new_value)\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore original values\n        for attr_name, original_value in self.original_values.items():\n            setattr(self.target_class, attr_name, original_value)\n        \n        # Remove attributes that didn't exist originally\n        for attr_name in self.config_changes:\n            if attr_name not in self.original_values:\n                delattr(self.target_class, attr_name)\n\n\n# Example Classes Using Metaprogramming\n\nclass BaseModel(metaclass=ModelRegistryMeta):\n    \"\"\"Base class for AI models with validation.\"\"\"\n    \n    # Validated properties using descriptors\n    temperature = ValidatedProperty(\n        lambda x: 0.0 <= x <= 2.0,\n        \"Temperature must be between 0.0 and 2.0\"\n    )\n    \n    max_tokens = ValidatedProperty(\n        lambda x: isinstance(x, int) and x > 0,\n        \"Max tokens must be a positive integer\"\n    )\n    \n    def __init__(self, temperature: float = 0.7, max_tokens: int = 1000):\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self._call_count = 0\n    \n    @CachedProperty\n    def model_signature(self) -> str:\n        \"\"\"Expensive computation that gets cached.\"\"\"\n        console.print(\"[dim]🔍 Computing model signature...[/dim]\")\n        time.sleep(0.1)  # Simulate expensive computation\n        return f\"{self.__class__.__name__}_{hash(str(self.temperature) + str(self.max_tokens))}\"\n\n\nclass GPTModel(BaseModel, metaclass=InstrumentedMeta):\n    \"\"\"GPT model with automatic instrumentation.\"\"\"\n    \n    def predict_text(self, text: str) -> str:\n        \"\"\"Predict text completion.\"\"\"\n        time.sleep(0.05)  # Simulate prediction\n        self._call_count += 1\n        return f\"GPT prediction for: {text}\"\n    \n    def predict_classification(self, text: str) -> Dict[str, float]:\n        \"\"\"Predict text classification.\"\"\"\n        time.sleep(0.03)\n        self._call_count += 1\n        return {\"positive\": 0.8, \"negative\": 0.1, \"neutral\": 0.1}\n\n\nclass ClaudeModel(BaseModel, metaclass=InstrumentedMeta):\n    \"\"\"Claude model with automatic instrumentation.\"\"\"\n    \n    def predict_text(self, text: str) -> str:\n        \"\"\"Predict text completion.\"\"\"\n        time.sleep(0.04)\n        self._call_count += 1\n        return f\"Claude prediction for: {text}\"\n\n\nclass ConfigurableModel:\n    \"\"\"Model with typed properties using property factory.\"\"\"\n    \n    @typed_property(float, default=0.7, validator=lambda x: 0.0 <= x <= 2.0)\n    def temperature(self):\n        \"\"\"Model temperature parameter.\"\"\"\n        pass\n    \n    @typed_property(int, default=1000, validator=lambda x: x > 0)\n    def max_tokens(self):\n        \"\"\"Maximum tokens to generate.\"\"\"\n        pass\n    \n    @typed_property(str, default=\"gpt-4\")\n    def model_name(self):\n        \"\"\"Name of the model.\"\"\"\n        pass\n\n\n# Dynamic Model Factory\nclass ModelFactory:\n    \"\"\"Factory for creating model classes dynamically.\"\"\"\n    \n    @staticmethod\n    def create_simple_model(name: str, prediction_delay: float = 0.1) -> Type:\n        \"\"\"Create a simple model class.\"\"\"\n        \n        def predict(self, text: str) -> str:\n            time.sleep(prediction_delay)\n            return f\"{name} processed: {text}\"\n        \n        def get_info(self) -> Dict[str, Any]:\n            return {\n                \"model_name\": name,\n                \"prediction_delay\": prediction_delay,\n                \"class_name\": self.__class__.__name__\n            }\n        \n        return create_model_class(\n            name=f\"{name}Model\",\n            attributes={\n                \"model_name\": name,\n                \"prediction_delay\": prediction_delay\n            },\n            methods={\n                \"predict\": predict,\n                \"get_info\": get_info\n            },\n            base_classes=(BaseModel,)\n        )\n    \n    @staticmethod\n    def create_ensemble_model(models: List[Type]) -> Type:\n        \"\"\"Create an ensemble model from multiple model classes.\"\"\"\n        \n        def predict(self, text: str) -> List[str]:\n            results = []\n            for model_class in models:\n                model = model_class()\n                if hasattr(model, 'predict'):\n                    results.append(model.predict(text))\n                elif hasattr(model, 'predict_text'):\n                    results.append(model.predict_text(text))\n            return results\n        \n        def get_model_info(self) -> List[str]:\n            return [model.__name__ for model in models]\n        \n        return create_model_class(\n            name=\"EnsembleModel\",\n            attributes={\n                \"component_models\": models\n            },\n            methods={\n                \"predict\": predict,\n                \"get_model_info\": get_model_info\n            }\n        )\n\n\nasync def demonstrate_descriptors():\n    \"\"\"Show descriptor patterns.\"\"\"\n    \n    console.print(Panel(\"🔍 Descriptors\", style=\"bold blue\"))\n    \n    console.print(\"[bold yellow]Creating model with validated properties:[/bold yellow]\")\n    \n    model = GPTModel()\n    console.print(f\"  Default temperature: {model.temperature}\")\n    console.print(f\"  Default max_tokens: {model.max_tokens}\")\n    \n    # Test validation\n    try:\n        model.temperature = 1.5\n        console.print(f\"  ✅ Set temperature to {model.temperature}\")\n    except ValueError as e:\n        console.print(f\"  ❌ Validation error: {e}\")\n    \n    try:\n        model.temperature = 3.0  # Should fail\n    except ValueError as e:\n        console.print(f\"  ❌ Validation caught: {e}\")\n    \n    # Test cached property\n    console.print(f\"\\n[bold cyan]Testing cached property:[/bold cyan]\")\n    sig1 = model.model_signature  # Should compute\n    sig2 = model.model_signature  # Should use cache\n    console.print(f\"  Signatures match: {sig1 == sig2}\")\n\n\nasync def demonstrate_metaclasses():\n    \"\"\"Show metaclass patterns.\"\"\"\n    \n    console.print(Panel(\"🏭 Metaclasses\", style=\"bold green\"))\n    \n    console.print(\"[bold yellow]Automatic model registration:[/bold yellow]\")\n    registered_models = ModelRegistryMeta.list_models()\n    console.print(f\"  Registered models: {registered_models}\")\n    \n    # Get model by ID\n    gpt_class = ModelRegistryMeta.get_model('gpt')\n    claude_class = ModelRegistryMeta.get_model('claude')\n    \n    console.print(f\"\\n[bold cyan]Creating models from registry:[/bold cyan]\")\n    if gpt_class:\n        gpt = gpt_class()\n        result = gpt.predict_text(\"Hello world\")\n        console.print(f\"  GPT: {result}\")\n    \n    if claude_class:\n        claude = claude_class()\n        result = claude.predict_text(\"Hello world\")\n        console.print(f\"  Claude: {result}\")\n    \n    # Show automatic instrumentation\n    console.print(f\"\\n[bold magenta]Instrumentation metrics:[/bold magenta]\")\n    if hasattr(gpt, '_metrics'):\n        for method, metrics in gpt._metrics.items():\n            avg_duration = sum(m['duration'] for m in metrics) / len(metrics)\n            console.print(f\"  {method}: {len(metrics)} calls, avg {avg_duration:.4f}s\")\n\n\nasync def demonstrate_dynamic_classes():\n    \"\"\"Show dynamic class creation.\"\"\"\n    \n    console.print(Panel(\"⚡ Dynamic Class Creation\", style=\"bold magenta\"))\n    \n    # Create models using factory\n    console.print(\"[bold yellow]Creating models dynamically:[/bold yellow]\")\n    \n    FastModel = ModelFactory.create_simple_model(\"FastAI\", prediction_delay=0.02)\n    SlowModel = ModelFactory.create_simple_model(\"DeepThink\", prediction_delay=0.08)\n    \n    fast_model = FastModel()\n    slow_model = SlowModel()\n    \n    console.print(f\"  Fast model: {fast_model.get_info()}\")\n    console.print(f\"  Slow model: {slow_model.get_info()}\")\n    \n    # Test predictions\n    start_time = time.time()\n    fast_result = fast_model.predict(\"test input\")\n    fast_time = time.time() - start_time\n    \n    start_time = time.time()\n    slow_result = slow_model.predict(\"test input\")\n    slow_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold cyan]Performance comparison:[/bold cyan]\")\n    console.print(f\"  Fast: {fast_time:.4f}s - {fast_result}\")\n    console.print(f\"  Slow: {slow_time:.4f}s - {slow_result}\")\n    \n    # Create ensemble model\n    console.print(f\"\\n[bold yellow]Creating ensemble model:[/bold yellow]\")\n    EnsembleModel = ModelFactory.create_ensemble_model([FastModel, SlowModel])\n    ensemble = EnsembleModel()\n    \n    ensemble_results = ensemble.predict(\"ensemble test\")\n    console.print(f\"  Ensemble results: {ensemble_results}\")\n    console.print(f\"  Component models: {ensemble.get_model_info()}\")\n\n\nasync def demonstrate_decorators():\n    \"\"\"Show advanced decorator patterns.\"\"\"\n    \n    console.print(Panel(\"🎨 Advanced Decorators\", style=\"bold cyan\"))\n    \n    @memoize_with_ttl(ttl_seconds=2.0)\n    @retry_with_backoff(max_retries=2, base_delay=0.5)\n    def expensive_ai_call(query: str, fail_rate: float = 0.0) -> str:\n        \"\"\"Simulate expensive AI call that might fail.\"\"\"\n        import random\n        \n        if random.random() < fail_rate:\n            raise ConnectionError(\"Simulated API failure\")\n        \n        time.sleep(0.1)  # Simulate processing time\n        return f\"AI response to: {query}\"\n    \n    console.print(\"[bold yellow]Testing decorated function:[/bold yellow]\")\n    \n    # First call - should compute\n    result1 = expensive_ai_call(\"What is AI?\")\n    console.print(f\"  Result 1: {result1}\")\n    \n    # Second call - should use cache\n    result2 = expensive_ai_call(\"What is AI?\")\n    console.print(f\"  Result 2: {result2}\")\n    \n    # Test with failures\n    console.print(f\"\\n[bold cyan]Testing with simulated failures:[/bold cyan]\")\n    try:\n        result3 = expensive_ai_call(\"Risky query\", fail_rate=0.7)\n        console.print(f\"  Success: {result3}\")\n    except Exception as e:\n        console.print(f\"  Failed: {e}\")\n    \n    # Show cache info\n    cache_info = expensive_ai_call.cache_info()\n    console.print(f\"\\n[bold green]Cache info: {cache_info}[/bold green]\")\n\n\nasync def demonstrate_typed_properties():\n    \"\"\"Show typed property factory.\"\"\"\n    \n    console.print(Panel(\"🏷️ Typed Properties\", style=\"bold red\"))\n    \n    console.print(\"[bold yellow]Creating model with typed properties:[/bold yellow]\")\n    \n    model = ConfigurableModel()\n    \n    # Test default values\n    console.print(f\"  Default temperature: {model.temperature}\")\n    console.print(f\"  Default max_tokens: {model.max_tokens}\")\n    console.print(f\"  Default model_name: {model.model_name}\")\n    \n    # Test type conversion\n    console.print(f\"\\n[bold cyan]Testing type conversion:[/bold cyan]\")\n    model.temperature = \"1.2\"  # String should convert to float\n    model.max_tokens = \"500\"   # String should convert to int\n    \n    console.print(f\"  Converted temperature: {model.temperature} (type: {type(model.temperature)})\")\n    console.print(f\"  Converted max_tokens: {model.max_tokens} (type: {type(model.max_tokens)})\")\n    \n    # Test validation\n    console.print(f\"\\n[bold yellow]Testing validation:[/bold yellow]\")\n    try:\n        model.temperature = 3.0  # Should fail validation\n    except ValueError as e:\n        console.print(f\"  ❌ Validation error: {e}\")\n    \n    try:\n        model.max_tokens = -100  # Should fail validation\n    except ValueError as e:\n        console.print(f\"  ❌ Validation error: {e}\")\n\n\nasync def demonstrate_dynamic_config():\n    \"\"\"Show dynamic configuration context manager.\"\"\"\n    \n    console.print(Panel(\"⚙️ Dynamic Configuration\", style=\"bold yellow\"))\n    \n    # Original class configuration\n    class AIService:\n        timeout = 30.0\n        retry_count = 3\n        debug_mode = False\n        \n        @classmethod\n        def get_config(cls) -> Dict[str, Any]:\n            return {\n                \"timeout\": cls.timeout,\n                \"retry_count\": cls.retry_count,\n                \"debug_mode\": cls.debug_mode\n            }\n    \n    console.print(\"[bold yellow]Original configuration:[/bold yellow]\")\n    console.print(f\"  {AIService.get_config()}\")\n    \n    # Temporarily modify configuration\n    console.print(f\"\\n[bold cyan]Using dynamic configuration:[/bold cyan]\")\n    with DynamicConfig(AIService, timeout=60.0, debug_mode=True, new_feature=True):\n        console.print(f\"  Inside context: {AIService.get_config()}\")\n        console.print(f\"  New feature: {getattr(AIService, 'new_feature', 'Not found')}\")\n    \n    console.print(f\"\\n[bold green]After context:[/bold green]\")\n    console.print(f\"  Restored: {AIService.get_config()}\")\n    console.print(f\"  New feature: {getattr(AIService, 'new_feature', 'Not found')}\")\n\n\ndef analyze_function_signature(func: Callable) -> Dict[str, Any]:\n    \"\"\"Metaprogramming function to analyze function signatures.\"\"\"\n    sig = inspect.signature(func)\n    type_hints = get_type_hints(func)\n    \n    return {\n        \"name\": func.__name__,\n        \"parameters\": {\n            name: {\n                \"annotation\": type_hints.get(name, param.annotation),\n                \"default\": param.default if param.default != param.empty else None,\n                \"kind\": param.kind.name\n            }\n            for name, param in sig.parameters.items()\n        },\n        \"return_type\": type_hints.get('return', sig.return_annotation),\n        \"docstring\": inspect.getdoc(func)\n    }\n\n\nasync def demonstrate_reflection():\n    \"\"\"Show reflection and introspection.\"\"\"\n    \n    console.print(Panel(\"🔍 Reflection & Introspection\", style=\"bold purple\"))\n    \n    # Analyze function signatures\n    def sample_ai_function(text: str, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\n        \"\"\"Sample AI function for analysis.\"\"\"\n        return {\"processed\": text, \"config\": {\"temperature\": temperature, \"max_tokens\": max_tokens}}\n    \n    analysis = analyze_function_signature(sample_ai_function)\n    \n    console.print(\"[bold yellow]Function signature analysis:[/bold yellow]\")\n    console.print(f\"  Name: {analysis['name']}\")\n    console.print(f\"  Return type: {analysis['return_type']}\")\n    console.print(f\"  Parameters:\")\n    \n    for param_name, param_info in analysis['parameters'].items():\n        console.print(f\"    {param_name}: {param_info['annotation']} (default: {param_info['default']})\")\n    \n    # Analyze class structure\n    console.print(f\"\\n[bold cyan]Class structure analysis:[/bold cyan]\")\n    gpt_model = GPTModel()\n    \n    methods = [name for name, method in inspect.getmembers(gpt_model, predicate=inspect.ismethod)]\n    properties = [name for name in dir(gpt_model) if isinstance(getattr(type(gpt_model), name, None), property)]\n    \n    console.print(f\"  Methods: {[m for m in methods if not m.startswith('_')]}\")\n    console.print(f\"  Properties: {properties}\")\n    \n    if hasattr(gpt_model, '_metrics'):\n        console.print(f\"  Metrics: {list(gpt_model._metrics.keys())}\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for metaprogramming.\"\"\"\n    \n    console.print(Panel(\"🎮 Interactive Exercise: Metaprogramming AI Framework\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"challenge\": \"Dynamic Model Registry\",\n            \"requirements\": [\n                \"Automatic registration of model classes\",\n                \"Type validation for model parameters\",\n                \"Runtime model discovery and instantiation\",\n                \"Automatic instrumentation of model methods\"\n            ],\n            \"tools\": \"Metaclasses, descriptors, registration patterns\"\n        },\n        {\n            \"challenge\": \"Self-Configuring AI Pipeline\",\n            \"requirements\": [\n                \"Dynamic property creation based on configuration\",\n                \"Automatic validation and type conversion\",\n                \"Context-based configuration overrides\",\n                \"Runtime pipeline modification\"\n            ],\n            \"tools\": \"Property factories, context managers, dynamic classes\"\n        },\n        {\n            \"challenge\": \"Adaptive Model Behavior\",\n            \"requirements\": [\n                \"Runtime method modification based on usage patterns\",\n                \"Automatic caching with TTL for expensive operations\",\n                \"Retry logic with configurable backoff strategies\",\n                \"Performance monitoring and optimization\"\n            ],\n            \"tools\": \"Decorators, method instrumentation, caching patterns\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Challenge {i}: {scenario['challenge']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  • {req}\")\n        console.print(f\"\\n[bold green]Metaprogramming Tools: {scenario['tools']}[/bold green]\")\n        console.print(\"\\n[bold magenta]Consider:[/bold magenta]\")\n        console.print(\"  • When to use metaclasses vs. decorators\")\n        console.print(\"  • How to balance flexibility with performance\")\n        console.print(\"  • Type safety in dynamic code generation\")\n        console.print(\"  • Debugging and introspection capabilities\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 AI Architecture Patterns: Metaprogramming[/bold blue]\")\n    console.print(\"Learn to create flexible, self-adapting AI systems\\n\")\n    \n    async def main():\n        await demonstrate_descriptors()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_metaclasses()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_dynamic_classes()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_decorators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_typed_properties()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_dynamic_config()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_reflection()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]✅ Completed![/bold green] Next: Run example 05_performance_optimization.py\")\n        console.print(\"💡 Pro tip: Metaprogramming enables flexible AI architectures but use with care!\")\n    \n    import asyncio\n    asyncio.run(main())","structuredPatch":[]}}
{"parentUuid":"3d5c1010-98e6-4018-8e48-e767734dd64d","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01EL15gs2UwSAX755fpHjuW6","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01Nf55ePEhZroBCG4k3iJGxC","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Create 03_functional_programming.py example file","status":"completed","priority":"high"},{"id":"2","content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high"},{"id":"3","content":"Create 05_performance_optimization.py example file","status":"in_progress","priority":"high"},{"id":"4","content":"Create tests for the Python idioms examples","status":"pending","priority":"medium"},{"id":"5","content":"Create exercises for hands-on practice","status":"pending","priority":"medium"}]}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":8399,"cache_read_input_tokens":47322,"output_tokens":223}},"requestId":"req_vrtx_011CRddCYv4YSL4kTM1tUqQo","type":"assistant","uuid":"ecac7fc5-2eee-4a49-9b78-1040179f73cd","timestamp":"2025-07-30T17:49:09.526Z"}
{"parentUuid":"ecac7fc5-2eee-4a49-9b78-1040179f73cd","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01Nf55ePEhZroBCG4k3iJGxC","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"2cd2c367-1546-4491-aad4-dbe0ec51e876","timestamp":"2025-07-30T17:49:09.553Z","toolUseResult":{"oldTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"in_progress","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"pending","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"pending","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}],"newTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"in_progress","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"pending","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}]}}
{"parentUuid":"2cd2c367-1546-4491-aad4-dbe0ec51e876","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_017ALqQG3hS2akc7ngyy8CGj","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01FdrPoV5pg5ArrgGG2RDZL7","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/05_performance_optimization.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 5: Performance Optimization for AI Systems\n\nLearn performance optimization techniques for AI applications including\nprofiling, memory management, vectorization, and parallel processing.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/05_performance_optimization.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nimport time\nimport gc\nimport weakref\nimport threading\nimport multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nfrom typing import List, Dict, Any, Callable, Iterator, Optional, Tuple\nfrom dataclasses import dataclass\nfrom functools import lru_cache, partial\nimport cProfile\nimport pstats\nimport tracemalloc\nfrom contextlib import contextmanager\nfrom collections import deque, defaultdict\nimport numpy as np\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.progress import Progress, TaskID\n\nconsole = Console()\n\n# 🎯 LEARNING GOAL: Master performance optimization for AI systems\n\n\n# Memory-Efficient Data Structures\nclass MemoryEfficientCache:\n    \"\"\"Memory-efficient LRU cache with size limits.\"\"\"\n    \n    def __init__(self, max_size: int = 1000, max_memory_mb: float = 100.0):\n        self.max_size = max_size\n        self.max_memory_bytes = max_memory_mb * 1024 * 1024\n        self.cache = {}\n        self.access_order = deque()\n        self.current_memory = 0\n    \n    def _estimate_size(self, obj: Any) -> int:\n        \"\"\"Estimate object size in bytes.\"\"\"\n        return sys.getsizeof(obj)\n    \n    def _evict_if_needed(self):\n        \"\"\"Evict items if cache is too large.\"\"\"\n        while (len(self.cache) > self.max_size or \n               self.current_memory > self.max_memory_bytes):\n            if not self.access_order:\n                break\n            \n            key = self.access_order.popleft()\n            if key in self.cache:\n                value = self.cache.pop(key)\n                self.current_memory -= self._estimate_size(value)\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"Get item from cache.\"\"\"\n        if key in self.cache:\n            # Move to end (most recent)\n            self.access_order.remove(key)\n            self.access_order.append(key)\n            return self.cache[key]\n        return None\n    \n    def put(self, key: str, value: Any) -> None:\n        \"\"\"Put item in cache.\"\"\"\n        if key in self.cache:\n            # Update existing\n            old_value = self.cache[key]\n            self.current_memory -= self._estimate_size(old_value)\n            self.access_order.remove(key)\n        \n        size = self._estimate_size(value)\n        self.cache[key] = value\n        self.access_order.append(key)\n        self.current_memory += size\n        \n        self._evict_if_needed()\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            \"size\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"memory_mb\": self.current_memory / (1024 * 1024),\n            \"max_memory_mb\": self.max_memory_bytes / (1024 * 1024),\n            \"hit_ratio\": 0.0  # Simplified for demo\n        }\n\n\nclass ObjectPool:\n    \"\"\"Object pool for expensive-to-create objects.\"\"\"\n    \n    def __init__(self, factory: Callable, max_size: int = 10):\n        self.factory = factory\n        self.max_size = max_size\n        self.available = deque()\n        self.in_use = weakref.WeakSet()\n        self.total_created = 0\n    \n    def acquire(self):\n        \"\"\"Acquire object from pool.\"\"\"\n        if self.available:\n            obj = self.available.popleft()\n        else:\n            if self.total_created < self.max_size:\n                obj = self.factory()\n                self.total_created += 1\n            else:\n                # Wait or create temporary\n                obj = self.factory()\n        \n        self.in_use.add(obj)\n        return obj\n    \n    def release(self, obj):\n        \"\"\"Release object back to pool.\"\"\"\n        if obj in self.in_use:\n            self.in_use.remove(obj)\n            if len(self.available) < self.max_size:\n                # Reset object state if needed\n                if hasattr(obj, 'reset'):\n                    obj.reset()\n                self.available.append(obj)\n    \n    @contextmanager\n    def borrow(self):\n        \"\"\"Context manager for automatic object management.\"\"\"\n        obj = self.acquire()\n        try:\n            yield obj\n        finally:\n            self.release(obj)\n    \n    def get_stats(self) -> Dict[str, Any]:\n        return {\n            \"available\": len(self.available),\n            \"in_use\": len(self.in_use),\n            \"total_created\": self.total_created,\n            \"max_size\": self.max_size\n        }\n\n\n# Performance Profiling Utilities\n@contextmanager\ndef profile_memory():\n    \"\"\"Context manager for memory profiling.\"\"\"\n    tracemalloc.start()\n    start_memory = tracemalloc.get_traced_memory()[0]\n    \n    yield\n    \n    end_memory = tracemalloc.get_traced_memory()[0]\n    tracemalloc.stop()\n    \n    memory_used = (end_memory - start_memory) / (1024 * 1024)  # MB\n    console.print(f\"[dim]🧠 Memory used: {memory_used:.2f} MB[/dim]\")\n\n\n@contextmanager\ndef profile_time():\n    \"\"\"Context manager for timing operations.\"\"\"\n    start_time = time.perf_counter()\n    yield\n    end_time = time.perf_counter()\n    duration = end_time - start_time\n    console.print(f\"[dim]⏱️  Execution time: {duration:.4f}s[/dim]\")\n\n\nclass PerformanceMonitor:\n    \"\"\"Monitor for tracking performance metrics.\"\"\"\n    \n    def __init__(self):\n        self.metrics = defaultdict(list)\n        self.counters = defaultdict(int)\n    \n    def record_time(self, operation: str, duration: float):\n        \"\"\"Record operation timing.\"\"\"\n        self.metrics[f\"{operation}_time\"].append(duration)\n        self.counters[f\"{operation}_count\"] += 1\n    \n    def record_memory(self, operation: str, memory_mb: float):\n        \"\"\"Record memory usage.\"\"\"\n        self.metrics[f\"{operation}_memory\"].append(memory_mb)\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get performance summary.\"\"\"\n        summary = {}\n        \n        for metric_name, values in self.metrics.items():\n            if values:\n                summary[metric_name] = {\n                    \"avg\": sum(values) / len(values),\n                    \"min\": min(values),\n                    \"max\": max(values),\n                    \"count\": len(values)\n                }\n        \n        summary[\"counters\"] = dict(self.counters)\n        return summary\n\n\n# Vectorized Operations\nclass VectorizedTextProcessor:\n    \"\"\"Text processor using vectorized operations.\"\"\"\n    \n    @staticmethod\n    def batch_tokenize(texts: List[str]) -> List[List[str]]:\n        \"\"\"Vectorized tokenization.\"\"\"\n        # Simulate vectorized operation\n        return [text.lower().split() for text in texts]\n    \n    @staticmethod\n    def batch_compute_lengths(texts: List[str]) -> np.ndarray:\n        \"\"\"Compute text lengths using NumPy.\"\"\"\n        return np.array([len(text) for text in texts])\n    \n    @staticmethod\n    def batch_filter_by_length(texts: List[str], min_length: int) -> List[str]:\n        \"\"\"Filter texts by length using vectorized operations.\"\"\"\n        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\n        mask = lengths >= min_length\n        return [text for text, keep in zip(texts, mask) if keep]\n    \n    @staticmethod\n    def batch_similarity_matrix(texts: List[str]) -> np.ndarray:\n        \"\"\"Compute similarity matrix (simplified).\"\"\"\n        n = len(texts)\n        similarity = np.zeros((n, n))\n        \n        # Simplified similarity based on length\n        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    similarity[i, j] = 1.0 / (1.0 + abs(lengths[i] - lengths[j]))\n                else:\n                    similarity[i, j] = 1.0\n        \n        return similarity\n\n\n# Parallel Processing Patterns\nclass ParallelProcessor:\n    \"\"\"Utilities for parallel processing.\"\"\"\n    \n    @staticmethod\n    def parallel_map_threads(func: Callable, items: List[Any], max_workers: int = 4) -> List[Any]:\n        \"\"\"Parallel map using threads (I/O bound tasks).\"\"\"\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            results = list(executor.map(func, items))\n        return results\n    \n    @staticmethod\n    def parallel_map_processes(func: Callable, items: List[Any], max_workers: int = None) -> List[Any]:\n        \"\"\"Parallel map using processes (CPU bound tasks).\"\"\"\n        if max_workers is None:\n            max_workers = mp.cpu_count()\n        \n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            results = list(executor.map(func, items))\n        return results\n    \n    @staticmethod\n    def parallel_batch_process(\n        items: List[Any], \n        batch_func: Callable[[List[Any]], List[Any]], \n        batch_size: int = 100,\n        max_workers: int = 4\n    ) -> List[Any]:\n        \"\"\"Process items in parallel batches.\"\"\"\n        \n        # Split into batches\n        batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n        \n        # Process batches in parallel\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            batch_results = list(executor.map(batch_func, batches))\n        \n        # Flatten results\n        results = []\n        for batch_result in batch_results:\n            results.extend(batch_result)\n        \n        return results\n\n\n# Lazy Loading and Generators\nclass LazyDataLoader:\n    \"\"\"Lazy data loader for large datasets.\"\"\"\n    \n    def __init__(self, data_source: Callable[[], Iterator[Any]]):\n        self.data_source = data_source\n        self._cache = {}\n        self._loaded_chunks = set()\n    \n    def get_chunk(self, chunk_id: int, chunk_size: int = 1000) -> List[Any]:\n        \"\"\"Get data chunk by ID.\"\"\"\n        if chunk_id in self._cache:\n            return self._cache[chunk_id]\n        \n        # Load chunk lazily\n        data_iter = self.data_source()\n        chunk_data = []\n        \n        # Skip to the right position\n        for i, item in enumerate(data_iter):\n            if i >= chunk_id * chunk_size and i < (chunk_id + 1) * chunk_size:\n                chunk_data.append(item)\n            elif i >= (chunk_id + 1) * chunk_size:\n                break\n        \n        self._cache[chunk_id] = chunk_data\n        self._loaded_chunks.add(chunk_id)\n        return chunk_data\n    \n    def get_stats(self) -> Dict[str, Any]:\n        return {\n            \"loaded_chunks\": len(self._loaded_chunks),\n            \"cache_size\": len(self._cache)\n        }\n\n\ndef create_sample_data_generator(size: int = 10000) -> Iterator[str]:\n    \"\"\"Create sample data generator.\"\"\"\n    for i in range(size):\n        yield f\"Sample text item {i} with some content for processing\"\n\n\n# Optimization Decorators\ndef memoize_disk(cache_dir: str = \"/tmp/ai_cache\"):\n    \"\"\"Decorator for disk-based memoization.\"\"\"\n    import pickle\n    import hashlib\n    import os\n    \n    os.makedirs(cache_dir, exist_ok=True)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = hashlib.md5(str(args).encode() + str(kwargs).encode()).hexdigest()\n            cache_file = os.path.join(cache_dir, f\"{func.__name__}_{key}.pkl\")\n            \n            # Check cache\n            if os.path.exists(cache_file):\n                try:\n                    with open(cache_file, 'rb') as f:\n                        result = pickle.load(f)\n                    console.print(f\"[dim green]💾 Disk cache hit for {func.__name__}[/dim green]\")\n                    return result\n                except:\n                    pass\n            \n            # Compute and cache\n            result = func(*args, **kwargs)\n            try:\n                with open(cache_file, 'wb') as f:\n                    pickle.dump(result, f)\n                console.print(f\"[dim yellow]💿 Cached {func.__name__} to disk[/dim yellow]\")\n            except:\n                pass\n            \n            return result\n        return wrapper\n    return decorator\n\n\ndef batch_decorator(batch_size: int = 10):\n    \"\"\"Decorator to automatically batch function calls.\"\"\"\n    def decorator(func):\n        def wrapper(items: List[Any], *args, **kwargs):\n            results = []\n            for i in range(0, len(items), batch_size):\n                batch = items[i:i + batch_size]\n                batch_results = func(batch, *args, **kwargs)\n                results.extend(batch_results)\n            return results\n        return wrapper\n    return decorator\n\n\n# Example AI Processing Functions\n@memoize_disk()\ndef expensive_model_inference(text: str) -> Dict[str, Any]:\n    \"\"\"Simulate expensive model inference.\"\"\"\n    time.sleep(0.1)  # Simulate processing time\n    \n    # Mock inference results\n    return {\n        \"sentiment\": \"positive\" if \"good\" in text.lower() else \"neutral\",\n        \"confidence\": 0.85,\n        \"processing_time\": 0.1,\n        \"tokens\": len(text.split())\n    }\n\n\n@batch_decorator(batch_size=5)\ndef batch_text_analysis(texts: List[str]) -> List[Dict[str, Any]]:\n    \"\"\"Batch text analysis.\"\"\"\n    # Simulate batch processing advantage\n    time.sleep(0.02)  # Much faster per item than individual calls\n    \n    return [\n        {\n            \"length\": len(text),\n            \"word_count\": len(text.split()),\n            \"char_diversity\": len(set(text.lower())),\n            \"batch_processed\": True\n        }\n        for text in texts\n    ]\n\n\ndef cpu_intensive_task(data: str) -> str:\n    \"\"\"CPU-intensive task for parallel processing demo.\"\"\"\n    # Simulate CPU work\n    result = \"\"\n    for _ in range(1000):\n        result = str(hash(data + result))[:10]\n    return result\n\n\ndef io_intensive_task(data: str) -> str:\n    \"\"\"I/O-intensive task for parallel processing demo.\"\"\"\n    # Simulate I/O wait\n    time.sleep(0.01)\n    return f\"Processed: {data.upper()}\"\n\n\nasync def demonstrate_memory_efficient_structures():\n    \"\"\"Show memory-efficient data structures.\"\"\"\n    \n    console.print(Panel(\"🧠 Memory-Efficient Structures\", style=\"bold blue\"))\n    \n    # Test memory-efficient cache\n    console.print(\"[bold yellow]Testing memory-efficient cache:[/bold yellow]\")\n    \n    cache = MemoryEfficientCache(max_size=5, max_memory_mb=0.1)\n    \n    # Add items to cache\n    test_data = [f\"large_data_item_{i}\" * 100 for i in range(10)]\n    \n    for i, data in enumerate(test_data):\n        cache.put(f\"key_{i}\", data)\n        stats = cache.get_stats()\n        console.print(f\"  Added item {i}: cache size={stats['size']}, memory={stats['memory_mb']:.3f}MB\")\n    \n    # Test retrieval\n    console.print(f\"\\n[bold cyan]Cache retrieval test:[/bold cyan]\")\n    for i in [0, 3, 7, 9]:\n        result = cache.get(f\"key_{i}\")\n        status = \"HIT\" if result else \"MISS\"\n        console.print(f\"  key_{i}: {status}\")\n    \n    # Test object pool\n    console.print(f\"\\n[bold yellow]Testing object pool:[/bold yellow]\")\n    \n    class ExpensiveResource:\n        def __init__(self):\n            time.sleep(0.01)  # Simulate expensive creation\n            self.created_at = time.time()\n            self.usage_count = 0\n        \n        def reset(self):\n            self.usage_count = 0\n    \n    pool = ObjectPool(ExpensiveResource, max_size=3)\n    \n    # Use objects from pool\n    with pool.borrow() as obj1:\n        obj1.usage_count += 1\n        console.print(f\"  Using object 1: created at {obj1.created_at:.3f}\")\n        \n        with pool.borrow() as obj2:\n            obj2.usage_count += 1\n            console.print(f\"  Using object 2: created at {obj2.created_at:.3f}\")\n    \n    console.print(f\"  Pool stats: {pool.get_stats()}\")\n\n\nasync def demonstrate_profiling():\n    \"\"\"Show profiling techniques.\"\"\"\n    \n    console.print(Panel(\"📊 Performance Profiling\", style=\"bold green\"))\n    \n    monitor = PerformanceMonitor()\n    \n    console.print(\"[bold yellow]Profiling different operations:[/bold yellow]\")\n    \n    # Profile memory usage\n    with profile_memory():\n        with profile_time():\n            # Create large data structure\n            large_list = [f\"item_{i}\" for i in range(100000)]\n            \n            start_time = time.perf_counter()\n            # Simulate processing\n            processed = [item.upper() for item in large_list[:1000]]\n            duration = time.perf_counter() - start_time\n            \n            monitor.record_time(\"list_processing\", duration)\n    \n    # Profile function calls\n    console.print(f\"\\n[bold cyan]Function call profiling:[/bold cyan]\")\n    \n    def profile_function():\n        \"\"\"Function to profile.\"\"\"\n        result = []\n        for i in range(1000):\n            result.append(expensive_model_inference(f\"test text {i % 10}\"))  # Will hit cache\n        return result\n    \n    # Use cProfile\n    profiler = cProfile.Profile()\n    profiler.enable()\n    \n    start_time = time.perf_counter()\n    results = profile_function()\n    duration = time.perf_counter() - start_time\n    \n    profiler.disable()\n    \n    # Show results\n    console.print(f\"  Processed {len(results)} items in {duration:.4f}s\")\n    console.print(f\"  Rate: {len(results) / duration:.1f} items/second\")\n    \n    # Show performance summary\n    summary = monitor.get_summary()\n    console.print(f\"\\n[bold magenta]Performance summary:[/bold magenta]\")\n    for metric, stats in summary.items():\n        if isinstance(stats, dict) and 'avg' in stats:\n            console.print(f\"  {metric}: avg={stats['avg']:.4f}, count={stats['count']}\")\n\n\nasync def demonstrate_vectorization():\n    \"\"\"Show vectorized operations.\"\"\"\n    \n    console.print(Panel(\"⚡ Vectorized Operations\", style=\"bold magenta\"))\n    \n    # Create test data\n    test_texts = [f\"Sample text number {i} for processing\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Comparing scalar vs vectorized operations:[/bold yellow]\")\n    \n    # Scalar operations\n    start_time = time.perf_counter()\n    scalar_lengths = [len(text) for text in test_texts]\n    scalar_time = time.perf_counter() - start_time\n    \n    # Vectorized operations\n    start_time = time.perf_counter()\n    vector_lengths = VectorizedTextProcessor.batch_compute_lengths(test_texts)\n    vector_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Scalar processing: {scalar_time:.4f}s\")\n    console.print(f\"  Vectorized processing: {vector_time:.4f}s\")\n    console.print(f\"  Speedup: {scalar_time / vector_time:.1f}x\")\n    \n    # Test batch operations\n    console.print(f\"\\n[bold cyan]Batch text processing:[/bold cyan]\")\n    \n    start_time = time.perf_counter()\n    tokenized = VectorizedTextProcessor.batch_tokenize(test_texts[:100])\n    filtered = VectorizedTextProcessor.batch_filter_by_length(test_texts[:100], min_length=20)\n    processing_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Processed {len(test_texts[:100])} texts in {processing_time:.4f}s\")\n    console.print(f\"  Tokenized: {len(tokenized)} texts\")\n    console.print(f\"  Filtered: {len(filtered)} texts (length >= 20)\")\n    \n    # Similarity matrix\n    console.print(f\"\\n[bold yellow]Computing similarity matrix:[/bold yellow]\")\n    sample_texts = test_texts[:10]\n    \n    start_time = time.perf_counter()\n    similarity_matrix = VectorizedTextProcessor.batch_similarity_matrix(sample_texts)\n    matrix_time = time.perf_counter() - start_time\n    \n    console.print(f\"  {len(sample_texts)}x{len(sample_texts)} matrix computed in {matrix_time:.4f}s\")\n    console.print(f\"  Average similarity: {np.mean(similarity_matrix[similarity_matrix != 1.0]):.3f}\")\n\n\nasync def demonstrate_parallel_processing():\n    \"\"\"Show parallel processing patterns.\"\"\"\n    \n    console.print(Panel(\"🚀 Parallel Processing\", style=\"bold cyan\"))\n    \n    # Create test data\n    test_data = [f\"data_item_{i}\" for i in range(100)]\n    \n    console.print(\"[bold yellow]Comparing sequential vs parallel processing:[/bold yellow]\")\n    \n    # Sequential processing (CPU-bound)\n    start_time = time.perf_counter()\n    sequential_results = [cpu_intensive_task(item) for item in test_data[:20]]\n    sequential_time = time.perf_counter() - start_time\n    \n    # Parallel processing (CPU-bound)\n    start_time = time.perf_counter()\n    parallel_results = ParallelProcessor.parallel_map_processes(\n        cpu_intensive_task, test_data[:20], max_workers=2\n    )\n    parallel_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Sequential (CPU): {sequential_time:.4f}s\")\n    console.print(f\"  Parallel (CPU): {parallel_time:.4f}s\")\n    console.print(f\"  Speedup: {sequential_time / parallel_time:.1f}x\")\n    \n    # I/O-bound comparison\n    console.print(f\"\\n[bold cyan]I/O-bound task comparison:[/bold cyan]\")\n    \n    start_time = time.perf_counter()\n    sequential_io = [io_intensive_task(item) for item in test_data[:50]]\n    sequential_io_time = time.perf_counter() - start_time\n    \n    start_time = time.perf_counter()\n    parallel_io = ParallelProcessor.parallel_map_threads(\n        io_intensive_task, test_data[:50], max_workers=10\n    )\n    parallel_io_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Sequential (I/O): {sequential_io_time:.4f}s\")\n    console.print(f\"  Parallel (I/O): {parallel_io_time:.4f}s\")\n    console.print(f\"  Speedup: {sequential_io_time / parallel_io_time:.1f}x\")\n    \n    # Batch processing\n    console.print(f\"\\n[bold yellow]Parallel batch processing:[/bold yellow]\")\n    \n    def process_batch(batch: List[str]) -> List[str]:\n        return [item.upper() for item in batch]\n    \n    start_time = time.perf_counter()\n    batch_results = ParallelProcessor.parallel_batch_process(\n        test_data, process_batch, batch_size=25, max_workers=4\n    )\n    batch_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Processed {len(batch_results)} items in {batch_time:.4f}s\")\n    console.print(f\"  Rate: {len(batch_results) / batch_time:.1f} items/second\")\n\n\nasync def demonstrate_lazy_loading():\n    \"\"\"Show lazy loading patterns.\"\"\"\n    \n    console.print(Panel(\"😴 Lazy Loading\", style=\"bold red\"))\n    \n    console.print(\"[bold yellow]Creating lazy data loader:[/bold yellow]\")\n    \n    # Create lazy loader\n    loader = LazyDataLoader(lambda: create_sample_data_generator(10000))\n    \n    console.print(f\"  Initial stats: {loader.get_stats()}\")\n    \n    # Load some chunks\n    console.print(f\"\\n[bold cyan]Loading data chunks on demand:[/bold cyan]\")\n    \n    for chunk_id in [0, 2, 5]:\n        start_time = time.perf_counter()\n        chunk_data = loader.get_chunk(chunk_id, chunk_size=100)\n        load_time = time.perf_counter() - start_time\n        \n        console.print(f\"  Chunk {chunk_id}: {len(chunk_data)} items loaded in {load_time:.4f}s\")\n    \n    console.print(f\"  Final stats: {loader.get_stats()}\")\n    \n    # Access cached chunk\n    start_time = time.perf_counter()\n    cached_chunk = loader.get_chunk(0, chunk_size=100)  # Should be cached\n    cached_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Cached access: {len(cached_chunk)} items in {cached_time:.6f}s\")\n\n\nasync def demonstrate_batch_processing():\n    \"\"\"Show batch processing optimization.\"\"\"\n    \n    console.print(Panel(\"📦 Batch Processing\", style=\"bold yellow\"))\n    \n    # Create test data\n    test_texts = [f\"Text sample {i} for analysis\" for i in range(100)]\n    \n    console.print(\"[bold yellow]Comparing individual vs batch processing:[/bold yellow]\")\n    \n    # Individual processing\n    start_time = time.perf_counter()\n    individual_results = []\n    for text in test_texts[:20]:\n        result = {\n            \"length\": len(text),\n            \"word_count\": len(text.split()),\n            \"char_diversity\": len(set(text.lower())),\n            \"batch_processed\": False\n        }\n        individual_results.append(result)\n    individual_time = time.perf_counter() - start_time\n    \n    # Batch processing\n    start_time = time.perf_counter()\n    batch_results = batch_text_analysis(test_texts[:20])\n    batch_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Individual processing: {individual_time:.4f}s\")\n    console.print(f\"  Batch processing: {batch_time:.4f}s\")\n    console.print(f\"  Speedup: {individual_time / batch_time:.1f}x\")\n    \n    # Verify results\n    console.print(f\"\\n[bold cyan]Result verification:[/bold cyan]\")\n    console.print(f\"  Individual results: {len(individual_results)} items\")\n    console.print(f\"  Batch results: {len(batch_results)} items\")\n    console.print(f\"  Batch flag in results: {batch_results[0]['batch_processed']}\")\n\n\nasync def demonstrate_gc_optimization():\n    \"\"\"Show garbage collection optimization.\"\"\"\n    \n    console.print(Panel(\"🗑️ Garbage Collection Optimization\", style=\"bold purple\"))\n    \n    console.print(\"[bold yellow]Memory management with garbage collection:[/bold yellow]\")\n    \n    # Check initial memory\n    gc.collect()  # Force collection\n    initial_objects = len(gc.get_objects())\n    \n    console.print(f\"  Initial objects: {initial_objects}\")\n    \n    # Create many objects\n    large_objects = []\n    for i in range(1000):\n        obj = {\n            \"id\": i,\n            \"data\": [f\"item_{j}\" for j in range(100)],\n            \"metadata\": {\"created\": time.time(), \"type\": \"test\"}\n        }\n        large_objects.append(obj)\n    \n    mid_objects = len(gc.get_objects())\n    console.print(f\"  After creation: {mid_objects} objects (+{mid_objects - initial_objects})\")\n    \n    # Clear references and force collection\n    del large_objects\n    gc.collect()\n    \n    final_objects = len(gc.get_objects())\n    console.print(f\"  After cleanup: {final_objects} objects (-{mid_objects - final_objects})\")\n    \n    # Show GC stats\n    console.print(f\"\\n[bold cyan]Garbage collection stats:[/bold cyan]\")\n    for generation, stats in enumerate(gc.get_stats()):\n        console.print(f\"  Generation {generation}: {stats}\")\n\n\ndef benchmark_comparison():\n    \"\"\"Compare different optimization techniques.\"\"\"\n    \n    console.print(Panel(\"🏆 Performance Benchmark\", style=\"bold orange\"))\n    \n    test_data = [f\"benchmark text {i}\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Benchmarking optimization techniques:[/bold yellow]\")\n    \n    techniques = [\n        (\"No optimization\", lambda data: [len(item) for item in data]),\n        (\"List comprehension\", lambda data: [len(item) for item in data]),\n        (\"Generator expression\", lambda data: list(len(item) for item in data)),\n        (\"NumPy vectorized\", lambda data: VectorizedTextProcessor.batch_compute_lengths(data)),\n        (\"Parallel threads\", lambda data: ParallelProcessor.parallel_map_threads(len, data)),\n    ]\n    \n    results = []\n    \n    for name, technique in techniques:\n        # Warm up\n        technique(test_data[:10])\n        \n        # Benchmark\n        start_time = time.perf_counter()\n        result = technique(test_data)\n        end_time = time.perf_counter()\n        \n        duration = end_time - start_time\n        rate = len(test_data) / duration\n        \n        results.append((name, duration, rate))\n        console.print(f\"  {name}: {duration:.4f}s ({rate:.1f} items/sec)\")\n    \n    # Find best technique\n    best_technique = min(results, key=lambda x: x[1])\n    console.print(f\"\\n[bold green]Best technique: {best_technique[0]} ({best_technique[1]:.4f}s)[/bold green]\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for performance optimization.\"\"\"\n    \n    console.print(Panel(\"🎮 Interactive Exercise: Optimize AI Performance\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"challenge\": \"Large-Scale Text Processing\",\n            \"problem\": \"Process 1M+ text documents with multiple AI models\",\n            \"bottlenecks\": [\n                \"Memory usage grows unbounded\",\n                \"CPU utilization is poor\",\n                \"I/O operations are blocking\",\n                \"Cache misses are frequent\"\n            ],\n            \"solutions\": \"Memory pooling, vectorization, async I/O, intelligent caching\"\n        },\n        {\n            \"challenge\": \"Real-Time AI Inference\",\n            \"problem\": \"Serve AI predictions with <100ms latency\",\n            \"bottlenecks\": [\n                \"Model loading takes too long\",\n                \"Batch processing adds latency\",\n                \"Memory allocations cause GC pauses\",\n                \"Network serialization is slow\"\n            ],\n            \"solutions\": \"Model pooling, streaming inference, object reuse, binary protocols\"\n        },\n        {\n            \"challenge\": \"Multi-Model Ensemble\",\n            \"problem\": \"Run multiple AI models concurrently\",\n            \"bottlenecks\": [\n                \"Models compete for resources\",\n                \"Sequential processing is slow\",\n                \"Memory pressure causes swapping\",\n                \"Error handling blocks pipeline\"\n            ],\n            \"solutions\": \"Resource isolation, parallel execution, memory management, circuit breakers\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Challenge {i}: {scenario['challenge']}[/bold cyan]\")\n        console.print(f\"[bold yellow]Problem: {scenario['problem']}[/bold yellow]\")\n        console.print(\"[bold red]Common Bottlenecks:[/bold red]\")\n        for bottleneck in scenario[\"bottlenecks\"]:\n            console.print(f\"  • {bottleneck}\")\n        console.print(f\"\\n[bold green]Optimization Strategies: {scenario['solutions']}[/bold green]\")\n        console.print(\"\\n[bold magenta]Consider:[/bold magenta]\")\n        console.print(\"  • Profile first, optimize second\")\n        console.print(\"  • Memory vs CPU vs I/O trade-offs\")\n        console.print(\"  • Caching strategies and invalidation\")\n        console.print(\"  • Parallel vs concurrent execution\")\n        console.print(\"  • Resource pooling and lifecycle management\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 AI Architecture Patterns: Performance Optimization[/bold blue]\")\n    console.print(\"Learn to build high-performance AI systems\\n\")\n    \n    async def main():\n        await demonstrate_memory_efficient_structures()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_profiling()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_vectorization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_parallel_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_lazy_loading()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_batch_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_gc_optimization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        benchmark_comparison()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]✅ Completed![/bold green] All Python idioms examples are ready!\")\n        console.print(\"💡 Pro tip: Always profile before optimizing - measure, don't guess!\")\n    \n    import asyncio\n    asyncio.run(main())"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":447,"cache_read_input_tokens":55721,"output_tokens":22}},"requestId":"req_vrtx_011CRddD1KoUkmpPi4b2xPhF","type":"assistant","uuid":"ac397c7a-8809-436a-93fe-f02bc7f2eacb","timestamp":"2025-07-30T17:51:23.205Z"}
{"parentUuid":"ac397c7a-8809-436a-93fe-f02bc7f2eacb","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01FdrPoV5pg5ArrgGG2RDZL7","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/05_performance_optimization.py"}]},"uuid":"a247f5c1-b516-498f-bfc1-c6e59757d36f","timestamp":"2025-07-30T17:51:23.242Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/05_performance_optimization.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 5: Performance Optimization for AI Systems\n\nLearn performance optimization techniques for AI applications including\nprofiling, memory management, vectorization, and parallel processing.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/05_performance_optimization.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nimport time\nimport gc\nimport weakref\nimport threading\nimport multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nfrom typing import List, Dict, Any, Callable, Iterator, Optional, Tuple\nfrom dataclasses import dataclass\nfrom functools import lru_cache, partial\nimport cProfile\nimport pstats\nimport tracemalloc\nfrom contextlib import contextmanager\nfrom collections import deque, defaultdict\nimport numpy as np\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.progress import Progress, TaskID\n\nconsole = Console()\n\n# 🎯 LEARNING GOAL: Master performance optimization for AI systems\n\n\n# Memory-Efficient Data Structures\nclass MemoryEfficientCache:\n    \"\"\"Memory-efficient LRU cache with size limits.\"\"\"\n    \n    def __init__(self, max_size: int = 1000, max_memory_mb: float = 100.0):\n        self.max_size = max_size\n        self.max_memory_bytes = max_memory_mb * 1024 * 1024\n        self.cache = {}\n        self.access_order = deque()\n        self.current_memory = 0\n    \n    def _estimate_size(self, obj: Any) -> int:\n        \"\"\"Estimate object size in bytes.\"\"\"\n        return sys.getsizeof(obj)\n    \n    def _evict_if_needed(self):\n        \"\"\"Evict items if cache is too large.\"\"\"\n        while (len(self.cache) > self.max_size or \n               self.current_memory > self.max_memory_bytes):\n            if not self.access_order:\n                break\n            \n            key = self.access_order.popleft()\n            if key in self.cache:\n                value = self.cache.pop(key)\n                self.current_memory -= self._estimate_size(value)\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"Get item from cache.\"\"\"\n        if key in self.cache:\n            # Move to end (most recent)\n            self.access_order.remove(key)\n            self.access_order.append(key)\n            return self.cache[key]\n        return None\n    \n    def put(self, key: str, value: Any) -> None:\n        \"\"\"Put item in cache.\"\"\"\n        if key in self.cache:\n            # Update existing\n            old_value = self.cache[key]\n            self.current_memory -= self._estimate_size(old_value)\n            self.access_order.remove(key)\n        \n        size = self._estimate_size(value)\n        self.cache[key] = value\n        self.access_order.append(key)\n        self.current_memory += size\n        \n        self._evict_if_needed()\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            \"size\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"memory_mb\": self.current_memory / (1024 * 1024),\n            \"max_memory_mb\": self.max_memory_bytes / (1024 * 1024),\n            \"hit_ratio\": 0.0  # Simplified for demo\n        }\n\n\nclass ObjectPool:\n    \"\"\"Object pool for expensive-to-create objects.\"\"\"\n    \n    def __init__(self, factory: Callable, max_size: int = 10):\n        self.factory = factory\n        self.max_size = max_size\n        self.available = deque()\n        self.in_use = weakref.WeakSet()\n        self.total_created = 0\n    \n    def acquire(self):\n        \"\"\"Acquire object from pool.\"\"\"\n        if self.available:\n            obj = self.available.popleft()\n        else:\n            if self.total_created < self.max_size:\n                obj = self.factory()\n                self.total_created += 1\n            else:\n                # Wait or create temporary\n                obj = self.factory()\n        \n        self.in_use.add(obj)\n        return obj\n    \n    def release(self, obj):\n        \"\"\"Release object back to pool.\"\"\"\n        if obj in self.in_use:\n            self.in_use.remove(obj)\n            if len(self.available) < self.max_size:\n                # Reset object state if needed\n                if hasattr(obj, 'reset'):\n                    obj.reset()\n                self.available.append(obj)\n    \n    @contextmanager\n    def borrow(self):\n        \"\"\"Context manager for automatic object management.\"\"\"\n        obj = self.acquire()\n        try:\n            yield obj\n        finally:\n            self.release(obj)\n    \n    def get_stats(self) -> Dict[str, Any]:\n        return {\n            \"available\": len(self.available),\n            \"in_use\": len(self.in_use),\n            \"total_created\": self.total_created,\n            \"max_size\": self.max_size\n        }\n\n\n# Performance Profiling Utilities\n@contextmanager\ndef profile_memory():\n    \"\"\"Context manager for memory profiling.\"\"\"\n    tracemalloc.start()\n    start_memory = tracemalloc.get_traced_memory()[0]\n    \n    yield\n    \n    end_memory = tracemalloc.get_traced_memory()[0]\n    tracemalloc.stop()\n    \n    memory_used = (end_memory - start_memory) / (1024 * 1024)  # MB\n    console.print(f\"[dim]🧠 Memory used: {memory_used:.2f} MB[/dim]\")\n\n\n@contextmanager\ndef profile_time():\n    \"\"\"Context manager for timing operations.\"\"\"\n    start_time = time.perf_counter()\n    yield\n    end_time = time.perf_counter()\n    duration = end_time - start_time\n    console.print(f\"[dim]⏱️  Execution time: {duration:.4f}s[/dim]\")\n\n\nclass PerformanceMonitor:\n    \"\"\"Monitor for tracking performance metrics.\"\"\"\n    \n    def __init__(self):\n        self.metrics = defaultdict(list)\n        self.counters = defaultdict(int)\n    \n    def record_time(self, operation: str, duration: float):\n        \"\"\"Record operation timing.\"\"\"\n        self.metrics[f\"{operation}_time\"].append(duration)\n        self.counters[f\"{operation}_count\"] += 1\n    \n    def record_memory(self, operation: str, memory_mb: float):\n        \"\"\"Record memory usage.\"\"\"\n        self.metrics[f\"{operation}_memory\"].append(memory_mb)\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get performance summary.\"\"\"\n        summary = {}\n        \n        for metric_name, values in self.metrics.items():\n            if values:\n                summary[metric_name] = {\n                    \"avg\": sum(values) / len(values),\n                    \"min\": min(values),\n                    \"max\": max(values),\n                    \"count\": len(values)\n                }\n        \n        summary[\"counters\"] = dict(self.counters)\n        return summary\n\n\n# Vectorized Operations\nclass VectorizedTextProcessor:\n    \"\"\"Text processor using vectorized operations.\"\"\"\n    \n    @staticmethod\n    def batch_tokenize(texts: List[str]) -> List[List[str]]:\n        \"\"\"Vectorized tokenization.\"\"\"\n        # Simulate vectorized operation\n        return [text.lower().split() for text in texts]\n    \n    @staticmethod\n    def batch_compute_lengths(texts: List[str]) -> np.ndarray:\n        \"\"\"Compute text lengths using NumPy.\"\"\"\n        return np.array([len(text) for text in texts])\n    \n    @staticmethod\n    def batch_filter_by_length(texts: List[str], min_length: int) -> List[str]:\n        \"\"\"Filter texts by length using vectorized operations.\"\"\"\n        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\n        mask = lengths >= min_length\n        return [text for text, keep in zip(texts, mask) if keep]\n    \n    @staticmethod\n    def batch_similarity_matrix(texts: List[str]) -> np.ndarray:\n        \"\"\"Compute similarity matrix (simplified).\"\"\"\n        n = len(texts)\n        similarity = np.zeros((n, n))\n        \n        # Simplified similarity based on length\n        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    similarity[i, j] = 1.0 / (1.0 + abs(lengths[i] - lengths[j]))\n                else:\n                    similarity[i, j] = 1.0\n        \n        return similarity\n\n\n# Parallel Processing Patterns\nclass ParallelProcessor:\n    \"\"\"Utilities for parallel processing.\"\"\"\n    \n    @staticmethod\n    def parallel_map_threads(func: Callable, items: List[Any], max_workers: int = 4) -> List[Any]:\n        \"\"\"Parallel map using threads (I/O bound tasks).\"\"\"\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            results = list(executor.map(func, items))\n        return results\n    \n    @staticmethod\n    def parallel_map_processes(func: Callable, items: List[Any], max_workers: int = None) -> List[Any]:\n        \"\"\"Parallel map using processes (CPU bound tasks).\"\"\"\n        if max_workers is None:\n            max_workers = mp.cpu_count()\n        \n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            results = list(executor.map(func, items))\n        return results\n    \n    @staticmethod\n    def parallel_batch_process(\n        items: List[Any], \n        batch_func: Callable[[List[Any]], List[Any]], \n        batch_size: int = 100,\n        max_workers: int = 4\n    ) -> List[Any]:\n        \"\"\"Process items in parallel batches.\"\"\"\n        \n        # Split into batches\n        batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n        \n        # Process batches in parallel\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            batch_results = list(executor.map(batch_func, batches))\n        \n        # Flatten results\n        results = []\n        for batch_result in batch_results:\n            results.extend(batch_result)\n        \n        return results\n\n\n# Lazy Loading and Generators\nclass LazyDataLoader:\n    \"\"\"Lazy data loader for large datasets.\"\"\"\n    \n    def __init__(self, data_source: Callable[[], Iterator[Any]]):\n        self.data_source = data_source\n        self._cache = {}\n        self._loaded_chunks = set()\n    \n    def get_chunk(self, chunk_id: int, chunk_size: int = 1000) -> List[Any]:\n        \"\"\"Get data chunk by ID.\"\"\"\n        if chunk_id in self._cache:\n            return self._cache[chunk_id]\n        \n        # Load chunk lazily\n        data_iter = self.data_source()\n        chunk_data = []\n        \n        # Skip to the right position\n        for i, item in enumerate(data_iter):\n            if i >= chunk_id * chunk_size and i < (chunk_id + 1) * chunk_size:\n                chunk_data.append(item)\n            elif i >= (chunk_id + 1) * chunk_size:\n                break\n        \n        self._cache[chunk_id] = chunk_data\n        self._loaded_chunks.add(chunk_id)\n        return chunk_data\n    \n    def get_stats(self) -> Dict[str, Any]:\n        return {\n            \"loaded_chunks\": len(self._loaded_chunks),\n            \"cache_size\": len(self._cache)\n        }\n\n\ndef create_sample_data_generator(size: int = 10000) -> Iterator[str]:\n    \"\"\"Create sample data generator.\"\"\"\n    for i in range(size):\n        yield f\"Sample text item {i} with some content for processing\"\n\n\n# Optimization Decorators\ndef memoize_disk(cache_dir: str = \"/tmp/ai_cache\"):\n    \"\"\"Decorator for disk-based memoization.\"\"\"\n    import pickle\n    import hashlib\n    import os\n    \n    os.makedirs(cache_dir, exist_ok=True)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = hashlib.md5(str(args).encode() + str(kwargs).encode()).hexdigest()\n            cache_file = os.path.join(cache_dir, f\"{func.__name__}_{key}.pkl\")\n            \n            # Check cache\n            if os.path.exists(cache_file):\n                try:\n                    with open(cache_file, 'rb') as f:\n                        result = pickle.load(f)\n                    console.print(f\"[dim green]💾 Disk cache hit for {func.__name__}[/dim green]\")\n                    return result\n                except:\n                    pass\n            \n            # Compute and cache\n            result = func(*args, **kwargs)\n            try:\n                with open(cache_file, 'wb') as f:\n                    pickle.dump(result, f)\n                console.print(f\"[dim yellow]💿 Cached {func.__name__} to disk[/dim yellow]\")\n            except:\n                pass\n            \n            return result\n        return wrapper\n    return decorator\n\n\ndef batch_decorator(batch_size: int = 10):\n    \"\"\"Decorator to automatically batch function calls.\"\"\"\n    def decorator(func):\n        def wrapper(items: List[Any], *args, **kwargs):\n            results = []\n            for i in range(0, len(items), batch_size):\n                batch = items[i:i + batch_size]\n                batch_results = func(batch, *args, **kwargs)\n                results.extend(batch_results)\n            return results\n        return wrapper\n    return decorator\n\n\n# Example AI Processing Functions\n@memoize_disk()\ndef expensive_model_inference(text: str) -> Dict[str, Any]:\n    \"\"\"Simulate expensive model inference.\"\"\"\n    time.sleep(0.1)  # Simulate processing time\n    \n    # Mock inference results\n    return {\n        \"sentiment\": \"positive\" if \"good\" in text.lower() else \"neutral\",\n        \"confidence\": 0.85,\n        \"processing_time\": 0.1,\n        \"tokens\": len(text.split())\n    }\n\n\n@batch_decorator(batch_size=5)\ndef batch_text_analysis(texts: List[str]) -> List[Dict[str, Any]]:\n    \"\"\"Batch text analysis.\"\"\"\n    # Simulate batch processing advantage\n    time.sleep(0.02)  # Much faster per item than individual calls\n    \n    return [\n        {\n            \"length\": len(text),\n            \"word_count\": len(text.split()),\n            \"char_diversity\": len(set(text.lower())),\n            \"batch_processed\": True\n        }\n        for text in texts\n    ]\n\n\ndef cpu_intensive_task(data: str) -> str:\n    \"\"\"CPU-intensive task for parallel processing demo.\"\"\"\n    # Simulate CPU work\n    result = \"\"\n    for _ in range(1000):\n        result = str(hash(data + result))[:10]\n    return result\n\n\ndef io_intensive_task(data: str) -> str:\n    \"\"\"I/O-intensive task for parallel processing demo.\"\"\"\n    # Simulate I/O wait\n    time.sleep(0.01)\n    return f\"Processed: {data.upper()}\"\n\n\nasync def demonstrate_memory_efficient_structures():\n    \"\"\"Show memory-efficient data structures.\"\"\"\n    \n    console.print(Panel(\"🧠 Memory-Efficient Structures\", style=\"bold blue\"))\n    \n    # Test memory-efficient cache\n    console.print(\"[bold yellow]Testing memory-efficient cache:[/bold yellow]\")\n    \n    cache = MemoryEfficientCache(max_size=5, max_memory_mb=0.1)\n    \n    # Add items to cache\n    test_data = [f\"large_data_item_{i}\" * 100 for i in range(10)]\n    \n    for i, data in enumerate(test_data):\n        cache.put(f\"key_{i}\", data)\n        stats = cache.get_stats()\n        console.print(f\"  Added item {i}: cache size={stats['size']}, memory={stats['memory_mb']:.3f}MB\")\n    \n    # Test retrieval\n    console.print(f\"\\n[bold cyan]Cache retrieval test:[/bold cyan]\")\n    for i in [0, 3, 7, 9]:\n        result = cache.get(f\"key_{i}\")\n        status = \"HIT\" if result else \"MISS\"\n        console.print(f\"  key_{i}: {status}\")\n    \n    # Test object pool\n    console.print(f\"\\n[bold yellow]Testing object pool:[/bold yellow]\")\n    \n    class ExpensiveResource:\n        def __init__(self):\n            time.sleep(0.01)  # Simulate expensive creation\n            self.created_at = time.time()\n            self.usage_count = 0\n        \n        def reset(self):\n            self.usage_count = 0\n    \n    pool = ObjectPool(ExpensiveResource, max_size=3)\n    \n    # Use objects from pool\n    with pool.borrow() as obj1:\n        obj1.usage_count += 1\n        console.print(f\"  Using object 1: created at {obj1.created_at:.3f}\")\n        \n        with pool.borrow() as obj2:\n            obj2.usage_count += 1\n            console.print(f\"  Using object 2: created at {obj2.created_at:.3f}\")\n    \n    console.print(f\"  Pool stats: {pool.get_stats()}\")\n\n\nasync def demonstrate_profiling():\n    \"\"\"Show profiling techniques.\"\"\"\n    \n    console.print(Panel(\"📊 Performance Profiling\", style=\"bold green\"))\n    \n    monitor = PerformanceMonitor()\n    \n    console.print(\"[bold yellow]Profiling different operations:[/bold yellow]\")\n    \n    # Profile memory usage\n    with profile_memory():\n        with profile_time():\n            # Create large data structure\n            large_list = [f\"item_{i}\" for i in range(100000)]\n            \n            start_time = time.perf_counter()\n            # Simulate processing\n            processed = [item.upper() for item in large_list[:1000]]\n            duration = time.perf_counter() - start_time\n            \n            monitor.record_time(\"list_processing\", duration)\n    \n    # Profile function calls\n    console.print(f\"\\n[bold cyan]Function call profiling:[/bold cyan]\")\n    \n    def profile_function():\n        \"\"\"Function to profile.\"\"\"\n        result = []\n        for i in range(1000):\n            result.append(expensive_model_inference(f\"test text {i % 10}\"))  # Will hit cache\n        return result\n    \n    # Use cProfile\n    profiler = cProfile.Profile()\n    profiler.enable()\n    \n    start_time = time.perf_counter()\n    results = profile_function()\n    duration = time.perf_counter() - start_time\n    \n    profiler.disable()\n    \n    # Show results\n    console.print(f\"  Processed {len(results)} items in {duration:.4f}s\")\n    console.print(f\"  Rate: {len(results) / duration:.1f} items/second\")\n    \n    # Show performance summary\n    summary = monitor.get_summary()\n    console.print(f\"\\n[bold magenta]Performance summary:[/bold magenta]\")\n    for metric, stats in summary.items():\n        if isinstance(stats, dict) and 'avg' in stats:\n            console.print(f\"  {metric}: avg={stats['avg']:.4f}, count={stats['count']}\")\n\n\nasync def demonstrate_vectorization():\n    \"\"\"Show vectorized operations.\"\"\"\n    \n    console.print(Panel(\"⚡ Vectorized Operations\", style=\"bold magenta\"))\n    \n    # Create test data\n    test_texts = [f\"Sample text number {i} for processing\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Comparing scalar vs vectorized operations:[/bold yellow]\")\n    \n    # Scalar operations\n    start_time = time.perf_counter()\n    scalar_lengths = [len(text) for text in test_texts]\n    scalar_time = time.perf_counter() - start_time\n    \n    # Vectorized operations\n    start_time = time.perf_counter()\n    vector_lengths = VectorizedTextProcessor.batch_compute_lengths(test_texts)\n    vector_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Scalar processing: {scalar_time:.4f}s\")\n    console.print(f\"  Vectorized processing: {vector_time:.4f}s\")\n    console.print(f\"  Speedup: {scalar_time / vector_time:.1f}x\")\n    \n    # Test batch operations\n    console.print(f\"\\n[bold cyan]Batch text processing:[/bold cyan]\")\n    \n    start_time = time.perf_counter()\n    tokenized = VectorizedTextProcessor.batch_tokenize(test_texts[:100])\n    filtered = VectorizedTextProcessor.batch_filter_by_length(test_texts[:100], min_length=20)\n    processing_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Processed {len(test_texts[:100])} texts in {processing_time:.4f}s\")\n    console.print(f\"  Tokenized: {len(tokenized)} texts\")\n    console.print(f\"  Filtered: {len(filtered)} texts (length >= 20)\")\n    \n    # Similarity matrix\n    console.print(f\"\\n[bold yellow]Computing similarity matrix:[/bold yellow]\")\n    sample_texts = test_texts[:10]\n    \n    start_time = time.perf_counter()\n    similarity_matrix = VectorizedTextProcessor.batch_similarity_matrix(sample_texts)\n    matrix_time = time.perf_counter() - start_time\n    \n    console.print(f\"  {len(sample_texts)}x{len(sample_texts)} matrix computed in {matrix_time:.4f}s\")\n    console.print(f\"  Average similarity: {np.mean(similarity_matrix[similarity_matrix != 1.0]):.3f}\")\n\n\nasync def demonstrate_parallel_processing():\n    \"\"\"Show parallel processing patterns.\"\"\"\n    \n    console.print(Panel(\"🚀 Parallel Processing\", style=\"bold cyan\"))\n    \n    # Create test data\n    test_data = [f\"data_item_{i}\" for i in range(100)]\n    \n    console.print(\"[bold yellow]Comparing sequential vs parallel processing:[/bold yellow]\")\n    \n    # Sequential processing (CPU-bound)\n    start_time = time.perf_counter()\n    sequential_results = [cpu_intensive_task(item) for item in test_data[:20]]\n    sequential_time = time.perf_counter() - start_time\n    \n    # Parallel processing (CPU-bound)\n    start_time = time.perf_counter()\n    parallel_results = ParallelProcessor.parallel_map_processes(\n        cpu_intensive_task, test_data[:20], max_workers=2\n    )\n    parallel_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Sequential (CPU): {sequential_time:.4f}s\")\n    console.print(f\"  Parallel (CPU): {parallel_time:.4f}s\")\n    console.print(f\"  Speedup: {sequential_time / parallel_time:.1f}x\")\n    \n    # I/O-bound comparison\n    console.print(f\"\\n[bold cyan]I/O-bound task comparison:[/bold cyan]\")\n    \n    start_time = time.perf_counter()\n    sequential_io = [io_intensive_task(item) for item in test_data[:50]]\n    sequential_io_time = time.perf_counter() - start_time\n    \n    start_time = time.perf_counter()\n    parallel_io = ParallelProcessor.parallel_map_threads(\n        io_intensive_task, test_data[:50], max_workers=10\n    )\n    parallel_io_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Sequential (I/O): {sequential_io_time:.4f}s\")\n    console.print(f\"  Parallel (I/O): {parallel_io_time:.4f}s\")\n    console.print(f\"  Speedup: {sequential_io_time / parallel_io_time:.1f}x\")\n    \n    # Batch processing\n    console.print(f\"\\n[bold yellow]Parallel batch processing:[/bold yellow]\")\n    \n    def process_batch(batch: List[str]) -> List[str]:\n        return [item.upper() for item in batch]\n    \n    start_time = time.perf_counter()\n    batch_results = ParallelProcessor.parallel_batch_process(\n        test_data, process_batch, batch_size=25, max_workers=4\n    )\n    batch_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Processed {len(batch_results)} items in {batch_time:.4f}s\")\n    console.print(f\"  Rate: {len(batch_results) / batch_time:.1f} items/second\")\n\n\nasync def demonstrate_lazy_loading():\n    \"\"\"Show lazy loading patterns.\"\"\"\n    \n    console.print(Panel(\"😴 Lazy Loading\", style=\"bold red\"))\n    \n    console.print(\"[bold yellow]Creating lazy data loader:[/bold yellow]\")\n    \n    # Create lazy loader\n    loader = LazyDataLoader(lambda: create_sample_data_generator(10000))\n    \n    console.print(f\"  Initial stats: {loader.get_stats()}\")\n    \n    # Load some chunks\n    console.print(f\"\\n[bold cyan]Loading data chunks on demand:[/bold cyan]\")\n    \n    for chunk_id in [0, 2, 5]:\n        start_time = time.perf_counter()\n        chunk_data = loader.get_chunk(chunk_id, chunk_size=100)\n        load_time = time.perf_counter() - start_time\n        \n        console.print(f\"  Chunk {chunk_id}: {len(chunk_data)} items loaded in {load_time:.4f}s\")\n    \n    console.print(f\"  Final stats: {loader.get_stats()}\")\n    \n    # Access cached chunk\n    start_time = time.perf_counter()\n    cached_chunk = loader.get_chunk(0, chunk_size=100)  # Should be cached\n    cached_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Cached access: {len(cached_chunk)} items in {cached_time:.6f}s\")\n\n\nasync def demonstrate_batch_processing():\n    \"\"\"Show batch processing optimization.\"\"\"\n    \n    console.print(Panel(\"📦 Batch Processing\", style=\"bold yellow\"))\n    \n    # Create test data\n    test_texts = [f\"Text sample {i} for analysis\" for i in range(100)]\n    \n    console.print(\"[bold yellow]Comparing individual vs batch processing:[/bold yellow]\")\n    \n    # Individual processing\n    start_time = time.perf_counter()\n    individual_results = []\n    for text in test_texts[:20]:\n        result = {\n            \"length\": len(text),\n            \"word_count\": len(text.split()),\n            \"char_diversity\": len(set(text.lower())),\n            \"batch_processed\": False\n        }\n        individual_results.append(result)\n    individual_time = time.perf_counter() - start_time\n    \n    # Batch processing\n    start_time = time.perf_counter()\n    batch_results = batch_text_analysis(test_texts[:20])\n    batch_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Individual processing: {individual_time:.4f}s\")\n    console.print(f\"  Batch processing: {batch_time:.4f}s\")\n    console.print(f\"  Speedup: {individual_time / batch_time:.1f}x\")\n    \n    # Verify results\n    console.print(f\"\\n[bold cyan]Result verification:[/bold cyan]\")\n    console.print(f\"  Individual results: {len(individual_results)} items\")\n    console.print(f\"  Batch results: {len(batch_results)} items\")\n    console.print(f\"  Batch flag in results: {batch_results[0]['batch_processed']}\")\n\n\nasync def demonstrate_gc_optimization():\n    \"\"\"Show garbage collection optimization.\"\"\"\n    \n    console.print(Panel(\"🗑️ Garbage Collection Optimization\", style=\"bold purple\"))\n    \n    console.print(\"[bold yellow]Memory management with garbage collection:[/bold yellow]\")\n    \n    # Check initial memory\n    gc.collect()  # Force collection\n    initial_objects = len(gc.get_objects())\n    \n    console.print(f\"  Initial objects: {initial_objects}\")\n    \n    # Create many objects\n    large_objects = []\n    for i in range(1000):\n        obj = {\n            \"id\": i,\n            \"data\": [f\"item_{j}\" for j in range(100)],\n            \"metadata\": {\"created\": time.time(), \"type\": \"test\"}\n        }\n        large_objects.append(obj)\n    \n    mid_objects = len(gc.get_objects())\n    console.print(f\"  After creation: {mid_objects} objects (+{mid_objects - initial_objects})\")\n    \n    # Clear references and force collection\n    del large_objects\n    gc.collect()\n    \n    final_objects = len(gc.get_objects())\n    console.print(f\"  After cleanup: {final_objects} objects (-{mid_objects - final_objects})\")\n    \n    # Show GC stats\n    console.print(f\"\\n[bold cyan]Garbage collection stats:[/bold cyan]\")\n    for generation, stats in enumerate(gc.get_stats()):\n        console.print(f\"  Generation {generation}: {stats}\")\n\n\ndef benchmark_comparison():\n    \"\"\"Compare different optimization techniques.\"\"\"\n    \n    console.print(Panel(\"🏆 Performance Benchmark\", style=\"bold orange\"))\n    \n    test_data = [f\"benchmark text {i}\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Benchmarking optimization techniques:[/bold yellow]\")\n    \n    techniques = [\n        (\"No optimization\", lambda data: [len(item) for item in data]),\n        (\"List comprehension\", lambda data: [len(item) for item in data]),\n        (\"Generator expression\", lambda data: list(len(item) for item in data)),\n        (\"NumPy vectorized\", lambda data: VectorizedTextProcessor.batch_compute_lengths(data)),\n        (\"Parallel threads\", lambda data: ParallelProcessor.parallel_map_threads(len, data)),\n    ]\n    \n    results = []\n    \n    for name, technique in techniques:\n        # Warm up\n        technique(test_data[:10])\n        \n        # Benchmark\n        start_time = time.perf_counter()\n        result = technique(test_data)\n        end_time = time.perf_counter()\n        \n        duration = end_time - start_time\n        rate = len(test_data) / duration\n        \n        results.append((name, duration, rate))\n        console.print(f\"  {name}: {duration:.4f}s ({rate:.1f} items/sec)\")\n    \n    # Find best technique\n    best_technique = min(results, key=lambda x: x[1])\n    console.print(f\"\\n[bold green]Best technique: {best_technique[0]} ({best_technique[1]:.4f}s)[/bold green]\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for performance optimization.\"\"\"\n    \n    console.print(Panel(\"🎮 Interactive Exercise: Optimize AI Performance\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"challenge\": \"Large-Scale Text Processing\",\n            \"problem\": \"Process 1M+ text documents with multiple AI models\",\n            \"bottlenecks\": [\n                \"Memory usage grows unbounded\",\n                \"CPU utilization is poor\",\n                \"I/O operations are blocking\",\n                \"Cache misses are frequent\"\n            ],\n            \"solutions\": \"Memory pooling, vectorization, async I/O, intelligent caching\"\n        },\n        {\n            \"challenge\": \"Real-Time AI Inference\",\n            \"problem\": \"Serve AI predictions with <100ms latency\",\n            \"bottlenecks\": [\n                \"Model loading takes too long\",\n                \"Batch processing adds latency\",\n                \"Memory allocations cause GC pauses\",\n                \"Network serialization is slow\"\n            ],\n            \"solutions\": \"Model pooling, streaming inference, object reuse, binary protocols\"\n        },\n        {\n            \"challenge\": \"Multi-Model Ensemble\",\n            \"problem\": \"Run multiple AI models concurrently\",\n            \"bottlenecks\": [\n                \"Models compete for resources\",\n                \"Sequential processing is slow\",\n                \"Memory pressure causes swapping\",\n                \"Error handling blocks pipeline\"\n            ],\n            \"solutions\": \"Resource isolation, parallel execution, memory management, circuit breakers\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Challenge {i}: {scenario['challenge']}[/bold cyan]\")\n        console.print(f\"[bold yellow]Problem: {scenario['problem']}[/bold yellow]\")\n        console.print(\"[bold red]Common Bottlenecks:[/bold red]\")\n        for bottleneck in scenario[\"bottlenecks\"]:\n            console.print(f\"  • {bottleneck}\")\n        console.print(f\"\\n[bold green]Optimization Strategies: {scenario['solutions']}[/bold green]\")\n        console.print(\"\\n[bold magenta]Consider:[/bold magenta]\")\n        console.print(\"  • Profile first, optimize second\")\n        console.print(\"  • Memory vs CPU vs I/O trade-offs\")\n        console.print(\"  • Caching strategies and invalidation\")\n        console.print(\"  • Parallel vs concurrent execution\")\n        console.print(\"  • Resource pooling and lifecycle management\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 AI Architecture Patterns: Performance Optimization[/bold blue]\")\n    console.print(\"Learn to build high-performance AI systems\\n\")\n    \n    async def main():\n        await demonstrate_memory_efficient_structures()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_profiling()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_vectorization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_parallel_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_lazy_loading()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_batch_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_gc_optimization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        benchmark_comparison()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]✅ Completed![/bold green] All Python idioms examples are ready!\")\n        console.print(\"💡 Pro tip: Always profile before optimizing - measure, don't guess!\")\n    \n    import asyncio\n    asyncio.run(main())","structuredPatch":[]}}
{"parentUuid":"a247f5c1-b516-498f-bfc1-c6e59757d36f","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01C81NHKnBXBGai1o8gCcQBj","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01G8bsDNRxiEKjbDAK2f7FtG","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Create 03_functional_programming.py example file","status":"completed","priority":"high"},{"id":"2","content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high"},{"id":"3","content":"Create 05_performance_optimization.py example file","status":"completed","priority":"high"},{"id":"4","content":"Create tests for the Python idioms examples","status":"in_progress","priority":"medium"},{"id":"5","content":"Create exercises for hands-on practice","status":"pending","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":9240,"cache_read_input_tokens":56168,"output_tokens":23}},"requestId":"req_vrtx_011CRddNsbjmXqmqziEPdznZ","type":"assistant","uuid":"57601520-6931-4a0e-8ebc-dbae4f8fc841","timestamp":"2025-07-30T17:51:29.968Z"}
{"parentUuid":"57601520-6931-4a0e-8ebc-dbae4f8fc841","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01G8bsDNRxiEKjbDAK2f7FtG","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"7ba91338-f5e2-4fd6-a10a-f5da2a1e1e5b","timestamp":"2025-07-30T17:51:29.995Z","toolUseResult":{"oldTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"in_progress","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"pending","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}],"newTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"completed","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"in_progress","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}]}}
{"parentUuid":"7ba91338-f5e2-4fd6-a10a-f5da2a1e1e5b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01Q6hgdtJgEEu63GLxvtTCGs","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me create tests for the Python idioms examples:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":446,"cache_read_input_tokens":65408,"output_tokens":1}},"requestId":"req_vrtx_011CRddPN8E2n1vBZeccRagT","type":"assistant","uuid":"6691b257-8a67-48d5-8435-2522177fb2b6","timestamp":"2025-07-30T17:51:33.921Z"}
{"parentUuid":"6691b257-8a67-48d5-8435-2522177fb2b6","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01Q6hgdtJgEEu63GLxvtTCGs","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01FtFYHZZVVzxbBFjVEArYyM","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_type_system.py","content":"#!/usr/bin/env python3\n\"\"\"\nTests for Advanced Type System Examples\n\nRun with: python -m pytest 03-python-idioms/tests/test_type_system.py -v\n\"\"\"\n\nimport pytest\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import Dict, Any\nfrom examples.example_01_type_system_advanced import (\n    TextInput, ClassificationOutput, TextClassifier,\n    TypedCache, ModelManager, create_model_config,\n    ModelConfig, AdvancedModelConfig\n)\n\n\nclass TestTextInput:\n    \"\"\"Test TextInput dataclass.\"\"\"\n    \n    def test_basic_creation(self):\n        \"\"\"Test basic TextInput creation.\"\"\"\n        text_input = TextInput(text=\"Hello world\")\n        assert text_input.text == \"Hello world\"\n        assert text_input.context is None\n        assert text_input.metadata == {}\n    \n    def test_with_context_and_metadata(self):\n        \"\"\"Test TextInput with context and metadata.\"\"\"\n        metadata = {\"source\": \"test\", \"priority\": \"high\"}\n        text_input = TextInput(\n            text=\"Test text\",\n            context=\"Test context\",\n            metadata=metadata\n        )\n        \n        assert text_input.text == \"Test text\"\n        assert text_input.context == \"Test context\"\n        assert text_input.metadata == metadata\n    \n    def test_post_init_metadata(self):\n        \"\"\"Test that metadata is initialized to empty dict.\"\"\"\n        text_input = TextInput(text=\"Hello\", metadata=None)\n        assert text_input.metadata == {}\n\n\nclass TestClassificationOutput:\n    \"\"\"Test ClassificationOutput dataclass.\"\"\"\n    \n    def test_creation(self):\n        \"\"\"Test ClassificationOutput creation.\"\"\"\n        output = ClassificationOutput(\n            label=\"positive\",\n            confidence=0.95,\n            alternatives=[(\"negative\", 0.05)]\n        )\n        \n        assert output.label == \"positive\"\n        assert output.confidence == 0.95\n        assert output.alternatives == [(\"negative\", 0.05)]\n    \n    def test_serialization(self):\n        \"\"\"Test to_dict and from_dict methods.\"\"\"\n        output = ClassificationOutput(\n            label=\"neutral\",\n            confidence=0.75,\n            alternatives=[(\"positive\", 0.15), (\"negative\", 0.10)]\n        )\n        \n        # Test serialization\n        serialized = output.to_dict()\n        expected = {\n            \"label\": \"neutral\",\n            \"confidence\": 0.75,\n            \"alternatives\": [(\"positive\", 0.15), (\"negative\", 0.10)]\n        }\n        assert serialized == expected\n        \n        # Test deserialization\n        restored = ClassificationOutput.from_dict(serialized)\n        assert restored.label == output.label\n        assert restored.confidence == output.confidence\n        assert restored.alternatives == output.alternatives\n\n\nclass TestTextClassifier:\n    \"\"\"Test TextClassifier implementation.\"\"\"\n    \n    @pytest.fixture\n    def classifier(self):\n        \"\"\"Create a TextClassifier instance.\"\"\"\n        config = create_model_config(\"openai\", \"large\")\n        return TextClassifier(config)\n    \n    @pytest.mark.asyncio\n    async def test_predict_positive(self, classifier):\n        \"\"\"Test prediction with positive text.\"\"\"\n        text_input = TextInput(text=\"This is a positive example\")\n        result = await classifier.predict(text_input)\n        \n        assert isinstance(result, ClassificationOutput)\n        assert result.label == \"positive\"\n        assert result.confidence > 0.8\n        assert len(result.alternatives) == 3\n    \n    @pytest.mark.asyncio\n    async def test_predict_negative(self, classifier):\n        \"\"\"Test prediction with negative text.\"\"\"\n        text_input = TextInput(text=\"This is a negative example\")\n        result = await classifier.predict(text_input)\n        \n        assert isinstance(result, ClassificationOutput)\n        assert result.label == \"negative\"\n        assert result.confidence > 0.8\n    \n    @pytest.mark.asyncio\n    async def test_predict_neutral(self, classifier):\n        \"\"\"Test prediction with neutral text.\"\"\"\n        text_input = TextInput(text=\"This is neutral text\")\n        result = await classifier.predict(text_input)\n        \n        assert isinstance(result, ClassificationOutput)\n        assert result.label == \"neutral\"\n        assert result.confidence > 0.7\n    \n    def test_validate_input_valid(self, classifier):\n        \"\"\"Test input validation with valid input.\"\"\"\n        valid_input = TextInput(text=\"Valid text input\")\n        assert classifier.validate_input(valid_input) is True\n    \n    def test_validate_input_empty(self, classifier):\n        \"\"\"Test input validation with empty input.\"\"\"\n        empty_input = TextInput(text=\"\")\n        assert classifier.validate_input(empty_input) is False\n        \n        whitespace_input = TextInput(text=\"   \")\n        assert classifier.validate_input(whitespace_input) is False\n    \n    def test_validate_input_too_long(self, classifier):\n        \"\"\"Test input validation with too long input.\"\"\"\n        long_input = TextInput(text=\"x\" * 10001)\n        assert classifier.validate_input(long_input) is False\n\n\nclass TestTypedCache:\n    \"\"\"Test TypedCache implementation.\"\"\"\n    \n    def test_basic_operations(self):\n        \"\"\"Test basic cache operations.\"\"\"\n        cache = TypedCache[str]()\n        \n        # Test empty cache\n        assert cache.get(\"nonexistent\") is None\n        \n        # Test put and get\n        cache.put(\"key1\", \"value1\")\n        assert cache.get(\"key1\") == \"value1\"\n        \n        # Test overwrite\n        cache.put(\"key1\", \"new_value1\")\n        assert cache.get(\"key1\") == \"new_value1\"\n    \n    def test_invalidate(self):\n        \"\"\"Test cache invalidation.\"\"\"\n        cache = TypedCache[int]()\n        \n        cache.put(\"key1\", 42)\n        assert cache.get(\"key1\") == 42\n        \n        # Test successful invalidation\n        assert cache.invalidate(\"key1\") is True\n        assert cache.get(\"key1\") is None\n        \n        # Test invalidation of non-existent key\n        assert cache.invalidate(\"nonexistent\") is False\n    \n    def test_stats(self):\n        \"\"\"Test cache statistics.\"\"\"\n        cache = TypedCache[str]()\n        \n        # Empty cache stats\n        stats = cache.get_stats()\n        assert stats[\"size\"] == 0\n        assert stats[\"total_access\"] == 0\n        assert stats[\"keys\"] == []\n        \n        # Add items and check stats\n        cache.put(\"key1\", \"value1\")\n        cache.put(\"key2\", \"value2\")\n        cache.get(\"key1\")  # Access key1\n        \n        stats = cache.get_stats()\n        assert stats[\"size\"] == 2\n        assert stats[\"total_access\"] == 1\n        assert set(stats[\"keys\"]) == {\"key1\", \"key2\"}\n\n\nclass TestModelManager:\n    \"\"\"Test ModelManager implementation.\"\"\"\n    \n    @pytest.fixture\n    def manager(self):\n        \"\"\"Create a ModelManager instance.\"\"\"\n        return ModelManager()\n    \n    def test_register_and_get_model(self, manager):\n        \"\"\"Test model registration and retrieval.\"\"\"\n        config = create_model_config(\"openai\", \"large\")\n        classifier = TextClassifier(config)\n        \n        # Register model\n        manager.register_model(\"test_classifier\", classifier)\n        \n        # Test retrieval\n        retrieved = manager.get_model(\"test_classifier\")\n        assert retrieved is classifier\n        \n        # Test non-existent model\n        assert manager.get_model(\"nonexistent\") is None\n    \n    def test_get_model_with_default(self, manager):\n        \"\"\"Test get_model with default value.\"\"\"\n        # Test with non-existent model and default\n        default_value = \"default_model\"\n        result = manager.get_model(\"nonexistent\", default_value)\n        assert result == default_value\n    \n    @pytest.mark.asyncio\n    async def test_predict_with_caching(self, manager):\n        \"\"\"Test prediction with caching.\"\"\"\n        config = create_model_config(\"openai\", \"large\")\n        classifier = TextClassifier(config)\n        manager.register_model(\"test_classifier\", classifier)\n        \n        text_input = TextInput(text=\"Test caching\")\n        \n        # First prediction\n        result1 = await manager.predict(\"test_classifier\", text_input)\n        assert isinstance(result1, ClassificationOutput)\n        \n        # Second prediction (should use cache)\n        result2 = await manager.predict(\"test_classifier\", text_input)\n        assert result2.label == result1.label\n        assert result2.confidence == result1.confidence\n        \n        # Check cache stats\n        cache_stats = manager.cache.get_stats()\n        assert cache_stats[\"size\"] >= 1\n    \n    @pytest.mark.asyncio\n    async def test_predict_nonexistent_model(self, manager):\n        \"\"\"Test prediction with non-existent model.\"\"\"\n        text_input = TextInput(text=\"Test error\")\n        \n        with pytest.raises(ValueError, match=\"Model 'nonexistent' not found\"):\n            await manager.predict(\"nonexistent\", text_input)\n\n\nclass TestModelConfig:\n    \"\"\"Test model configuration types.\"\"\"\n    \n    def test_create_model_config(self):\n        \"\"\"Test model config creation.\"\"\"\n        config = create_model_config(\"openai\", \"large\", temperature=0.8)\n        \n        assert isinstance(config, dict)  # TypedDict creates regular dict\n        assert config[\"provider\"] == \"openai\"\n        assert config[\"temperature\"] == 0.8\n        assert config[\"max_tokens\"] == 4000  # large model tokens\n        assert isinstance(config[\"model_id\"], str)\n    \n    def test_different_providers_and_sizes(self):\n        \"\"\"Test different provider and size combinations.\"\"\"\n        # Test anthropic small\n        config1 = create_model_config(\"anthropic\", \"small\")\n        assert config1[\"provider\"] == \"anthropic\"\n        assert config1[\"max_tokens\"] == 1000\n        \n        # Test local model\n        config2 = create_model_config(\"local\", \"medium\")\n        assert config2[\"provider\"] == \"local\"\n        assert config2[\"max_tokens\"] == 2000\n    \n    def test_advanced_config_fields(self):\n        \"\"\"Test that AdvancedModelConfig can include optional fields.\"\"\"\n        # This test ensures type checking works correctly\n        advanced_config: AdvancedModelConfig = {\n            \"model_id\": \"test-model\",\n            \"max_tokens\": 2000,\n            \"temperature\": 0.7,\n            \"provider\": \"openai\",\n            \"timeout_seconds\": 30.0,\n            \"retry_attempts\": 3,\n            \"fallback_model\": \"backup-model\",\n            \"cost_limit_usd\": 10.0\n        }\n        \n        assert advanced_config[\"retry_attempts\"] == 3\n        assert advanced_config[\"fallback_model\"] == \"backup-model\"\n        assert advanced_config[\"cost_limit_usd\"] == 10.0\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":446,"cache_read_input_tokens":65408,"output_tokens":1}},"requestId":"req_vrtx_011CRddPN8E2n1vBZeccRagT","type":"assistant","uuid":"b53678e2-d66f-4b1e-87bc-b4c4cc9f0ffa","timestamp":"2025-07-30T17:52:13.441Z"}
{"parentUuid":"b53678e2-d66f-4b1e-87bc-b4c4cc9f0ffa","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01FtFYHZZVVzxbBFjVEArYyM","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_type_system.py"}]},"uuid":"fa158a43-4802-49b9-9bad-504d0cf350cc","timestamp":"2025-07-30T17:52:13.471Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_type_system.py","content":"#!/usr/bin/env python3\n\"\"\"\nTests for Advanced Type System Examples\n\nRun with: python -m pytest 03-python-idioms/tests/test_type_system.py -v\n\"\"\"\n\nimport pytest\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import Dict, Any\nfrom examples.example_01_type_system_advanced import (\n    TextInput, ClassificationOutput, TextClassifier,\n    TypedCache, ModelManager, create_model_config,\n    ModelConfig, AdvancedModelConfig\n)\n\n\nclass TestTextInput:\n    \"\"\"Test TextInput dataclass.\"\"\"\n    \n    def test_basic_creation(self):\n        \"\"\"Test basic TextInput creation.\"\"\"\n        text_input = TextInput(text=\"Hello world\")\n        assert text_input.text == \"Hello world\"\n        assert text_input.context is None\n        assert text_input.metadata == {}\n    \n    def test_with_context_and_metadata(self):\n        \"\"\"Test TextInput with context and metadata.\"\"\"\n        metadata = {\"source\": \"test\", \"priority\": \"high\"}\n        text_input = TextInput(\n            text=\"Test text\",\n            context=\"Test context\",\n            metadata=metadata\n        )\n        \n        assert text_input.text == \"Test text\"\n        assert text_input.context == \"Test context\"\n        assert text_input.metadata == metadata\n    \n    def test_post_init_metadata(self):\n        \"\"\"Test that metadata is initialized to empty dict.\"\"\"\n        text_input = TextInput(text=\"Hello\", metadata=None)\n        assert text_input.metadata == {}\n\n\nclass TestClassificationOutput:\n    \"\"\"Test ClassificationOutput dataclass.\"\"\"\n    \n    def test_creation(self):\n        \"\"\"Test ClassificationOutput creation.\"\"\"\n        output = ClassificationOutput(\n            label=\"positive\",\n            confidence=0.95,\n            alternatives=[(\"negative\", 0.05)]\n        )\n        \n        assert output.label == \"positive\"\n        assert output.confidence == 0.95\n        assert output.alternatives == [(\"negative\", 0.05)]\n    \n    def test_serialization(self):\n        \"\"\"Test to_dict and from_dict methods.\"\"\"\n        output = ClassificationOutput(\n            label=\"neutral\",\n            confidence=0.75,\n            alternatives=[(\"positive\", 0.15), (\"negative\", 0.10)]\n        )\n        \n        # Test serialization\n        serialized = output.to_dict()\n        expected = {\n            \"label\": \"neutral\",\n            \"confidence\": 0.75,\n            \"alternatives\": [(\"positive\", 0.15), (\"negative\", 0.10)]\n        }\n        assert serialized == expected\n        \n        # Test deserialization\n        restored = ClassificationOutput.from_dict(serialized)\n        assert restored.label == output.label\n        assert restored.confidence == output.confidence\n        assert restored.alternatives == output.alternatives\n\n\nclass TestTextClassifier:\n    \"\"\"Test TextClassifier implementation.\"\"\"\n    \n    @pytest.fixture\n    def classifier(self):\n        \"\"\"Create a TextClassifier instance.\"\"\"\n        config = create_model_config(\"openai\", \"large\")\n        return TextClassifier(config)\n    \n    @pytest.mark.asyncio\n    async def test_predict_positive(self, classifier):\n        \"\"\"Test prediction with positive text.\"\"\"\n        text_input = TextInput(text=\"This is a positive example\")\n        result = await classifier.predict(text_input)\n        \n        assert isinstance(result, ClassificationOutput)\n        assert result.label == \"positive\"\n        assert result.confidence > 0.8\n        assert len(result.alternatives) == 3\n    \n    @pytest.mark.asyncio\n    async def test_predict_negative(self, classifier):\n        \"\"\"Test prediction with negative text.\"\"\"\n        text_input = TextInput(text=\"This is a negative example\")\n        result = await classifier.predict(text_input)\n        \n        assert isinstance(result, ClassificationOutput)\n        assert result.label == \"negative\"\n        assert result.confidence > 0.8\n    \n    @pytest.mark.asyncio\n    async def test_predict_neutral(self, classifier):\n        \"\"\"Test prediction with neutral text.\"\"\"\n        text_input = TextInput(text=\"This is neutral text\")\n        result = await classifier.predict(text_input)\n        \n        assert isinstance(result, ClassificationOutput)\n        assert result.label == \"neutral\"\n        assert result.confidence > 0.7\n    \n    def test_validate_input_valid(self, classifier):\n        \"\"\"Test input validation with valid input.\"\"\"\n        valid_input = TextInput(text=\"Valid text input\")\n        assert classifier.validate_input(valid_input) is True\n    \n    def test_validate_input_empty(self, classifier):\n        \"\"\"Test input validation with empty input.\"\"\"\n        empty_input = TextInput(text=\"\")\n        assert classifier.validate_input(empty_input) is False\n        \n        whitespace_input = TextInput(text=\"   \")\n        assert classifier.validate_input(whitespace_input) is False\n    \n    def test_validate_input_too_long(self, classifier):\n        \"\"\"Test input validation with too long input.\"\"\"\n        long_input = TextInput(text=\"x\" * 10001)\n        assert classifier.validate_input(long_input) is False\n\n\nclass TestTypedCache:\n    \"\"\"Test TypedCache implementation.\"\"\"\n    \n    def test_basic_operations(self):\n        \"\"\"Test basic cache operations.\"\"\"\n        cache = TypedCache[str]()\n        \n        # Test empty cache\n        assert cache.get(\"nonexistent\") is None\n        \n        # Test put and get\n        cache.put(\"key1\", \"value1\")\n        assert cache.get(\"key1\") == \"value1\"\n        \n        # Test overwrite\n        cache.put(\"key1\", \"new_value1\")\n        assert cache.get(\"key1\") == \"new_value1\"\n    \n    def test_invalidate(self):\n        \"\"\"Test cache invalidation.\"\"\"\n        cache = TypedCache[int]()\n        \n        cache.put(\"key1\", 42)\n        assert cache.get(\"key1\") == 42\n        \n        # Test successful invalidation\n        assert cache.invalidate(\"key1\") is True\n        assert cache.get(\"key1\") is None\n        \n        # Test invalidation of non-existent key\n        assert cache.invalidate(\"nonexistent\") is False\n    \n    def test_stats(self):\n        \"\"\"Test cache statistics.\"\"\"\n        cache = TypedCache[str]()\n        \n        # Empty cache stats\n        stats = cache.get_stats()\n        assert stats[\"size\"] == 0\n        assert stats[\"total_access\"] == 0\n        assert stats[\"keys\"] == []\n        \n        # Add items and check stats\n        cache.put(\"key1\", \"value1\")\n        cache.put(\"key2\", \"value2\")\n        cache.get(\"key1\")  # Access key1\n        \n        stats = cache.get_stats()\n        assert stats[\"size\"] == 2\n        assert stats[\"total_access\"] == 1\n        assert set(stats[\"keys\"]) == {\"key1\", \"key2\"}\n\n\nclass TestModelManager:\n    \"\"\"Test ModelManager implementation.\"\"\"\n    \n    @pytest.fixture\n    def manager(self):\n        \"\"\"Create a ModelManager instance.\"\"\"\n        return ModelManager()\n    \n    def test_register_and_get_model(self, manager):\n        \"\"\"Test model registration and retrieval.\"\"\"\n        config = create_model_config(\"openai\", \"large\")\n        classifier = TextClassifier(config)\n        \n        # Register model\n        manager.register_model(\"test_classifier\", classifier)\n        \n        # Test retrieval\n        retrieved = manager.get_model(\"test_classifier\")\n        assert retrieved is classifier\n        \n        # Test non-existent model\n        assert manager.get_model(\"nonexistent\") is None\n    \n    def test_get_model_with_default(self, manager):\n        \"\"\"Test get_model with default value.\"\"\"\n        # Test with non-existent model and default\n        default_value = \"default_model\"\n        result = manager.get_model(\"nonexistent\", default_value)\n        assert result == default_value\n    \n    @pytest.mark.asyncio\n    async def test_predict_with_caching(self, manager):\n        \"\"\"Test prediction with caching.\"\"\"\n        config = create_model_config(\"openai\", \"large\")\n        classifier = TextClassifier(config)\n        manager.register_model(\"test_classifier\", classifier)\n        \n        text_input = TextInput(text=\"Test caching\")\n        \n        # First prediction\n        result1 = await manager.predict(\"test_classifier\", text_input)\n        assert isinstance(result1, ClassificationOutput)\n        \n        # Second prediction (should use cache)\n        result2 = await manager.predict(\"test_classifier\", text_input)\n        assert result2.label == result1.label\n        assert result2.confidence == result1.confidence\n        \n        # Check cache stats\n        cache_stats = manager.cache.get_stats()\n        assert cache_stats[\"size\"] >= 1\n    \n    @pytest.mark.asyncio\n    async def test_predict_nonexistent_model(self, manager):\n        \"\"\"Test prediction with non-existent model.\"\"\"\n        text_input = TextInput(text=\"Test error\")\n        \n        with pytest.raises(ValueError, match=\"Model 'nonexistent' not found\"):\n            await manager.predict(\"nonexistent\", text_input)\n\n\nclass TestModelConfig:\n    \"\"\"Test model configuration types.\"\"\"\n    \n    def test_create_model_config(self):\n        \"\"\"Test model config creation.\"\"\"\n        config = create_model_config(\"openai\", \"large\", temperature=0.8)\n        \n        assert isinstance(config, dict)  # TypedDict creates regular dict\n        assert config[\"provider\"] == \"openai\"\n        assert config[\"temperature\"] == 0.8\n        assert config[\"max_tokens\"] == 4000  # large model tokens\n        assert isinstance(config[\"model_id\"], str)\n    \n    def test_different_providers_and_sizes(self):\n        \"\"\"Test different provider and size combinations.\"\"\"\n        # Test anthropic small\n        config1 = create_model_config(\"anthropic\", \"small\")\n        assert config1[\"provider\"] == \"anthropic\"\n        assert config1[\"max_tokens\"] == 1000\n        \n        # Test local model\n        config2 = create_model_config(\"local\", \"medium\")\n        assert config2[\"provider\"] == \"local\"\n        assert config2[\"max_tokens\"] == 2000\n    \n    def test_advanced_config_fields(self):\n        \"\"\"Test that AdvancedModelConfig can include optional fields.\"\"\"\n        # This test ensures type checking works correctly\n        advanced_config: AdvancedModelConfig = {\n            \"model_id\": \"test-model\",\n            \"max_tokens\": 2000,\n            \"temperature\": 0.7,\n            \"provider\": \"openai\",\n            \"timeout_seconds\": 30.0,\n            \"retry_attempts\": 3,\n            \"fallback_model\": \"backup-model\",\n            \"cost_limit_usd\": 10.0\n        }\n        \n        assert advanced_config[\"retry_attempts\"] == 3\n        assert advanced_config[\"fallback_model\"] == \"backup-model\"\n        assert advanced_config[\"cost_limit_usd\"] == 10.0\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])","structuredPatch":[]}}
{"parentUuid":"fa158a43-4802-49b9-9bad-504d0cf350cc","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01K9zAg8vVL8TfyDTSEfDJmB","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01BpGwQwtxfsWug987XtPkoA","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_async_patterns.py","content":"#!/usr/bin/env python3\n\"\"\"\nTests for Async Patterns Examples\n\nRun with: python -m pytest 03-python-idioms/tests/test_async_patterns.py -v\n\"\"\"\n\nimport pytest\nimport asyncio\nimport time\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom examples.example_02_async_patterns import (\n    StreamingChunk, AsyncRateLimiter, AsyncConnectionPool,\n    StreamingAIProvider, ConcurrentInferenceEngine,\n    async_batch_processor, async_resource_manager\n)\nfrom common import Message\n\n\nclass TestStreamingChunk:\n    \"\"\"Test StreamingChunk dataclass.\"\"\"\n    \n    def test_basic_creation(self):\n        \"\"\"Test basic StreamingChunk creation.\"\"\"\n        chunk = StreamingChunk(\n            content=\"Hello\",\n            chunk_id=1,\n            is_final=False\n        )\n        \n        assert chunk.content == \"Hello\"\n        assert chunk.chunk_id == 1\n        assert chunk.is_final is False\n        assert chunk.metadata == {}\n    \n    def test_with_metadata(self):\n        \"\"\"Test StreamingChunk with metadata.\"\"\"\n        metadata = {\"model\": \"gpt-4\", \"timestamp\": 123456}\n        chunk = StreamingChunk(\n            content=\"World\",\n            chunk_id=2,\n            is_final=True,\n            metadata=metadata\n        )\n        \n        assert chunk.metadata == metadata\n        assert chunk.is_final is True\n\n\nclass TestAsyncRateLimiter:\n    \"\"\"Test AsyncRateLimiter implementation.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_basic_rate_limiting(self):\n        \"\"\"Test basic rate limiting functionality.\"\"\"\n        # Allow 2 calls per 0.5 seconds\n        limiter = AsyncRateLimiter(max_calls=2, time_window=0.5)\n        \n        # First two calls should be immediate\n        start_time = time.time()\n        await limiter.acquire()\n        await limiter.acquire()\n        first_two_time = time.time() - start_time\n        \n        # Should be very fast (< 0.1 seconds)\n        assert first_two_time < 0.1\n        \n        # Third call should be delayed\n        start_time = time.time()\n        await limiter.acquire()\n        third_call_time = time.time() - start_time\n        \n        # Should wait approximately 0.5 seconds\n        assert 0.4 < third_call_time < 0.7\n    \n    @pytest.mark.asyncio\n    async def test_stats(self):\n        \"\"\"Test rate limiter statistics.\"\"\"\n        limiter = AsyncRateLimiter(max_calls=3, time_window=1.0)\n        \n        # Initial stats\n        stats = limiter.get_stats()\n        assert stats[\"current_calls\"] == 0\n        assert stats[\"max_calls\"] == 3\n        assert stats[\"time_window\"] == 1.0\n        assert stats[\"utilization\"] == 0.0\n        \n        # After some calls\n        await limiter.acquire()\n        await limiter.acquire()\n        \n        stats = limiter.get_stats()\n        assert stats[\"current_calls\"] == 2\n        assert stats[\"utilization\"] == 2/3\n\n\nclass TestAsyncConnectionPool:\n    \"\"\"Test AsyncConnectionPool implementation.\"\"\"\n    \n    async def create_mock_connection(self):\n        \"\"\"Create a mock connection.\"\"\"\n        await asyncio.sleep(0.01)  # Simulate connection time\n        return {\"id\": id(object()), \"created_at\": time.time()}\n    \n    @pytest.mark.asyncio\n    async def test_connection_creation(self):\n        \"\"\"Test connection pool creation and basic operations.\"\"\"\n        pool = AsyncConnectionPool(self.create_mock_connection, max_size=2)\n        \n        # Test getting connections\n        conn1 = await pool.get_connection()\n        conn2 = await pool.get_connection()\n        \n        assert conn1 is not None\n        assert conn2 is not None\n        assert conn1[\"id\"] != conn2[\"id\"]\n        \n        # Return connections\n        await pool.return_connection(conn1)\n        await pool.return_connection(conn2)\n        \n        stats = pool.get_stats()\n        assert stats[\"created_count\"] == 2\n        assert stats[\"active_connections\"] == 0\n        assert stats[\"pool_size\"] == 2\n    \n    @pytest.mark.asyncio\n    async def test_context_manager(self):\n        \"\"\"Test connection pool context manager.\"\"\"\n        pool = AsyncConnectionPool(self.create_mock_connection, max_size=1)\n        \n        async with pool.connection() as conn:\n            assert conn is not None\n            assert \"id\" in conn\n            \n            # Check that connection is active\n            stats = pool.get_stats()\n            assert stats[\"active_connections\"] == 1\n        \n        # After context, connection should be returned\n        stats = pool.get_stats()\n        assert stats[\"active_connections\"] == 0\n        assert stats[\"pool_size\"] == 1\n    \n    @pytest.mark.asyncio\n    async def test_pool_reuse(self):\n        \"\"\"Test that connections are reused from pool.\"\"\"\n        pool = AsyncConnectionPool(self.create_mock_connection, max_size=1)\n        \n        # Get and return a connection\n        conn1 = await pool.get_connection()\n        conn1_id = conn1[\"id\"]\n        await pool.return_connection(conn1)\n        \n        # Get connection again - should be the same one\n        conn2 = await pool.get_connection()\n        assert conn2[\"id\"] == conn1_id\n        \n        await pool.return_connection(conn2)\n\n\nclass TestStreamingAIProvider:\n    \"\"\"Test StreamingAIProvider implementation.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_stream_completion(self):\n        \"\"\"Test streaming completion functionality.\"\"\"\n        provider = StreamingAIProvider(response_delay=0.01)\n        messages = [Message(role=\"user\", content=\"Hello\")]\n        \n        chunks = []\n        async for chunk in provider.stream_completion(\"gpt-4\", messages):\n            chunks.append(chunk)\n        \n        # Should have multiple chunks\n        assert len(chunks) > 1\n        \n        # Check chunk properties\n        for i, chunk in enumerate(chunks):\n            assert isinstance(chunk, StreamingChunk)\n            assert chunk.chunk_id == i\n            assert chunk.metadata[\"model\"] == \"gpt-4\"\n            \n            # Last chunk should be marked as final\n            if i == len(chunks) - 1:\n                assert chunk.is_final is True\n            else:\n                assert chunk.is_final is False\n        \n        # Reconstruct full response\n        full_response = \"\".join(chunk.content for chunk in chunks)\n        assert \"Hello\" in full_response\n        assert \"streaming response\" in full_response\n    \n    @pytest.mark.asyncio\n    async def test_call_count(self):\n        \"\"\"Test that call count is tracked.\"\"\"\n        provider = StreamingAIProvider(response_delay=0.01)\n        messages = [Message(role=\"user\", content=\"Test\")]\n        \n        initial_count = provider.call_count\n        \n        async for chunk in provider.stream_completion(\"gpt-4\", messages):\n            pass\n        \n        assert provider.call_count == initial_count + 1\n\n\nclass TestConcurrentInferenceEngine:\n    \"\"\"Test ConcurrentInferenceEngine implementation.\"\"\"\n    \n    async def mock_inference(self, text: str, delay: float = 0.01) -> str:\n        \"\"\"Mock inference function for testing.\"\"\"\n        await asyncio.sleep(delay)\n        return f\"Processed: {text}\"\n    \n    @pytest.mark.asyncio\n    async def test_submit_and_wait(self):\n        \"\"\"Test task submission and waiting.\"\"\"\n        engine = ConcurrentInferenceEngine(max_concurrent=2)\n        \n        # Submit a task\n        task_id = await engine.submit_task(\n            \"test_task\",\n            self.mock_inference,\n            \"test input\",\n            0.01\n        )\n        \n        assert task_id == \"test_task\"\n        \n        # Wait for completion\n        result = await engine.wait_for_task(task_id)\n        assert result == \"Processed: test input\"\n        \n        # Check stats\n        stats = engine.get_stats()\n        assert stats[\"completed_tasks\"] == 1\n        assert stats[\"active_tasks\"] == 0\n    \n    @pytest.mark.asyncio\n    async def test_concurrent_execution(self):\n        \"\"\"Test concurrent execution of multiple tasks.\"\"\"\n        engine = ConcurrentInferenceEngine(max_concurrent=3)\n        \n        # Submit multiple tasks\n        tasks = []\n        for i in range(5):\n            task_id = await engine.submit_task(\n                f\"task_{i}\",\n                self.mock_inference,\n                f\"input_{i}\",\n                0.02\n            )\n            tasks.append(task_id)\n        \n        # Wait for all tasks\n        results = await engine.wait_for_all()\n        \n        assert len(results) == 5\n        for result in results:\n            assert result.startswith(\"Processed: input_\")\n        \n        # Check final stats\n        stats = engine.get_stats()\n        assert stats[\"completed_tasks\"] == 5\n        assert stats[\"active_tasks\"] == 0\n    \n    @pytest.mark.asyncio\n    async def test_error_handling(self):\n        \"\"\"Test error handling in concurrent execution.\"\"\"\n        engine = ConcurrentInferenceEngine(max_concurrent=2)\n        \n        async def failing_task(text: str) -> str:\n            raise ValueError(f\"Failed to process: {text}\")\n        \n        # Submit failing task\n        task_id = await engine.submit_task(\"failing_task\", failing_task, \"test\")\n        \n        # Should raise the original exception\n        with pytest.raises(ValueError, match=\"Failed to process: test\"):\n            await engine.wait_for_task(task_id)\n        \n        # Check that error is recorded\n        assert len(engine.completed_tasks) == 1\n        completed_task = engine.completed_tasks[0]\n        assert completed_task[\"status\"] == \"error\"\n        assert \"Failed to process\" in completed_task[\"error\"]\n\n\nclass TestAsyncBatchProcessor:\n    \"\"\"Test async_batch_processor function.\"\"\"\n    \n    async def mock_processor(self, item: str) -> str:\n        \"\"\"Mock processor for testing.\"\"\"\n        await asyncio.sleep(0.001)\n        return f\"processed_{item}\"\n    \n    @pytest.mark.asyncio\n    async def test_batch_processing(self):\n        \"\"\"Test basic batch processing.\"\"\"\n        items = [f\"item_{i}\" for i in range(10)]\n        \n        results = await async_batch_processor(\n            items=items,\n            processor=self.mock_processor,\n            batch_size=3,\n            max_concurrent=2\n        )\n        \n        assert len(results) == 10\n        for i, result in enumerate(results):\n            assert result == f\"processed_item_{i}\"\n    \n    @pytest.mark.asyncio\n    async def test_batch_size_and_concurrency(self):\n        \"\"\"Test different batch sizes and concurrency levels.\"\"\"\n        items = [f\"item_{i}\" for i in range(7)]\n        \n        # Test with batch_size=2, should create 4 batches (2,2,2,1)\n        results = await async_batch_processor(\n            items=items,\n            processor=self.mock_processor,\n            batch_size=2,\n            max_concurrent=1\n        )\n        \n        assert len(results) == 7\n        assert all(result.startswith(\"processed_\") for result in results)\n    \n    @pytest.mark.asyncio\n    async def test_error_handling_in_batch(self):\n        \"\"\"Test error handling within batch processing.\"\"\"\n        \n        async def sometimes_failing_processor(item: str) -> str:\n            if \"fail\" in item:\n                raise ValueError(f\"Processing failed for {item}\")\n            await asyncio.sleep(0.001)\n            return f\"processed_{item}\"\n        \n        items = [\"item_1\", \"item_fail\", \"item_3\"]\n        \n        results = await async_batch_processor(\n            items=items,\n            processor=sometimes_failing_processor,\n            batch_size=2,\n            max_concurrent=1\n        )\n        \n        assert len(results) == 3\n        assert results[0] == \"processed_item_1\"\n        assert isinstance(results[1], ValueError)  # Exception in results\n        assert results[2] == \"processed_item_3\"\n\n\nclass TestAsyncResourceManager:\n    \"\"\"Test async_resource_manager context manager.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_resource_lifecycle(self):\n        \"\"\"Test resource creation and cleanup.\"\"\"\n        created_resources = []\n        cleaned_resources = []\n        \n        async def create_resource():\n            resource = {\"id\": \"test_resource\", \"active\": True}\n            created_resources.append(resource)\n            return resource\n        \n        async def cleanup_resource(resource):\n            resource[\"active\"] = False\n            cleaned_resources.append(resource)\n        \n        async with async_resource_manager(create_resource, cleanup_resource) as resource:\n            assert resource[\"id\"] == \"test_resource\"\n            assert resource[\"active\"] is True\n            assert len(created_resources) == 1\n            assert len(cleaned_resources) == 0\n        \n        # After context, cleanup should have been called\n        assert len(cleaned_resources) == 1\n        assert cleaned_resources[0][\"active\"] is False\n    \n    @pytest.mark.asyncio\n    async def test_resource_cleanup_on_exception(self):\n        \"\"\"Test that resources are cleaned up even when exceptions occur.\"\"\"\n        cleanup_called = []\n        \n        async def create_resource():\n            return {\"id\": \"test\"}\n        \n        async def cleanup_resource(resource):\n            cleanup_called.append(True)\n        \n        with pytest.raises(ValueError, match=\"Test exception\"):\n            async with async_resource_manager(create_resource, cleanup_resource) as resource:\n                raise ValueError(\"Test exception\")\n        \n        # Cleanup should still be called\n        assert len(cleanup_called) == 1\n    \n    @pytest.mark.asyncio\n    async def test_no_cleanup_function(self):\n        \"\"\"Test resource manager without cleanup function.\"\"\"\n        \n        async def create_resource():\n            return {\"id\": \"no_cleanup\"}\n        \n        # Should work without cleanup function\n        async with async_resource_manager(create_resource) as resource:\n            assert resource[\"id\"] == \"no_cleanup\"\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2954,"cache_read_input_tokens":65854,"output_tokens":3769}},"requestId":"req_vrtx_011CRddSa8NbH2Yg9TVJMfYQ","type":"assistant","uuid":"a8424021-9fbc-4846-8873-961467282b44","timestamp":"2025-07-30T17:53:07.954Z"}
{"parentUuid":"a8424021-9fbc-4846-8873-961467282b44","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01BpGwQwtxfsWug987XtPkoA","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_async_patterns.py"}]},"uuid":"badfe4ea-98ba-44a0-96e9-dfa20d3cc7a2","timestamp":"2025-07-30T17:53:07.982Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_async_patterns.py","content":"#!/usr/bin/env python3\n\"\"\"\nTests for Async Patterns Examples\n\nRun with: python -m pytest 03-python-idioms/tests/test_async_patterns.py -v\n\"\"\"\n\nimport pytest\nimport asyncio\nimport time\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom examples.example_02_async_patterns import (\n    StreamingChunk, AsyncRateLimiter, AsyncConnectionPool,\n    StreamingAIProvider, ConcurrentInferenceEngine,\n    async_batch_processor, async_resource_manager\n)\nfrom common import Message\n\n\nclass TestStreamingChunk:\n    \"\"\"Test StreamingChunk dataclass.\"\"\"\n    \n    def test_basic_creation(self):\n        \"\"\"Test basic StreamingChunk creation.\"\"\"\n        chunk = StreamingChunk(\n            content=\"Hello\",\n            chunk_id=1,\n            is_final=False\n        )\n        \n        assert chunk.content == \"Hello\"\n        assert chunk.chunk_id == 1\n        assert chunk.is_final is False\n        assert chunk.metadata == {}\n    \n    def test_with_metadata(self):\n        \"\"\"Test StreamingChunk with metadata.\"\"\"\n        metadata = {\"model\": \"gpt-4\", \"timestamp\": 123456}\n        chunk = StreamingChunk(\n            content=\"World\",\n            chunk_id=2,\n            is_final=True,\n            metadata=metadata\n        )\n        \n        assert chunk.metadata == metadata\n        assert chunk.is_final is True\n\n\nclass TestAsyncRateLimiter:\n    \"\"\"Test AsyncRateLimiter implementation.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_basic_rate_limiting(self):\n        \"\"\"Test basic rate limiting functionality.\"\"\"\n        # Allow 2 calls per 0.5 seconds\n        limiter = AsyncRateLimiter(max_calls=2, time_window=0.5)\n        \n        # First two calls should be immediate\n        start_time = time.time()\n        await limiter.acquire()\n        await limiter.acquire()\n        first_two_time = time.time() - start_time\n        \n        # Should be very fast (< 0.1 seconds)\n        assert first_two_time < 0.1\n        \n        # Third call should be delayed\n        start_time = time.time()\n        await limiter.acquire()\n        third_call_time = time.time() - start_time\n        \n        # Should wait approximately 0.5 seconds\n        assert 0.4 < third_call_time < 0.7\n    \n    @pytest.mark.asyncio\n    async def test_stats(self):\n        \"\"\"Test rate limiter statistics.\"\"\"\n        limiter = AsyncRateLimiter(max_calls=3, time_window=1.0)\n        \n        # Initial stats\n        stats = limiter.get_stats()\n        assert stats[\"current_calls\"] == 0\n        assert stats[\"max_calls\"] == 3\n        assert stats[\"time_window\"] == 1.0\n        assert stats[\"utilization\"] == 0.0\n        \n        # After some calls\n        await limiter.acquire()\n        await limiter.acquire()\n        \n        stats = limiter.get_stats()\n        assert stats[\"current_calls\"] == 2\n        assert stats[\"utilization\"] == 2/3\n\n\nclass TestAsyncConnectionPool:\n    \"\"\"Test AsyncConnectionPool implementation.\"\"\"\n    \n    async def create_mock_connection(self):\n        \"\"\"Create a mock connection.\"\"\"\n        await asyncio.sleep(0.01)  # Simulate connection time\n        return {\"id\": id(object()), \"created_at\": time.time()}\n    \n    @pytest.mark.asyncio\n    async def test_connection_creation(self):\n        \"\"\"Test connection pool creation and basic operations.\"\"\"\n        pool = AsyncConnectionPool(self.create_mock_connection, max_size=2)\n        \n        # Test getting connections\n        conn1 = await pool.get_connection()\n        conn2 = await pool.get_connection()\n        \n        assert conn1 is not None\n        assert conn2 is not None\n        assert conn1[\"id\"] != conn2[\"id\"]\n        \n        # Return connections\n        await pool.return_connection(conn1)\n        await pool.return_connection(conn2)\n        \n        stats = pool.get_stats()\n        assert stats[\"created_count\"] == 2\n        assert stats[\"active_connections\"] == 0\n        assert stats[\"pool_size\"] == 2\n    \n    @pytest.mark.asyncio\n    async def test_context_manager(self):\n        \"\"\"Test connection pool context manager.\"\"\"\n        pool = AsyncConnectionPool(self.create_mock_connection, max_size=1)\n        \n        async with pool.connection() as conn:\n            assert conn is not None\n            assert \"id\" in conn\n            \n            # Check that connection is active\n            stats = pool.get_stats()\n            assert stats[\"active_connections\"] == 1\n        \n        # After context, connection should be returned\n        stats = pool.get_stats()\n        assert stats[\"active_connections\"] == 0\n        assert stats[\"pool_size\"] == 1\n    \n    @pytest.mark.asyncio\n    async def test_pool_reuse(self):\n        \"\"\"Test that connections are reused from pool.\"\"\"\n        pool = AsyncConnectionPool(self.create_mock_connection, max_size=1)\n        \n        # Get and return a connection\n        conn1 = await pool.get_connection()\n        conn1_id = conn1[\"id\"]\n        await pool.return_connection(conn1)\n        \n        # Get connection again - should be the same one\n        conn2 = await pool.get_connection()\n        assert conn2[\"id\"] == conn1_id\n        \n        await pool.return_connection(conn2)\n\n\nclass TestStreamingAIProvider:\n    \"\"\"Test StreamingAIProvider implementation.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_stream_completion(self):\n        \"\"\"Test streaming completion functionality.\"\"\"\n        provider = StreamingAIProvider(response_delay=0.01)\n        messages = [Message(role=\"user\", content=\"Hello\")]\n        \n        chunks = []\n        async for chunk in provider.stream_completion(\"gpt-4\", messages):\n            chunks.append(chunk)\n        \n        # Should have multiple chunks\n        assert len(chunks) > 1\n        \n        # Check chunk properties\n        for i, chunk in enumerate(chunks):\n            assert isinstance(chunk, StreamingChunk)\n            assert chunk.chunk_id == i\n            assert chunk.metadata[\"model\"] == \"gpt-4\"\n            \n            # Last chunk should be marked as final\n            if i == len(chunks) - 1:\n                assert chunk.is_final is True\n            else:\n                assert chunk.is_final is False\n        \n        # Reconstruct full response\n        full_response = \"\".join(chunk.content for chunk in chunks)\n        assert \"Hello\" in full_response\n        assert \"streaming response\" in full_response\n    \n    @pytest.mark.asyncio\n    async def test_call_count(self):\n        \"\"\"Test that call count is tracked.\"\"\"\n        provider = StreamingAIProvider(response_delay=0.01)\n        messages = [Message(role=\"user\", content=\"Test\")]\n        \n        initial_count = provider.call_count\n        \n        async for chunk in provider.stream_completion(\"gpt-4\", messages):\n            pass\n        \n        assert provider.call_count == initial_count + 1\n\n\nclass TestConcurrentInferenceEngine:\n    \"\"\"Test ConcurrentInferenceEngine implementation.\"\"\"\n    \n    async def mock_inference(self, text: str, delay: float = 0.01) -> str:\n        \"\"\"Mock inference function for testing.\"\"\"\n        await asyncio.sleep(delay)\n        return f\"Processed: {text}\"\n    \n    @pytest.mark.asyncio\n    async def test_submit_and_wait(self):\n        \"\"\"Test task submission and waiting.\"\"\"\n        engine = ConcurrentInferenceEngine(max_concurrent=2)\n        \n        # Submit a task\n        task_id = await engine.submit_task(\n            \"test_task\",\n            self.mock_inference,\n            \"test input\",\n            0.01\n        )\n        \n        assert task_id == \"test_task\"\n        \n        # Wait for completion\n        result = await engine.wait_for_task(task_id)\n        assert result == \"Processed: test input\"\n        \n        # Check stats\n        stats = engine.get_stats()\n        assert stats[\"completed_tasks\"] == 1\n        assert stats[\"active_tasks\"] == 0\n    \n    @pytest.mark.asyncio\n    async def test_concurrent_execution(self):\n        \"\"\"Test concurrent execution of multiple tasks.\"\"\"\n        engine = ConcurrentInferenceEngine(max_concurrent=3)\n        \n        # Submit multiple tasks\n        tasks = []\n        for i in range(5):\n            task_id = await engine.submit_task(\n                f\"task_{i}\",\n                self.mock_inference,\n                f\"input_{i}\",\n                0.02\n            )\n            tasks.append(task_id)\n        \n        # Wait for all tasks\n        results = await engine.wait_for_all()\n        \n        assert len(results) == 5\n        for result in results:\n            assert result.startswith(\"Processed: input_\")\n        \n        # Check final stats\n        stats = engine.get_stats()\n        assert stats[\"completed_tasks\"] == 5\n        assert stats[\"active_tasks\"] == 0\n    \n    @pytest.mark.asyncio\n    async def test_error_handling(self):\n        \"\"\"Test error handling in concurrent execution.\"\"\"\n        engine = ConcurrentInferenceEngine(max_concurrent=2)\n        \n        async def failing_task(text: str) -> str:\n            raise ValueError(f\"Failed to process: {text}\")\n        \n        # Submit failing task\n        task_id = await engine.submit_task(\"failing_task\", failing_task, \"test\")\n        \n        # Should raise the original exception\n        with pytest.raises(ValueError, match=\"Failed to process: test\"):\n            await engine.wait_for_task(task_id)\n        \n        # Check that error is recorded\n        assert len(engine.completed_tasks) == 1\n        completed_task = engine.completed_tasks[0]\n        assert completed_task[\"status\"] == \"error\"\n        assert \"Failed to process\" in completed_task[\"error\"]\n\n\nclass TestAsyncBatchProcessor:\n    \"\"\"Test async_batch_processor function.\"\"\"\n    \n    async def mock_processor(self, item: str) -> str:\n        \"\"\"Mock processor for testing.\"\"\"\n        await asyncio.sleep(0.001)\n        return f\"processed_{item}\"\n    \n    @pytest.mark.asyncio\n    async def test_batch_processing(self):\n        \"\"\"Test basic batch processing.\"\"\"\n        items = [f\"item_{i}\" for i in range(10)]\n        \n        results = await async_batch_processor(\n            items=items,\n            processor=self.mock_processor,\n            batch_size=3,\n            max_concurrent=2\n        )\n        \n        assert len(results) == 10\n        for i, result in enumerate(results):\n            assert result == f\"processed_item_{i}\"\n    \n    @pytest.mark.asyncio\n    async def test_batch_size_and_concurrency(self):\n        \"\"\"Test different batch sizes and concurrency levels.\"\"\"\n        items = [f\"item_{i}\" for i in range(7)]\n        \n        # Test with batch_size=2, should create 4 batches (2,2,2,1)\n        results = await async_batch_processor(\n            items=items,\n            processor=self.mock_processor,\n            batch_size=2,\n            max_concurrent=1\n        )\n        \n        assert len(results) == 7\n        assert all(result.startswith(\"processed_\") for result in results)\n    \n    @pytest.mark.asyncio\n    async def test_error_handling_in_batch(self):\n        \"\"\"Test error handling within batch processing.\"\"\"\n        \n        async def sometimes_failing_processor(item: str) -> str:\n            if \"fail\" in item:\n                raise ValueError(f\"Processing failed for {item}\")\n            await asyncio.sleep(0.001)\n            return f\"processed_{item}\"\n        \n        items = [\"item_1\", \"item_fail\", \"item_3\"]\n        \n        results = await async_batch_processor(\n            items=items,\n            processor=sometimes_failing_processor,\n            batch_size=2,\n            max_concurrent=1\n        )\n        \n        assert len(results) == 3\n        assert results[0] == \"processed_item_1\"\n        assert isinstance(results[1], ValueError)  # Exception in results\n        assert results[2] == \"processed_item_3\"\n\n\nclass TestAsyncResourceManager:\n    \"\"\"Test async_resource_manager context manager.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_resource_lifecycle(self):\n        \"\"\"Test resource creation and cleanup.\"\"\"\n        created_resources = []\n        cleaned_resources = []\n        \n        async def create_resource():\n            resource = {\"id\": \"test_resource\", \"active\": True}\n            created_resources.append(resource)\n            return resource\n        \n        async def cleanup_resource(resource):\n            resource[\"active\"] = False\n            cleaned_resources.append(resource)\n        \n        async with async_resource_manager(create_resource, cleanup_resource) as resource:\n            assert resource[\"id\"] == \"test_resource\"\n            assert resource[\"active\"] is True\n            assert len(created_resources) == 1\n            assert len(cleaned_resources) == 0\n        \n        # After context, cleanup should have been called\n        assert len(cleaned_resources) == 1\n        assert cleaned_resources[0][\"active\"] is False\n    \n    @pytest.mark.asyncio\n    async def test_resource_cleanup_on_exception(self):\n        \"\"\"Test that resources are cleaned up even when exceptions occur.\"\"\"\n        cleanup_called = []\n        \n        async def create_resource():\n            return {\"id\": \"test\"}\n        \n        async def cleanup_resource(resource):\n            cleanup_called.append(True)\n        \n        with pytest.raises(ValueError, match=\"Test exception\"):\n            async with async_resource_manager(create_resource, cleanup_resource) as resource:\n                raise ValueError(\"Test exception\")\n        \n        # Cleanup should still be called\n        assert len(cleanup_called) == 1\n    \n    @pytest.mark.asyncio\n    async def test_no_cleanup_function(self):\n        \"\"\"Test resource manager without cleanup function.\"\"\"\n        \n        async def create_resource():\n            return {\"id\": \"no_cleanup\"}\n        \n        # Should work without cleanup function\n        async with async_resource_manager(create_resource) as resource:\n            assert resource[\"id\"] == \"no_cleanup\"\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])","structuredPatch":[]}}
{"parentUuid":"badfe4ea-98ba-44a0-96e9-dfa20d3cc7a2","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01CGHSXhEk4P6fnRWxqEeVGd","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01XS3cjXouuGRoZVmymAzoai","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_functional_programming.py","content":"#!/usr/bin/env python3\n\"\"\"\nTests for Functional Programming Examples\n\nRun with: python -m pytest 03-python-idioms/tests/test_functional_programming.py -v\n\"\"\"\n\nimport pytest\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom examples.example_03_functional_programming import (\n    ImmutableConfig, ModelPrediction, tokenize_text, count_tokens,\n    filter_tokens, calculate_text_stats, create_text_processor,\n    create_confidence_filter, FunctionalPipeline, curry_model_predict,\n    Result, safe_divide, safe_model_predict, compose, pipe,\n    expensive_text_analysis\n)\nfrom dataclasses import replace\n\n\nclass TestImmutableConfig:\n    \"\"\"Test ImmutableConfig immutable dataclass.\"\"\"\n    \n    def test_basic_creation(self):\n        \"\"\"Test basic configuration creation.\"\"\"\n        config = ImmutableConfig(\n            model_name=\"gpt-4\",\n            temperature=0.7,\n            max_tokens=1000,\n            provider=\"openai\"\n        )\n        \n        assert config.model_name == \"gpt-4\"\n        assert config.temperature == 0.7\n        assert config.max_tokens == 1000\n        assert config.provider == \"openai\"\n        assert config.api_key is None\n    \n    def test_immutability(self):\n        \"\"\"Test that config is immutable.\"\"\"\n        config = ImmutableConfig(\n            model_name=\"gpt-4\",\n            temperature=0.7,\n            max_tokens=1000,\n            provider=\"openai\"\n        )\n        \n        # Should not be able to modify\n        with pytest.raises(AttributeError):\n            config.temperature = 0.8\n    \n    def test_with_temperature(self):\n        \"\"\"Test creating new config with different temperature.\"\"\"\n        original = ImmutableConfig(\n            model_name=\"gpt-4\",\n            temperature=0.7,\n            max_tokens=1000,\n            provider=\"openai\"\n        )\n        \n        new_config = original.with_temperature(0.9)\n        \n        # Original should be unchanged\n        assert original.temperature == 0.7\n        # New config should have new temperature\n        assert new_config.temperature == 0.9\n        # Other fields should be the same\n        assert new_config.model_name == original.model_name\n        assert new_config.max_tokens == original.max_tokens\n        assert new_config.provider == original.provider\n    \n    def test_with_provider(self):\n        \"\"\"Test creating new config with different provider.\"\"\"\n        original = ImmutableConfig(\n            model_name=\"gpt-4\",\n            temperature=0.7,\n            max_tokens=1000,\n            provider=\"openai\"\n        )\n        \n        new_config = original.with_provider(\"anthropic\", \"api-key-123\")\n        \n        # Original should be unchanged\n        assert original.provider == \"openai\"\n        assert original.api_key is None\n        # New config should have new provider and API key\n        assert new_config.provider == \"anthropic\"\n        assert new_config.api_key == \"api-key-123\"\n\n\nclass TestModelPrediction:\n    \"\"\"Test ModelPrediction immutable dataclass.\"\"\"\n    \n    def test_basic_creation(self):\n        \"\"\"Test basic prediction creation.\"\"\"\n        prediction = ModelPrediction(\n            text=\"Hello world\",\n            confidence=0.95,\n            model_used=\"gpt-4\",\n            processing_time=0.1,\n            metadata={\"temperature\": 0.7}\n        )\n        \n        assert prediction.text == \"Hello world\"\n        assert prediction.confidence == 0.95\n        assert prediction.model_used == \"gpt-4\"\n        assert prediction.processing_time == 0.1\n        assert prediction.metadata == {\"temperature\": 0.7}\n    \n    def test_with_confidence_threshold(self):\n        \"\"\"Test confidence threshold filtering.\"\"\"\n        prediction = ModelPrediction(\n            text=\"Test\",\n            confidence=0.85,\n            model_used=\"gpt-4\",\n            processing_time=0.1,\n            metadata={}\n        )\n        \n        # Should pass threshold\n        result_pass = prediction.with_confidence_threshold(0.8)\n        assert result_pass is prediction\n        \n        # Should fail threshold\n        result_fail = prediction.with_confidence_threshold(0.9)\n        assert result_fail is None\n    \n    def test_transform_text(self):\n        \"\"\"Test text transformation while preserving other fields.\"\"\"\n        prediction = ModelPrediction(\n            text=\"hello world\",\n            confidence=0.95,\n            model_used=\"gpt-4\",\n            processing_time=0.1,\n            metadata={\"test\": True}\n        )\n        \n        # Transform text to uppercase\n        transformed = prediction.transform_text(str.upper)\n        \n        # Text should be transformed\n        assert transformed.text == \"HELLO WORLD\"\n        # Other fields should be preserved\n        assert transformed.confidence == prediction.confidence\n        assert transformed.model_used == prediction.model_used\n        assert transformed.processing_time == prediction.processing_time\n        assert transformed.metadata == prediction.metadata\n\n\nclass TestPureFunctions:\n    \"\"\"Test pure functions for text processing.\"\"\"\n    \n    def test_tokenize_text(self):\n        \"\"\"Test text tokenization.\"\"\"\n        result = tokenize_text(\"Hello World Python\")\n        assert result == [\"hello\", \"world\", \"python\"]\n        \n        # Test with punctuation\n        result = tokenize_text(\"Hello, World!\")\n        assert result == [\"hello,\", \"world!\"]\n    \n    def test_count_tokens(self):\n        \"\"\"Test token counting.\"\"\"\n        tokens = [\"hello\", \"world\", \"python\"]\n        assert count_tokens(tokens) == 3\n        \n        assert count_tokens([]) == 0\n    \n    def test_filter_tokens(self):\n        \"\"\"Test token filtering by length.\"\"\"\n        tokens = [\"a\", \"bb\", \"ccc\", \"dddd\"]\n        \n        # Default min_length=2\n        result = filter_tokens(tokens)\n        assert result == [\"bb\", \"ccc\", \"dddd\"]\n        \n        # Custom min_length=3\n        result = filter_tokens(tokens, min_length=3)\n        assert result == [\"ccc\", \"dddd\"]\n        \n        # No tokens pass filter\n        result = filter_tokens(tokens, min_length=5)\n        assert result == []\n    \n    def test_calculate_text_stats(self):\n        \"\"\"Test text statistics calculation.\"\"\"\n        text = \"Hello world hello\"\n        stats = calculate_text_stats(text)\n        \n        assert stats[\"character_count\"] == 17\n        assert stats[\"word_count\"] == 3\n        assert stats[\"unique_words\"] == 2  # \"hello\" appears twice\n        assert stats[\"filtered_words\"] == 2  # Both \"hello\" and \"world\" are >=2 chars\n        assert stats[\"avg_word_length\"] == (5 + 5 + 5) / 3  # hello, world, hello\n\n\nclass TestHigherOrderFunctions:\n    \"\"\"Test higher-order functions.\"\"\"\n    \n    def test_create_text_processor(self):\n        \"\"\"Test text processor creation.\"\"\"\n        transformers = [\n            str.lower,\n            lambda text: text.replace(\"hello\", \"hi\"),\n            str.upper\n        ]\n        \n        processor = create_text_processor(transformers)\n        result = processor(\"Hello World\")\n        \n        # Should apply transformations in order: lower -> replace -> upper\n        assert result == \"HI WORLD\"\n    \n    def test_create_confidence_filter(self):\n        \"\"\"Test confidence filter creation.\"\"\"\n        filter_func = create_confidence_filter(0.8)\n        \n        high_conf = ModelPrediction(\"test\", 0.9, \"model\", 0.1, {})\n        low_conf = ModelPrediction(\"test\", 0.7, \"model\", 0.1, {})\n        \n        assert filter_func(high_conf) is True\n        assert filter_func(low_conf) is False\n\n\nclass TestFunctionComposition:\n    \"\"\"Test function composition utilities.\"\"\"\n    \n    def test_compose(self):\n        \"\"\"Test mathematical function composition (right to left).\"\"\"\n        add_one = lambda x: x + 1\n        multiply_two = lambda x: x * 2\n        \n        # compose(f, g)(x) = f(g(x))\n        composed = compose(add_one, multiply_two)\n        result = composed(3)  # multiply_two(3) = 6, add_one(6) = 7\n        assert result == 7\n    \n    def test_pipe(self):\n        \"\"\"Test pipe function composition (left to right).\"\"\"\n        add_one = lambda x: x + 1\n        multiply_two = lambda x: x * 2\n        \n        # pipe(f, g)(x) = g(f(x))\n        piped = pipe(add_one, multiply_two)\n        result = piped(3)  # add_one(3) = 4, multiply_two(4) = 8\n        assert result == 8\n    \n    def test_multiple_functions(self):\n        \"\"\"Test composition with multiple functions.\"\"\"\n        f1 = lambda x: x + 1\n        f2 = lambda x: x * 2\n        f3 = lambda x: x - 5\n        \n        piped = pipe(f1, f2, f3)\n        result = piped(3)  # (3+1)*2-5 = 3\n        assert result == 3\n\n\nclass TestFunctionalPipeline:\n    \"\"\"Test FunctionalPipeline class.\"\"\"\n    \n    def test_empty_pipeline(self):\n        \"\"\"Test empty pipeline.\"\"\"\n        pipeline = FunctionalPipeline()\n        result = pipeline.process(\"test\")\n        assert result == \"test\"\n    \n    def test_add_steps(self):\n        \"\"\"Test adding steps to pipeline.\"\"\"\n        pipeline = FunctionalPipeline()\n        new_pipeline = pipeline.add_step(str.upper)\n        newer_pipeline = new_pipeline.add_step(lambda x: x + \"!\")\n        \n        # Original pipeline should be unchanged\n        assert len(pipeline.steps) == 0\n        assert len(new_pipeline.steps) == 1\n        assert len(newer_pipeline.steps) == 2\n        \n        # Test processing\n        result = newer_pipeline.process(\"hello\")\n        assert result == \"HELLO!\"\n    \n    def test_callable_interface(self):\n        \"\"\"Test that pipeline is callable.\"\"\"\n        pipeline = (FunctionalPipeline()\n                   .add_step(str.strip)\n                   .add_step(str.upper)\n                   .add_step(lambda x: f\"[{x}]\"))\n        \n        result = pipeline(\"  hello world  \")\n        assert result == \"[HELLO WORLD]\"\n\n\nclass TestCurrying:\n    \"\"\"Test currying and partial application.\"\"\"\n    \n    def test_curry_model_predict(self):\n        \"\"\"Test curried model prediction function.\"\"\"\n        predictor = curry_model_predict(\"gpt-4\", 0.8)\n        \n        # Should return a function\n        assert callable(predictor)\n        \n        # Test prediction\n        result = predictor(\"Hello world\")\n        assert isinstance(result, ModelPrediction)\n        assert result.text == \"Hello world\"\n        assert result.model_used == \"gpt-4\"\n        assert result.metadata[\"temperature\"] == 0.8\n        assert 0.8 <= result.confidence <= 1.0\n    \n    def test_deterministic_prediction(self):\n        \"\"\"Test that predictions are deterministic for same input.\"\"\"\n        predictor = curry_model_predict(\"gpt-4\", 0.7)\n        \n        result1 = predictor(\"test input\")\n        result2 = predictor(\"test input\")\n        \n        # Should be identical\n        assert result1.confidence == result2.confidence\n        assert result1.processing_time == result2.processing_time\n\n\nclass TestResultMonad:\n    \"\"\"Test Result monad for error handling.\"\"\"\n    \n    def test_success_creation(self):\n        \"\"\"Test creating successful results.\"\"\"\n        result = Result.success(42)\n        assert result.is_success is True\n        assert result.value == 42\n        assert result.error is None\n    \n    def test_failure_creation(self):\n        \"\"\"Test creating failed results.\"\"\"\n        result = Result.failure(\"Something went wrong\")\n        assert result.is_success is False\n        assert result.value is None\n        assert result.error == \"Something went wrong\"\n    \n    def test_map_success(self):\n        \"\"\"Test mapping over successful result.\"\"\"\n        result = Result.success(10)\n        mapped = result.map(lambda x: x * 2)\n        \n        assert mapped.is_success is True\n        assert mapped.value == 20\n    \n    def test_map_failure(self):\n        \"\"\"Test mapping over failed result.\"\"\"\n        result = Result.failure(\"error\")\n        mapped = result.map(lambda x: x * 2)\n        \n        assert mapped.is_success is False\n        assert mapped.error == \"error\"\n    \n    def test_map_exception(self):\n        \"\"\"Test map handling exceptions.\"\"\"\n        result = Result.success(\"not a number\")\n        mapped = result.map(lambda x: int(x))  # Will raise ValueError\n        \n        assert mapped.is_success is False\n        assert \"invalid literal\" in mapped.error\n    \n    def test_flat_map(self):\n        \"\"\"Test flat_map operation.\"\"\"\n        result = Result.success(10)\n        flat_mapped = result.flat_map(lambda x: Result.success(x * 2))\n        \n        assert flat_mapped.is_success is True\n        assert flat_mapped.value == 20\n    \n    def test_filter(self):\n        \"\"\"Test filter operation.\"\"\"\n        result = Result.success(10)\n        \n        # Should pass filter\n        filtered_pass = result.filter(lambda x: x > 5)\n        assert filtered_pass.is_success is True\n        assert filtered_pass.value == 10\n        \n        # Should fail filter\n        filtered_fail = result.filter(lambda x: x > 15, \"Too small\")\n        assert filtered_fail.is_success is False\n        assert filtered_fail.error == \"Too small\"\n    \n    def test_get_or_else(self):\n        \"\"\"Test get_or_else operation.\"\"\"\n        success = Result.success(42)\n        failure = Result.failure(\"error\")\n        \n        assert success.get_or_else(0) == 42\n        assert failure.get_or_else(0) == 0\n\n\nclass TestSafeFunctions:\n    \"\"\"Test safe functions that return Result.\"\"\"\n    \n    def test_safe_divide_success(self):\n        \"\"\"Test successful safe division.\"\"\"\n        result = safe_divide(10, 2)\n        assert result.is_success is True\n        assert result.value == 5.0\n    \n    def test_safe_divide_by_zero(self):\n        \"\"\"Test division by zero.\"\"\"\n        result = safe_divide(10, 0)\n        assert result.is_success is False\n        assert \"Division by zero\" in result.error\n    \n    def test_safe_model_predict_success(self):\n        \"\"\"Test successful safe model prediction.\"\"\"\n        result = safe_model_predict(\"Hello world\")\n        assert result.is_success is True\n        assert isinstance(result.value, ModelPrediction)\n        assert result.value.text == \"Hello world\"\n        assert result.value.confidence == 0.85\n    \n    def test_safe_model_predict_empty_input(self):\n        \"\"\"Test safe prediction with empty input.\"\"\"\n        result = safe_model_predict(\"\")\n        assert result.is_success is False\n        assert \"Empty input text\" in result.error\n        \n        result = safe_model_predict(\"   \")\n        assert result.is_success is False\n        assert \"Empty input text\" in result.error\n    \n    def test_safe_model_predict_too_long(self):\n        \"\"\"Test safe prediction with too long input.\"\"\"\n        long_text = \"x\" * 1001\n        result = safe_model_predict(long_text)\n        assert result.is_success is False\n        assert \"Text too long\" in result.error\n\n\nclass TestMemoization:\n    \"\"\"Test memoization functionality.\"\"\"\n    \n    def test_expensive_text_analysis_caching(self):\n        \"\"\"Test that expensive analysis is cached.\"\"\"\n        # Clear any existing cache\n        expensive_text_analysis.cache_clear()\n        \n        text = \"test caching behavior\"\n        \n        # First call\n        result1 = expensive_text_analysis(text)\n        cache_info1 = expensive_text_analysis.cache_info()\n        \n        # Second call (should hit cache)\n        result2 = expensive_text_analysis(text)\n        cache_info2 = expensive_text_analysis.cache_info()\n        \n        # Results should be identical\n        assert result1 == result2\n        \n        # Cache should have been hit\n        assert cache_info2.hits == cache_info1.hits + 1\n        assert cache_info2.misses == cache_info1.misses\n    \n    def test_different_inputs_different_results(self):\n        \"\"\"Test that different inputs produce different cached results.\"\"\"\n        expensive_text_analysis.cache_clear()\n        \n        result1 = expensive_text_analysis(\"text one\")\n        result2 = expensive_text_analysis(\"text two\")\n        \n        # Should be different results\n        assert result1 != result2\n        \n        # Should have 2 cache misses\n        cache_info = expensive_text_analysis.cache_info()\n        assert cache_info.misses == 2\n        assert cache_info.hits == 0\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":3821,"cache_read_input_tokens":68808,"output_tokens":4501}},"requestId":"req_vrtx_011CRddWbQdF6d2TFTG3rtPW","type":"assistant","uuid":"af21dca8-1937-474c-951e-9f522eb9ef5a","timestamp":"2025-07-30T17:54:13.500Z"}
{"parentUuid":"af21dca8-1937-474c-951e-9f522eb9ef5a","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01XS3cjXouuGRoZVmymAzoai","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_functional_programming.py"}]},"uuid":"15e470c5-ea3f-46d3-940e-24c03b714d61","timestamp":"2025-07-30T17:54:13.533Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/tests/test_functional_programming.py","content":"#!/usr/bin/env python3\n\"\"\"\nTests for Functional Programming Examples\n\nRun with: python -m pytest 03-python-idioms/tests/test_functional_programming.py -v\n\"\"\"\n\nimport pytest\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom examples.example_03_functional_programming import (\n    ImmutableConfig, ModelPrediction, tokenize_text, count_tokens,\n    filter_tokens, calculate_text_stats, create_text_processor,\n    create_confidence_filter, FunctionalPipeline, curry_model_predict,\n    Result, safe_divide, safe_model_predict, compose, pipe,\n    expensive_text_analysis\n)\nfrom dataclasses import replace\n\n\nclass TestImmutableConfig:\n    \"\"\"Test ImmutableConfig immutable dataclass.\"\"\"\n    \n    def test_basic_creation(self):\n        \"\"\"Test basic configuration creation.\"\"\"\n        config = ImmutableConfig(\n            model_name=\"gpt-4\",\n            temperature=0.7,\n            max_tokens=1000,\n            provider=\"openai\"\n        )\n        \n        assert config.model_name == \"gpt-4\"\n        assert config.temperature == 0.7\n        assert config.max_tokens == 1000\n        assert config.provider == \"openai\"\n        assert config.api_key is None\n    \n    def test_immutability(self):\n        \"\"\"Test that config is immutable.\"\"\"\n        config = ImmutableConfig(\n            model_name=\"gpt-4\",\n            temperature=0.7,\n            max_tokens=1000,\n            provider=\"openai\"\n        )\n        \n        # Should not be able to modify\n        with pytest.raises(AttributeError):\n            config.temperature = 0.8\n    \n    def test_with_temperature(self):\n        \"\"\"Test creating new config with different temperature.\"\"\"\n        original = ImmutableConfig(\n            model_name=\"gpt-4\",\n            temperature=0.7,\n            max_tokens=1000,\n            provider=\"openai\"\n        )\n        \n        new_config = original.with_temperature(0.9)\n        \n        # Original should be unchanged\n        assert original.temperature == 0.7\n        # New config should have new temperature\n        assert new_config.temperature == 0.9\n        # Other fields should be the same\n        assert new_config.model_name == original.model_name\n        assert new_config.max_tokens == original.max_tokens\n        assert new_config.provider == original.provider\n    \n    def test_with_provider(self):\n        \"\"\"Test creating new config with different provider.\"\"\"\n        original = ImmutableConfig(\n            model_name=\"gpt-4\",\n            temperature=0.7,\n            max_tokens=1000,\n            provider=\"openai\"\n        )\n        \n        new_config = original.with_provider(\"anthropic\", \"api-key-123\")\n        \n        # Original should be unchanged\n        assert original.provider == \"openai\"\n        assert original.api_key is None\n        # New config should have new provider and API key\n        assert new_config.provider == \"anthropic\"\n        assert new_config.api_key == \"api-key-123\"\n\n\nclass TestModelPrediction:\n    \"\"\"Test ModelPrediction immutable dataclass.\"\"\"\n    \n    def test_basic_creation(self):\n        \"\"\"Test basic prediction creation.\"\"\"\n        prediction = ModelPrediction(\n            text=\"Hello world\",\n            confidence=0.95,\n            model_used=\"gpt-4\",\n            processing_time=0.1,\n            metadata={\"temperature\": 0.7}\n        )\n        \n        assert prediction.text == \"Hello world\"\n        assert prediction.confidence == 0.95\n        assert prediction.model_used == \"gpt-4\"\n        assert prediction.processing_time == 0.1\n        assert prediction.metadata == {\"temperature\": 0.7}\n    \n    def test_with_confidence_threshold(self):\n        \"\"\"Test confidence threshold filtering.\"\"\"\n        prediction = ModelPrediction(\n            text=\"Test\",\n            confidence=0.85,\n            model_used=\"gpt-4\",\n            processing_time=0.1,\n            metadata={}\n        )\n        \n        # Should pass threshold\n        result_pass = prediction.with_confidence_threshold(0.8)\n        assert result_pass is prediction\n        \n        # Should fail threshold\n        result_fail = prediction.with_confidence_threshold(0.9)\n        assert result_fail is None\n    \n    def test_transform_text(self):\n        \"\"\"Test text transformation while preserving other fields.\"\"\"\n        prediction = ModelPrediction(\n            text=\"hello world\",\n            confidence=0.95,\n            model_used=\"gpt-4\",\n            processing_time=0.1,\n            metadata={\"test\": True}\n        )\n        \n        # Transform text to uppercase\n        transformed = prediction.transform_text(str.upper)\n        \n        # Text should be transformed\n        assert transformed.text == \"HELLO WORLD\"\n        # Other fields should be preserved\n        assert transformed.confidence == prediction.confidence\n        assert transformed.model_used == prediction.model_used\n        assert transformed.processing_time == prediction.processing_time\n        assert transformed.metadata == prediction.metadata\n\n\nclass TestPureFunctions:\n    \"\"\"Test pure functions for text processing.\"\"\"\n    \n    def test_tokenize_text(self):\n        \"\"\"Test text tokenization.\"\"\"\n        result = tokenize_text(\"Hello World Python\")\n        assert result == [\"hello\", \"world\", \"python\"]\n        \n        # Test with punctuation\n        result = tokenize_text(\"Hello, World!\")\n        assert result == [\"hello,\", \"world!\"]\n    \n    def test_count_tokens(self):\n        \"\"\"Test token counting.\"\"\"\n        tokens = [\"hello\", \"world\", \"python\"]\n        assert count_tokens(tokens) == 3\n        \n        assert count_tokens([]) == 0\n    \n    def test_filter_tokens(self):\n        \"\"\"Test token filtering by length.\"\"\"\n        tokens = [\"a\", \"bb\", \"ccc\", \"dddd\"]\n        \n        # Default min_length=2\n        result = filter_tokens(tokens)\n        assert result == [\"bb\", \"ccc\", \"dddd\"]\n        \n        # Custom min_length=3\n        result = filter_tokens(tokens, min_length=3)\n        assert result == [\"ccc\", \"dddd\"]\n        \n        # No tokens pass filter\n        result = filter_tokens(tokens, min_length=5)\n        assert result == []\n    \n    def test_calculate_text_stats(self):\n        \"\"\"Test text statistics calculation.\"\"\"\n        text = \"Hello world hello\"\n        stats = calculate_text_stats(text)\n        \n        assert stats[\"character_count\"] == 17\n        assert stats[\"word_count\"] == 3\n        assert stats[\"unique_words\"] == 2  # \"hello\" appears twice\n        assert stats[\"filtered_words\"] == 2  # Both \"hello\" and \"world\" are >=2 chars\n        assert stats[\"avg_word_length\"] == (5 + 5 + 5) / 3  # hello, world, hello\n\n\nclass TestHigherOrderFunctions:\n    \"\"\"Test higher-order functions.\"\"\"\n    \n    def test_create_text_processor(self):\n        \"\"\"Test text processor creation.\"\"\"\n        transformers = [\n            str.lower,\n            lambda text: text.replace(\"hello\", \"hi\"),\n            str.upper\n        ]\n        \n        processor = create_text_processor(transformers)\n        result = processor(\"Hello World\")\n        \n        # Should apply transformations in order: lower -> replace -> upper\n        assert result == \"HI WORLD\"\n    \n    def test_create_confidence_filter(self):\n        \"\"\"Test confidence filter creation.\"\"\"\n        filter_func = create_confidence_filter(0.8)\n        \n        high_conf = ModelPrediction(\"test\", 0.9, \"model\", 0.1, {})\n        low_conf = ModelPrediction(\"test\", 0.7, \"model\", 0.1, {})\n        \n        assert filter_func(high_conf) is True\n        assert filter_func(low_conf) is False\n\n\nclass TestFunctionComposition:\n    \"\"\"Test function composition utilities.\"\"\"\n    \n    def test_compose(self):\n        \"\"\"Test mathematical function composition (right to left).\"\"\"\n        add_one = lambda x: x + 1\n        multiply_two = lambda x: x * 2\n        \n        # compose(f, g)(x) = f(g(x))\n        composed = compose(add_one, multiply_two)\n        result = composed(3)  # multiply_two(3) = 6, add_one(6) = 7\n        assert result == 7\n    \n    def test_pipe(self):\n        \"\"\"Test pipe function composition (left to right).\"\"\"\n        add_one = lambda x: x + 1\n        multiply_two = lambda x: x * 2\n        \n        # pipe(f, g)(x) = g(f(x))\n        piped = pipe(add_one, multiply_two)\n        result = piped(3)  # add_one(3) = 4, multiply_two(4) = 8\n        assert result == 8\n    \n    def test_multiple_functions(self):\n        \"\"\"Test composition with multiple functions.\"\"\"\n        f1 = lambda x: x + 1\n        f2 = lambda x: x * 2\n        f3 = lambda x: x - 5\n        \n        piped = pipe(f1, f2, f3)\n        result = piped(3)  # (3+1)*2-5 = 3\n        assert result == 3\n\n\nclass TestFunctionalPipeline:\n    \"\"\"Test FunctionalPipeline class.\"\"\"\n    \n    def test_empty_pipeline(self):\n        \"\"\"Test empty pipeline.\"\"\"\n        pipeline = FunctionalPipeline()\n        result = pipeline.process(\"test\")\n        assert result == \"test\"\n    \n    def test_add_steps(self):\n        \"\"\"Test adding steps to pipeline.\"\"\"\n        pipeline = FunctionalPipeline()\n        new_pipeline = pipeline.add_step(str.upper)\n        newer_pipeline = new_pipeline.add_step(lambda x: x + \"!\")\n        \n        # Original pipeline should be unchanged\n        assert len(pipeline.steps) == 0\n        assert len(new_pipeline.steps) == 1\n        assert len(newer_pipeline.steps) == 2\n        \n        # Test processing\n        result = newer_pipeline.process(\"hello\")\n        assert result == \"HELLO!\"\n    \n    def test_callable_interface(self):\n        \"\"\"Test that pipeline is callable.\"\"\"\n        pipeline = (FunctionalPipeline()\n                   .add_step(str.strip)\n                   .add_step(str.upper)\n                   .add_step(lambda x: f\"[{x}]\"))\n        \n        result = pipeline(\"  hello world  \")\n        assert result == \"[HELLO WORLD]\"\n\n\nclass TestCurrying:\n    \"\"\"Test currying and partial application.\"\"\"\n    \n    def test_curry_model_predict(self):\n        \"\"\"Test curried model prediction function.\"\"\"\n        predictor = curry_model_predict(\"gpt-4\", 0.8)\n        \n        # Should return a function\n        assert callable(predictor)\n        \n        # Test prediction\n        result = predictor(\"Hello world\")\n        assert isinstance(result, ModelPrediction)\n        assert result.text == \"Hello world\"\n        assert result.model_used == \"gpt-4\"\n        assert result.metadata[\"temperature\"] == 0.8\n        assert 0.8 <= result.confidence <= 1.0\n    \n    def test_deterministic_prediction(self):\n        \"\"\"Test that predictions are deterministic for same input.\"\"\"\n        predictor = curry_model_predict(\"gpt-4\", 0.7)\n        \n        result1 = predictor(\"test input\")\n        result2 = predictor(\"test input\")\n        \n        # Should be identical\n        assert result1.confidence == result2.confidence\n        assert result1.processing_time == result2.processing_time\n\n\nclass TestResultMonad:\n    \"\"\"Test Result monad for error handling.\"\"\"\n    \n    def test_success_creation(self):\n        \"\"\"Test creating successful results.\"\"\"\n        result = Result.success(42)\n        assert result.is_success is True\n        assert result.value == 42\n        assert result.error is None\n    \n    def test_failure_creation(self):\n        \"\"\"Test creating failed results.\"\"\"\n        result = Result.failure(\"Something went wrong\")\n        assert result.is_success is False\n        assert result.value is None\n        assert result.error == \"Something went wrong\"\n    \n    def test_map_success(self):\n        \"\"\"Test mapping over successful result.\"\"\"\n        result = Result.success(10)\n        mapped = result.map(lambda x: x * 2)\n        \n        assert mapped.is_success is True\n        assert mapped.value == 20\n    \n    def test_map_failure(self):\n        \"\"\"Test mapping over failed result.\"\"\"\n        result = Result.failure(\"error\")\n        mapped = result.map(lambda x: x * 2)\n        \n        assert mapped.is_success is False\n        assert mapped.error == \"error\"\n    \n    def test_map_exception(self):\n        \"\"\"Test map handling exceptions.\"\"\"\n        result = Result.success(\"not a number\")\n        mapped = result.map(lambda x: int(x))  # Will raise ValueError\n        \n        assert mapped.is_success is False\n        assert \"invalid literal\" in mapped.error\n    \n    def test_flat_map(self):\n        \"\"\"Test flat_map operation.\"\"\"\n        result = Result.success(10)\n        flat_mapped = result.flat_map(lambda x: Result.success(x * 2))\n        \n        assert flat_mapped.is_success is True\n        assert flat_mapped.value == 20\n    \n    def test_filter(self):\n        \"\"\"Test filter operation.\"\"\"\n        result = Result.success(10)\n        \n        # Should pass filter\n        filtered_pass = result.filter(lambda x: x > 5)\n        assert filtered_pass.is_success is True\n        assert filtered_pass.value == 10\n        \n        # Should fail filter\n        filtered_fail = result.filter(lambda x: x > 15, \"Too small\")\n        assert filtered_fail.is_success is False\n        assert filtered_fail.error == \"Too small\"\n    \n    def test_get_or_else(self):\n        \"\"\"Test get_or_else operation.\"\"\"\n        success = Result.success(42)\n        failure = Result.failure(\"error\")\n        \n        assert success.get_or_else(0) == 42\n        assert failure.get_or_else(0) == 0\n\n\nclass TestSafeFunctions:\n    \"\"\"Test safe functions that return Result.\"\"\"\n    \n    def test_safe_divide_success(self):\n        \"\"\"Test successful safe division.\"\"\"\n        result = safe_divide(10, 2)\n        assert result.is_success is True\n        assert result.value == 5.0\n    \n    def test_safe_divide_by_zero(self):\n        \"\"\"Test division by zero.\"\"\"\n        result = safe_divide(10, 0)\n        assert result.is_success is False\n        assert \"Division by zero\" in result.error\n    \n    def test_safe_model_predict_success(self):\n        \"\"\"Test successful safe model prediction.\"\"\"\n        result = safe_model_predict(\"Hello world\")\n        assert result.is_success is True\n        assert isinstance(result.value, ModelPrediction)\n        assert result.value.text == \"Hello world\"\n        assert result.value.confidence == 0.85\n    \n    def test_safe_model_predict_empty_input(self):\n        \"\"\"Test safe prediction with empty input.\"\"\"\n        result = safe_model_predict(\"\")\n        assert result.is_success is False\n        assert \"Empty input text\" in result.error\n        \n        result = safe_model_predict(\"   \")\n        assert result.is_success is False\n        assert \"Empty input text\" in result.error\n    \n    def test_safe_model_predict_too_long(self):\n        \"\"\"Test safe prediction with too long input.\"\"\"\n        long_text = \"x\" * 1001\n        result = safe_model_predict(long_text)\n        assert result.is_success is False\n        assert \"Text too long\" in result.error\n\n\nclass TestMemoization:\n    \"\"\"Test memoization functionality.\"\"\"\n    \n    def test_expensive_text_analysis_caching(self):\n        \"\"\"Test that expensive analysis is cached.\"\"\"\n        # Clear any existing cache\n        expensive_text_analysis.cache_clear()\n        \n        text = \"test caching behavior\"\n        \n        # First call\n        result1 = expensive_text_analysis(text)\n        cache_info1 = expensive_text_analysis.cache_info()\n        \n        # Second call (should hit cache)\n        result2 = expensive_text_analysis(text)\n        cache_info2 = expensive_text_analysis.cache_info()\n        \n        # Results should be identical\n        assert result1 == result2\n        \n        # Cache should have been hit\n        assert cache_info2.hits == cache_info1.hits + 1\n        assert cache_info2.misses == cache_info1.misses\n    \n    def test_different_inputs_different_results(self):\n        \"\"\"Test that different inputs produce different cached results.\"\"\"\n        expensive_text_analysis.cache_clear()\n        \n        result1 = expensive_text_analysis(\"text one\")\n        result2 = expensive_text_analysis(\"text two\")\n        \n        # Should be different results\n        assert result1 != result2\n        \n        # Should have 2 cache misses\n        cache_info = expensive_text_analysis.cache_info()\n        assert cache_info.misses == 2\n        assert cache_info.hits == 0\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])","structuredPatch":[]}}
{"parentUuid":"15e470c5-ea3f-46d3-940e-24c03b714d61","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01Ai2E2hNAyodRaMRUYG8vmi","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01LatXc5NXVsRz7gPzThgUCB","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Create 03_functional_programming.py example file","status":"completed","priority":"high"},{"id":"2","content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high"},{"id":"3","content":"Create 05_performance_optimization.py example file","status":"completed","priority":"high"},{"id":"4","content":"Create tests for the Python idioms examples","status":"completed","priority":"medium"},{"id":"5","content":"Create exercises for hands-on practice","status":"in_progress","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":4553,"cache_read_input_tokens":72629,"output_tokens":23}},"requestId":"req_vrtx_011CRddbRYh7azdinckQv9SP","type":"assistant","uuid":"0a80c8a1-fa3d-40a6-9ecd-f5307ed31464","timestamp":"2025-07-30T17:54:19.387Z"}
{"parentUuid":"0a80c8a1-fa3d-40a6-9ecd-f5307ed31464","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01LatXc5NXVsRz7gPzThgUCB","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"31a156d0-c514-42cf-9b89-55bd57850f58","timestamp":"2025-07-30T17:54:19.418Z","toolUseResult":{"oldTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"completed","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"in_progress","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"pending","priority":"medium","id":"5"}],"newTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"completed","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"completed","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"in_progress","priority":"medium","id":"5"}]}}
{"parentUuid":"31a156d0-c514-42cf-9b89-55bd57850f58","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01H3JWFPU3zSs8kwaBHinyeA","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me create exercises for hands-on practice:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":446,"cache_read_input_tokens":77182,"output_tokens":1}},"requestId":"req_vrtx_011CRddbrWcUVasYe5n2RH3t","type":"assistant","uuid":"db1df784-41cf-45b1-9a98-3d5443e6080b","timestamp":"2025-07-30T17:54:22.077Z"}
{"parentUuid":"db1df784-41cf-45b1-9a98-3d5443e6080b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01H3JWFPU3zSs8kwaBHinyeA","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01RGCydn7YpqsE5qFH9Pc3Ec","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_01_build_typed_ai_pipeline.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 1: Build a Typed AI Pipeline\n\nYour task: Create a type-safe AI processing pipeline that can handle\ndifferent types of inputs and outputs with proper validation.\n\nRequirements:\n1. Create typed input/output classes for different AI tasks\n2. Implement a generic pipeline that preserves type information\n3. Add validation for input data\n4. Support multiple AI model types with different capabilities\n5. Implement proper error handling with Result types\n\nRun with: python 03-python-idioms/exercises/exercise_01_build_typed_ai_pipeline.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import TypeVar, Generic, Protocol, List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom rich.console import Console\n\nconsole = Console()\n\n# TODO: Define your types here\n\n# Generic type variables\nT = TypeVar('T')  # Input type\nU = TypeVar('U')  # Output type\n\n# TODO: Create input classes for different AI tasks\n# Hint: Create classes for TextInput, ImageInput, AudioInput with proper validation\n\n@dataclass\nclass TextInput:\n    \"\"\"TODO: Implement TextInput with validation.\"\"\"\n    pass\n\n@dataclass \nclass ImageInput:\n    \"\"\"TODO: Implement ImageInput with validation.\"\"\"\n    pass\n\n# TODO: Create output classes for different AI results\n# Hint: Create classes for ClassificationResult, GenerationResult, etc.\n\n@dataclass\nclass ClassificationResult:\n    \"\"\"TODO: Implement ClassificationResult.\"\"\"\n    pass\n\n@dataclass\nclass GenerationResult:\n    \"\"\"TODO: Implement GenerationResult.\"\"\"\n    pass\n\n# TODO: Create protocols for different AI model capabilities\n# Hint: Use Protocol to define what methods models should have\n\nclass Classifier(Protocol[T, U]):\n    \"\"\"TODO: Define classifier protocol.\"\"\"\n    pass\n\nclass Generator(Protocol[T, U]):\n    \"\"\"TODO: Define generator protocol.\"\"\"\n    pass\n\n# TODO: Create a generic AI model interface\n# Hint: Use Generic[T, U] to make it type-safe\n\nclass AIModel(Generic[T, U], ABC):\n    \"\"\"TODO: Implement generic AI model base class.\"\"\"\n    pass\n\n# TODO: Create concrete model implementations\n# Hint: Implement specific models like TextClassifier, ImageClassifier\n\nclass TextClassifier:\n    \"\"\"TODO: Implement text classifier that works with TextInput -> ClassificationResult.\"\"\"\n    pass\n\nclass TextGenerator:\n    \"\"\"TODO: Implement text generator that works with TextInput -> GenerationResult.\"\"\"\n    pass\n\n# TODO: Create a typed pipeline that preserves type information\n# Hint: Use generics to maintain type safety throughout the pipeline\n\nclass TypedPipeline(Generic[T, U]):\n    \"\"\"TODO: Implement typed pipeline that processes T -> U.\"\"\"\n    pass\n\n# TODO: Create a Result type for error handling\n# Hint: Implement a monadic Result type for safe operations\n\nclass Result(Generic[T]):\n    \"\"\"TODO: Implement Result monad for error handling.\"\"\"\n    pass\n\n# TODO: Create validation functions\n# Hint: Implement functions that return Result[T] for safe validation\n\ndef validate_text_input(text: str) -> Result[TextInput]:\n    \"\"\"TODO: Validate text input and return Result.\"\"\"\n    pass\n\ndef validate_image_input(image_path: str) -> Result[ImageInput]:\n    \"\"\"TODO: Validate image input and return Result.\"\"\"\n    pass\n\n# TODO: Create a model registry with type safety\n# Hint: Use TypedDict or similar for configuration\n\nclass ModelRegistry:\n    \"\"\"TODO: Implement type-safe model registry.\"\"\"\n    pass\n\n# Test functions to verify your implementation\n\ndef test_basic_types():\n    \"\"\"Test basic type functionality.\"\"\"\n    console.print(\"[bold blue]Testing Basic Types...[/bold blue]\")\n    \n    # TODO: Create instances of your input/output classes\n    # TODO: Test validation\n    # TODO: Show type safety in action\n    \n    console.print(\"✅ Basic types test completed!\")\n\ndef test_pipeline():\n    \"\"\"Test pipeline functionality.\"\"\"\n    console.print(\"[bold green]Testing Pipeline...[/bold green]\")\n    \n    # TODO: Create a pipeline\n    # TODO: Process some data through it\n    # TODO: Show type preservation\n    \n    console.print(\"✅ Pipeline test completed!\")\n\ndef test_error_handling():\n    \"\"\"Test error handling with Result type.\"\"\"\n    console.print(\"[bold yellow]Testing Error Handling...[/bold yellow]\")\n    \n    # TODO: Test successful operations\n    # TODO: Test error cases\n    # TODO: Show Result monad chaining\n    \n    console.print(\"✅ Error handling test completed!\")\n\ndef test_model_registry():\n    \"\"\"Test model registry functionality.\"\"\"\n    console.print(\"[bold magenta]Testing Model Registry...[/bold magenta]\")\n    \n    # TODO: Register some models\n    # TODO: Retrieve and use them\n    # TODO: Show type safety\n    \n    console.print(\"✅ Model registry test completed!\")\n\n# Challenge tasks\n\ndef challenge_multi_modal_pipeline():\n    \"\"\"Challenge: Create a multi-modal pipeline that can process different input types.\"\"\"\n    console.print(\"[bold red]Challenge: Multi-Modal Pipeline[/bold red]\")\n    \n    # TODO: Create a pipeline that can handle TextInput, ImageInput, etc.\n    # TODO: Route to appropriate models based on input type\n    # TODO: Combine results from multiple models\n    \n    console.print(\"🎯 Multi-modal pipeline challenge completed!\")\n\ndef challenge_streaming_pipeline():\n    \"\"\"Challenge: Create a streaming version of your pipeline.\"\"\"\n    console.print(\"[bold cyan]Challenge: Streaming Pipeline[/bold cyan]\")\n    \n    # TODO: Modify your pipeline to work with async generators\n    # TODO: Process data streams with type safety\n    # TODO: Handle backpressure and errors in streams\n    \n    console.print(\"🎯 Streaming pipeline challenge completed!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 1: Build a Typed AI Pipeline[/bold blue]\")\n    console.print(\"Create a type-safe AI processing system\\n\")\n    \n    # Run basic tests\n    test_basic_types()\n    console.print()\n    \n    test_pipeline() \n    console.print()\n    \n    test_error_handling()\n    console.print()\n    \n    test_model_registry()\n    console.print()\n    \n    # Run challenges\n    challenge_multi_modal_pipeline()\n    console.print()\n    \n    challenge_streaming_pipeline()\n    console.print()\n    \n    console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n    console.print(\"💡 Key learnings:\")\n    console.print(\"  • Generic types enable reusable, type-safe code\")\n    console.print(\"  • Protocols define interfaces without inheritance\")\n    console.print(\"  • Result types provide monadic error handling\")\n    console.print(\"  • TypedDict ensures configuration safety\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":446,"cache_read_input_tokens":77182,"output_tokens":1}},"requestId":"req_vrtx_011CRddbrWcUVasYe5n2RH3t","type":"assistant","uuid":"0d48e91d-ef50-443f-9bb0-63f5e94a048b","timestamp":"2025-07-30T17:54:51.308Z"}
{"parentUuid":"0d48e91d-ef50-443f-9bb0-63f5e94a048b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01RGCydn7YpqsE5qFH9Pc3Ec","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_01_build_typed_ai_pipeline.py"}]},"uuid":"e4871aa6-58b1-4b89-9b44-f626ca79b183","timestamp":"2025-07-30T17:54:51.371Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_01_build_typed_ai_pipeline.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 1: Build a Typed AI Pipeline\n\nYour task: Create a type-safe AI processing pipeline that can handle\ndifferent types of inputs and outputs with proper validation.\n\nRequirements:\n1. Create typed input/output classes for different AI tasks\n2. Implement a generic pipeline that preserves type information\n3. Add validation for input data\n4. Support multiple AI model types with different capabilities\n5. Implement proper error handling with Result types\n\nRun with: python 03-python-idioms/exercises/exercise_01_build_typed_ai_pipeline.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import TypeVar, Generic, Protocol, List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom rich.console import Console\n\nconsole = Console()\n\n# TODO: Define your types here\n\n# Generic type variables\nT = TypeVar('T')  # Input type\nU = TypeVar('U')  # Output type\n\n# TODO: Create input classes for different AI tasks\n# Hint: Create classes for TextInput, ImageInput, AudioInput with proper validation\n\n@dataclass\nclass TextInput:\n    \"\"\"TODO: Implement TextInput with validation.\"\"\"\n    pass\n\n@dataclass \nclass ImageInput:\n    \"\"\"TODO: Implement ImageInput with validation.\"\"\"\n    pass\n\n# TODO: Create output classes for different AI results\n# Hint: Create classes for ClassificationResult, GenerationResult, etc.\n\n@dataclass\nclass ClassificationResult:\n    \"\"\"TODO: Implement ClassificationResult.\"\"\"\n    pass\n\n@dataclass\nclass GenerationResult:\n    \"\"\"TODO: Implement GenerationResult.\"\"\"\n    pass\n\n# TODO: Create protocols for different AI model capabilities\n# Hint: Use Protocol to define what methods models should have\n\nclass Classifier(Protocol[T, U]):\n    \"\"\"TODO: Define classifier protocol.\"\"\"\n    pass\n\nclass Generator(Protocol[T, U]):\n    \"\"\"TODO: Define generator protocol.\"\"\"\n    pass\n\n# TODO: Create a generic AI model interface\n# Hint: Use Generic[T, U] to make it type-safe\n\nclass AIModel(Generic[T, U], ABC):\n    \"\"\"TODO: Implement generic AI model base class.\"\"\"\n    pass\n\n# TODO: Create concrete model implementations\n# Hint: Implement specific models like TextClassifier, ImageClassifier\n\nclass TextClassifier:\n    \"\"\"TODO: Implement text classifier that works with TextInput -> ClassificationResult.\"\"\"\n    pass\n\nclass TextGenerator:\n    \"\"\"TODO: Implement text generator that works with TextInput -> GenerationResult.\"\"\"\n    pass\n\n# TODO: Create a typed pipeline that preserves type information\n# Hint: Use generics to maintain type safety throughout the pipeline\n\nclass TypedPipeline(Generic[T, U]):\n    \"\"\"TODO: Implement typed pipeline that processes T -> U.\"\"\"\n    pass\n\n# TODO: Create a Result type for error handling\n# Hint: Implement a monadic Result type for safe operations\n\nclass Result(Generic[T]):\n    \"\"\"TODO: Implement Result monad for error handling.\"\"\"\n    pass\n\n# TODO: Create validation functions\n# Hint: Implement functions that return Result[T] for safe validation\n\ndef validate_text_input(text: str) -> Result[TextInput]:\n    \"\"\"TODO: Validate text input and return Result.\"\"\"\n    pass\n\ndef validate_image_input(image_path: str) -> Result[ImageInput]:\n    \"\"\"TODO: Validate image input and return Result.\"\"\"\n    pass\n\n# TODO: Create a model registry with type safety\n# Hint: Use TypedDict or similar for configuration\n\nclass ModelRegistry:\n    \"\"\"TODO: Implement type-safe model registry.\"\"\"\n    pass\n\n# Test functions to verify your implementation\n\ndef test_basic_types():\n    \"\"\"Test basic type functionality.\"\"\"\n    console.print(\"[bold blue]Testing Basic Types...[/bold blue]\")\n    \n    # TODO: Create instances of your input/output classes\n    # TODO: Test validation\n    # TODO: Show type safety in action\n    \n    console.print(\"✅ Basic types test completed!\")\n\ndef test_pipeline():\n    \"\"\"Test pipeline functionality.\"\"\"\n    console.print(\"[bold green]Testing Pipeline...[/bold green]\")\n    \n    # TODO: Create a pipeline\n    # TODO: Process some data through it\n    # TODO: Show type preservation\n    \n    console.print(\"✅ Pipeline test completed!\")\n\ndef test_error_handling():\n    \"\"\"Test error handling with Result type.\"\"\"\n    console.print(\"[bold yellow]Testing Error Handling...[/bold yellow]\")\n    \n    # TODO: Test successful operations\n    # TODO: Test error cases\n    # TODO: Show Result monad chaining\n    \n    console.print(\"✅ Error handling test completed!\")\n\ndef test_model_registry():\n    \"\"\"Test model registry functionality.\"\"\"\n    console.print(\"[bold magenta]Testing Model Registry...[/bold magenta]\")\n    \n    # TODO: Register some models\n    # TODO: Retrieve and use them\n    # TODO: Show type safety\n    \n    console.print(\"✅ Model registry test completed!\")\n\n# Challenge tasks\n\ndef challenge_multi_modal_pipeline():\n    \"\"\"Challenge: Create a multi-modal pipeline that can process different input types.\"\"\"\n    console.print(\"[bold red]Challenge: Multi-Modal Pipeline[/bold red]\")\n    \n    # TODO: Create a pipeline that can handle TextInput, ImageInput, etc.\n    # TODO: Route to appropriate models based on input type\n    # TODO: Combine results from multiple models\n    \n    console.print(\"🎯 Multi-modal pipeline challenge completed!\")\n\ndef challenge_streaming_pipeline():\n    \"\"\"Challenge: Create a streaming version of your pipeline.\"\"\"\n    console.print(\"[bold cyan]Challenge: Streaming Pipeline[/bold cyan]\")\n    \n    # TODO: Modify your pipeline to work with async generators\n    # TODO: Process data streams with type safety\n    # TODO: Handle backpressure and errors in streams\n    \n    console.print(\"🎯 Streaming pipeline challenge completed!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 1: Build a Typed AI Pipeline[/bold blue]\")\n    console.print(\"Create a type-safe AI processing system\\n\")\n    \n    # Run basic tests\n    test_basic_types()\n    console.print()\n    \n    test_pipeline() \n    console.print()\n    \n    test_error_handling()\n    console.print()\n    \n    test_model_registry()\n    console.print()\n    \n    # Run challenges\n    challenge_multi_modal_pipeline()\n    console.print()\n    \n    challenge_streaming_pipeline()\n    console.print()\n    \n    console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n    console.print(\"💡 Key learnings:\")\n    console.print(\"  • Generic types enable reusable, type-safe code\")\n    console.print(\"  • Protocols define interfaces without inheritance\")\n    console.print(\"  • Result types provide monadic error handling\")\n    console.print(\"  • TypedDict ensures configuration safety\")","structuredPatch":[]}}
{"parentUuid":"e4871aa6-58b1-4b89-9b44-f626ca79b183","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01MKR1fFEcdjSsQeEmeQixBu","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01VQ4oGU4Tz4zWwsbvJSEMaD","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_02_async_ai_service.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 2: Build an Async AI Service\n\nYour task: Create a high-performance async AI service that can handle\nconcurrent requests with proper resource management and error handling.\n\nRequirements:\n1. Create async AI models that support streaming responses\n2. Implement connection pooling for external AI services\n3. Add rate limiting to prevent API abuse\n4. Support concurrent request processing with proper error isolation\n5. Implement graceful shutdown and resource cleanup\n\nRun with: python 03-python-idioms/exercises/exercise_02_async_ai_service.py\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import AsyncGenerator, List, Dict, Any, Optional, Callable\nfrom dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nimport time\nimport weakref\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.panel import Panel\n\nconsole = Console()\n\n# TODO: Define your async data structures\n\n@dataclass\nclass AIRequest:\n    \"\"\"TODO: Implement AI request with proper typing.\"\"\"\n    pass\n\n@dataclass\nclass AIResponse:\n    \"\"\"TODO: Implement AI response with metadata.\"\"\"\n    pass\n\n@dataclass\nclass StreamingChunk:\n    \"\"\"TODO: Implement streaming chunk for real-time responses.\"\"\"\n    pass\n\n# TODO: Create async AI model interface\n# Hint: Use AsyncGenerator for streaming responses\n\nclass AsyncAIModel:\n    \"\"\"TODO: Implement async AI model with streaming support.\"\"\"\n    \n    async def predict(self, request: AIRequest) -> AIResponse:\n        \"\"\"TODO: Implement async prediction.\"\"\"\n        pass\n    \n    async def stream_predict(self, request: AIRequest) -> AsyncGenerator[StreamingChunk, None]:\n        \"\"\"TODO: Implement streaming prediction.\"\"\"\n        pass\n\n# TODO: Create connection pool for external services\n# Hint: Manage async connections with proper lifecycle\n\nclass AsyncConnectionPool:\n    \"\"\"TODO: Implement async connection pool.\"\"\"\n    pass\n\n# TODO: Create rate limiter for API protection\n# Hint: Use asyncio.Semaphore and time-based windows\n\nclass AsyncRateLimiter:\n    \"\"\"TODO: Implement async rate limiter.\"\"\"\n    pass\n\n# TODO: Create concurrent request processor\n# Hint: Use asyncio.Semaphore for concurrency control\n\nclass ConcurrentProcessor:\n    \"\"\"TODO: Implement concurrent request processor.\"\"\"\n    pass\n\n# TODO: Create resource manager with async context managers\n# Hint: Use @asynccontextmanager for proper cleanup\n\n@asynccontextmanager\nasync def managed_ai_service():\n    \"\"\"TODO: Implement resource manager for AI service.\"\"\"\n    pass\n\n# TODO: Create the main AI service class\n# Hint: Combine all components into a cohesive service\n\nclass AsyncAIService:\n    \"\"\"TODO: Implement the main async AI service.\"\"\"\n    pass\n\n# Test functions to verify your implementation\n\nasync def test_basic_async_model():\n    \"\"\"Test basic async model functionality.\"\"\"\n    console.print(\"[bold blue]Testing Basic Async Model...[/bold blue]\")\n    \n    # TODO: Create and test async model\n    # TODO: Test both regular and streaming responses\n    # TODO: Measure response times\n    \n    console.print(\"✅ Basic async model test completed!\")\n\nasync def test_connection_pooling():\n    \"\"\"Test connection pooling.\"\"\"\n    console.print(\"[bold green]Testing Connection Pooling...[/bold green]\")\n    \n    # TODO: Create connection pool\n    # TODO: Test concurrent connection usage\n    # TODO: Verify connection reuse\n    \n    console.print(\"✅ Connection pooling test completed!\")\n\nasync def test_rate_limiting():\n    \"\"\"Test rate limiting functionality.\"\"\"\n    console.print(\"[bold yellow]Testing Rate Limiting...[/bold yellow]\")\n    \n    # TODO: Create rate limiter\n    # TODO: Test rate limiting behavior\n    # TODO: Measure timing to verify limits\n    \n    console.print(\"✅ Rate limiting test completed!\")\n\nasync def test_concurrent_processing():\n    \"\"\"Test concurrent request processing.\"\"\"\n    console.print(\"[bold magenta]Testing Concurrent Processing...[/bold magenta]\")\n    \n    # TODO: Create concurrent processor\n    # TODO: Submit multiple requests\n    # TODO: Verify concurrent execution\n    \n    console.print(\"✅ Concurrent processing test completed!\")\n\nasync def test_full_service():\n    \"\"\"Test the complete AI service.\"\"\"\n    console.print(\"[bold cyan]Testing Full AI Service...[/bold cyan]\")\n    \n    # TODO: Create full service\n    # TODO: Test multiple concurrent requests\n    # TODO: Test both streaming and regular responses\n    \n    console.print(\"✅ Full service test completed!\")\n\n# Challenge tasks\n\nasync def challenge_load_balancing():\n    \"\"\"Challenge: Add load balancing across multiple AI models.\"\"\"\n    console.print(\"[bold red]Challenge: Load Balancing[/bold red]\")\n    \n    # TODO: Create multiple AI model instances\n    # TODO: Implement load balancing strategy (round-robin, least-loaded, etc.)\n    # TODO: Handle model failures gracefully\n    \n    console.print(\"🎯 Load balancing challenge completed!\")\n\nasync def challenge_circuit_breaker():\n    \"\"\"Challenge: Implement circuit breaker pattern for fault tolerance.\"\"\"\n    console.print(\"[bold purple]Challenge: Circuit Breaker[/bold purple]\")\n    \n    # TODO: Implement circuit breaker for AI models\n    # TODO: Track failure rates and trip breaker when needed\n    # TODO: Support automatic recovery\n    \n    console.print(\"🎯 Circuit breaker challenge completed!\")\n\nasync def challenge_streaming_aggregation():\n    \"\"\"Challenge: Aggregate multiple streaming responses.\"\"\"\n    console.print(\"[bold orange]Challenge: Streaming Aggregation[/bold orange]\")\n    \n    # TODO: Send same request to multiple models\n    # TODO: Aggregate streaming responses in real-time\n    # TODO: Handle different response speeds\n    \n    console.print(\"🎯 Streaming aggregation challenge completed!\")\n\nasync def challenge_metrics_monitoring():\n    \"\"\"Challenge: Add comprehensive metrics and monitoring.\"\"\"\n    console.print(\"[bold teal]Challenge: Metrics & Monitoring[/bold teal]\")\n    \n    # TODO: Track request latency, throughput, error rates\n    # TODO: Monitor resource usage (connections, memory, etc.)\n    # TODO: Implement health checks\n    \n    console.print(\"🎯 Metrics monitoring challenge completed!\")\n\n# Performance benchmark\n\nasync def benchmark_service():\n    \"\"\"Benchmark the AI service performance.\"\"\"\n    console.print(\"[bold gold]🏆 Performance Benchmark[/bold gold]\")\n    \n    # TODO: Create benchmark scenarios\n    # TODO: Test with different concurrency levels\n    # TODO: Measure latency, throughput, resource usage\n    # TODO: Compare with and without optimizations\n    \n    console.print(\"📊 Benchmark completed!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 2: Build an Async AI Service[/bold blue]\")\n    console.print(\"Create a high-performance async AI processing service\\n\")\n    \n    async def main():\n        # Run basic tests\n        await test_basic_async_model()\n        console.print()\n        \n        await test_connection_pooling()\n        console.print()\n        \n        await test_rate_limiting()\n        console.print()\n        \n        await test_concurrent_processing()\n        console.print()\n        \n        await test_full_service()\n        console.print()\n        \n        # Run challenges\n        await challenge_load_balancing()\n        console.print()\n        \n        await challenge_circuit_breaker()\n        console.print()\n        \n        await challenge_streaming_aggregation()\n        console.print()\n        \n        await challenge_metrics_monitoring()\n        console.print()\n        \n        # Run benchmark\n        await benchmark_service()\n        console.print()\n        \n        console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n        console.print(\"💡 Key learnings:\")\n        console.print(\"  • Async programming enables high concurrency\")\n        console.print(\"  • Resource pooling improves efficiency\")\n        console.print(\"  • Rate limiting protects services\")\n        console.print(\"  • Proper error isolation prevents cascade failures\")\n        console.print(\"  • Context managers ensure resource cleanup\")\n    \n    asyncio.run(main())"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1859,"cache_read_input_tokens":77628,"output_tokens":2120}},"requestId":"req_vrtx_011CRddeDGqezzQjcZaYWdEk","type":"assistant","uuid":"bd72c9d4-2301-4cd9-89c8-77bb82922e56","timestamp":"2025-07-30T17:55:25.475Z"}
{"parentUuid":"bd72c9d4-2301-4cd9-89c8-77bb82922e56","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01VQ4oGU4Tz4zWwsbvJSEMaD","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_02_async_ai_service.py"}]},"uuid":"cd4297ef-a097-4dfa-8d7a-64d55a7d1a30","timestamp":"2025-07-30T17:55:25.507Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_02_async_ai_service.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 2: Build an Async AI Service\n\nYour task: Create a high-performance async AI service that can handle\nconcurrent requests with proper resource management and error handling.\n\nRequirements:\n1. Create async AI models that support streaming responses\n2. Implement connection pooling for external AI services\n3. Add rate limiting to prevent API abuse\n4. Support concurrent request processing with proper error isolation\n5. Implement graceful shutdown and resource cleanup\n\nRun with: python 03-python-idioms/exercises/exercise_02_async_ai_service.py\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import AsyncGenerator, List, Dict, Any, Optional, Callable\nfrom dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nimport time\nimport weakref\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.panel import Panel\n\nconsole = Console()\n\n# TODO: Define your async data structures\n\n@dataclass\nclass AIRequest:\n    \"\"\"TODO: Implement AI request with proper typing.\"\"\"\n    pass\n\n@dataclass\nclass AIResponse:\n    \"\"\"TODO: Implement AI response with metadata.\"\"\"\n    pass\n\n@dataclass\nclass StreamingChunk:\n    \"\"\"TODO: Implement streaming chunk for real-time responses.\"\"\"\n    pass\n\n# TODO: Create async AI model interface\n# Hint: Use AsyncGenerator for streaming responses\n\nclass AsyncAIModel:\n    \"\"\"TODO: Implement async AI model with streaming support.\"\"\"\n    \n    async def predict(self, request: AIRequest) -> AIResponse:\n        \"\"\"TODO: Implement async prediction.\"\"\"\n        pass\n    \n    async def stream_predict(self, request: AIRequest) -> AsyncGenerator[StreamingChunk, None]:\n        \"\"\"TODO: Implement streaming prediction.\"\"\"\n        pass\n\n# TODO: Create connection pool for external services\n# Hint: Manage async connections with proper lifecycle\n\nclass AsyncConnectionPool:\n    \"\"\"TODO: Implement async connection pool.\"\"\"\n    pass\n\n# TODO: Create rate limiter for API protection\n# Hint: Use asyncio.Semaphore and time-based windows\n\nclass AsyncRateLimiter:\n    \"\"\"TODO: Implement async rate limiter.\"\"\"\n    pass\n\n# TODO: Create concurrent request processor\n# Hint: Use asyncio.Semaphore for concurrency control\n\nclass ConcurrentProcessor:\n    \"\"\"TODO: Implement concurrent request processor.\"\"\"\n    pass\n\n# TODO: Create resource manager with async context managers\n# Hint: Use @asynccontextmanager for proper cleanup\n\n@asynccontextmanager\nasync def managed_ai_service():\n    \"\"\"TODO: Implement resource manager for AI service.\"\"\"\n    pass\n\n# TODO: Create the main AI service class\n# Hint: Combine all components into a cohesive service\n\nclass AsyncAIService:\n    \"\"\"TODO: Implement the main async AI service.\"\"\"\n    pass\n\n# Test functions to verify your implementation\n\nasync def test_basic_async_model():\n    \"\"\"Test basic async model functionality.\"\"\"\n    console.print(\"[bold blue]Testing Basic Async Model...[/bold blue]\")\n    \n    # TODO: Create and test async model\n    # TODO: Test both regular and streaming responses\n    # TODO: Measure response times\n    \n    console.print(\"✅ Basic async model test completed!\")\n\nasync def test_connection_pooling():\n    \"\"\"Test connection pooling.\"\"\"\n    console.print(\"[bold green]Testing Connection Pooling...[/bold green]\")\n    \n    # TODO: Create connection pool\n    # TODO: Test concurrent connection usage\n    # TODO: Verify connection reuse\n    \n    console.print(\"✅ Connection pooling test completed!\")\n\nasync def test_rate_limiting():\n    \"\"\"Test rate limiting functionality.\"\"\"\n    console.print(\"[bold yellow]Testing Rate Limiting...[/bold yellow]\")\n    \n    # TODO: Create rate limiter\n    # TODO: Test rate limiting behavior\n    # TODO: Measure timing to verify limits\n    \n    console.print(\"✅ Rate limiting test completed!\")\n\nasync def test_concurrent_processing():\n    \"\"\"Test concurrent request processing.\"\"\"\n    console.print(\"[bold magenta]Testing Concurrent Processing...[/bold magenta]\")\n    \n    # TODO: Create concurrent processor\n    # TODO: Submit multiple requests\n    # TODO: Verify concurrent execution\n    \n    console.print(\"✅ Concurrent processing test completed!\")\n\nasync def test_full_service():\n    \"\"\"Test the complete AI service.\"\"\"\n    console.print(\"[bold cyan]Testing Full AI Service...[/bold cyan]\")\n    \n    # TODO: Create full service\n    # TODO: Test multiple concurrent requests\n    # TODO: Test both streaming and regular responses\n    \n    console.print(\"✅ Full service test completed!\")\n\n# Challenge tasks\n\nasync def challenge_load_balancing():\n    \"\"\"Challenge: Add load balancing across multiple AI models.\"\"\"\n    console.print(\"[bold red]Challenge: Load Balancing[/bold red]\")\n    \n    # TODO: Create multiple AI model instances\n    # TODO: Implement load balancing strategy (round-robin, least-loaded, etc.)\n    # TODO: Handle model failures gracefully\n    \n    console.print(\"🎯 Load balancing challenge completed!\")\n\nasync def challenge_circuit_breaker():\n    \"\"\"Challenge: Implement circuit breaker pattern for fault tolerance.\"\"\"\n    console.print(\"[bold purple]Challenge: Circuit Breaker[/bold purple]\")\n    \n    # TODO: Implement circuit breaker for AI models\n    # TODO: Track failure rates and trip breaker when needed\n    # TODO: Support automatic recovery\n    \n    console.print(\"🎯 Circuit breaker challenge completed!\")\n\nasync def challenge_streaming_aggregation():\n    \"\"\"Challenge: Aggregate multiple streaming responses.\"\"\"\n    console.print(\"[bold orange]Challenge: Streaming Aggregation[/bold orange]\")\n    \n    # TODO: Send same request to multiple models\n    # TODO: Aggregate streaming responses in real-time\n    # TODO: Handle different response speeds\n    \n    console.print(\"🎯 Streaming aggregation challenge completed!\")\n\nasync def challenge_metrics_monitoring():\n    \"\"\"Challenge: Add comprehensive metrics and monitoring.\"\"\"\n    console.print(\"[bold teal]Challenge: Metrics & Monitoring[/bold teal]\")\n    \n    # TODO: Track request latency, throughput, error rates\n    # TODO: Monitor resource usage (connections, memory, etc.)\n    # TODO: Implement health checks\n    \n    console.print(\"🎯 Metrics monitoring challenge completed!\")\n\n# Performance benchmark\n\nasync def benchmark_service():\n    \"\"\"Benchmark the AI service performance.\"\"\"\n    console.print(\"[bold gold]🏆 Performance Benchmark[/bold gold]\")\n    \n    # TODO: Create benchmark scenarios\n    # TODO: Test with different concurrency levels\n    # TODO: Measure latency, throughput, resource usage\n    # TODO: Compare with and without optimizations\n    \n    console.print(\"📊 Benchmark completed!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 2: Build an Async AI Service[/bold blue]\")\n    console.print(\"Create a high-performance async AI processing service\\n\")\n    \n    async def main():\n        # Run basic tests\n        await test_basic_async_model()\n        console.print()\n        \n        await test_connection_pooling()\n        console.print()\n        \n        await test_rate_limiting()\n        console.print()\n        \n        await test_concurrent_processing()\n        console.print()\n        \n        await test_full_service()\n        console.print()\n        \n        # Run challenges\n        await challenge_load_balancing()\n        console.print()\n        \n        await challenge_circuit_breaker()\n        console.print()\n        \n        await challenge_streaming_aggregation()\n        console.print()\n        \n        await challenge_metrics_monitoring()\n        console.print()\n        \n        # Run benchmark\n        await benchmark_service()\n        console.print()\n        \n        console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n        console.print(\"💡 Key learnings:\")\n        console.print(\"  • Async programming enables high concurrency\")\n        console.print(\"  • Resource pooling improves efficiency\")\n        console.print(\"  • Rate limiting protects services\")\n        console.print(\"  • Proper error isolation prevents cascade failures\")\n        console.print(\"  • Context managers ensure resource cleanup\")\n    \n    asyncio.run(main())","structuredPatch":[]}}
{"parentUuid":"cd4297ef-a097-4dfa-8d7a-64d55a7d1a30","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01RaL94RJwp6LQAGHX46244q","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01HgBSEwNQbAcXHX9uwQfi8M","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_03_functional_ai_pipeline.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 3: Build a Functional AI Pipeline\n\nYour task: Create a functional programming-based AI pipeline that emphasizes\nimmutability, pure functions, and composability for predictable AI workflows.\n\nRequirements:\n1. Design immutable data structures for AI operations\n2. Create pure functions for all data transformations\n3. Implement function composition for pipeline building\n4. Use monadic error handling for robust operation\n5. Add lazy evaluation for efficient processing\n\nRun with: python 03-python-idioms/exercises/exercise_03_functional_ai_pipeline.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import Callable, List, Dict, Any, Optional, TypeVar, Iterator, Tuple\nfrom dataclasses import dataclass, replace\nfrom functools import reduce, partial, lru_cache\nfrom itertools import chain, groupby\nimport time\nfrom rich.console import Console\n\nconsole = Console()\n\n# Type variables\nT = TypeVar('T')\nU = TypeVar('U')\nV = TypeVar('V')\n\n# TODO: Create immutable data structures for AI operations\n\n@dataclass(frozen=True)\nclass AIData:\n    \"\"\"TODO: Implement immutable AI data container.\"\"\"\n    pass\n\n@dataclass(frozen=True)\nclass ProcessingStep:\n    \"\"\"TODO: Implement immutable processing step configuration.\"\"\"\n    pass\n\n@dataclass(frozen=True)\nclass PipelineResult:\n    \"\"\"TODO: Implement immutable pipeline result.\"\"\"\n    pass\n\n# TODO: Create Result monad for error handling\n\nclass Result:\n    \"\"\"TODO: Implement Result monad for functional error handling.\"\"\"\n    \n    @classmethod\n    def success(cls, value: Any):\n        \"\"\"TODO: Create successful result.\"\"\"\n        pass\n    \n    @classmethod\n    def failure(cls, error: str):\n        \"\"\"TODO: Create failed result.\"\"\"\n        pass\n    \n    def map(self, func: Callable):\n        \"\"\"TODO: Apply function if successful.\"\"\"\n        pass\n    \n    def flat_map(self, func: Callable):\n        \"\"\"TODO: Apply function that returns Result.\"\"\"\n        pass\n    \n    def filter(self, predicate: Callable, error_msg: str = \"Filter failed\"):\n        \"\"\"TODO: Filter result based on predicate.\"\"\"\n        pass\n\n# TODO: Create pure functions for data processing\n# Hint: These should have no side effects and always return the same output for the same input\n\ndef tokenize_text(text: str) -> List[str]:\n    \"\"\"TODO: Pure function to tokenize text.\"\"\"\n    pass\n\ndef normalize_text(text: str) -> str:\n    \"\"\"TODO: Pure function to normalize text.\"\"\"\n    pass\n\ndef extract_features(tokens: List[str]) -> Dict[str, Any]:\n    \"\"\"TODO: Pure function to extract features from tokens.\"\"\"\n    pass\n\ndef classify_sentiment(features: Dict[str, Any]) -> Dict[str, float]:\n    \"\"\"TODO: Pure function to classify sentiment.\"\"\"\n    pass\n\n# TODO: Create higher-order functions for pipeline composition\n\ndef compose(*functions: Callable) -> Callable:\n    \"\"\"TODO: Compose functions right to left.\"\"\"\n    pass\n\ndef pipe(*functions: Callable) -> Callable:\n    \"\"\"TODO: Pipe functions left to right.\"\"\"\n    pass\n\ndef curry(func: Callable) -> Callable:\n    \"\"\"TODO: Convert function to curried form.\"\"\"\n    pass\n\n# TODO: Create lazy evaluation utilities\n\ndef lazy_map(func: Callable, iterable: Iterator[T]) -> Iterator[U]:\n    \"\"\"TODO: Lazy map that processes items on demand.\"\"\"\n    pass\n\ndef lazy_filter(predicate: Callable, iterable: Iterator[T]) -> Iterator[T]:\n    \"\"\"TODO: Lazy filter that filters items on demand.\"\"\"\n    pass\n\ndef lazy_batch(iterable: Iterator[T], batch_size: int) -> Iterator[List[T]]:\n    \"\"\"TODO: Lazy batching of items.\"\"\"\n    pass\n\n# TODO: Create functional pipeline class\n\nclass FunctionalPipeline:\n    \"\"\"TODO: Implement functional pipeline with immutable operations.\"\"\"\n    \n    def __init__(self, steps: List[Callable] = None):\n        \"\"\"TODO: Initialize pipeline with optional steps.\"\"\"\n        pass\n    \n    def add_step(self, step: Callable) -> 'FunctionalPipeline':\n        \"\"\"TODO: Add step returning new pipeline (immutable).\"\"\"\n        pass\n    \n    def process(self, data: T) -> Result[U]:\n        \"\"\"TODO: Process data through pipeline.\"\"\"\n        pass\n    \n    def process_lazy(self, data_stream: Iterator[T]) -> Iterator[Result[U]]:\n        \"\"\"TODO: Process stream lazily.\"\"\"\n        pass\n\n# TODO: Create memoization utilities for expensive operations\n\n@lru_cache(maxsize=128)\ndef expensive_ai_operation(data: str) -> Dict[str, Any]:\n    \"\"\"TODO: Expensive operation that benefits from memoization.\"\"\"\n    pass\n\ndef memoize_with_key(key_func: Callable) -> Callable:\n    \"\"\"TODO: Custom memoization with key function.\"\"\"\n    pass\n\n# TODO: Create validation functions that return Results\n\ndef validate_text_length(text: str, max_length: int = 1000) -> Result[str]:\n    \"\"\"TODO: Validate text length.\"\"\"\n    pass\n\ndef validate_data_format(data: Dict[str, Any]) -> Result[Dict[str, Any]]:\n    \"\"\"TODO: Validate data format.\"\"\"\n    pass\n\n# TODO: Create transformation functions\n\ndef transform_to_features(data: AIData) -> Result[Dict[str, Any]]:\n    \"\"\"TODO: Transform AI data to features.\"\"\"\n    pass\n\ndef transform_to_predictions(features: Dict[str, Any]) -> Result[Dict[str, float]]:\n    \"\"\"TODO: Transform features to predictions.\"\"\"\n    pass\n\n# Test functions to verify your implementation\n\ndef test_immutable_data():\n    \"\"\"Test immutable data structures.\"\"\"\n    console.print(\"[bold blue]Testing Immutable Data Structures...[/bold blue]\")\n    \n    # TODO: Create immutable data instances\n    # TODO: Test that they cannot be modified\n    # TODO: Test creating new instances with changes\n    \n    console.print(\"✅ Immutable data test completed!\")\n\ndef test_pure_functions():\n    \"\"\"Test pure functions.\"\"\"\n    console.print(\"[bold green]Testing Pure Functions...[/bold green]\")\n    \n    # TODO: Test that functions are deterministic\n    # TODO: Test that they have no side effects\n    # TODO: Test function composition\n    \n    console.print(\"✅ Pure functions test completed!\")\n\ndef test_result_monad():\n    \"\"\"Test Result monad for error handling.\"\"\"\n    console.print(\"[bold yellow]Testing Result Monad...[/bold yellow]\")\n    \n    # TODO: Test successful operations\n    # TODO: Test error handling\n    # TODO: Test chaining operations\n    \n    console.print(\"✅ Result monad test completed!\")\n\ndef test_lazy_evaluation():\n    \"\"\"Test lazy evaluation.\"\"\"\n    console.print(\"[bold magenta]Testing Lazy Evaluation...[/bold magenta]\")\n    \n    # TODO: Test lazy map and filter\n    # TODO: Show that processing happens on demand\n    # TODO: Test lazy batching\n    \n    console.print(\"✅ Lazy evaluation test completed!\")\n\ndef test_functional_pipeline():\n    \"\"\"Test functional pipeline.\"\"\"\n    console.print(\"[bold cyan]Testing Functional Pipeline...[/bold cyan]\")\n    \n    # TODO: Create pipeline with multiple steps\n    # TODO: Test immutable addition of steps\n    # TODO: Test processing with error handling\n    \n    console.print(\"✅ Functional pipeline test completed!\")\n\n# Challenge tasks\n\ndef challenge_parallel_functional_processing():\n    \"\"\"Challenge: Add parallel processing while maintaining functional principles.\"\"\"\n    console.print(\"[bold red]Challenge: Parallel Functional Processing[/bold red]\")\n    \n    # TODO: Process data in parallel while keeping functions pure\n    # TODO: Use multiprocessing with functional approach\n    # TODO: Ensure thread safety through immutability\n    \n    console.print(\"🎯 Parallel functional processing challenge completed!\")\n\ndef challenge_streaming_functional_pipeline():\n    \"\"\"Challenge: Create a streaming version that processes infinite data.\"\"\"\n    console.print(\"[bold purple]Challenge: Streaming Functional Pipeline[/bold purple]\")\n    \n    # TODO: Handle infinite streams of data\n    # TODO: Process data lazily without loading everything into memory\n    # TODO: Support backpressure and flow control\n    \n    console.print(\"🎯 Streaming functional pipeline challenge completed!\")\n\ndef challenge_functional_error_recovery():\n    \"\"\"Challenge: Implement sophisticated error recovery with functional patterns.\"\"\"\n    console.print(\"[bold orange]Challenge: Functional Error Recovery[/bold orange]\")\n    \n    # TODO: Implement retry logic with exponential backoff\n    # TODO: Support fallback operations for failed steps\n    # TODO: Accumulate errors for batch reporting\n    \n    console.print(\"🎯 Functional error recovery challenge completed!\")\n\ndef challenge_functional_optimization():\n    \"\"\"Challenge: Optimize the pipeline using functional techniques.\"\"\"\n    console.print(\"[bold teal]Challenge: Functional Optimization[/bold teal]\")\n    \n    # TODO: Implement fusion optimization for composed functions\n    # TODO: Add intelligent memoization strategies\n    # TODO: Optimize using partial evaluation\n    \n    console.print(\"🎯 Functional optimization challenge completed!\")\n\n# Performance comparison\n\ndef compare_functional_vs_imperative():\n    \"\"\"Compare functional vs imperative approaches.\"\"\"\n    console.print(\"[bold gold]🏆 Functional vs Imperative Comparison[/bold gold]\")\n    \n    # TODO: Implement same pipeline in imperative style\n    # TODO: Compare performance, memory usage, maintainability\n    # TODO: Show benefits and trade-offs of functional approach\n    \n    console.print(\"📊 Comparison completed!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 3: Build a Functional AI Pipeline[/bold blue]\")\n    console.print(\"Create a functional programming-based AI processing system\\n\")\n    \n    # Run basic tests\n    test_immutable_data()\n    console.print()\n    \n    test_pure_functions()\n    console.print()\n    \n    test_result_monad()\n    console.print()\n    \n    test_lazy_evaluation()\n    console.print()\n    \n    test_functional_pipeline()\n    console.print()\n    \n    # Run challenges\n    challenge_parallel_functional_processing()\n    console.print()\n    \n    challenge_streaming_functional_pipeline()\n    console.print()\n    \n    challenge_functional_error_recovery()\n    console.print()\n    \n    challenge_functional_optimization()\n    console.print()\n    \n    # Run comparison\n    compare_functional_vs_imperative()\n    console.print()\n    \n    console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n    console.print(\"💡 Key learnings:\")\n    console.print(\"  • Immutability prevents bugs and enables parallelism\")\n    console.print(\"  • Pure functions are predictable and testable\")\n    console.print(\"  • Function composition enables modular design\")\n    console.print(\"  • Monadic error handling provides robust operation\")\n    console.print(\"  • Lazy evaluation optimizes resource usage\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2176,"cache_read_input_tokens":79487,"output_tokens":22}},"requestId":"req_vrtx_011CRddgjByHNqG81gu2x1Uy","type":"assistant","uuid":"7fcff093-5dd4-45a0-9132-6995b0201d51","timestamp":"2025-07-30T17:56:12.669Z"}
{"parentUuid":"7fcff093-5dd4-45a0-9132-6995b0201d51","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01HgBSEwNQbAcXHX9uwQfi8M","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_03_functional_ai_pipeline.py"}]},"uuid":"65210e57-851f-4bcf-aab9-aa77f501737a","timestamp":"2025-07-30T17:56:12.702Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_03_functional_ai_pipeline.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 3: Build a Functional AI Pipeline\n\nYour task: Create a functional programming-based AI pipeline that emphasizes\nimmutability, pure functions, and composability for predictable AI workflows.\n\nRequirements:\n1. Design immutable data structures for AI operations\n2. Create pure functions for all data transformations\n3. Implement function composition for pipeline building\n4. Use monadic error handling for robust operation\n5. Add lazy evaluation for efficient processing\n\nRun with: python 03-python-idioms/exercises/exercise_03_functional_ai_pipeline.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import Callable, List, Dict, Any, Optional, TypeVar, Iterator, Tuple\nfrom dataclasses import dataclass, replace\nfrom functools import reduce, partial, lru_cache\nfrom itertools import chain, groupby\nimport time\nfrom rich.console import Console\n\nconsole = Console()\n\n# Type variables\nT = TypeVar('T')\nU = TypeVar('U')\nV = TypeVar('V')\n\n# TODO: Create immutable data structures for AI operations\n\n@dataclass(frozen=True)\nclass AIData:\n    \"\"\"TODO: Implement immutable AI data container.\"\"\"\n    pass\n\n@dataclass(frozen=True)\nclass ProcessingStep:\n    \"\"\"TODO: Implement immutable processing step configuration.\"\"\"\n    pass\n\n@dataclass(frozen=True)\nclass PipelineResult:\n    \"\"\"TODO: Implement immutable pipeline result.\"\"\"\n    pass\n\n# TODO: Create Result monad for error handling\n\nclass Result:\n    \"\"\"TODO: Implement Result monad for functional error handling.\"\"\"\n    \n    @classmethod\n    def success(cls, value: Any):\n        \"\"\"TODO: Create successful result.\"\"\"\n        pass\n    \n    @classmethod\n    def failure(cls, error: str):\n        \"\"\"TODO: Create failed result.\"\"\"\n        pass\n    \n    def map(self, func: Callable):\n        \"\"\"TODO: Apply function if successful.\"\"\"\n        pass\n    \n    def flat_map(self, func: Callable):\n        \"\"\"TODO: Apply function that returns Result.\"\"\"\n        pass\n    \n    def filter(self, predicate: Callable, error_msg: str = \"Filter failed\"):\n        \"\"\"TODO: Filter result based on predicate.\"\"\"\n        pass\n\n# TODO: Create pure functions for data processing\n# Hint: These should have no side effects and always return the same output for the same input\n\ndef tokenize_text(text: str) -> List[str]:\n    \"\"\"TODO: Pure function to tokenize text.\"\"\"\n    pass\n\ndef normalize_text(text: str) -> str:\n    \"\"\"TODO: Pure function to normalize text.\"\"\"\n    pass\n\ndef extract_features(tokens: List[str]) -> Dict[str, Any]:\n    \"\"\"TODO: Pure function to extract features from tokens.\"\"\"\n    pass\n\ndef classify_sentiment(features: Dict[str, Any]) -> Dict[str, float]:\n    \"\"\"TODO: Pure function to classify sentiment.\"\"\"\n    pass\n\n# TODO: Create higher-order functions for pipeline composition\n\ndef compose(*functions: Callable) -> Callable:\n    \"\"\"TODO: Compose functions right to left.\"\"\"\n    pass\n\ndef pipe(*functions: Callable) -> Callable:\n    \"\"\"TODO: Pipe functions left to right.\"\"\"\n    pass\n\ndef curry(func: Callable) -> Callable:\n    \"\"\"TODO: Convert function to curried form.\"\"\"\n    pass\n\n# TODO: Create lazy evaluation utilities\n\ndef lazy_map(func: Callable, iterable: Iterator[T]) -> Iterator[U]:\n    \"\"\"TODO: Lazy map that processes items on demand.\"\"\"\n    pass\n\ndef lazy_filter(predicate: Callable, iterable: Iterator[T]) -> Iterator[T]:\n    \"\"\"TODO: Lazy filter that filters items on demand.\"\"\"\n    pass\n\ndef lazy_batch(iterable: Iterator[T], batch_size: int) -> Iterator[List[T]]:\n    \"\"\"TODO: Lazy batching of items.\"\"\"\n    pass\n\n# TODO: Create functional pipeline class\n\nclass FunctionalPipeline:\n    \"\"\"TODO: Implement functional pipeline with immutable operations.\"\"\"\n    \n    def __init__(self, steps: List[Callable] = None):\n        \"\"\"TODO: Initialize pipeline with optional steps.\"\"\"\n        pass\n    \n    def add_step(self, step: Callable) -> 'FunctionalPipeline':\n        \"\"\"TODO: Add step returning new pipeline (immutable).\"\"\"\n        pass\n    \n    def process(self, data: T) -> Result[U]:\n        \"\"\"TODO: Process data through pipeline.\"\"\"\n        pass\n    \n    def process_lazy(self, data_stream: Iterator[T]) -> Iterator[Result[U]]:\n        \"\"\"TODO: Process stream lazily.\"\"\"\n        pass\n\n# TODO: Create memoization utilities for expensive operations\n\n@lru_cache(maxsize=128)\ndef expensive_ai_operation(data: str) -> Dict[str, Any]:\n    \"\"\"TODO: Expensive operation that benefits from memoization.\"\"\"\n    pass\n\ndef memoize_with_key(key_func: Callable) -> Callable:\n    \"\"\"TODO: Custom memoization with key function.\"\"\"\n    pass\n\n# TODO: Create validation functions that return Results\n\ndef validate_text_length(text: str, max_length: int = 1000) -> Result[str]:\n    \"\"\"TODO: Validate text length.\"\"\"\n    pass\n\ndef validate_data_format(data: Dict[str, Any]) -> Result[Dict[str, Any]]:\n    \"\"\"TODO: Validate data format.\"\"\"\n    pass\n\n# TODO: Create transformation functions\n\ndef transform_to_features(data: AIData) -> Result[Dict[str, Any]]:\n    \"\"\"TODO: Transform AI data to features.\"\"\"\n    pass\n\ndef transform_to_predictions(features: Dict[str, Any]) -> Result[Dict[str, float]]:\n    \"\"\"TODO: Transform features to predictions.\"\"\"\n    pass\n\n# Test functions to verify your implementation\n\ndef test_immutable_data():\n    \"\"\"Test immutable data structures.\"\"\"\n    console.print(\"[bold blue]Testing Immutable Data Structures...[/bold blue]\")\n    \n    # TODO: Create immutable data instances\n    # TODO: Test that they cannot be modified\n    # TODO: Test creating new instances with changes\n    \n    console.print(\"✅ Immutable data test completed!\")\n\ndef test_pure_functions():\n    \"\"\"Test pure functions.\"\"\"\n    console.print(\"[bold green]Testing Pure Functions...[/bold green]\")\n    \n    # TODO: Test that functions are deterministic\n    # TODO: Test that they have no side effects\n    # TODO: Test function composition\n    \n    console.print(\"✅ Pure functions test completed!\")\n\ndef test_result_monad():\n    \"\"\"Test Result monad for error handling.\"\"\"\n    console.print(\"[bold yellow]Testing Result Monad...[/bold yellow]\")\n    \n    # TODO: Test successful operations\n    # TODO: Test error handling\n    # TODO: Test chaining operations\n    \n    console.print(\"✅ Result monad test completed!\")\n\ndef test_lazy_evaluation():\n    \"\"\"Test lazy evaluation.\"\"\"\n    console.print(\"[bold magenta]Testing Lazy Evaluation...[/bold magenta]\")\n    \n    # TODO: Test lazy map and filter\n    # TODO: Show that processing happens on demand\n    # TODO: Test lazy batching\n    \n    console.print(\"✅ Lazy evaluation test completed!\")\n\ndef test_functional_pipeline():\n    \"\"\"Test functional pipeline.\"\"\"\n    console.print(\"[bold cyan]Testing Functional Pipeline...[/bold cyan]\")\n    \n    # TODO: Create pipeline with multiple steps\n    # TODO: Test immutable addition of steps\n    # TODO: Test processing with error handling\n    \n    console.print(\"✅ Functional pipeline test completed!\")\n\n# Challenge tasks\n\ndef challenge_parallel_functional_processing():\n    \"\"\"Challenge: Add parallel processing while maintaining functional principles.\"\"\"\n    console.print(\"[bold red]Challenge: Parallel Functional Processing[/bold red]\")\n    \n    # TODO: Process data in parallel while keeping functions pure\n    # TODO: Use multiprocessing with functional approach\n    # TODO: Ensure thread safety through immutability\n    \n    console.print(\"🎯 Parallel functional processing challenge completed!\")\n\ndef challenge_streaming_functional_pipeline():\n    \"\"\"Challenge: Create a streaming version that processes infinite data.\"\"\"\n    console.print(\"[bold purple]Challenge: Streaming Functional Pipeline[/bold purple]\")\n    \n    # TODO: Handle infinite streams of data\n    # TODO: Process data lazily without loading everything into memory\n    # TODO: Support backpressure and flow control\n    \n    console.print(\"🎯 Streaming functional pipeline challenge completed!\")\n\ndef challenge_functional_error_recovery():\n    \"\"\"Challenge: Implement sophisticated error recovery with functional patterns.\"\"\"\n    console.print(\"[bold orange]Challenge: Functional Error Recovery[/bold orange]\")\n    \n    # TODO: Implement retry logic with exponential backoff\n    # TODO: Support fallback operations for failed steps\n    # TODO: Accumulate errors for batch reporting\n    \n    console.print(\"🎯 Functional error recovery challenge completed!\")\n\ndef challenge_functional_optimization():\n    \"\"\"Challenge: Optimize the pipeline using functional techniques.\"\"\"\n    console.print(\"[bold teal]Challenge: Functional Optimization[/bold teal]\")\n    \n    # TODO: Implement fusion optimization for composed functions\n    # TODO: Add intelligent memoization strategies\n    # TODO: Optimize using partial evaluation\n    \n    console.print(\"🎯 Functional optimization challenge completed!\")\n\n# Performance comparison\n\ndef compare_functional_vs_imperative():\n    \"\"\"Compare functional vs imperative approaches.\"\"\"\n    console.print(\"[bold gold]🏆 Functional vs Imperative Comparison[/bold gold]\")\n    \n    # TODO: Implement same pipeline in imperative style\n    # TODO: Compare performance, memory usage, maintainability\n    # TODO: Show benefits and trade-offs of functional approach\n    \n    console.print(\"📊 Comparison completed!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 3: Build a Functional AI Pipeline[/bold blue]\")\n    console.print(\"Create a functional programming-based AI processing system\\n\")\n    \n    # Run basic tests\n    test_immutable_data()\n    console.print()\n    \n    test_pure_functions()\n    console.print()\n    \n    test_result_monad()\n    console.print()\n    \n    test_lazy_evaluation()\n    console.print()\n    \n    test_functional_pipeline()\n    console.print()\n    \n    # Run challenges\n    challenge_parallel_functional_processing()\n    console.print()\n    \n    challenge_streaming_functional_pipeline()\n    console.print()\n    \n    challenge_functional_error_recovery()\n    console.print()\n    \n    challenge_functional_optimization()\n    console.print()\n    \n    # Run comparison\n    compare_functional_vs_imperative()\n    console.print()\n    \n    console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n    console.print(\"💡 Key learnings:\")\n    console.print(\"  • Immutability prevents bugs and enables parallelism\")\n    console.print(\"  • Pure functions are predictable and testable\")\n    console.print(\"  • Function composition enables modular design\")\n    console.print(\"  • Monadic error handling provides robust operation\")\n    console.print(\"  • Lazy evaluation optimizes resource usage\")","structuredPatch":[]}}
{"parentUuid":"65210e57-851f-4bcf-aab9-aa77f501737a","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01LFsTL3mTeqyBBiouu63Upg","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01Hakhk5yPYLsuaNaP11Xpka","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_04_dynamic_ai_framework.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 4: Build a Dynamic AI Framework\n\nYour task: Create a metaprogramming-powered AI framework that can dynamically\nadapt, self-configure, and extend its capabilities at runtime.\n\nRequirements:\n1. Use metaclasses to automatically register and configure AI models\n2. Create descriptors for intelligent property management\n3. Implement dynamic class generation for custom AI components\n4. Build decorators that add instrumentation and optimization\n5. Support runtime introspection and modification\n\nRun with: python 03-python-idioms/exercises/exercise_04_dynamic_ai_framework.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import Any, Dict, List, Type, Callable, Optional\nimport inspect\nimport time\nfrom functools import wraps\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom rich.console import Console\nfrom rich.table import Table\n\nconsole = Console()\n\n# TODO: Create custom descriptors for AI model properties\n\nclass ValidatedProperty:\n    \"\"\"TODO: Implement descriptor that validates property values.\"\"\"\n    pass\n\nclass CachedProperty:\n    \"\"\"TODO: Implement descriptor that caches expensive computations.\"\"\"\n    pass\n\nclass MetricProperty:\n    \"\"\"TODO: Implement descriptor that tracks property access metrics.\"\"\"\n    pass\n\n# TODO: Create metaclasses for automatic model management\n\nclass ModelRegistryMeta(type):\n    \"\"\"TODO: Implement metaclass that automatically registers AI models.\"\"\"\n    pass\n\nclass InstrumentedMeta(type):\n    \"\"\"TODO: Implement metaclass that adds automatic instrumentation.\"\"\"\n    pass\n\nclass ConfigurableMeta(type):\n    \"\"\"TODO: Implement metaclass that handles dynamic configuration.\"\"\"\n    pass\n\n# TODO: Create dynamic class generation utilities\n\ndef create_ai_model_class(\n    name: str,\n    capabilities: List[str],\n    config: Dict[str, Any]\n) -> Type:\n    \"\"\"TODO: Dynamically create AI model class with specified capabilities.\"\"\"\n    pass\n\ndef create_ensemble_class(model_classes: List[Type]) -> Type:\n    \"\"\"TODO: Create ensemble class from multiple model classes.\"\"\"\n    pass\n\ndef create_proxy_class(target_class: Type, middleware: List[Callable]) -> Type:\n    \"\"\"TODO: Create proxy class with middleware injection.\"\"\"\n    pass\n\n# TODO: Create advanced decorators\n\nclass PerformanceMonitor:\n    \"\"\"TODO: Decorator class that monitors method performance.\"\"\"\n    pass\n\nclass RetryWithBackoff:\n    \"\"\"TODO: Decorator class that adds retry logic with exponential backoff.\"\"\"\n    pass\n\nclass CacheWithTTL:\n    \"\"\"TODO: Decorator class that adds caching with time-to-live.\"\"\"\n    pass\n\ndef auto_optimize(optimization_type: str = \"memory\"):\n    \"\"\"TODO: Decorator that automatically optimizes method execution.\"\"\"\n    pass\n\ndef feature_flag(flag_name: str, default: bool = False):\n    \"\"\"TODO: Decorator that enables/disables features based on flags.\"\"\"\n    pass\n\n# TODO: Create base AI model class with metaclass\n\nclass BaseAIModel(metaclass=ModelRegistryMeta):\n    \"\"\"TODO: Implement base AI model with automatic registration.\"\"\"\n    pass\n\n# TODO: Create specific AI model implementations\n\nclass TextClassifier(BaseAIModel):\n    \"\"\"TODO: Implement text classifier with metaprogramming features.\"\"\"\n    pass\n\nclass ImageGenerator(BaseAIModel):\n    \"\"\"TODO: Implement image generator with metaprogramming features.\"\"\"\n    pass\n\n# TODO: Create dynamic configuration system\n\nclass DynamicConfig:\n    \"\"\"TODO: Implement dynamic configuration that can be modified at runtime.\"\"\"\n    pass\n\nclass ConfigurationManager:\n    \"\"\"TODO: Implement manager for dynamic configurations.\"\"\"\n    pass\n\n# TODO: Create runtime introspection utilities\n\ndef analyze_model_capabilities(model_class: Type) -> Dict[str, Any]:\n    \"\"\"TODO: Analyze model class and return capability information.\"\"\"\n    pass\n\ndef generate_model_documentation(model_class: Type) -> str:\n    \"\"\"TODO: Generate documentation for model class dynamically.\"\"\"\n    pass\n\ndef optimize_model_for_hardware(model_class: Type, hardware_info: Dict[str, Any]) -> Type:\n    \"\"\"TODO: Optimize model class for specific hardware configuration.\"\"\"\n    pass\n\n# TODO: Create plugin system\n\nclass PluginManager:\n    \"\"\"TODO: Implement plugin manager that can load capabilities dynamically.\"\"\"\n    pass\n\nclass AIPlugin:\n    \"\"\"TODO: Base class for AI plugins.\"\"\"\n    pass\n\n# TODO: Create adaptive framework\n\nclass AdaptiveAIFramework:\n    \"\"\"TODO: Main framework class that adapts based on usage patterns.\"\"\"\n    pass\n\n# Test functions to verify your implementation\n\ndef test_descriptors():\n    \"\"\"Test custom descriptors.\"\"\"\n    console.print(\"[bold blue]Testing Custom Descriptors...[/bold blue]\")\n    \n    # TODO: Create class with custom descriptors\n    # TODO: Test validation, caching, and metrics\n    # TODO: Show descriptor behavior\n    \n    console.print(\"✅ Descriptors test completed!\")\n\ndef test_metaclasses():\n    \"\"\"Test metaclass functionality.\"\"\"\n    console.print(\"[bold green]Testing Metaclasses...[/bold green]\")\n    \n    # TODO: Create classes using metaclasses\n    # TODO: Test automatic registration and instrumentation\n    # TODO: Show metaclass magic in action\n    \n    console.print(\"✅ Metaclasses test completed!\")\n\ndef test_dynamic_class_generation():\n    \"\"\"Test dynamic class generation.\"\"\"\n    console.print(\"[bold yellow]Testing Dynamic Class Generation...[/bold yellow]\")\n    \n    # TODO: Generate classes dynamically\n    # TODO: Test their functionality\n    # TODO: Show runtime class creation\n    \n    console.print(\"✅ Dynamic class generation test completed!\")\n\ndef test_advanced_decorators():\n    \"\"\"Test advanced decorator functionality.\"\"\"\n    console.print(\"[bold magenta]Testing Advanced Decorators...[/bold magenta]\")\n    \n    # TODO: Apply various decorators to methods\n    # TODO: Test performance monitoring, retries, caching\n    # TODO: Show decorator composition\n    \n    console.print(\"✅ Advanced decorators test completed!\")\n\ndef test_runtime_introspection():\n    \"\"\"Test runtime introspection and modification.\"\"\"\n    console.print(\"[bold cyan]Testing Runtime Introspection...[/bold cyan]\")\n    \n    # TODO: Analyze model capabilities\n    # TODO: Generate documentation\n    # TODO: Modify behavior at runtime\n    \n    console.print(\"✅ Runtime introspection test completed!\")\n\n# Challenge tasks\n\ndef challenge_self_optimizing_framework():\n    \"\"\"Challenge: Create a framework that optimizes itself based on usage patterns.\"\"\"\n    console.print(\"[bold red]Challenge: Self-Optimizing Framework[/bold red]\")\n    \n    # TODO: Track usage patterns and performance metrics\n    # TODO: Automatically optimize based on patterns\n    # TODO: Support A/B testing of optimizations\n    \n    console.print(\"🎯 Self-optimizing framework challenge completed!\")\n\ndef challenge_hot_swappable_models():\n    \"\"\"Challenge: Implement hot-swappable model updates without downtime.\"\"\"\n    console.print(\"[bold purple]Challenge: Hot-Swappable Models[/bold purple]\")\n    \n    # TODO: Support model updates without stopping service\n    # TODO: Implement gradual rollout of new models\n    # TODO: Handle version compatibility and rollback\n    \n    console.print(\"🎯 Hot-swappable models challenge completed!\")\n\ndef challenge_ai_model_marketplace():\n    \"\"\"Challenge: Create a marketplace where models can be discovered and loaded dynamically.\"\"\"\n    console.print(\"[bold orange]Challenge: AI Model Marketplace[/bold orange]\")\n    \n    # TODO: Support model discovery from registry\n    # TODO: Dynamic loading and validation of external models\n    # TODO: Sandboxing and security for untrusted models\n    \n    console.print(\"🎯 AI model marketplace challenge completed!\")\n\ndef challenge_code_generation():\n    \"\"\"Challenge: Generate optimized code at runtime based on requirements.\"\"\"\n    console.print(\"[bold teal]Challenge: Runtime Code Generation[/bold teal]\")\n    \n    # TODO: Generate optimal implementations based on data characteristics\n    # TODO: Support just-in-time compilation for performance\n    # TODO: Cache generated code for reuse\n    \n    console.print(\"🎯 Runtime code generation challenge completed!\")\n\n# Advanced features demonstration\n\ndef demonstrate_framework_evolution():\n    \"\"\"Demonstrate how the framework can evolve at runtime.\"\"\"\n    console.print(\"[bold gold]🧬 Framework Evolution Demonstration[/bold gold]\")\n    \n    # TODO: Show framework adapting to new requirements\n    # TODO: Demonstrate capability extension without restart\n    # TODO: Show learning from usage patterns\n    \n    console.print(\"🔄 Framework evolution demonstrated!\")\n\ndef demonstrate_metaprogramming_benefits():\n    \"\"\"Demonstrate benefits of metaprogramming approach.\"\"\"\n    console.print(\"[bold silver]💎 Metaprogramming Benefits[/bold silver]\")\n    \n    # TODO: Compare with traditional approaches\n    # TODO: Show reduced boilerplate and increased flexibility\n    # TODO: Demonstrate runtime adaptability\n    \n    console.print(\"✨ Metaprogramming benefits demonstrated!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 4: Build a Dynamic AI Framework[/bold blue]\")\n    console.print(\"Create a metaprogramming-powered adaptive AI system\\n\")\n    \n    # Run basic tests\n    test_descriptors()\n    console.print()\n    \n    test_metaclasses()\n    console.print()\n    \n    test_dynamic_class_generation()\n    console.print()\n    \n    test_advanced_decorators()\n    console.print()\n    \n    test_runtime_introspection()\n    console.print()\n    \n    # Run challenges\n    challenge_self_optimizing_framework()\n    console.print()\n    \n    challenge_hot_swappable_models()\n    console.print()\n    \n    challenge_ai_model_marketplace()\n    console.print()\n    \n    challenge_code_generation()\n    console.print()\n    \n    # Run demonstrations\n    demonstrate_framework_evolution()\n    console.print()\n    \n    demonstrate_metaprogramming_benefits()\n    console.print()\n    \n    console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n    console.print(\"💡 Key learnings:\")\n    console.print(\"  • Metaclasses enable automatic code generation\")\n    console.print(\"  • Descriptors provide intelligent property management\")\n    console.print(\"  • Dynamic class generation enables runtime flexibility\")\n    console.print(\"  • Advanced decorators add powerful cross-cutting concerns\")\n    console.print(\"  • Introspection enables self-adapting systems\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2886,"cache_read_input_tokens":81663,"output_tokens":23}},"requestId":"req_vrtx_011CRddkCyth7RjyX2XHaKrt","type":"assistant","uuid":"551ed036-fde6-4132-ac82-b52f2b5f3f40","timestamp":"2025-07-30T17:56:56.820Z"}
{"parentUuid":"551ed036-fde6-4132-ac82-b52f2b5f3f40","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01Hakhk5yPYLsuaNaP11Xpka","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_04_dynamic_ai_framework.py"}]},"uuid":"0c04a241-aee7-43de-82c0-654ddf919089","timestamp":"2025-07-30T17:56:56.855Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_04_dynamic_ai_framework.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 4: Build a Dynamic AI Framework\n\nYour task: Create a metaprogramming-powered AI framework that can dynamically\nadapt, self-configure, and extend its capabilities at runtime.\n\nRequirements:\n1. Use metaclasses to automatically register and configure AI models\n2. Create descriptors for intelligent property management\n3. Implement dynamic class generation for custom AI components\n4. Build decorators that add instrumentation and optimization\n5. Support runtime introspection and modification\n\nRun with: python 03-python-idioms/exercises/exercise_04_dynamic_ai_framework.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import Any, Dict, List, Type, Callable, Optional\nimport inspect\nimport time\nfrom functools import wraps\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom rich.console import Console\nfrom rich.table import Table\n\nconsole = Console()\n\n# TODO: Create custom descriptors for AI model properties\n\nclass ValidatedProperty:\n    \"\"\"TODO: Implement descriptor that validates property values.\"\"\"\n    pass\n\nclass CachedProperty:\n    \"\"\"TODO: Implement descriptor that caches expensive computations.\"\"\"\n    pass\n\nclass MetricProperty:\n    \"\"\"TODO: Implement descriptor that tracks property access metrics.\"\"\"\n    pass\n\n# TODO: Create metaclasses for automatic model management\n\nclass ModelRegistryMeta(type):\n    \"\"\"TODO: Implement metaclass that automatically registers AI models.\"\"\"\n    pass\n\nclass InstrumentedMeta(type):\n    \"\"\"TODO: Implement metaclass that adds automatic instrumentation.\"\"\"\n    pass\n\nclass ConfigurableMeta(type):\n    \"\"\"TODO: Implement metaclass that handles dynamic configuration.\"\"\"\n    pass\n\n# TODO: Create dynamic class generation utilities\n\ndef create_ai_model_class(\n    name: str,\n    capabilities: List[str],\n    config: Dict[str, Any]\n) -> Type:\n    \"\"\"TODO: Dynamically create AI model class with specified capabilities.\"\"\"\n    pass\n\ndef create_ensemble_class(model_classes: List[Type]) -> Type:\n    \"\"\"TODO: Create ensemble class from multiple model classes.\"\"\"\n    pass\n\ndef create_proxy_class(target_class: Type, middleware: List[Callable]) -> Type:\n    \"\"\"TODO: Create proxy class with middleware injection.\"\"\"\n    pass\n\n# TODO: Create advanced decorators\n\nclass PerformanceMonitor:\n    \"\"\"TODO: Decorator class that monitors method performance.\"\"\"\n    pass\n\nclass RetryWithBackoff:\n    \"\"\"TODO: Decorator class that adds retry logic with exponential backoff.\"\"\"\n    pass\n\nclass CacheWithTTL:\n    \"\"\"TODO: Decorator class that adds caching with time-to-live.\"\"\"\n    pass\n\ndef auto_optimize(optimization_type: str = \"memory\"):\n    \"\"\"TODO: Decorator that automatically optimizes method execution.\"\"\"\n    pass\n\ndef feature_flag(flag_name: str, default: bool = False):\n    \"\"\"TODO: Decorator that enables/disables features based on flags.\"\"\"\n    pass\n\n# TODO: Create base AI model class with metaclass\n\nclass BaseAIModel(metaclass=ModelRegistryMeta):\n    \"\"\"TODO: Implement base AI model with automatic registration.\"\"\"\n    pass\n\n# TODO: Create specific AI model implementations\n\nclass TextClassifier(BaseAIModel):\n    \"\"\"TODO: Implement text classifier with metaprogramming features.\"\"\"\n    pass\n\nclass ImageGenerator(BaseAIModel):\n    \"\"\"TODO: Implement image generator with metaprogramming features.\"\"\"\n    pass\n\n# TODO: Create dynamic configuration system\n\nclass DynamicConfig:\n    \"\"\"TODO: Implement dynamic configuration that can be modified at runtime.\"\"\"\n    pass\n\nclass ConfigurationManager:\n    \"\"\"TODO: Implement manager for dynamic configurations.\"\"\"\n    pass\n\n# TODO: Create runtime introspection utilities\n\ndef analyze_model_capabilities(model_class: Type) -> Dict[str, Any]:\n    \"\"\"TODO: Analyze model class and return capability information.\"\"\"\n    pass\n\ndef generate_model_documentation(model_class: Type) -> str:\n    \"\"\"TODO: Generate documentation for model class dynamically.\"\"\"\n    pass\n\ndef optimize_model_for_hardware(model_class: Type, hardware_info: Dict[str, Any]) -> Type:\n    \"\"\"TODO: Optimize model class for specific hardware configuration.\"\"\"\n    pass\n\n# TODO: Create plugin system\n\nclass PluginManager:\n    \"\"\"TODO: Implement plugin manager that can load capabilities dynamically.\"\"\"\n    pass\n\nclass AIPlugin:\n    \"\"\"TODO: Base class for AI plugins.\"\"\"\n    pass\n\n# TODO: Create adaptive framework\n\nclass AdaptiveAIFramework:\n    \"\"\"TODO: Main framework class that adapts based on usage patterns.\"\"\"\n    pass\n\n# Test functions to verify your implementation\n\ndef test_descriptors():\n    \"\"\"Test custom descriptors.\"\"\"\n    console.print(\"[bold blue]Testing Custom Descriptors...[/bold blue]\")\n    \n    # TODO: Create class with custom descriptors\n    # TODO: Test validation, caching, and metrics\n    # TODO: Show descriptor behavior\n    \n    console.print(\"✅ Descriptors test completed!\")\n\ndef test_metaclasses():\n    \"\"\"Test metaclass functionality.\"\"\"\n    console.print(\"[bold green]Testing Metaclasses...[/bold green]\")\n    \n    # TODO: Create classes using metaclasses\n    # TODO: Test automatic registration and instrumentation\n    # TODO: Show metaclass magic in action\n    \n    console.print(\"✅ Metaclasses test completed!\")\n\ndef test_dynamic_class_generation():\n    \"\"\"Test dynamic class generation.\"\"\"\n    console.print(\"[bold yellow]Testing Dynamic Class Generation...[/bold yellow]\")\n    \n    # TODO: Generate classes dynamically\n    # TODO: Test their functionality\n    # TODO: Show runtime class creation\n    \n    console.print(\"✅ Dynamic class generation test completed!\")\n\ndef test_advanced_decorators():\n    \"\"\"Test advanced decorator functionality.\"\"\"\n    console.print(\"[bold magenta]Testing Advanced Decorators...[/bold magenta]\")\n    \n    # TODO: Apply various decorators to methods\n    # TODO: Test performance monitoring, retries, caching\n    # TODO: Show decorator composition\n    \n    console.print(\"✅ Advanced decorators test completed!\")\n\ndef test_runtime_introspection():\n    \"\"\"Test runtime introspection and modification.\"\"\"\n    console.print(\"[bold cyan]Testing Runtime Introspection...[/bold cyan]\")\n    \n    # TODO: Analyze model capabilities\n    # TODO: Generate documentation\n    # TODO: Modify behavior at runtime\n    \n    console.print(\"✅ Runtime introspection test completed!\")\n\n# Challenge tasks\n\ndef challenge_self_optimizing_framework():\n    \"\"\"Challenge: Create a framework that optimizes itself based on usage patterns.\"\"\"\n    console.print(\"[bold red]Challenge: Self-Optimizing Framework[/bold red]\")\n    \n    # TODO: Track usage patterns and performance metrics\n    # TODO: Automatically optimize based on patterns\n    # TODO: Support A/B testing of optimizations\n    \n    console.print(\"🎯 Self-optimizing framework challenge completed!\")\n\ndef challenge_hot_swappable_models():\n    \"\"\"Challenge: Implement hot-swappable model updates without downtime.\"\"\"\n    console.print(\"[bold purple]Challenge: Hot-Swappable Models[/bold purple]\")\n    \n    # TODO: Support model updates without stopping service\n    # TODO: Implement gradual rollout of new models\n    # TODO: Handle version compatibility and rollback\n    \n    console.print(\"🎯 Hot-swappable models challenge completed!\")\n\ndef challenge_ai_model_marketplace():\n    \"\"\"Challenge: Create a marketplace where models can be discovered and loaded dynamically.\"\"\"\n    console.print(\"[bold orange]Challenge: AI Model Marketplace[/bold orange]\")\n    \n    # TODO: Support model discovery from registry\n    # TODO: Dynamic loading and validation of external models\n    # TODO: Sandboxing and security for untrusted models\n    \n    console.print(\"🎯 AI model marketplace challenge completed!\")\n\ndef challenge_code_generation():\n    \"\"\"Challenge: Generate optimized code at runtime based on requirements.\"\"\"\n    console.print(\"[bold teal]Challenge: Runtime Code Generation[/bold teal]\")\n    \n    # TODO: Generate optimal implementations based on data characteristics\n    # TODO: Support just-in-time compilation for performance\n    # TODO: Cache generated code for reuse\n    \n    console.print(\"🎯 Runtime code generation challenge completed!\")\n\n# Advanced features demonstration\n\ndef demonstrate_framework_evolution():\n    \"\"\"Demonstrate how the framework can evolve at runtime.\"\"\"\n    console.print(\"[bold gold]🧬 Framework Evolution Demonstration[/bold gold]\")\n    \n    # TODO: Show framework adapting to new requirements\n    # TODO: Demonstrate capability extension without restart\n    # TODO: Show learning from usage patterns\n    \n    console.print(\"🔄 Framework evolution demonstrated!\")\n\ndef demonstrate_metaprogramming_benefits():\n    \"\"\"Demonstrate benefits of metaprogramming approach.\"\"\"\n    console.print(\"[bold silver]💎 Metaprogramming Benefits[/bold silver]\")\n    \n    # TODO: Compare with traditional approaches\n    # TODO: Show reduced boilerplate and increased flexibility\n    # TODO: Demonstrate runtime adaptability\n    \n    console.print(\"✨ Metaprogramming benefits demonstrated!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 4: Build a Dynamic AI Framework[/bold blue]\")\n    console.print(\"Create a metaprogramming-powered adaptive AI system\\n\")\n    \n    # Run basic tests\n    test_descriptors()\n    console.print()\n    \n    test_metaclasses()\n    console.print()\n    \n    test_dynamic_class_generation()\n    console.print()\n    \n    test_advanced_decorators()\n    console.print()\n    \n    test_runtime_introspection()\n    console.print()\n    \n    # Run challenges\n    challenge_self_optimizing_framework()\n    console.print()\n    \n    challenge_hot_swappable_models()\n    console.print()\n    \n    challenge_ai_model_marketplace()\n    console.print()\n    \n    challenge_code_generation()\n    console.print()\n    \n    # Run demonstrations\n    demonstrate_framework_evolution()\n    console.print()\n    \n    demonstrate_metaprogramming_benefits()\n    console.print()\n    \n    console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n    console.print(\"💡 Key learnings:\")\n    console.print(\"  • Metaclasses enable automatic code generation\")\n    console.print(\"  • Descriptors provide intelligent property management\")\n    console.print(\"  • Dynamic class generation enables runtime flexibility\")\n    console.print(\"  • Advanced decorators add powerful cross-cutting concerns\")\n    console.print(\"  • Introspection enables self-adapting systems\")","structuredPatch":[]}}
{"parentUuid":"0c04a241-aee7-43de-82c0-654ddf919089","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01Pw2NncZGhsoyZzdoMtugSP","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01RE7845RNUJ3QZFcRi98qiy","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_05_high_performance_ai.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 5: Build a High-Performance AI System\n\nYour task: Create a high-performance AI system that maximizes throughput,\nminimizes latency, and efficiently manages resources for production workloads.\n\nRequirements:\n1. Implement efficient memory management and object pooling\n2. Create vectorized operations for batch processing\n3. Add parallel and concurrent processing capabilities\n4. Implement intelligent caching and memoization strategies\n5. Optimize garbage collection and resource lifecycle\n\nRun with: python 03-python-idioms/exercises/exercise_05_high_performance_ai.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nimport time\nimport gc\nimport threading\nimport multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nfrom typing import List, Dict, Any, Callable, Iterator, Optional\nfrom dataclasses import dataclass\nfrom contextlib import contextmanager\nimport numpy as np\nfrom collections import deque, defaultdict\nimport weakref\nfrom rich.console import Console\nfrom rich.progress import Progress\n\nconsole = Console()\n\n# TODO: Create memory-efficient data structures\n\nclass RingBuffer:\n    \"\"\"TODO: Implement memory-efficient ring buffer for streaming data.\"\"\"\n    pass\n\nclass ObjectPool:\n    \"\"\"TODO: Implement object pool for expensive-to-create objects.\"\"\"\n    pass\n\nclass MemoryEfficientCache:\n    \"\"\"TODO: Implement cache with memory limits and smart eviction.\"\"\"\n    pass\n\n# TODO: Create performance monitoring utilities\n\nclass PerformanceProfiler:\n    \"\"\"TODO: Implement performance profiler for AI operations.\"\"\"\n    pass\n\n@contextmanager\ndef memory_tracker():\n    \"\"\"TODO: Context manager for tracking memory usage.\"\"\"\n    pass\n\n@contextmanager\ndef cpu_timer():\n    \"\"\"TODO: Context manager for timing CPU operations.\"\"\"\n    pass\n\n# TODO: Create vectorized processing utilities\n\nclass VectorizedProcessor:\n    \"\"\"TODO: Implement vectorized operations for AI data processing.\"\"\"\n    \n    @staticmethod\n    def batch_normalize(data: np.ndarray) -> np.ndarray:\n        \"\"\"TODO: Vectorized batch normalization.\"\"\"\n        pass\n    \n    @staticmethod\n    def batch_tokenize(texts: List[str]) -> List[List[int]]:\n        \"\"\"TODO: Vectorized text tokenization.\"\"\"\n        pass\n    \n    @staticmethod\n    def batch_similarity(vectors: np.ndarray) -> np.ndarray:\n        \"\"\"TODO: Vectorized similarity computation.\"\"\"\n        pass\n\n# TODO: Create parallel processing framework\n\nclass ParallelAIProcessor:\n    \"\"\"TODO: Implement parallel processing for AI workloads.\"\"\"\n    \n    def __init__(self, max_workers: int = None):\n        \"\"\"TODO: Initialize with optimal worker count.\"\"\"\n        pass\n    \n    def process_parallel_cpu(self, func: Callable, data: List[Any]) -> List[Any]:\n        \"\"\"TODO: Process CPU-bound tasks in parallel.\"\"\"\n        pass\n    \n    def process_parallel_io(self, func: Callable, data: List[Any]) -> List[Any]:\n        \"\"\"TODO: Process I/O-bound tasks in parallel.\"\"\"\n        pass\n    \n    def process_streaming(self, func: Callable, data_stream: Iterator[Any]) -> Iterator[Any]:\n        \"\"\"TODO: Process streaming data with parallelism.\"\"\"\n        pass\n\n# TODO: Create intelligent caching system\n\nclass IntelligentCache:\n    \"\"\"TODO: Implement cache with adaptive strategies.\"\"\"\n    \n    def __init__(self, max_memory_mb: float = 100):\n        \"\"\"TODO: Initialize cache with memory limit.\"\"\"\n        pass\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"TODO: Get item from cache with access tracking.\"\"\"\n        pass\n    \n    def put(self, key: str, value: Any, ttl: Optional[float] = None) -> None:\n        \"\"\"TODO: Put item in cache with optional TTL.\"\"\"\n        pass\n    \n    def optimize_cache(self) -> None:\n        \"\"\"TODO: Optimize cache based on access patterns.\"\"\"\n        pass\n\n# TODO: Create batch processing optimizations\n\nclass BatchProcessor:\n    \"\"\"TODO: Implement optimized batch processing.\"\"\"\n    \n    def __init__(self, batch_size: int = 32, max_memory_mb: float = 50):\n        \"\"\"TODO: Initialize batch processor.\"\"\"\n        pass\n    \n    def process_batches(self, func: Callable, data: List[Any]) -> List[Any]:\n        \"\"\"TODO: Process data in optimized batches.\"\"\"\n        pass\n    \n    def adaptive_batch_size(self, func: Callable, sample_data: List[Any]) -> int:\n        \"\"\"TODO: Determine optimal batch size automatically.\"\"\"\n        pass\n\n# TODO: Create resource management system\n\nclass ResourceManager:\n    \"\"\"TODO: Implement intelligent resource management.\"\"\"\n    \n    def __init__(self):\n        \"\"\"TODO: Initialize resource manager.\"\"\"\n        pass\n    \n    def allocate_resources(self, operation_type: str, estimated_load: float) -> Dict[str, Any]:\n        \"\"\"TODO: Allocate resources based on operation requirements.\"\"\"\n        pass\n    \n    def monitor_usage(self) -> Dict[str, float]:\n        \"\"\"TODO: Monitor current resource usage.\"\"\"\n        pass\n    \n    def optimize_allocation(self) -> None:\n        \"\"\"TODO: Optimize resource allocation based on usage patterns.\"\"\"\n        pass\n\n# TODO: Create high-performance AI model interface\n\nclass HighPerformanceAIModel:\n    \"\"\"TODO: Implement high-performance AI model with optimizations.\"\"\"\n    \n    def __init__(self, model_config: Dict[str, Any]):\n        \"\"\"TODO: Initialize model with performance optimizations.\"\"\"\n        pass\n    \n    def predict_single(self, input_data: Any) -> Any:\n        \"\"\"TODO: Single prediction with optimizations.\"\"\"\n        pass\n    \n    def predict_batch(self, input_batch: List[Any]) -> List[Any]:\n        \"\"\"TODO: Batch prediction with vectorization.\"\"\"\n        pass\n    \n    def predict_streaming(self, input_stream: Iterator[Any]) -> Iterator[Any]:\n        \"\"\"TODO: Streaming prediction with efficient processing.\"\"\"\n        pass\n\n# TODO: Create garbage collection optimizations\n\nclass GCOptimizer:\n    \"\"\"TODO: Implement garbage collection optimizations.\"\"\"\n    \n    @staticmethod\n    def optimize_for_ai_workload():\n        \"\"\"TODO: Configure GC settings for AI workloads.\"\"\"\n        pass\n    \n    @staticmethod\n    def minimize_gc_pauses():\n        \"\"\"TODO: Minimize garbage collection pauses.\"\"\"\n        pass\n    \n    @contextmanager\n    def gc_disabled():\n        \"\"\"TODO: Context manager to temporarily disable GC.\"\"\"\n        pass\n\n# TODO: Create load balancing and scaling utilities\n\nclass LoadBalancer:\n    \"\"\"TODO: Implement load balancer for AI model instances.\"\"\"\n    \n    def __init__(self, models: List[HighPerformanceAIModel]):\n        \"\"\"TODO: Initialize load balancer.\"\"\"\n        pass\n    \n    def get_best_model(self, request_size: int) -> HighPerformanceAIModel:\n        \"\"\"TODO: Select best model based on current load.\"\"\"\n        pass\n    \n    def distribute_load(self, requests: List[Any]) -> Dict[HighPerformanceAIModel, List[Any]]:\n        \"\"\"TODO: Distribute requests across models optimally.\"\"\"\n        pass\n\n# Test functions to verify your implementation\n\ndef test_memory_efficiency():\n    \"\"\"Test memory-efficient data structures.\"\"\"\n    console.print(\"[bold blue]Testing Memory Efficiency...[/bold blue]\")\n    \n    # TODO: Test object pools, caches, and memory tracking\n    # TODO: Show memory usage improvements\n    # TODO: Test memory leak prevention\n    \n    console.print(\"✅ Memory efficiency test completed!\")\n\ndef test_vectorized_operations():\n    \"\"\"Test vectorized processing.\"\"\"\n    console.print(\"[bold green]Testing Vectorized Operations...[/bold green]\")\n    \n    # TODO: Compare scalar vs vectorized operations\n    # TODO: Show performance improvements\n    # TODO: Test different data sizes\n    \n    console.print(\"✅ Vectorized operations test completed!\")\n\ndef test_parallel_processing():\n    \"\"\"Test parallel processing capabilities.\"\"\"\n    console.print(\"[bold yellow]Testing Parallel Processing...[/bold yellow]\")\n    \n    # TODO: Test CPU and I/O bound parallel processing\n    # TODO: Show scalability with worker count\n    # TODO: Test streaming parallel processing\n    \n    console.print(\"✅ Parallel processing test completed!\")\n\ndef test_intelligent_caching():\n    \"\"\"Test intelligent caching system.\"\"\"\n    console.print(\"[bold magenta]Testing Intelligent Caching...[/bold magenta]\")\n    \n    # TODO: Test cache hit rates and optimization\n    # TODO: Show cache adaptation to access patterns\n    # TODO: Test memory-bounded caching\n    \n    console.print(\"✅ Intelligent caching test completed!\")\n\ndef test_batch_optimization():\n    \"\"\"Test batch processing optimizations.\"\"\"\n    console.print(\"[bold cyan]Testing Batch Optimization...[/bold cyan]\")\n    \n    # TODO: Test adaptive batch sizing\n    # TODO: Show throughput improvements\n    # TODO: Test memory-efficient batching\n    \n    console.print(\"✅ Batch optimization test completed!\")\n\n# Challenge tasks\n\ndef challenge_auto_scaling():\n    \"\"\"Challenge: Implement auto-scaling based on load and performance metrics.\"\"\"\n    console.print(\"[bold red]Challenge: Auto-Scaling[/bold red]\")\n    \n    # TODO: Monitor performance metrics\n    # TODO: Automatically scale resources up/down\n    # TODO: Predict load and pre-scale\n    \n    console.print(\"🎯 Auto-scaling challenge completed!\")\n\ndef challenge_numa_optimization():\n    \"\"\"Challenge: Optimize for NUMA (Non-Uniform Memory Access) systems.\"\"\"\n    console.print(\"[bold purple]Challenge: NUMA Optimization[/bold purple]\")\n    \n    # TODO: Detect NUMA topology\n    # TODO: Optimize memory allocation and thread placement\n    # TODO: Minimize cross-NUMA traffic\n    \n    console.print(\"🎯 NUMA optimization challenge completed!\")\n\ndef challenge_gpu_acceleration():\n    \"\"\"Challenge: Add GPU acceleration for compute-intensive operations.\"\"\"\n    console.print(\"[bold orange]Challenge: GPU Acceleration[/bold orange]\")\n    \n    # TODO: Detect GPU capabilities\n    # TODO: Offload appropriate operations to GPU\n    # TODO: Manage GPU memory efficiently\n    \n    console.print(\"🎯 GPU acceleration challenge completed!\")\n\ndef challenge_real_time_optimization():\n    \"\"\"Challenge: Optimize for real-time, low-latency requirements.\"\"\"\n    console.print(\"[bold teal]Challenge: Real-Time Optimization[/bold teal]\")\n    \n    # TODO: Minimize allocation in hot paths\n    # TODO: Pre-allocate resources and warm up models\n    # TODO: Implement deterministic execution times\n    \n    console.print(\"🎯 Real-time optimization challenge completed!\")\n\n# Performance benchmarks\n\ndef benchmark_throughput():\n    \"\"\"Benchmark system throughput under various conditions.\"\"\"\n    console.print(\"[bold gold]🏆 Throughput Benchmark[/bold gold]\")\n    \n    # TODO: Test throughput with different configurations\n    # TODO: Compare optimized vs unoptimized versions\n    # TODO: Show scalability curves\n    \n    console.print(\"📊 Throughput benchmark completed!\")\n\ndef benchmark_latency():\n    \"\"\"Benchmark system latency and responsiveness.\"\"\"\n    console.print(\"[bold silver]⚡ Latency Benchmark[/bold silver]\")\n    \n    # TODO: Test latency under different loads\n    # TODO: Measure p99 latencies\n    # TODO: Show latency improvements\n    \n    console.print(\"📊 Latency benchmark completed!\")\n\ndef benchmark_resource_efficiency():\n    \"\"\"Benchmark resource efficiency (CPU, memory, etc.).\"\"\"\n    console.print(\"[bold bronze]🔋 Resource Efficiency Benchmark[/bold bronze]\")\n    \n    # TODO: Measure CPU and memory efficiency\n    # TODO: Test resource scaling characteristics\n    # TODO: Compare resource usage before/after optimizations\n    \n    console.print(\"📊 Resource efficiency benchmark completed!\")\n\n# Comprehensive performance analysis\n\ndef analyze_performance_profile():\n    \"\"\"Analyze comprehensive performance profile of the system.\"\"\"\n    console.print(\"[bold violet]🔬 Performance Analysis[/bold violet]\")\n    \n    # TODO: Profile CPU usage, memory allocation, I/O patterns\n    # TODO: Identify bottlenecks and optimization opportunities\n    # TODO: Generate performance report with recommendations\n    \n    console.print(\"📈 Performance analysis completed!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 5: Build a High-Performance AI System[/bold blue]\")\n    console.print(\"Create a production-ready, high-performance AI processing system\\n\")\n    \n    # Run basic tests\n    test_memory_efficiency()\n    console.print()\n    \n    test_vectorized_operations()\n    console.print()\n    \n    test_parallel_processing()\n    console.print()\n    \n    test_intelligent_caching()\n    console.print()\n    \n    test_batch_optimization()\n    console.print()\n    \n    # Run challenges\n    challenge_auto_scaling()\n    console.print()\n    \n    challenge_numa_optimization()\n    console.print()\n    \n    challenge_gpu_acceleration()\n    console.print()\n    \n    challenge_real_time_optimization()\n    console.print()\n    \n    # Run benchmarks\n    benchmark_throughput()\n    console.print()\n    \n    benchmark_latency()\n    console.print()\n    \n    benchmark_resource_efficiency()\n    console.print()\n    \n    # Run analysis\n    analyze_performance_profile()\n    console.print()\n    \n    console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n    console.print(\"💡 Key learnings:\")\n    console.print(\"  • Memory management is crucial for performance\")\n    console.print(\"  • Vectorization provides significant speedups\")\n    console.print(\"  • Parallelism scales performance with hardware\")\n    console.print(\"  • Intelligent caching reduces redundant computation\")\n    console.print(\"  • Profiling guides optimization efforts\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2739,"cache_read_input_tokens":84549,"output_tokens":18}},"requestId":"req_vrtx_011CRddoU4LZmhX7FNCxNkyc","type":"assistant","uuid":"0ef8b26b-cbd6-4493-8820-88297d22d414","timestamp":"2025-07-30T17:57:52.792Z"}
{"parentUuid":"0ef8b26b-cbd6-4493-8820-88297d22d414","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01RE7845RNUJ3QZFcRi98qiy","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_05_high_performance_ai.py"}]},"uuid":"671f3d60-f604-4996-8e0c-d13c0d015009","timestamp":"2025-07-30T17:57:52.825Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/exercises/exercise_05_high_performance_ai.py","content":"#!/usr/bin/env python3\n\"\"\"\nExercise 5: Build a High-Performance AI System\n\nYour task: Create a high-performance AI system that maximizes throughput,\nminimizes latency, and efficiently manages resources for production workloads.\n\nRequirements:\n1. Implement efficient memory management and object pooling\n2. Create vectorized operations for batch processing\n3. Add parallel and concurrent processing capabilities\n4. Implement intelligent caching and memoization strategies\n5. Optimize garbage collection and resource lifecycle\n\nRun with: python 03-python-idioms/exercises/exercise_05_high_performance_ai.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nimport time\nimport gc\nimport threading\nimport multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nfrom typing import List, Dict, Any, Callable, Iterator, Optional\nfrom dataclasses import dataclass\nfrom contextlib import contextmanager\nimport numpy as np\nfrom collections import deque, defaultdict\nimport weakref\nfrom rich.console import Console\nfrom rich.progress import Progress\n\nconsole = Console()\n\n# TODO: Create memory-efficient data structures\n\nclass RingBuffer:\n    \"\"\"TODO: Implement memory-efficient ring buffer for streaming data.\"\"\"\n    pass\n\nclass ObjectPool:\n    \"\"\"TODO: Implement object pool for expensive-to-create objects.\"\"\"\n    pass\n\nclass MemoryEfficientCache:\n    \"\"\"TODO: Implement cache with memory limits and smart eviction.\"\"\"\n    pass\n\n# TODO: Create performance monitoring utilities\n\nclass PerformanceProfiler:\n    \"\"\"TODO: Implement performance profiler for AI operations.\"\"\"\n    pass\n\n@contextmanager\ndef memory_tracker():\n    \"\"\"TODO: Context manager for tracking memory usage.\"\"\"\n    pass\n\n@contextmanager\ndef cpu_timer():\n    \"\"\"TODO: Context manager for timing CPU operations.\"\"\"\n    pass\n\n# TODO: Create vectorized processing utilities\n\nclass VectorizedProcessor:\n    \"\"\"TODO: Implement vectorized operations for AI data processing.\"\"\"\n    \n    @staticmethod\n    def batch_normalize(data: np.ndarray) -> np.ndarray:\n        \"\"\"TODO: Vectorized batch normalization.\"\"\"\n        pass\n    \n    @staticmethod\n    def batch_tokenize(texts: List[str]) -> List[List[int]]:\n        \"\"\"TODO: Vectorized text tokenization.\"\"\"\n        pass\n    \n    @staticmethod\n    def batch_similarity(vectors: np.ndarray) -> np.ndarray:\n        \"\"\"TODO: Vectorized similarity computation.\"\"\"\n        pass\n\n# TODO: Create parallel processing framework\n\nclass ParallelAIProcessor:\n    \"\"\"TODO: Implement parallel processing for AI workloads.\"\"\"\n    \n    def __init__(self, max_workers: int = None):\n        \"\"\"TODO: Initialize with optimal worker count.\"\"\"\n        pass\n    \n    def process_parallel_cpu(self, func: Callable, data: List[Any]) -> List[Any]:\n        \"\"\"TODO: Process CPU-bound tasks in parallel.\"\"\"\n        pass\n    \n    def process_parallel_io(self, func: Callable, data: List[Any]) -> List[Any]:\n        \"\"\"TODO: Process I/O-bound tasks in parallel.\"\"\"\n        pass\n    \n    def process_streaming(self, func: Callable, data_stream: Iterator[Any]) -> Iterator[Any]:\n        \"\"\"TODO: Process streaming data with parallelism.\"\"\"\n        pass\n\n# TODO: Create intelligent caching system\n\nclass IntelligentCache:\n    \"\"\"TODO: Implement cache with adaptive strategies.\"\"\"\n    \n    def __init__(self, max_memory_mb: float = 100):\n        \"\"\"TODO: Initialize cache with memory limit.\"\"\"\n        pass\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"TODO: Get item from cache with access tracking.\"\"\"\n        pass\n    \n    def put(self, key: str, value: Any, ttl: Optional[float] = None) -> None:\n        \"\"\"TODO: Put item in cache with optional TTL.\"\"\"\n        pass\n    \n    def optimize_cache(self) -> None:\n        \"\"\"TODO: Optimize cache based on access patterns.\"\"\"\n        pass\n\n# TODO: Create batch processing optimizations\n\nclass BatchProcessor:\n    \"\"\"TODO: Implement optimized batch processing.\"\"\"\n    \n    def __init__(self, batch_size: int = 32, max_memory_mb: float = 50):\n        \"\"\"TODO: Initialize batch processor.\"\"\"\n        pass\n    \n    def process_batches(self, func: Callable, data: List[Any]) -> List[Any]:\n        \"\"\"TODO: Process data in optimized batches.\"\"\"\n        pass\n    \n    def adaptive_batch_size(self, func: Callable, sample_data: List[Any]) -> int:\n        \"\"\"TODO: Determine optimal batch size automatically.\"\"\"\n        pass\n\n# TODO: Create resource management system\n\nclass ResourceManager:\n    \"\"\"TODO: Implement intelligent resource management.\"\"\"\n    \n    def __init__(self):\n        \"\"\"TODO: Initialize resource manager.\"\"\"\n        pass\n    \n    def allocate_resources(self, operation_type: str, estimated_load: float) -> Dict[str, Any]:\n        \"\"\"TODO: Allocate resources based on operation requirements.\"\"\"\n        pass\n    \n    def monitor_usage(self) -> Dict[str, float]:\n        \"\"\"TODO: Monitor current resource usage.\"\"\"\n        pass\n    \n    def optimize_allocation(self) -> None:\n        \"\"\"TODO: Optimize resource allocation based on usage patterns.\"\"\"\n        pass\n\n# TODO: Create high-performance AI model interface\n\nclass HighPerformanceAIModel:\n    \"\"\"TODO: Implement high-performance AI model with optimizations.\"\"\"\n    \n    def __init__(self, model_config: Dict[str, Any]):\n        \"\"\"TODO: Initialize model with performance optimizations.\"\"\"\n        pass\n    \n    def predict_single(self, input_data: Any) -> Any:\n        \"\"\"TODO: Single prediction with optimizations.\"\"\"\n        pass\n    \n    def predict_batch(self, input_batch: List[Any]) -> List[Any]:\n        \"\"\"TODO: Batch prediction with vectorization.\"\"\"\n        pass\n    \n    def predict_streaming(self, input_stream: Iterator[Any]) -> Iterator[Any]:\n        \"\"\"TODO: Streaming prediction with efficient processing.\"\"\"\n        pass\n\n# TODO: Create garbage collection optimizations\n\nclass GCOptimizer:\n    \"\"\"TODO: Implement garbage collection optimizations.\"\"\"\n    \n    @staticmethod\n    def optimize_for_ai_workload():\n        \"\"\"TODO: Configure GC settings for AI workloads.\"\"\"\n        pass\n    \n    @staticmethod\n    def minimize_gc_pauses():\n        \"\"\"TODO: Minimize garbage collection pauses.\"\"\"\n        pass\n    \n    @contextmanager\n    def gc_disabled():\n        \"\"\"TODO: Context manager to temporarily disable GC.\"\"\"\n        pass\n\n# TODO: Create load balancing and scaling utilities\n\nclass LoadBalancer:\n    \"\"\"TODO: Implement load balancer for AI model instances.\"\"\"\n    \n    def __init__(self, models: List[HighPerformanceAIModel]):\n        \"\"\"TODO: Initialize load balancer.\"\"\"\n        pass\n    \n    def get_best_model(self, request_size: int) -> HighPerformanceAIModel:\n        \"\"\"TODO: Select best model based on current load.\"\"\"\n        pass\n    \n    def distribute_load(self, requests: List[Any]) -> Dict[HighPerformanceAIModel, List[Any]]:\n        \"\"\"TODO: Distribute requests across models optimally.\"\"\"\n        pass\n\n# Test functions to verify your implementation\n\ndef test_memory_efficiency():\n    \"\"\"Test memory-efficient data structures.\"\"\"\n    console.print(\"[bold blue]Testing Memory Efficiency...[/bold blue]\")\n    \n    # TODO: Test object pools, caches, and memory tracking\n    # TODO: Show memory usage improvements\n    # TODO: Test memory leak prevention\n    \n    console.print(\"✅ Memory efficiency test completed!\")\n\ndef test_vectorized_operations():\n    \"\"\"Test vectorized processing.\"\"\"\n    console.print(\"[bold green]Testing Vectorized Operations...[/bold green]\")\n    \n    # TODO: Compare scalar vs vectorized operations\n    # TODO: Show performance improvements\n    # TODO: Test different data sizes\n    \n    console.print(\"✅ Vectorized operations test completed!\")\n\ndef test_parallel_processing():\n    \"\"\"Test parallel processing capabilities.\"\"\"\n    console.print(\"[bold yellow]Testing Parallel Processing...[/bold yellow]\")\n    \n    # TODO: Test CPU and I/O bound parallel processing\n    # TODO: Show scalability with worker count\n    # TODO: Test streaming parallel processing\n    \n    console.print(\"✅ Parallel processing test completed!\")\n\ndef test_intelligent_caching():\n    \"\"\"Test intelligent caching system.\"\"\"\n    console.print(\"[bold magenta]Testing Intelligent Caching...[/bold magenta]\")\n    \n    # TODO: Test cache hit rates and optimization\n    # TODO: Show cache adaptation to access patterns\n    # TODO: Test memory-bounded caching\n    \n    console.print(\"✅ Intelligent caching test completed!\")\n\ndef test_batch_optimization():\n    \"\"\"Test batch processing optimizations.\"\"\"\n    console.print(\"[bold cyan]Testing Batch Optimization...[/bold cyan]\")\n    \n    # TODO: Test adaptive batch sizing\n    # TODO: Show throughput improvements\n    # TODO: Test memory-efficient batching\n    \n    console.print(\"✅ Batch optimization test completed!\")\n\n# Challenge tasks\n\ndef challenge_auto_scaling():\n    \"\"\"Challenge: Implement auto-scaling based on load and performance metrics.\"\"\"\n    console.print(\"[bold red]Challenge: Auto-Scaling[/bold red]\")\n    \n    # TODO: Monitor performance metrics\n    # TODO: Automatically scale resources up/down\n    # TODO: Predict load and pre-scale\n    \n    console.print(\"🎯 Auto-scaling challenge completed!\")\n\ndef challenge_numa_optimization():\n    \"\"\"Challenge: Optimize for NUMA (Non-Uniform Memory Access) systems.\"\"\"\n    console.print(\"[bold purple]Challenge: NUMA Optimization[/bold purple]\")\n    \n    # TODO: Detect NUMA topology\n    # TODO: Optimize memory allocation and thread placement\n    # TODO: Minimize cross-NUMA traffic\n    \n    console.print(\"🎯 NUMA optimization challenge completed!\")\n\ndef challenge_gpu_acceleration():\n    \"\"\"Challenge: Add GPU acceleration for compute-intensive operations.\"\"\"\n    console.print(\"[bold orange]Challenge: GPU Acceleration[/bold orange]\")\n    \n    # TODO: Detect GPU capabilities\n    # TODO: Offload appropriate operations to GPU\n    # TODO: Manage GPU memory efficiently\n    \n    console.print(\"🎯 GPU acceleration challenge completed!\")\n\ndef challenge_real_time_optimization():\n    \"\"\"Challenge: Optimize for real-time, low-latency requirements.\"\"\"\n    console.print(\"[bold teal]Challenge: Real-Time Optimization[/bold teal]\")\n    \n    # TODO: Minimize allocation in hot paths\n    # TODO: Pre-allocate resources and warm up models\n    # TODO: Implement deterministic execution times\n    \n    console.print(\"🎯 Real-time optimization challenge completed!\")\n\n# Performance benchmarks\n\ndef benchmark_throughput():\n    \"\"\"Benchmark system throughput under various conditions.\"\"\"\n    console.print(\"[bold gold]🏆 Throughput Benchmark[/bold gold]\")\n    \n    # TODO: Test throughput with different configurations\n    # TODO: Compare optimized vs unoptimized versions\n    # TODO: Show scalability curves\n    \n    console.print(\"📊 Throughput benchmark completed!\")\n\ndef benchmark_latency():\n    \"\"\"Benchmark system latency and responsiveness.\"\"\"\n    console.print(\"[bold silver]⚡ Latency Benchmark[/bold silver]\")\n    \n    # TODO: Test latency under different loads\n    # TODO: Measure p99 latencies\n    # TODO: Show latency improvements\n    \n    console.print(\"📊 Latency benchmark completed!\")\n\ndef benchmark_resource_efficiency():\n    \"\"\"Benchmark resource efficiency (CPU, memory, etc.).\"\"\"\n    console.print(\"[bold bronze]🔋 Resource Efficiency Benchmark[/bold bronze]\")\n    \n    # TODO: Measure CPU and memory efficiency\n    # TODO: Test resource scaling characteristics\n    # TODO: Compare resource usage before/after optimizations\n    \n    console.print(\"📊 Resource efficiency benchmark completed!\")\n\n# Comprehensive performance analysis\n\ndef analyze_performance_profile():\n    \"\"\"Analyze comprehensive performance profile of the system.\"\"\"\n    console.print(\"[bold violet]🔬 Performance Analysis[/bold violet]\")\n    \n    # TODO: Profile CPU usage, memory allocation, I/O patterns\n    # TODO: Identify bottlenecks and optimization opportunities\n    # TODO: Generate performance report with recommendations\n    \n    console.print(\"📈 Performance analysis completed!\")\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]🚀 Exercise 5: Build a High-Performance AI System[/bold blue]\")\n    console.print(\"Create a production-ready, high-performance AI processing system\\n\")\n    \n    # Run basic tests\n    test_memory_efficiency()\n    console.print()\n    \n    test_vectorized_operations()\n    console.print()\n    \n    test_parallel_processing()\n    console.print()\n    \n    test_intelligent_caching()\n    console.print()\n    \n    test_batch_optimization()\n    console.print()\n    \n    # Run challenges\n    challenge_auto_scaling()\n    console.print()\n    \n    challenge_numa_optimization()\n    console.print()\n    \n    challenge_gpu_acceleration()\n    console.print()\n    \n    challenge_real_time_optimization()\n    console.print()\n    \n    # Run benchmarks\n    benchmark_throughput()\n    console.print()\n    \n    benchmark_latency()\n    console.print()\n    \n    benchmark_resource_efficiency()\n    console.print()\n    \n    # Run analysis\n    analyze_performance_profile()\n    console.print()\n    \n    console.print(\"[bold green]🎉 Exercise completed![/bold green]\")\n    console.print(\"💡 Key learnings:\")\n    console.print(\"  • Memory management is crucial for performance\")\n    console.print(\"  • Vectorization provides significant speedups\")\n    console.print(\"  • Parallelism scales performance with hardware\")\n    console.print(\"  • Intelligent caching reduces redundant computation\")\n    console.print(\"  • Profiling guides optimization efforts\")","structuredPatch":[]}}
{"parentUuid":"671f3d60-f604-4996-8e0c-d13c0d015009","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_012yA5PZj7MhSLGudeS7umN8","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01D6fXYtrKVEBSADgQzKdATJ","name":"TodoWrite","input":{"todos":[{"id":"1","content":"Create 03_functional_programming.py example file","status":"completed","priority":"high"},{"id":"2","content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high"},{"id":"3","content":"Create 05_performance_optimization.py example file","status":"completed","priority":"high"},{"id":"4","content":"Create tests for the Python idioms examples","status":"completed","priority":"medium"},{"id":"5","content":"Create exercises for hands-on practice","status":"completed","priority":"medium"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":3600,"cache_read_input_tokens":87288,"output_tokens":23}},"requestId":"req_vrtx_011CRddsaqjW4y7PPh83DMM8","type":"assistant","uuid":"f48cab07-e6ef-45ea-a99a-0ebfb3d65319","timestamp":"2025-07-30T17:57:58.586Z"}
{"parentUuid":"f48cab07-e6ef-45ea-a99a-0ebfb3d65319","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01D6fXYtrKVEBSADgQzKdATJ","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"4df55a14-4754-4588-99f2-f4a2d17f3c3b","timestamp":"2025-07-30T17:57:58.621Z","toolUseResult":{"oldTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"completed","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"completed","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"in_progress","priority":"medium","id":"5"}],"newTodos":[{"content":"Create 03_functional_programming.py example file","status":"completed","priority":"high","id":"1"},{"content":"Create 04_metaprogramming.py example file","status":"completed","priority":"high","id":"2"},{"content":"Create 05_performance_optimization.py example file","status":"completed","priority":"high","id":"3"},{"content":"Create tests for the Python idioms examples","status":"completed","priority":"medium","id":"4"},{"content":"Create exercises for hands-on practice","status":"completed","priority":"medium","id":"5"}]}}
{"parentUuid":"4df55a14-4754-4588-99f2-f4a2d17f3c3b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"a8c407f7-5e1d-4ef6-a98e-93eb1f9d0a7b","version":"1.0.63","gitBranch":"","message":{"id":"msg_vrtx_01P2NobZBcu2Jwvv6RYGXgKW","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Perfect! I've completed adding all the Python idioms files to continue your hands-on AI patterns project. Here's what I've added:\n\n## ✅ Completed Files\n\n### Examples (03-python-idioms/examples/):\n1. **03_functional_programming.py** - Functional programming patterns with immutable data, pure functions, monadic error handling, and composition\n2. **04_metaprogramming.py** - Metaprogramming techniques with descriptors, metaclasses, dynamic class generation, and runtime introspection  \n3. **05_performance_optimization.py** - Performance optimization with memory management, vectorization, parallel processing, and profiling\n\n### Tests (03-python-idioms/tests/):\n1. **test_type_system.py** - Comprehensive tests for the advanced type system examples\n2. **test_async_patterns.py** - Tests for async patterns including streaming, rate limiting, and concurrent processing\n3. **test_functional_programming.py** - Tests for functional programming patterns and immutable data structures\n\n### Exercises (03-python-idioms/exercises/):\n1. **exercise_01_build_typed_ai_pipeline.py** - Build a type-safe AI pipeline\n2. **exercise_02_async_ai_service.py** - Create a high-performance async AI service\n3. **exercise_03_functional_ai_pipeline.py** - Build a functional programming-based pipeline\n4. **exercise_04_dynamic_ai_framework.py** - Create a metaprogramming-powered framework\n5. **exercise_05_high_performance_ai.py** - Build a high-performance AI system\n\n## 🎯 Key Features Added\n\n**Advanced Type System**: Generic types, protocols, TypedDict configurations, and overloaded methods\n\n**Async Patterns**: Streaming responses, connection pooling, rate limiting, and concurrent processing\n\n**Functional Programming**: Immutable data structures, pure functions, monadic error handling, and lazy evaluation\n\n**Metaprogramming**: Custom descriptors, metaclasses, dynamic class generation, and runtime introspection\n\n**Performance Optimization**: Memory-efficient data structures, vectorized operations, parallel processing, and intelligent caching\n\nEach example includes interactive demonstrations, comprehensive tests, and hands-on exercises to help you master these advanced Python patterns for AI systems!"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":442,"cache_read_input_tokens":90888,"output_tokens":1}},"requestId":"req_vrtx_011CRddt1JQ9mxRM3dCWSoaU","type":"assistant","uuid":"3f684232-a111-422a-a3c6-57d2ab10620d","timestamp":"2025-07-30T17:58:08.870Z"}
