{"type":"summary","summary":"Python Idioms: Advanced AI Programming Patterns","leafUuid":"3f684232-a111-422a-a3c6-57d2ab10620d"}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":"@03-python-idioms/examples/01_type_system_advanced.py can you create a juptyer notebook for me to learn form these examples. explain each cell what are the first priciples, python idioms also so i come out well rounded "},"uuid":"1f2aa446-cc66-4981-b58e-803c83605e1b","timestamp":"2025-08-01T17:52:07.465Z"}
{"parentUuid":"1f2aa446-cc66-4981-b58e-803c83605e1b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01LyYkVKpMWQmKVWW9LLvTr5","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I'll create a comprehensive Jupyter notebook to help you learn Python's advanced type system from these examples. Let me break this down into digestible cells with clear explanations of first principles and Python idioms."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":30960,"cache_read_input_tokens":0,"output_tokens":3}},"requestId":"req_vrtx_011CRhR3xQo8muxNwPWVLsa2","type":"assistant","uuid":"ee2dc2c2-40f2-46ad-a323-94367c8506e7","timestamp":"2025-08-01T17:52:15.009Z"}
{"parentUuid":"ee2dc2c2-40f2-46ad-a323-94367c8506e7","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01LyYkVKpMWQmKVWW9LLvTr5","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01SrddcxhPQRr3p8dn3a67XD","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/01_type_system_advanced_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Advanced Python Type System for AI Development\\n\",\n    \"\\n\",\n    \"This notebook will teach you Python's advanced type system through practical AI examples. You'll learn:\\n\",\n    \"\\n\",\n    \"1. **Type Variables & Generics** - Building reusable, type-safe components\\n\",\n    \"2. **Protocols** - Defining interfaces without inheritance\\n\",\n    \"3. **TypedDict** - Structured configuration with type safety\\n\",\n    \"4. **Dataclasses** - Clean data structures with automatic methods\\n\",\n    \"5. **Function Overloading** - Multiple signatures for flexible APIs\\n\",\n    \"6. **Advanced Decorators** - Type-preserving function enhancement\\n\",\n    \"\\n\",\n    \"## Prerequisites\\n\",\n    \"```bash\\n\",\n    \"pip install rich\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 1: Essential Imports and Setup\\n\",\n    \"\\n\",\n    \"**First Principle**: Python's typing module provides static type checking capabilities without runtime performance cost.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Import typing components explicitly for better readability and IDE support.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Essential imports for advanced typing\\n\",\n    \"from typing import (\\n\",\n    \"    TypeVar, Generic, Protocol, runtime_checkable, \\n\",\n    \"    Union, Optional, Literal, TypedDict, overload,\\n\",\n    \"    ParamSpec, Concatenate, Callable, Any, Dict, List\\n\",\n    \")\\n\",\n    \"from dataclasses import dataclass\\n\",\n    \"from abc import ABC, abstractmethod\\n\",\n    \"import asyncio\\n\",\n    \"import hashlib\\n\",\n    \"from rich.console import Console\\n\",\n    \"from rich.panel import Panel\\n\",\n    \"from rich.table import Table\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"print(\\\"‚úÖ All imports successful!\\\")\\n\",\n    \"print(\\\"üìö Ready to explore advanced Python typing!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 2: Understanding Type Variables\\n\",\n    \"\\n\",\n    \"**First Principle**: Type variables create generic, reusable code that maintains type safety across different data types.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use descriptive names for type variables (`T` for generic, `ModelInput`/`ModelOutput` for domain-specific).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Type Variables - The foundation of generic programming\\n\",\n    \"T = TypeVar('T')  # Generic type variable\\n\",\n    \"ModelInput = TypeVar('ModelInput')  # Domain-specific type variable\\n\",\n    \"ModelOutput = TypeVar('ModelOutput')  # Domain-specific type variable\\n\",\n    \"P = ParamSpec('P')  # Parameter specification for functions\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üî§ Type Variables Explained\\\", style=\\\"bold blue\\\"))\\n\",\n    \"console.print(\\\"\\\"\\\"\\n\",\n    \"Type Variables are placeholders that represent \\\"any type\\\" but maintain \\n\",\n    \"consistency within their scope.\\n\",\n    \"\\n\",\n    \"Key Benefits:\\n\",\n    \"‚Ä¢ Type safety: Prevents mixing incompatible types\\n\",\n    \"‚Ä¢ Reusability: Same code works with different types\\n\",\n    \"‚Ä¢ IDE support: Better autocomplete and error detection\\n\",\n    \"‚Ä¢ Documentation: Clear contracts about input/output relationships\\n\",\n    \"\\\"\\\"\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüìù Type variables defined:\\\")\\n\",\n    \"print(f\\\"T: {T}\\\")\\n\",\n    \"print(f\\\"ModelInput: {ModelInput}\\\")\\n\",\n    \"print(f\\\"ModelOutput: {ModelOutput}\\\")\\n\",\n    \"print(f\\\"P: {P}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 3: Dataclasses - Clean Data Structures\\n\",\n    \"\\n\",\n    \"**First Principle**: Dataclasses automatically generate boilerplate methods (`__init__`, `__repr__`, `__eq__`) for data-focused classes.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `__post_init__` for validation and default value initialization that depends on other fields.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"@dataclass\\n\",\n    \"class TextInput:\\n\",\n    \"    \\\"\\\"\\\"Structured text input with automatic method generation.\\\"\\\"\\\"\\n\",\n    \"    text: str\\n\",\n    \"    context: Optional[str] = None\\n\",\n    \"    metadata: Dict[str, Any] = None\\n\",\n    \"    \\n\",\n    \"    def __post_init__(self):\\n\",\n    \"        \\\"\\\"\\\"Called after __init__ for custom initialization.\\\"\\\"\\\"\\n\",\n    \"        if self.metadata is None:\\n\",\n    \"            self.metadata = {}  # Avoid mutable default arguments\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class ClassificationOutput:\\n\",\n    \"    \\\"\\\"\\\"Classification result with confidence scores.\\\"\\\"\\\"\\n\",\n    \"    label: str\\n\",\n    \"    confidence: float\\n\",\n    \"    alternatives: List[tuple[str, float]]\\n\",\n    \"    \\n\",\n    \"    def to_dict(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Convert to dictionary for serialization.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"label\\\": self.label,\\n\",\n    \"            \\\"confidence\\\": self.confidence,\\n\",\n    \"            \\\"alternatives\\\": self.alternatives\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def from_dict(cls, data: Dict[str, Any]) -> 'ClassificationOutput':\\n\",\n    \"        \\\"\\\"\\\"Create instance from dictionary.\\\"\\\"\\\"\\n\",\n    \"        return cls(\\n\",\n    \"            label=data[\\\"label\\\"],\\n\",\n    \"            confidence=data[\\\"confidence\\\"],\\n\",\n    \"            alternatives=data[\\\"alternatives\\\"]\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"# Demonstrate dataclass usage\\n\",\n    \"console.print(Panel(\\\"üìä Dataclasses in Action\\\", style=\\\"bold green\\\"))\\n\",\n    \"\\n\",\n    \"# Create instances\\n\",\n    \"text_input = TextInput(\\n\",\n    \"    text=\\\"This is a positive example!\\\",\\n\",\n    \"    context=\\\"Sentiment analysis demo\\\"\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"classification = ClassificationOutput(\\n\",\n    \"    label=\\\"positive\\\",\\n\",\n    \"    confidence=0.92,\\n\",\n    \"    alternatives=[(\\\"neutral\\\", 0.06), (\\\"negative\\\", 0.02)]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(f\\\"Text Input: {text_input}\\\")\\n\",\n    \"print(f\\\"Classification: {classification}\\\")\\n\",\n    \"print(f\\\"As dict: {classification.to_dict()}\\\")\\n\",\n    \"\\n\",\n    \"# Show automatic equality\\n\",\n    \"same_classification = ClassificationOutput(\\\"positive\\\", 0.92, [(\\\"neutral\\\", 0.06), (\\\"negative\\\", 0.02)])\\n\",\n    \"print(f\\\"\\\\nüìã Automatic equality: {classification == same_classification}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 4: Protocols - Interface Definition Without Inheritance\\n\",\n    \"\\n\",\n    \"**First Principle**: Protocols define \\\"what an object can do\\\" rather than \\\"what an object is\\\" (structural vs nominal typing).\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `@runtime_checkable` when you need `isinstance()` checks, use `Protocol[T]` for generic interfaces.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"@runtime_checkable\\n\",\n    \"class Serializable(Protocol):\\n\",\n    \"    \\\"\\\"\\\"Protocol for objects that can be serialized to/from dictionaries.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def to_dict(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Convert to dictionary.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def from_dict(cls, data: Dict[str, Any]) -> 'Serializable':\\n\",\n    \"        \\\"\\\"\\\"Create from dictionary.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"\\n\",\n    \"@runtime_checkable\\n\",\n    \"class Cacheable(Protocol[T]):\\n\",\n    \"    \\\"\\\"\\\"Generic protocol for cacheable objects.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def cache_key(self) -> str:\\n\",\n    \"        \\\"\\\"\\\"Get unique cache key.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    def serialize(self) -> bytes:\\n\",\n    \"        \\\"\\\"\\\"Serialize for caching.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def deserialize(cls, data: bytes) -> T:\\n\",\n    \"        \\\"\\\"\\\"Deserialize from cache.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"\\n\",\n    \"# Test protocol compliance\\n\",\n    \"console.print(Panel(\\\"üìã Protocol Checking\\\", style=\\\"bold yellow\\\"))\\n\",\n    \"\\n\",\n    \"# Our ClassificationOutput automatically satisfies Serializable!\\n\",\n    \"result = ClassificationOutput(\\\"positive\\\", 0.85, [(\\\"neutral\\\", 0.15)])\\n\",\n    \"\\n\",\n    \"print(f\\\"Does ClassificationOutput implement Serializable? {isinstance(result, Serializable)}\\\")\\n\",\n    \"\\n\",\n    \"if isinstance(result, Serializable):\\n\",\n    \"    serialized = result.to_dict()\\n\",\n    \"    restored = ClassificationOutput.from_dict(serialized)\\n\",\n    \"    print(f\\\"‚úÖ Serialization works: {restored.label} ({restored.confidence})\\\")\\n\",\n    \"\\n\",\n    \"# Test with other objects\\n\",\n    \"test_objects = [\\n\",\n    \"    (\\\"ClassificationOutput\\\", result),\\n\",\n    \"    (\\\"String\\\", \\\"hello\\\"),\\n\",\n    \"    (\\\"Dictionary\\\", {\\\"key\\\": \\\"value\\\"}),\\n\",\n    \"    (\\\"TextInput\\\", text_input)\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüîç Runtime Protocol Checks:\\\")\\n\",\n    \"for name, obj in test_objects:\\n\",\n    \"    is_serializable = isinstance(obj, Serializable)\\n\",\n    \"    print(f\\\"  {name}: Serializable = {is_serializable}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 5: TypedDict - Structured Configuration\\n\",\n    \"\\n\",\n    \"**First Principle**: TypedDict provides type safety for dictionaries while maintaining dict performance and JSON compatibility.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `Literal` types for constrained string values, `total=False` for optional fields in inherited TypedDicts.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Define typed dictionaries for configuration\\n\",\n    \"class ModelConfig(TypedDict):\\n\",\n    \"    \\\"\\\"\\\"Basic model configuration with required fields.\\\"\\\"\\\"\\n\",\n    \"    model_id: str\\n\",\n    \"    max_tokens: int\\n\",\n    \"    temperature: float\\n\",\n    \"    provider: Literal[\\\"openai\\\", \\\"anthropic\\\", \\\"local\\\"]  # Constrained values\\n\",\n    \"    timeout_seconds: Optional[float]\\n\",\n    \"\\n\",\n    \"class AdvancedModelConfig(ModelConfig, total=False):\\n\",\n    \"    \\\"\\\"\\\"Extended config with optional fields.\\\"\\\"\\\"\\n\",\n    \"    retry_attempts: int\\n\",\n    \"    fallback_model: str\\n\",\n    \"    cost_limit_usd: float\\n\",\n    \"    custom_headers: Dict[str, str]\\n\",\n    \"\\n\",\n    \"# Type aliases for clarity\\n\",\n    \"Provider = Literal[\\\"openai\\\", \\\"anthropic\\\", \\\"huggingface\\\", \\\"local\\\"]\\n\",\n    \"ModelSize = Literal[\\\"small\\\", \\\"medium\\\", \\\"large\\\", \\\"xl\\\"]\\n\",\n    \"\\n\",\n    \"def create_model_config(\\n\",\n    \"    provider: Provider,\\n\",\n    \"    model_size: ModelSize,\\n\",\n    \"    temperature: float = 0.7\\n\",\n    \") -> ModelConfig:\\n\",\n    \"    \\\"\\\"\\\"Factory function using literal types for validation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Mapping tuples to model IDs\\n\",\n    \"    model_mapping = {\\n\",\n    \"        (\\\"openai\\\", \\\"small\\\"): \\\"gpt-3.5-turbo\\\",\\n\",\n    \"        (\\\"openai\\\", \\\"large\\\"): \\\"gpt-4\\\",\\n\",\n    \"        (\\\"anthropic\\\", \\\"small\\\"): \\\"claude-3-haiku\\\",\\n\",\n    \"        (\\\"anthropic\\\", \\\"large\\\"): \\\"claude-3-opus\\\",\\n\",\n    \"        (\\\"local\\\", \\\"small\\\"): \\\"local-small-model\\\"\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    model_id = model_mapping.get((provider, model_size), \\\"default-model\\\")\\n\",\n    \"    \\n\",\n    \"    max_tokens_mapping = {\\n\",\n    \"        \\\"small\\\": 1000, \\\"medium\\\": 2000, \\\"large\\\": 4000, \\\"xl\\\": 8000\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    return ModelConfig(\\n\",\n    \"        model_id=model_id,\\n\",\n    \"        max_tokens=max_tokens_mapping[model_size],\\n\",\n    \"        temperature=temperature,\\n\",\n    \"        provider=provider,\\n\",\n    \"        timeout_seconds=30.0\\n\",\n    \"    )\\n\",\n    \"\\n\",\n    \"# Demonstrate TypedDict usage\\n\",\n    \"console.print(Panel(\\\"üìù TypedDict Configuration\\\", style=\\\"bold magenta\\\"))\\n\",\n    \"\\n\",\n    \"# Create configurations\\n\",\n    \"basic_config: ModelConfig = {\\n\",\n    \"    \\\"model_id\\\": \\\"gpt-4\\\",\\n\",\n    \"    \\\"max_tokens\\\": 2000,\\n\",\n    \"    \\\"temperature\\\": 0.7,\\n\",\n    \"    \\\"provider\\\": \\\"openai\\\",\\n\",\n    \"    \\\"timeout_seconds\\\": 30.0\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"advanced_config: AdvancedModelConfig = {\\n\",\n    \"    \\\"model_id\\\": \\\"claude-3-opus\\\",\\n\",\n    \"    \\\"max_tokens\\\": 4000,\\n\",\n    \"    \\\"temperature\\\": 0.5,\\n\",\n    \"    \\\"provider\\\": \\\"anthropic\\\",\\n\",\n    \"    \\\"timeout_seconds\\\": 60.0,\\n\",\n    \"    \\\"retry_attempts\\\": 3,\\n\",\n    \"    \\\"fallback_model\\\": \\\"claude-3-haiku\\\",\\n\",\n    \"    \\\"cost_limit_usd\\\": 10.0\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"print(\\\"üìã Configuration Examples:\\\")\\n\",\n    \"print(f\\\"Basic: {basic_config['provider']} - {basic_config['model_id']}\\\")\\n\",\n    \"print(f\\\"Advanced: {advanced_config['provider']} - {advanced_config['model_id']}\\\")\\n\",\n    \"print(f\\\"Has retries: {advanced_config.get('retry_attempts', 'No')}\\\")\\n\",\n    \"\\n\",\n    \"# Test factory function\\n\",\n    \"print(\\\"\\\\nüè≠ Factory Function Results:\\\")\\n\",\n    \"configs = [\\n\",\n    \"    create_model_config(\\\"openai\\\", \\\"large\\\", 0.8),\\n\",\n    \"    create_model_config(\\\"anthropic\\\", \\\"small\\\", 0.3),\\n\",\n    \"    create_model_config(\\\"local\\\", \\\"small\\\", 1.0)\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for i, config in enumerate(configs, 1):\\n\",\n    \"    print(f\\\"  Config {i}: {config['provider']} - {config['model_id']}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 6: Generic Classes - Type-Safe Containers\\n\",\n    \"\\n\",\n    \"**First Principle**: Generic classes use type variables to create reusable containers that maintain type safety.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `Generic[T]` for classes that store or operate on arbitrary types, use descriptive variable names for domain-specific generics.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class TypedCache(Generic[T]):\\n\",\n    \"    \\\"\\\"\\\"Type-safe cache that remembers what type it stores.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self._cache: Dict[str, T] = {}\\n\",\n    \"        self._access_count: Dict[str, int] = {}\\n\",\n    \"    \\n\",\n    \"    def get(self, key: str) -> Optional[T]:\\n\",\n    \"        \\\"\\\"\\\"Get item from cache with type safety.\\\"\\\"\\\"\\n\",\n    \"        if key in self._cache:\\n\",\n    \"            self._access_count[key] = self._access_count.get(key, 0) + 1\\n\",\n    \"            return self._cache[key]\\n\",\n    \"        return None\\n\",\n    \"    \\n\",\n    \"    def put(self, key: str, value: T) -> None:\\n\",\n    \"        \\\"\\\"\\\"Store item in cache.\\\"\\\"\\\"\\n\",\n    \"        self._cache[key] = value\\n\",\n    \"        self._access_count[key] = 0\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get cache statistics.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"size\\\": len(self._cache),\\n\",\n    \"            \\\"total_access\\\": sum(self._access_count.values()),\\n\",\n    \"            \\\"keys\\\": list(self._cache.keys())\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"# Generic AI Model base class\\n\",\n    \"class AIModel(Generic[ModelInput, ModelOutput], ABC):\\n\",\n    \"    \\\"\\\"\\\"Generic AI model with typed input/output.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, config: ModelConfig):\\n\",\n    \"        self.config = config\\n\",\n    \"    \\n\",\n    \"    @abstractmethod\\n\",\n    \"    async def predict(self, input_data: ModelInput) -> ModelOutput:\\n\",\n    \"        \\\"\\\"\\\"Make prediction with type safety.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"    \\n\",\n    \"    @abstractmethod\\n\",\n    \"    def validate_input(self, input_data: ModelInput) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Validate input data.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"\\n\",\n    \"# Concrete implementation\\n\",\n    \"class TextClassifier(AIModel[TextInput, ClassificationOutput]):\\n\",\n    \"    \\\"\\\"\\\"Text classifier with specific input/output types.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    async def predict(self, input_data: TextInput) -> ClassificationOutput:\\n\",\n    \"        \\\"\\\"\\\"Classify text with mock implementation.\\\"\\\"\\\"\\n\",\n    \"        # Simple sentiment analysis simulation\\n\",\n    \"        text_lower = input_data.text.lower()\\n\",\n    \"        \\n\",\n    \"        if \\\"positive\\\" in text_lower or \\\"good\\\" in text_lower or \\\"great\\\" in text_lower:\\n\",\n    \"            return ClassificationOutput(\\\"positive\\\", 0.92, [(\\\"neutral\\\", 0.06), (\\\"negative\\\", 0.02)])\\n\",\n    \"        elif \\\"negative\\\" in text_lower or \\\"bad\\\" in text_lower or \\\"terrible\\\" in text_lower:\\n\",\n    \"            return ClassificationOutput(\\\"negative\\\", 0.88, [(\\\"neutral\\\", 0.10), (\\\"positive\\\", 0.02)])\\n\",\n    \"        else:\\n\",\n    \"            return ClassificationOutput(\\\"neutral\\\", 0.75, [(\\\"positive\\\", 0.15), (\\\"negative\\\", 0.10)])\\n\",\n    \"    \\n\",\n    \"    def validate_input(self, input_data: TextInput) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Validate text input.\\\"\\\"\\\"\\n\",\n    \"        return (\\n\",\n    \"            isinstance(input_data.text, str) and\\n\",\n    \"            len(input_data.text.strip()) > 0 and\\n\",\n    \"            len(input_data.text) <= 10000\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"# Demonstrate generic usage\\n\",\n    \"console.print(Panel(\\\"üéØ Generic Types in Action\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"# Create typed caches\\n\",\n    \"text_cache = TypedCache[str]()\\n\",\n    \"result_cache = TypedCache[ClassificationOutput]()\\n\",\n    \"\\n\",\n    \"# Use text cache\\n\",\n    \"text_cache.put(\\\"greeting\\\", \\\"Hello, world!\\\")\\n\",\n    \"text_cache.put(\\\"farewell\\\", \\\"Goodbye!\\\")\\n\",\n    \"cached_text = text_cache.get(\\\"greeting\\\")\\n\",\n    \"print(f\\\"üìù Cached text: {cached_text}\\\")\\n\",\n    \"\\n\",\n    \"# Use result cache\\n\",\n    \"sample_result = ClassificationOutput(\\\"positive\\\", 0.95, [(\\\"negative\\\", 0.05)])\\n\",\n    \"result_cache.put(\\\"sample\\\", sample_result)\\n\",\n    \"cached_result = result_cache.get(\\\"sample\\\")\\n\",\n    \"print(f\\\"üìä Cached result: {cached_result.label} ({cached_result.confidence})\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìà Cache Stats:\\\")\\n\",\n    \"print(f\\\"  Text cache: {text_cache.get_stats()}\\\")\\n\",\n    \"print(f\\\"  Result cache: {result_cache.get_stats()}\\\")\\n\",\n    \"\\n\",\n    \"# Test the classifier\\n\",\n    \"print(f\\\"\\\\nü§ñ Testing TextClassifier:\\\")\\n\",\n    \"config = create_model_config(\\\"openai\\\", \\\"large\\\")\\n\",\n    \"classifier = TextClassifier(config)\\n\",\n    \"\\n\",\n    \"test_inputs = [\\n\",\n    \"    TextInput(\\\"This is a great positive example!\\\"),\\n\",\n    \"    TextInput(\\\"This is a terrible negative example!\\\"),\\n\",\n    \"    TextInput(\\\"This is just a normal example.\\\")\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for i, test_input in enumerate(test_inputs, 1):\\n\",\n    \"    if classifier.validate_input(test_input):\\n\",\n    \"        result = await classifier.predict(test_input)\\n\",\n    \"        print(f\\\"  {i}. '{test_input.text}' -> {result.label} ({result.confidence:.2f})\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"  {i}. Invalid input: {test_input.text}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 7: Function Overloading\\n\",\n    \"\\n\",\n    \"**First Principle**: Function overloading allows multiple type signatures for the same function, improving API flexibility while maintaining type safety.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `@overload` for type hints only, implement the actual logic in the non-decorated function below.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class ModelManager:\\n\",\n    \"    \\\"\\\"\\\"Manages multiple AI models with overloaded methods.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.models: Dict[str, AIModel] = {}\\n\",\n    \"        self.cache = TypedCache[Any]()\\n\",\n    \"    \\n\",\n    \"    # Overloaded get_model methods\\n\",\n    \"    @overload\\n\",\n    \"    def get_model(self, name: str) -> Optional[AIModel]:\\n\",\n    \"        \\\"\\\"\\\"Get model by name, return None if not found.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    @overload\\n\",\n    \"    def get_model(self, name: str, default: T) -> Union[AIModel, T]:\\n\",\n    \"        \\\"\\\"\\\"Get model by name, return default if not found.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    def get_model(self, name: str, default=None):\\n\",\n    \"        \\\"\\\"\\\"Implementation for both overloads.\\\"\\\"\\\"\\n\",\n    \"        return self.models.get(name, default)\\n\",\n    \"    \\n\",\n    \"    def register_model(self, name: str, model: AIModel) -> None:\\n\",\n    \"        \\\"\\\"\\\"Register a model with a name.\\\"\\\"\\\"\\n\",\n    \"        self.models[name] = model\\n\",\n    \"    \\n\",\n    \"    # Overloaded predict methods\\n\",\n    \"    @overload\\n\",\n    \"    async def predict(self, model_name: str, input_data: TextInput) -> ClassificationOutput:\\n\",\n    \"        \\\"\\\"\\\"Predict with text classifier.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    @overload\\n\",\n    \"    async def predict(self, model_name: str, input_data: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Generic prediction method.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    async def predict(self, model_name: str, input_data: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Make prediction with caching.\\\"\\\"\\\"\\n\",\n    \"        model = self.get_model(model_name)\\n\",\n    \"        if model is None:\\n\",\n    \"            raise ValueError(f\\\"Model '{model_name}' not found\\\")\\n\",\n    \"        \\n\",\n    \"        # Simple cache key generation\\n\",\n    \"        cache_key = f\\\"{model_name}:{hash(str(input_data.text if hasattr(input_data, 'text') else input_data))}\\\"\\n\",\n    \"        \\n\",\n    \"        # Check cache first\\n\",\n    \"        cached_result = self.cache.get(cache_key)\\n\",\n    \"        if cached_result is not None:\\n\",\n    \"            print(f\\\"üóÑÔ∏è  Cache hit for {model_name}\\\")\\n\",\n    \"            return cached_result\\n\",\n    \"        \\n\",\n    \"        # Make prediction\\n\",\n    \"        print(f\\\"ü§ñ Making prediction with {model_name}\\\")\\n\",\n    \"        result = await model.predict(input_data)\\n\",\n    \"        \\n\",\n    \"        # Cache the result\\n\",\n    \"        self.cache.put(cache_key, result)\\n\",\n    \"        \\n\",\n    \"        return result\\n\",\n    \"\\n\",\n    \"# Demonstrate function overloading\\n\",\n    \"console.print(Panel(\\\"‚ö° Function Overloading\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"\\n\",\n    \"# Setup\\n\",\n    \"manager = ModelManager()\\n\",\n    \"config = create_model_config(\\\"openai\\\", \\\"large\\\")\\n\",\n    \"classifier = TextClassifier(config)\\n\",\n    \"manager.register_model(\\\"sentiment_classifier\\\", classifier)\\n\",\n    \"\\n\",\n    \"print(\\\"üîß Testing overloaded get_model:\\\")\\n\",\n    \"\\n\",\n    \"# Test get_model without default\\n\",\n    \"model1 = manager.get_model(\\\"sentiment_classifier\\\")\\n\",\n    \"print(f\\\"  Found model: {model1 is not None}\\\")\\n\",\n    \"\\n\",\n    \"# Test get_model with default\\n\",\n    \"model2 = manager.get_model(\\\"nonexistent\\\", \\\"default_value\\\")\\n\",\n    \"print(f\\\"  With default: {model2}\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüéØ Testing overloaded predict:\\\")\\n\",\n    \"\\n\",\n    \"# Test prediction with caching\\n\",\n    \"test_input = TextInput(\\\"This is a fantastic positive example!\\\")\\n\",\n    \"\\n\",\n    \"# First call - should compute\\n\",\n    \"result1 = await manager.predict(\\\"sentiment_classifier\\\", test_input)\\n\",\n    \"print(f\\\"  First call: {result1.label} ({result1.confidence:.2f})\\\")\\n\",\n    \"\\n\",\n    \"# Second call - should use cache\\n\",\n    \"result2 = await manager.predict(\\\"sentiment_classifier\\\", test_input)\\n\",\n    \"print(f\\\"  Second call: {result2.label} ({result2.confidence:.2f})\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìä Cache statistics: {manager.cache.get_stats()}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 8: Advanced Decorators with Type Preservation\\n\",\n    \"\\n\",\n    \"**First Principle**: Decorators can preserve function signatures and types using `ParamSpec` and proper generic typing.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `ParamSpec` to maintain parameter types, use `Callable[P, T]` for functions that accept any parameters and return type T.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def with_retries(max_retries: int = 3) -> Callable[[Callable[P, T]], Callable[P, T]]:\\n\",\n    \"    \\\"\\\"\\\"Decorator that adds retry logic with full type preservation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def decorator(func: Callable[P, T]) -> Callable[P, T]:\\n\",\n    \"        async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\\n\",\n    \"            last_exception = None\\n\",\n    \"            \\n\",\n    \"            for attempt in range(max_retries):\\n\",\n    \"                try:\\n\",\n    \"                    # Handle both sync and async functions\\n\",\n    \"                    if asyncio.iscoroutinefunction(func):\\n\",\n    \"                        return await func(*args, **kwargs)\\n\",\n    \"                    else:\\n\",\n    \"                        return func(*args, **kwargs)\\n\",\n    \"                except Exception as e:\\n\",\n    \"                    last_exception = e\\n\",\n    \"                    print(f\\\"‚ùå Attempt {attempt + 1} failed: {str(e)[:50]}...\\\")\\n\",\n    \"                    \\n\",\n    \"                    if attempt < max_retries - 1:\\n\",\n    \"                        wait_time = 2 ** attempt  # Exponential backoff\\n\",\n    \"                        print(f\\\"‚è≥ Waiting {wait_time}s before retry...\\\")\\n\",\n    \"                        await asyncio.sleep(wait_time)\\n\",\n    \"            \\n\",\n    \"            print(f\\\"üí• All {max_retries} attempts failed\\\")\\n\",\n    \"            raise last_exception\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"    return decorator\\n\",\n    \"\\n\",\n    \"def log_calls(func: Callable[P, T]) -> Callable[P, T]:\\n\",\n    \"    \\\"\\\"\\\"Decorator that logs function calls with type preservation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\\n\",\n    \"        func_name = func.__name__\\n\",\n    \"        print(f\\\"üöÄ Calling {func_name} with {len(args)} args, {len(kwargs)} kwargs\\\")\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            if asyncio.iscoroutinefunction(func):\\n\",\n    \"                result = await func(*args, **kwargs)\\n\",\n    \"            else:\\n\",\n    \"                result = func(*args, **kwargs)\\n\",\n    \"            \\n\",\n    \"            print(f\\\"‚úÖ {func_name} completed successfully\\\")\\n\",\n    \"            return result\\n\",\n    \"        except Exception as e:\\n\",\n    \"            print(f\\\"üí• {func_name} failed: {e}\\\")\\n\",\n    \"            raise\\n\",\n    \"    \\n\",\n    \"    return wrapper\\n\",\n    \"\\n\",\n    \"# Example functions using the decorators\\n\",\n    \"@with_retries(max_retries=3)\\n\",\n    \"@log_calls\\n\",\n    \"async def flaky_api_call(text: str, fail_rate: float = 0.7) -> str:\\n\",\n    \"    \\\"\\\"\\\"Simulates an unreliable API call.\\\"\\\"\\\"\\n\",\n    \"    import random\\n\",\n    \"    \\n\",\n    \"    if random.random() < fail_rate:\\n\",\n    \"        raise ConnectionError(f\\\"Simulated network failure for '{text}'\\\")\\n\",\n    \"    \\n\",\n    \"    # Simulate processing time\\n\",\n    \"    await asyncio.sleep(0.1)\\n\",\n    \"    return f\\\"Processed: {text.upper()}\\\"\\n\",\n    \"\\n\",\n    \"@with_retries(max_retries=2)\\n\",\n    \"async def reliable_classification(input_data: TextInput) -> ClassificationOutput:\\n\",\n    \"    \\\"\\\"\\\"Reliable classification with retries.\\\"\\\"\\\"\\n\",\n    \"    # Simulate occasional failures\\n\",\n    \"    import random\\n\",\n    \"    if random.random() < 0.3:\\n\",\n    \"        raise ValueError(\\\"Temporary classification error\\\")\\n\",\n    \"    \\n\",\n    \"    # Simple classification\\n\",\n    \"    text_lower = input_data.text.lower()\\n\",\n    \"    if \\\"good\\\" in text_lower or \\\"great\\\" in text_lower:\\n\",\n    \"        return ClassificationOutput(\\\"positive\\\", 0.90, [(\\\"neutral\\\", 0.08), (\\\"negative\\\", 0.02)])\\n\",\n    \"    else:\\n\",\n    \"        return ClassificationOutput(\\\"neutral\\\", 0.70, [(\\\"positive\\\", 0.20), (\\\"negative\\\", 0.10)])\\n\",\n    \"\\n\",\n    \"# Demonstrate advanced decorators\\n\",\n    \"console.print(Panel(\\\"üé® Advanced Decorators with Type Preservation\\\", style=\\\"bold red\\\"))\\n\",\n    \"\\n\",\n    \"print(\\\"üß™ Testing flaky API call:\\\")\\n\",\n    \"try:\\n\",\n    \"    result = await flaky_api_call(\\\"hello world\\\", fail_rate=0.8)\\n\",\n    \"    print(f\\\"üéâ Success: {result}\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"üö´ Final failure: {e}\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüß™ Testing reliable classification:\\\")\\n\",\n    \"test_input = TextInput(\\\"This is a good example\\\")\\n\",\n    \"try:\\n\",\n    \"    classification = await reliable_classification(test_input)\\n\",\n    \"    print(f\\\"üéØ Classification: {classification.label} ({classification.confidence:.2f})\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"üö´ Classification failed: {e}\\\")\\n\",\n    \"\\n\",\n    \"# Show that type information is preserved\\n\",\n    \"print(\\\"\\\\nüîç Type preservation check:\\\")\\n\",\n    \"print(f\\\"flaky_api_call annotations: {flaky_api_call.__annotations__}\\\")\\n\",\n    \"print(f\\\"reliable_classification return type preserved: {hasattr(reliable_classification, '__annotations__')}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 9: Real-World Integration Example\\n\",\n    \"\\n\",\n    \"**First Principle**: Combine all typing features to create a robust, type-safe AI system that's easy to extend and maintain.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use composition of typed components rather than complex inheritance hierarchies.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class AIServiceConfig(TypedDict):\\n\",\n    \"    \\\"\\\"\\\"Configuration for AI service.\\\"\\\"\\\"\\n\",\n    \"    default_provider: Provider\\n\",\n    \"    max_concurrent_requests: int\\n\",\n    \"    cache_ttl_seconds: int\\n\",\n    \"    retry_attempts: int\\n\",\n    \"\\n\",\n    \"class AIService:\\n\",\n    \"    \\\"\\\"\\\"Production-ready AI service using all typing features.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, config: AIServiceConfig):\\n\",\n    \"        self.config = config\\n\",\n    \"        self.manager = ModelManager()\\n\",\n    \"        self.request_cache = TypedCache[ClassificationOutput]()\\n\",\n    \"        self._setup_default_models()\\n\",\n    \"    \\n\",\n    \"    def _setup_default_models(self) -> None:\\n\",\n    \"        \\\"\\\"\\\"Initialize default models.\\\"\\\"\\\"\\n\",\n    \"        model_config = create_model_config(\\n\",\n    \"            self.config[\\\"default_provider\\\"], \\n\",\n    \"            \\\"large\\\", \\n\",\n    \"            temperature=0.5\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        classifier = TextClassifier(model_config)\\n\",\n    \"        self.manager.register_model(\\\"default_classifier\\\", classifier)\\n\",\n    \"    \\n\",\n    \"    @with_retries(max_retries=3)\\n\",\n    \"    @log_calls\\n\",\n    \"    async def classify_text(\\n\",\n    \"        self, \\n\",\n    \"        text: str, \\n\",\n    \"        context: Optional[str] = None,\\n\",\n    \"        use_cache: bool = True\\n\",\n    \"    ) -> ClassificationOutput:\\n\",\n    \"        \\\"\\\"\\\"Classify text with full error handling and caching.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Create structured input\\n\",\n    \"        text_input = TextInput(text=text, context=context)\\n\",\n    \"        \\n\",\n    \"        # Check cache if enabled\\n\",\n    \"        if use_cache:\\n\",\n    \"            cache_key = f\\\"classify:{hash(text)}:{hash(context or '')}\\\"\\n\",\n    \"            cached_result = self.request_cache.get(cache_key)\\n\",\n    \"            if cached_result is not None:\\n\",\n    \"                print(f\\\"üíæ Cache hit for text: '{text[:30]}...'\\\")\\n\",\n    \"                return cached_result\\n\",\n    \"        \\n\",\n    \"        # Make prediction\\n\",\n    \"        result = await self.manager.predict(\\\"default_classifier\\\", text_input)\\n\",\n    \"        \\n\",\n    \"        # Cache result\\n\",\n    \"        if use_cache and isinstance(result, ClassificationOutput):\\n\",\n    \"            self.request_cache.put(cache_key, result)\\n\",\n    \"        \\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    async def batch_classify(\\n\",\n    \"        self, \\n\",\n    \"        texts: List[str], \\n\",\n    \"        context: Optional[str] = None\\n\",\n    \"    ) -> List[ClassificationOutput]:\\n\",\n    \"        \\\"\\\"\\\"Classify multiple texts concurrently.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Create tasks for concurrent processing\\n\",\n    \"        tasks = [\\n\",\n    \"            self.classify_text(text, context)\\n\",\n    \"            for text in texts\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        # Wait for all classifications\\n\",\n    \"        results = await asyncio.gather(*tasks, return_exceptions=True)\\n\",\n    \"        \\n\",\n    \"        # Handle any failures\\n\",\n    \"        valid_results = []\\n\",\n    \"        for i, result in enumerate(results):\\n\",\n    \"            if isinstance(result, Exception):\\n\",\n    \"                print(f\\\"‚ùå Failed to classify text {i}: {result}\\\")\\n\",\n    \"                # Return neutral as fallback\\n\",\n    \"                valid_results.append(\\n\",\n    \"                    ClassificationOutput(\\\"neutral\\\", 0.5, [(\\\"positive\\\", 0.25), (\\\"negative\\\", 0.25)])\\n\",\n    \"                )\\n\",\n    \"            else:\\n\",\n    \"                valid_results.append(result)\\n\",\n    \"        \\n\",\n    \"        return valid_results\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get service statistics.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"config\\\": self.config,\\n\",\n    \"            \\\"models_registered\\\": len(self.manager.models),\\n\",\n    \"            \\\"cache_stats\\\": self.request_cache.get_stats(),\\n\",\n    \"            \\\"manager_cache_stats\\\": self.manager.cache.get_stats()\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"# Demonstrate the complete system\\n\",\n    \"console.print(Panel(\\\"üèóÔ∏è Complete AI Service Integration\\\", style=\\\"bold green\\\"))\\n\",\n    \"\\n\",\n    \"# Create service configuration\\n\",\n    \"service_config: AIServiceConfig = {\\n\",\n    \"    \\\"default_provider\\\": \\\"openai\\\",\\n\",\n    \"    \\\"max_concurrent_requests\\\": 10,\\n\",\n    \"    \\\"cache_ttl_seconds\\\": 3600,\\n\",\n    \"    \\\"retry_attempts\\\": 3\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"# Initialize service\\n\",\n    \"ai_service = AIService(service_config)\\n\",\n    \"\\n\",\n    \"print(\\\"üß™ Testing single classification:\\\")\\n\",\n    \"single_result = await ai_service.classify_text(\\n\",\n    \"    \\\"This is an amazing product with great features!\\\",\\n\",\n    \"    context=\\\"Product review\\\"\\n\",\n    \")\\n\",\n    \"print(f\\\"Result: {single_result.label} (confidence: {single_result.confidence:.2f})\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüß™ Testing batch classification:\\\")\\n\",\n    \"test_texts = [\\n\",\n    \"    \\\"This product is fantastic!\\\",\\n\",\n    \"    \\\"Terrible customer service experience.\\\",\\n\",\n    \"    \\\"The product works as expected.\\\",\\n\",\n    \"    \\\"Outstanding quality and great value!\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"batch_results = await ai_service.batch_classify(test_texts, \\\"Product reviews\\\")\\n\",\n    \"\\n\",\n    \"for i, (text, result) in enumerate(zip(test_texts, batch_results), 1):\\n\",\n    \"    print(f\\\"  {i}. '{text}' -> {result.label} ({result.confidence:.2f})\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüìä Service statistics:\\\")\\n\",\n    \"stats = ai_service.get_stats()\\n\",\n    \"print(f\\\"Models registered: {stats['models_registered']}\\\")\\n\",\n    \"print(f\\\"Cache entries: {stats['cache_stats']['size']}\\\")\\n\",\n    \"print(f\\\"Total cache accesses: {stats['cache_stats']['total_access']}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 10: Key Takeaways and Best Practices\\n\",\n    \"\\n\",\n    \"**Summary of Python Typing First Principles and Idioms**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"console.print(Panel(\\\"üéì Key Takeaways: Advanced Python Typing\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"takeaways = {\\n\",\n    \"    \\\"Type Variables (Generic[T])\\\": [\\n\",\n    \"        \\\"‚Ä¢ Enable reusable, type-safe components\\\",\\n\",\n    \"        \\\"‚Ä¢ Use descriptive names: ModelInput, ModelOutput\\\",\\n\",\n    \"        \\\"‚Ä¢ Maintain type relationships across function calls\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"Protocols\\\": [\\n\",\n    \"        \\\"‚Ä¢ Define 'what objects can do' not 'what they are'\\\",\\n\",\n    \"        \\\"‚Ä¢ Use @runtime_checkable for isinstance() support\\\",\\n\",\n    \"        \\\"‚Ä¢ Protocol[T] enables generic interfaces\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"TypedDict\\\": [\\n\",\n    \"        \\\"‚Ä¢ Type-safe dictionaries for configuration\\\",\\n\",\n    \"        \\\"‚Ä¢ Use Literal types for constrained values\\\",\\n\",\n    \"        \\\"‚Ä¢ total=False for optional fields in inheritance\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"Dataclasses\\\": [\\n\",\n    \"        \\\"‚Ä¢ Automatic __init__, __repr__, __eq__ generation\\\",\\n\",\n    \"        \\\"‚Ä¢ Use __post_init__ for validation/defaults\\\",\\n\",\n    \"        \\\"‚Ä¢ Avoid mutable defaults (use None + __post_init__)\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"Function Overloading\\\": [\\n\",\n    \"        \\\"‚Ä¢ Multiple type signatures for flexible APIs\\\",\\n\",\n    \"        \\\"‚Ä¢ @overload for type hints, implementation below\\\",\\n\",\n    \"        \\\"‚Ä¢ Improves IDE support and type checking\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"Advanced Decorators\\\": [\\n\",\n    \"        \\\"‚Ä¢ Use ParamSpec (P) to preserve parameter types\\\",\\n\",\n    \"        \\\"‚Ä¢ Callable[P, T] for type-preserving decorators\\\",\\n\",\n    \"        \\\"‚Ä¢ Handle both sync and async functions\\\"\\n\",\n    \"    ]\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"for concept, points in takeaways.items():\\n\",\n    \"    console.print(f\\\"\\\\n[bold yellow]{concept}:[/bold yellow]\\\")\\n\",\n    \"    for point in points:\\n\",\n    \"        console.print(f\\\"  {point}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]üöÄ Production Tips:[/bold green]\\\")\\n\",\n    \"production_tips = [\\n\",\n    \"    \\\"Use mypy or pyright for static type checking\\\",\\n\",\n    \"    \\\"Start with basic types, add generics as needed\\\",\\n\",\n    \"    \\\"Prefer composition over complex inheritance\\\",\\n\",\n    \"    \\\"Use typing for public APIs, relax for internal code\\\",\\n\",\n    \"    \\\"Document complex generic types with examples\\\",\\n\",\n    \"    \\\"Test type annotations with actual usage patterns\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for tip in production_tips:\\n\",\n    \"    console.print(f\\\"  üí° {tip}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold magenta]üéØ Next Steps:[/bold magenta]\\\")\\n\",\n    \"next_steps = [\\n\",\n    \"    \\\"Practice with your own AI components\\\",\\n\",\n    \"    \\\"Explore async patterns in the next example\\\",\\n\",\n    \"    \\\"Set up mypy in your development workflow\\\",\\n\",\n    \"    \\\"Build a typed plugin system using Protocols\\\",\\n\",\n    \"    \\\"Create generic data processing pipelines\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for step in next_steps:\\n\",\n    \"    console.print(f\\\"  üìö {step}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold blue]üéâ Congratulations![/bold blue] You now understand Python's advanced type system!\\\")\\n\",\n    \"console.print(\\\"Use these patterns to build robust, maintainable AI systems. ü§ñ‚ú®\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":30960,"cache_read_input_tokens":0,"output_tokens":3}},"requestId":"req_vrtx_011CRhR3xQo8muxNwPWVLsa2","type":"assistant","uuid":"7e9bdb2c-e451-4eee-896b-e5eb9cb0c374","timestamp":"2025-08-01T17:55:23.128Z"}
{"parentUuid":"7e9bdb2c-e451-4eee-896b-e5eb9cb0c374","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01SrddcxhPQRr3p8dn3a67XD","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/01_type_system_advanced_learning.ipynb"}]},"uuid":"c0ffa078-39fd-4f89-95f2-74dbbf3718b3","timestamp":"2025-08-01T17:57:59.228Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/01_type_system_advanced_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Advanced Python Type System for AI Development\\n\",\n    \"\\n\",\n    \"This notebook will teach you Python's advanced type system through practical AI examples. You'll learn:\\n\",\n    \"\\n\",\n    \"1. **Type Variables & Generics** - Building reusable, type-safe components\\n\",\n    \"2. **Protocols** - Defining interfaces without inheritance\\n\",\n    \"3. **TypedDict** - Structured configuration with type safety\\n\",\n    \"4. **Dataclasses** - Clean data structures with automatic methods\\n\",\n    \"5. **Function Overloading** - Multiple signatures for flexible APIs\\n\",\n    \"6. **Advanced Decorators** - Type-preserving function enhancement\\n\",\n    \"\\n\",\n    \"## Prerequisites\\n\",\n    \"```bash\\n\",\n    \"pip install rich\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 1: Essential Imports and Setup\\n\",\n    \"\\n\",\n    \"**First Principle**: Python's typing module provides static type checking capabilities without runtime performance cost.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Import typing components explicitly for better readability and IDE support.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Essential imports for advanced typing\\n\",\n    \"from typing import (\\n\",\n    \"    TypeVar, Generic, Protocol, runtime_checkable, \\n\",\n    \"    Union, Optional, Literal, TypedDict, overload,\\n\",\n    \"    ParamSpec, Concatenate, Callable, Any, Dict, List\\n\",\n    \")\\n\",\n    \"from dataclasses import dataclass\\n\",\n    \"from abc import ABC, abstractmethod\\n\",\n    \"import asyncio\\n\",\n    \"import hashlib\\n\",\n    \"from rich.console import Console\\n\",\n    \"from rich.panel import Panel\\n\",\n    \"from rich.table import Table\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"print(\\\"‚úÖ All imports successful!\\\")\\n\",\n    \"print(\\\"üìö Ready to explore advanced Python typing!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 2: Understanding Type Variables\\n\",\n    \"\\n\",\n    \"**First Principle**: Type variables create generic, reusable code that maintains type safety across different data types.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use descriptive names for type variables (`T` for generic, `ModelInput`/`ModelOutput` for domain-specific).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Type Variables - The foundation of generic programming\\n\",\n    \"T = TypeVar('T')  # Generic type variable\\n\",\n    \"ModelInput = TypeVar('ModelInput')  # Domain-specific type variable\\n\",\n    \"ModelOutput = TypeVar('ModelOutput')  # Domain-specific type variable\\n\",\n    \"P = ParamSpec('P')  # Parameter specification for functions\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üî§ Type Variables Explained\\\", style=\\\"bold blue\\\"))\\n\",\n    \"console.print(\\\"\\\"\\\"\\n\",\n    \"Type Variables are placeholders that represent \\\"any type\\\" but maintain \\n\",\n    \"consistency within their scope.\\n\",\n    \"\\n\",\n    \"Key Benefits:\\n\",\n    \"‚Ä¢ Type safety: Prevents mixing incompatible types\\n\",\n    \"‚Ä¢ Reusability: Same code works with different types\\n\",\n    \"‚Ä¢ IDE support: Better autocomplete and error detection\\n\",\n    \"‚Ä¢ Documentation: Clear contracts about input/output relationships\\n\",\n    \"\\\"\\\"\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüìù Type variables defined:\\\")\\n\",\n    \"print(f\\\"T: {T}\\\")\\n\",\n    \"print(f\\\"ModelInput: {ModelInput}\\\")\\n\",\n    \"print(f\\\"ModelOutput: {ModelOutput}\\\")\\n\",\n    \"print(f\\\"P: {P}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 3: Dataclasses - Clean Data Structures\\n\",\n    \"\\n\",\n    \"**First Principle**: Dataclasses automatically generate boilerplate methods (`__init__`, `__repr__`, `__eq__`) for data-focused classes.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `__post_init__` for validation and default value initialization that depends on other fields.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"@dataclass\\n\",\n    \"class TextInput:\\n\",\n    \"    \\\"\\\"\\\"Structured text input with automatic method generation.\\\"\\\"\\\"\\n\",\n    \"    text: str\\n\",\n    \"    context: Optional[str] = None\\n\",\n    \"    metadata: Dict[str, Any] = None\\n\",\n    \"    \\n\",\n    \"    def __post_init__(self):\\n\",\n    \"        \\\"\\\"\\\"Called after __init__ for custom initialization.\\\"\\\"\\\"\\n\",\n    \"        if self.metadata is None:\\n\",\n    \"            self.metadata = {}  # Avoid mutable default arguments\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class ClassificationOutput:\\n\",\n    \"    \\\"\\\"\\\"Classification result with confidence scores.\\\"\\\"\\\"\\n\",\n    \"    label: str\\n\",\n    \"    confidence: float\\n\",\n    \"    alternatives: List[tuple[str, float]]\\n\",\n    \"    \\n\",\n    \"    def to_dict(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Convert to dictionary for serialization.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"label\\\": self.label,\\n\",\n    \"            \\\"confidence\\\": self.confidence,\\n\",\n    \"            \\\"alternatives\\\": self.alternatives\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def from_dict(cls, data: Dict[str, Any]) -> 'ClassificationOutput':\\n\",\n    \"        \\\"\\\"\\\"Create instance from dictionary.\\\"\\\"\\\"\\n\",\n    \"        return cls(\\n\",\n    \"            label=data[\\\"label\\\"],\\n\",\n    \"            confidence=data[\\\"confidence\\\"],\\n\",\n    \"            alternatives=data[\\\"alternatives\\\"]\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"# Demonstrate dataclass usage\\n\",\n    \"console.print(Panel(\\\"üìä Dataclasses in Action\\\", style=\\\"bold green\\\"))\\n\",\n    \"\\n\",\n    \"# Create instances\\n\",\n    \"text_input = TextInput(\\n\",\n    \"    text=\\\"This is a positive example!\\\",\\n\",\n    \"    context=\\\"Sentiment analysis demo\\\"\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"classification = ClassificationOutput(\\n\",\n    \"    label=\\\"positive\\\",\\n\",\n    \"    confidence=0.92,\\n\",\n    \"    alternatives=[(\\\"neutral\\\", 0.06), (\\\"negative\\\", 0.02)]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(f\\\"Text Input: {text_input}\\\")\\n\",\n    \"print(f\\\"Classification: {classification}\\\")\\n\",\n    \"print(f\\\"As dict: {classification.to_dict()}\\\")\\n\",\n    \"\\n\",\n    \"# Show automatic equality\\n\",\n    \"same_classification = ClassificationOutput(\\\"positive\\\", 0.92, [(\\\"neutral\\\", 0.06), (\\\"negative\\\", 0.02)])\\n\",\n    \"print(f\\\"\\\\nüìã Automatic equality: {classification == same_classification}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 4: Protocols - Interface Definition Without Inheritance\\n\",\n    \"\\n\",\n    \"**First Principle**: Protocols define \\\"what an object can do\\\" rather than \\\"what an object is\\\" (structural vs nominal typing).\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `@runtime_checkable` when you need `isinstance()` checks, use `Protocol[T]` for generic interfaces.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"@runtime_checkable\\n\",\n    \"class Serializable(Protocol):\\n\",\n    \"    \\\"\\\"\\\"Protocol for objects that can be serialized to/from dictionaries.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def to_dict(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Convert to dictionary.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def from_dict(cls, data: Dict[str, Any]) -> 'Serializable':\\n\",\n    \"        \\\"\\\"\\\"Create from dictionary.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"\\n\",\n    \"@runtime_checkable\\n\",\n    \"class Cacheable(Protocol[T]):\\n\",\n    \"    \\\"\\\"\\\"Generic protocol for cacheable objects.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def cache_key(self) -> str:\\n\",\n    \"        \\\"\\\"\\\"Get unique cache key.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    def serialize(self) -> bytes:\\n\",\n    \"        \\\"\\\"\\\"Serialize for caching.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def deserialize(cls, data: bytes) -> T:\\n\",\n    \"        \\\"\\\"\\\"Deserialize from cache.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"\\n\",\n    \"# Test protocol compliance\\n\",\n    \"console.print(Panel(\\\"üìã Protocol Checking\\\", style=\\\"bold yellow\\\"))\\n\",\n    \"\\n\",\n    \"# Our ClassificationOutput automatically satisfies Serializable!\\n\",\n    \"result = ClassificationOutput(\\\"positive\\\", 0.85, [(\\\"neutral\\\", 0.15)])\\n\",\n    \"\\n\",\n    \"print(f\\\"Does ClassificationOutput implement Serializable? {isinstance(result, Serializable)}\\\")\\n\",\n    \"\\n\",\n    \"if isinstance(result, Serializable):\\n\",\n    \"    serialized = result.to_dict()\\n\",\n    \"    restored = ClassificationOutput.from_dict(serialized)\\n\",\n    \"    print(f\\\"‚úÖ Serialization works: {restored.label} ({restored.confidence})\\\")\\n\",\n    \"\\n\",\n    \"# Test with other objects\\n\",\n    \"test_objects = [\\n\",\n    \"    (\\\"ClassificationOutput\\\", result),\\n\",\n    \"    (\\\"String\\\", \\\"hello\\\"),\\n\",\n    \"    (\\\"Dictionary\\\", {\\\"key\\\": \\\"value\\\"}),\\n\",\n    \"    (\\\"TextInput\\\", text_input)\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüîç Runtime Protocol Checks:\\\")\\n\",\n    \"for name, obj in test_objects:\\n\",\n    \"    is_serializable = isinstance(obj, Serializable)\\n\",\n    \"    print(f\\\"  {name}: Serializable = {is_serializable}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 5: TypedDict - Structured Configuration\\n\",\n    \"\\n\",\n    \"**First Principle**: TypedDict provides type safety for dictionaries while maintaining dict performance and JSON compatibility.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `Literal` types for constrained string values, `total=False` for optional fields in inherited TypedDicts.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Define typed dictionaries for configuration\\n\",\n    \"class ModelConfig(TypedDict):\\n\",\n    \"    \\\"\\\"\\\"Basic model configuration with required fields.\\\"\\\"\\\"\\n\",\n    \"    model_id: str\\n\",\n    \"    max_tokens: int\\n\",\n    \"    temperature: float\\n\",\n    \"    provider: Literal[\\\"openai\\\", \\\"anthropic\\\", \\\"local\\\"]  # Constrained values\\n\",\n    \"    timeout_seconds: Optional[float]\\n\",\n    \"\\n\",\n    \"class AdvancedModelConfig(ModelConfig, total=False):\\n\",\n    \"    \\\"\\\"\\\"Extended config with optional fields.\\\"\\\"\\\"\\n\",\n    \"    retry_attempts: int\\n\",\n    \"    fallback_model: str\\n\",\n    \"    cost_limit_usd: float\\n\",\n    \"    custom_headers: Dict[str, str]\\n\",\n    \"\\n\",\n    \"# Type aliases for clarity\\n\",\n    \"Provider = Literal[\\\"openai\\\", \\\"anthropic\\\", \\\"huggingface\\\", \\\"local\\\"]\\n\",\n    \"ModelSize = Literal[\\\"small\\\", \\\"medium\\\", \\\"large\\\", \\\"xl\\\"]\\n\",\n    \"\\n\",\n    \"def create_model_config(\\n\",\n    \"    provider: Provider,\\n\",\n    \"    model_size: ModelSize,\\n\",\n    \"    temperature: float = 0.7\\n\",\n    \") -> ModelConfig:\\n\",\n    \"    \\\"\\\"\\\"Factory function using literal types for validation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Mapping tuples to model IDs\\n\",\n    \"    model_mapping = {\\n\",\n    \"        (\\\"openai\\\", \\\"small\\\"): \\\"gpt-3.5-turbo\\\",\\n\",\n    \"        (\\\"openai\\\", \\\"large\\\"): \\\"gpt-4\\\",\\n\",\n    \"        (\\\"anthropic\\\", \\\"small\\\"): \\\"claude-3-haiku\\\",\\n\",\n    \"        (\\\"anthropic\\\", \\\"large\\\"): \\\"claude-3-opus\\\",\\n\",\n    \"        (\\\"local\\\", \\\"small\\\"): \\\"local-small-model\\\"\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    model_id = model_mapping.get((provider, model_size), \\\"default-model\\\")\\n\",\n    \"    \\n\",\n    \"    max_tokens_mapping = {\\n\",\n    \"        \\\"small\\\": 1000, \\\"medium\\\": 2000, \\\"large\\\": 4000, \\\"xl\\\": 8000\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    return ModelConfig(\\n\",\n    \"        model_id=model_id,\\n\",\n    \"        max_tokens=max_tokens_mapping[model_size],\\n\",\n    \"        temperature=temperature,\\n\",\n    \"        provider=provider,\\n\",\n    \"        timeout_seconds=30.0\\n\",\n    \"    )\\n\",\n    \"\\n\",\n    \"# Demonstrate TypedDict usage\\n\",\n    \"console.print(Panel(\\\"üìù TypedDict Configuration\\\", style=\\\"bold magenta\\\"))\\n\",\n    \"\\n\",\n    \"# Create configurations\\n\",\n    \"basic_config: ModelConfig = {\\n\",\n    \"    \\\"model_id\\\": \\\"gpt-4\\\",\\n\",\n    \"    \\\"max_tokens\\\": 2000,\\n\",\n    \"    \\\"temperature\\\": 0.7,\\n\",\n    \"    \\\"provider\\\": \\\"openai\\\",\\n\",\n    \"    \\\"timeout_seconds\\\": 30.0\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"advanced_config: AdvancedModelConfig = {\\n\",\n    \"    \\\"model_id\\\": \\\"claude-3-opus\\\",\\n\",\n    \"    \\\"max_tokens\\\": 4000,\\n\",\n    \"    \\\"temperature\\\": 0.5,\\n\",\n    \"    \\\"provider\\\": \\\"anthropic\\\",\\n\",\n    \"    \\\"timeout_seconds\\\": 60.0,\\n\",\n    \"    \\\"retry_attempts\\\": 3,\\n\",\n    \"    \\\"fallback_model\\\": \\\"claude-3-haiku\\\",\\n\",\n    \"    \\\"cost_limit_usd\\\": 10.0\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"print(\\\"üìã Configuration Examples:\\\")\\n\",\n    \"print(f\\\"Basic: {basic_config['provider']} - {basic_config['model_id']}\\\")\\n\",\n    \"print(f\\\"Advanced: {advanced_config['provider']} - {advanced_config['model_id']}\\\")\\n\",\n    \"print(f\\\"Has retries: {advanced_config.get('retry_attempts', 'No')}\\\")\\n\",\n    \"\\n\",\n    \"# Test factory function\\n\",\n    \"print(\\\"\\\\nüè≠ Factory Function Results:\\\")\\n\",\n    \"configs = [\\n\",\n    \"    create_model_config(\\\"openai\\\", \\\"large\\\", 0.8),\\n\",\n    \"    create_model_config(\\\"anthropic\\\", \\\"small\\\", 0.3),\\n\",\n    \"    create_model_config(\\\"local\\\", \\\"small\\\", 1.0)\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for i, config in enumerate(configs, 1):\\n\",\n    \"    print(f\\\"  Config {i}: {config['provider']} - {config['model_id']}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 6: Generic Classes - Type-Safe Containers\\n\",\n    \"\\n\",\n    \"**First Principle**: Generic classes use type variables to create reusable containers that maintain type safety.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `Generic[T]` for classes that store or operate on arbitrary types, use descriptive variable names for domain-specific generics.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class TypedCache(Generic[T]):\\n\",\n    \"    \\\"\\\"\\\"Type-safe cache that remembers what type it stores.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self._cache: Dict[str, T] = {}\\n\",\n    \"        self._access_count: Dict[str, int] = {}\\n\",\n    \"    \\n\",\n    \"    def get(self, key: str) -> Optional[T]:\\n\",\n    \"        \\\"\\\"\\\"Get item from cache with type safety.\\\"\\\"\\\"\\n\",\n    \"        if key in self._cache:\\n\",\n    \"            self._access_count[key] = self._access_count.get(key, 0) + 1\\n\",\n    \"            return self._cache[key]\\n\",\n    \"        return None\\n\",\n    \"    \\n\",\n    \"    def put(self, key: str, value: T) -> None:\\n\",\n    \"        \\\"\\\"\\\"Store item in cache.\\\"\\\"\\\"\\n\",\n    \"        self._cache[key] = value\\n\",\n    \"        self._access_count[key] = 0\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get cache statistics.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"size\\\": len(self._cache),\\n\",\n    \"            \\\"total_access\\\": sum(self._access_count.values()),\\n\",\n    \"            \\\"keys\\\": list(self._cache.keys())\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"# Generic AI Model base class\\n\",\n    \"class AIModel(Generic[ModelInput, ModelOutput], ABC):\\n\",\n    \"    \\\"\\\"\\\"Generic AI model with typed input/output.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, config: ModelConfig):\\n\",\n    \"        self.config = config\\n\",\n    \"    \\n\",\n    \"    @abstractmethod\\n\",\n    \"    async def predict(self, input_data: ModelInput) -> ModelOutput:\\n\",\n    \"        \\\"\\\"\\\"Make prediction with type safety.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"    \\n\",\n    \"    @abstractmethod\\n\",\n    \"    def validate_input(self, input_data: ModelInput) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Validate input data.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"\\n\",\n    \"# Concrete implementation\\n\",\n    \"class TextClassifier(AIModel[TextInput, ClassificationOutput]):\\n\",\n    \"    \\\"\\\"\\\"Text classifier with specific input/output types.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    async def predict(self, input_data: TextInput) -> ClassificationOutput:\\n\",\n    \"        \\\"\\\"\\\"Classify text with mock implementation.\\\"\\\"\\\"\\n\",\n    \"        # Simple sentiment analysis simulation\\n\",\n    \"        text_lower = input_data.text.lower()\\n\",\n    \"        \\n\",\n    \"        if \\\"positive\\\" in text_lower or \\\"good\\\" in text_lower or \\\"great\\\" in text_lower:\\n\",\n    \"            return ClassificationOutput(\\\"positive\\\", 0.92, [(\\\"neutral\\\", 0.06), (\\\"negative\\\", 0.02)])\\n\",\n    \"        elif \\\"negative\\\" in text_lower or \\\"bad\\\" in text_lower or \\\"terrible\\\" in text_lower:\\n\",\n    \"            return ClassificationOutput(\\\"negative\\\", 0.88, [(\\\"neutral\\\", 0.10), (\\\"positive\\\", 0.02)])\\n\",\n    \"        else:\\n\",\n    \"            return ClassificationOutput(\\\"neutral\\\", 0.75, [(\\\"positive\\\", 0.15), (\\\"negative\\\", 0.10)])\\n\",\n    \"    \\n\",\n    \"    def validate_input(self, input_data: TextInput) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Validate text input.\\\"\\\"\\\"\\n\",\n    \"        return (\\n\",\n    \"            isinstance(input_data.text, str) and\\n\",\n    \"            len(input_data.text.strip()) > 0 and\\n\",\n    \"            len(input_data.text) <= 10000\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"# Demonstrate generic usage\\n\",\n    \"console.print(Panel(\\\"üéØ Generic Types in Action\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"# Create typed caches\\n\",\n    \"text_cache = TypedCache[str]()\\n\",\n    \"result_cache = TypedCache[ClassificationOutput]()\\n\",\n    \"\\n\",\n    \"# Use text cache\\n\",\n    \"text_cache.put(\\\"greeting\\\", \\\"Hello, world!\\\")\\n\",\n    \"text_cache.put(\\\"farewell\\\", \\\"Goodbye!\\\")\\n\",\n    \"cached_text = text_cache.get(\\\"greeting\\\")\\n\",\n    \"print(f\\\"üìù Cached text: {cached_text}\\\")\\n\",\n    \"\\n\",\n    \"# Use result cache\\n\",\n    \"sample_result = ClassificationOutput(\\\"positive\\\", 0.95, [(\\\"negative\\\", 0.05)])\\n\",\n    \"result_cache.put(\\\"sample\\\", sample_result)\\n\",\n    \"cached_result = result_cache.get(\\\"sample\\\")\\n\",\n    \"print(f\\\"üìä Cached result: {cached_result.label} ({cached_result.confidence})\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìà Cache Stats:\\\")\\n\",\n    \"print(f\\\"  Text cache: {text_cache.get_stats()}\\\")\\n\",\n    \"print(f\\\"  Result cache: {result_cache.get_stats()}\\\")\\n\",\n    \"\\n\",\n    \"# Test the classifier\\n\",\n    \"print(f\\\"\\\\nü§ñ Testing TextClassifier:\\\")\\n\",\n    \"config = create_model_config(\\\"openai\\\", \\\"large\\\")\\n\",\n    \"classifier = TextClassifier(config)\\n\",\n    \"\\n\",\n    \"test_inputs = [\\n\",\n    \"    TextInput(\\\"This is a great positive example!\\\"),\\n\",\n    \"    TextInput(\\\"This is a terrible negative example!\\\"),\\n\",\n    \"    TextInput(\\\"This is just a normal example.\\\")\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for i, test_input in enumerate(test_inputs, 1):\\n\",\n    \"    if classifier.validate_input(test_input):\\n\",\n    \"        result = await classifier.predict(test_input)\\n\",\n    \"        print(f\\\"  {i}. '{test_input.text}' -> {result.label} ({result.confidence:.2f})\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"  {i}. Invalid input: {test_input.text}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 7: Function Overloading\\n\",\n    \"\\n\",\n    \"**First Principle**: Function overloading allows multiple type signatures for the same function, improving API flexibility while maintaining type safety.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `@overload` for type hints only, implement the actual logic in the non-decorated function below.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class ModelManager:\\n\",\n    \"    \\\"\\\"\\\"Manages multiple AI models with overloaded methods.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.models: Dict[str, AIModel] = {}\\n\",\n    \"        self.cache = TypedCache[Any]()\\n\",\n    \"    \\n\",\n    \"    # Overloaded get_model methods\\n\",\n    \"    @overload\\n\",\n    \"    def get_model(self, name: str) -> Optional[AIModel]:\\n\",\n    \"        \\\"\\\"\\\"Get model by name, return None if not found.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    @overload\\n\",\n    \"    def get_model(self, name: str, default: T) -> Union[AIModel, T]:\\n\",\n    \"        \\\"\\\"\\\"Get model by name, return default if not found.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    def get_model(self, name: str, default=None):\\n\",\n    \"        \\\"\\\"\\\"Implementation for both overloads.\\\"\\\"\\\"\\n\",\n    \"        return self.models.get(name, default)\\n\",\n    \"    \\n\",\n    \"    def register_model(self, name: str, model: AIModel) -> None:\\n\",\n    \"        \\\"\\\"\\\"Register a model with a name.\\\"\\\"\\\"\\n\",\n    \"        self.models[name] = model\\n\",\n    \"    \\n\",\n    \"    # Overloaded predict methods\\n\",\n    \"    @overload\\n\",\n    \"    async def predict(self, model_name: str, input_data: TextInput) -> ClassificationOutput:\\n\",\n    \"        \\\"\\\"\\\"Predict with text classifier.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    @overload\\n\",\n    \"    async def predict(self, model_name: str, input_data: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Generic prediction method.\\\"\\\"\\\"\\n\",\n    \"        ...\\n\",\n    \"    \\n\",\n    \"    async def predict(self, model_name: str, input_data: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Make prediction with caching.\\\"\\\"\\\"\\n\",\n    \"        model = self.get_model(model_name)\\n\",\n    \"        if model is None:\\n\",\n    \"            raise ValueError(f\\\"Model '{model_name}' not found\\\")\\n\",\n    \"        \\n\",\n    \"        # Simple cache key generation\\n\",\n    \"        cache_key = f\\\"{model_name}:{hash(str(input_data.text if hasattr(input_data, 'text') else input_data))}\\\"\\n\",\n    \"        \\n\",\n    \"        # Check cache first\\n\",\n    \"        cached_result = self.cache.get(cache_key)\\n\",\n    \"        if cached_result is not None:\\n\",\n    \"            print(f\\\"üóÑÔ∏è  Cache hit for {model_name}\\\")\\n\",\n    \"            return cached_result\\n\",\n    \"        \\n\",\n    \"        # Make prediction\\n\",\n    \"        print(f\\\"ü§ñ Making prediction with {model_name}\\\")\\n\",\n    \"        result = await model.predict(input_data)\\n\",\n    \"        \\n\",\n    \"        # Cache the result\\n\",\n    \"        self.cache.put(cache_key, result)\\n\",\n    \"        \\n\",\n    \"        return result\\n\",\n    \"\\n\",\n    \"# Demonstrate function overloading\\n\",\n    \"console.print(Panel(\\\"‚ö° Function Overloading\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"\\n\",\n    \"# Setup\\n\",\n    \"manager = ModelManager()\\n\",\n    \"config = create_model_config(\\\"openai\\\", \\\"large\\\")\\n\",\n    \"classifier = TextClassifier(config)\\n\",\n    \"manager.register_model(\\\"sentiment_classifier\\\", classifier)\\n\",\n    \"\\n\",\n    \"print(\\\"üîß Testing overloaded get_model:\\\")\\n\",\n    \"\\n\",\n    \"# Test get_model without default\\n\",\n    \"model1 = manager.get_model(\\\"sentiment_classifier\\\")\\n\",\n    \"print(f\\\"  Found model: {model1 is not None}\\\")\\n\",\n    \"\\n\",\n    \"# Test get_model with default\\n\",\n    \"model2 = manager.get_model(\\\"nonexistent\\\", \\\"default_value\\\")\\n\",\n    \"print(f\\\"  With default: {model2}\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüéØ Testing overloaded predict:\\\")\\n\",\n    \"\\n\",\n    \"# Test prediction with caching\\n\",\n    \"test_input = TextInput(\\\"This is a fantastic positive example!\\\")\\n\",\n    \"\\n\",\n    \"# First call - should compute\\n\",\n    \"result1 = await manager.predict(\\\"sentiment_classifier\\\", test_input)\\n\",\n    \"print(f\\\"  First call: {result1.label} ({result1.confidence:.2f})\\\")\\n\",\n    \"\\n\",\n    \"# Second call - should use cache\\n\",\n    \"result2 = await manager.predict(\\\"sentiment_classifier\\\", test_input)\\n\",\n    \"print(f\\\"  Second call: {result2.label} ({result2.confidence:.2f})\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìä Cache statistics: {manager.cache.get_stats()}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 8: Advanced Decorators with Type Preservation\\n\",\n    \"\\n\",\n    \"**First Principle**: Decorators can preserve function signatures and types using `ParamSpec` and proper generic typing.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `ParamSpec` to maintain parameter types, use `Callable[P, T]` for functions that accept any parameters and return type T.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def with_retries(max_retries: int = 3) -> Callable[[Callable[P, T]], Callable[P, T]]:\\n\",\n    \"    \\\"\\\"\\\"Decorator that adds retry logic with full type preservation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def decorator(func: Callable[P, T]) -> Callable[P, T]:\\n\",\n    \"        async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\\n\",\n    \"            last_exception = None\\n\",\n    \"            \\n\",\n    \"            for attempt in range(max_retries):\\n\",\n    \"                try:\\n\",\n    \"                    # Handle both sync and async functions\\n\",\n    \"                    if asyncio.iscoroutinefunction(func):\\n\",\n    \"                        return await func(*args, **kwargs)\\n\",\n    \"                    else:\\n\",\n    \"                        return func(*args, **kwargs)\\n\",\n    \"                except Exception as e:\\n\",\n    \"                    last_exception = e\\n\",\n    \"                    print(f\\\"‚ùå Attempt {attempt + 1} failed: {str(e)[:50]}...\\\")\\n\",\n    \"                    \\n\",\n    \"                    if attempt < max_retries - 1:\\n\",\n    \"                        wait_time = 2 ** attempt  # Exponential backoff\\n\",\n    \"                        print(f\\\"‚è≥ Waiting {wait_time}s before retry...\\\")\\n\",\n    \"                        await asyncio.sleep(wait_time)\\n\",\n    \"            \\n\",\n    \"            print(f\\\"üí• All {max_retries} attempts failed\\\")\\n\",\n    \"            raise last_exception\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"    return decorator\\n\",\n    \"\\n\",\n    \"def log_calls(func: Callable[P, T]) -> Callable[P, T]:\\n\",\n    \"    \\\"\\\"\\\"Decorator that logs function calls with type preservation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\\n\",\n    \"        func_name = func.__name__\\n\",\n    \"        print(f\\\"üöÄ Calling {func_name} with {len(args)} args, {len(kwargs)} kwargs\\\")\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            if asyncio.iscoroutinefunction(func):\\n\",\n    \"                result = await func(*args, **kwargs)\\n\",\n    \"            else:\\n\",\n    \"                result = func(*args, **kwargs)\\n\",\n    \"            \\n\",\n    \"            print(f\\\"‚úÖ {func_name} completed successfully\\\")\\n\",\n    \"            return result\\n\",\n    \"        except Exception as e:\\n\",\n    \"            print(f\\\"üí• {func_name} failed: {e}\\\")\\n\",\n    \"            raise\\n\",\n    \"    \\n\",\n    \"    return wrapper\\n\",\n    \"\\n\",\n    \"# Example functions using the decorators\\n\",\n    \"@with_retries(max_retries=3)\\n\",\n    \"@log_calls\\n\",\n    \"async def flaky_api_call(text: str, fail_rate: float = 0.7) -> str:\\n\",\n    \"    \\\"\\\"\\\"Simulates an unreliable API call.\\\"\\\"\\\"\\n\",\n    \"    import random\\n\",\n    \"    \\n\",\n    \"    if random.random() < fail_rate:\\n\",\n    \"        raise ConnectionError(f\\\"Simulated network failure for '{text}'\\\")\\n\",\n    \"    \\n\",\n    \"    # Simulate processing time\\n\",\n    \"    await asyncio.sleep(0.1)\\n\",\n    \"    return f\\\"Processed: {text.upper()}\\\"\\n\",\n    \"\\n\",\n    \"@with_retries(max_retries=2)\\n\",\n    \"async def reliable_classification(input_data: TextInput) -> ClassificationOutput:\\n\",\n    \"    \\\"\\\"\\\"Reliable classification with retries.\\\"\\\"\\\"\\n\",\n    \"    # Simulate occasional failures\\n\",\n    \"    import random\\n\",\n    \"    if random.random() < 0.3:\\n\",\n    \"        raise ValueError(\\\"Temporary classification error\\\")\\n\",\n    \"    \\n\",\n    \"    # Simple classification\\n\",\n    \"    text_lower = input_data.text.lower()\\n\",\n    \"    if \\\"good\\\" in text_lower or \\\"great\\\" in text_lower:\\n\",\n    \"        return ClassificationOutput(\\\"positive\\\", 0.90, [(\\\"neutral\\\", 0.08), (\\\"negative\\\", 0.02)])\\n\",\n    \"    else:\\n\",\n    \"        return ClassificationOutput(\\\"neutral\\\", 0.70, [(\\\"positive\\\", 0.20), (\\\"negative\\\", 0.10)])\\n\",\n    \"\\n\",\n    \"# Demonstrate advanced decorators\\n\",\n    \"console.print(Panel(\\\"üé® Advanced Decorators with Type Preservation\\\", style=\\\"bold red\\\"))\\n\",\n    \"\\n\",\n    \"print(\\\"üß™ Testing flaky API call:\\\")\\n\",\n    \"try:\\n\",\n    \"    result = await flaky_api_call(\\\"hello world\\\", fail_rate=0.8)\\n\",\n    \"    print(f\\\"üéâ Success: {result}\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"üö´ Final failure: {e}\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüß™ Testing reliable classification:\\\")\\n\",\n    \"test_input = TextInput(\\\"This is a good example\\\")\\n\",\n    \"try:\\n\",\n    \"    classification = await reliable_classification(test_input)\\n\",\n    \"    print(f\\\"üéØ Classification: {classification.label} ({classification.confidence:.2f})\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"üö´ Classification failed: {e}\\\")\\n\",\n    \"\\n\",\n    \"# Show that type information is preserved\\n\",\n    \"print(\\\"\\\\nüîç Type preservation check:\\\")\\n\",\n    \"print(f\\\"flaky_api_call annotations: {flaky_api_call.__annotations__}\\\")\\n\",\n    \"print(f\\\"reliable_classification return type preserved: {hasattr(reliable_classification, '__annotations__')}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 9: Real-World Integration Example\\n\",\n    \"\\n\",\n    \"**First Principle**: Combine all typing features to create a robust, type-safe AI system that's easy to extend and maintain.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use composition of typed components rather than complex inheritance hierarchies.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class AIServiceConfig(TypedDict):\\n\",\n    \"    \\\"\\\"\\\"Configuration for AI service.\\\"\\\"\\\"\\n\",\n    \"    default_provider: Provider\\n\",\n    \"    max_concurrent_requests: int\\n\",\n    \"    cache_ttl_seconds: int\\n\",\n    \"    retry_attempts: int\\n\",\n    \"\\n\",\n    \"class AIService:\\n\",\n    \"    \\\"\\\"\\\"Production-ready AI service using all typing features.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, config: AIServiceConfig):\\n\",\n    \"        self.config = config\\n\",\n    \"        self.manager = ModelManager()\\n\",\n    \"        self.request_cache = TypedCache[ClassificationOutput]()\\n\",\n    \"        self._setup_default_models()\\n\",\n    \"    \\n\",\n    \"    def _setup_default_models(self) -> None:\\n\",\n    \"        \\\"\\\"\\\"Initialize default models.\\\"\\\"\\\"\\n\",\n    \"        model_config = create_model_config(\\n\",\n    \"            self.config[\\\"default_provider\\\"], \\n\",\n    \"            \\\"large\\\", \\n\",\n    \"            temperature=0.5\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        classifier = TextClassifier(model_config)\\n\",\n    \"        self.manager.register_model(\\\"default_classifier\\\", classifier)\\n\",\n    \"    \\n\",\n    \"    @with_retries(max_retries=3)\\n\",\n    \"    @log_calls\\n\",\n    \"    async def classify_text(\\n\",\n    \"        self, \\n\",\n    \"        text: str, \\n\",\n    \"        context: Optional[str] = None,\\n\",\n    \"        use_cache: bool = True\\n\",\n    \"    ) -> ClassificationOutput:\\n\",\n    \"        \\\"\\\"\\\"Classify text with full error handling and caching.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Create structured input\\n\",\n    \"        text_input = TextInput(text=text, context=context)\\n\",\n    \"        \\n\",\n    \"        # Check cache if enabled\\n\",\n    \"        if use_cache:\\n\",\n    \"            cache_key = f\\\"classify:{hash(text)}:{hash(context or '')}\\\"\\n\",\n    \"            cached_result = self.request_cache.get(cache_key)\\n\",\n    \"            if cached_result is not None:\\n\",\n    \"                print(f\\\"üíæ Cache hit for text: '{text[:30]}...'\\\")\\n\",\n    \"                return cached_result\\n\",\n    \"        \\n\",\n    \"        # Make prediction\\n\",\n    \"        result = await self.manager.predict(\\\"default_classifier\\\", text_input)\\n\",\n    \"        \\n\",\n    \"        # Cache result\\n\",\n    \"        if use_cache and isinstance(result, ClassificationOutput):\\n\",\n    \"            self.request_cache.put(cache_key, result)\\n\",\n    \"        \\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    async def batch_classify(\\n\",\n    \"        self, \\n\",\n    \"        texts: List[str], \\n\",\n    \"        context: Optional[str] = None\\n\",\n    \"    ) -> List[ClassificationOutput]:\\n\",\n    \"        \\\"\\\"\\\"Classify multiple texts concurrently.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Create tasks for concurrent processing\\n\",\n    \"        tasks = [\\n\",\n    \"            self.classify_text(text, context)\\n\",\n    \"            for text in texts\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        # Wait for all classifications\\n\",\n    \"        results = await asyncio.gather(*tasks, return_exceptions=True)\\n\",\n    \"        \\n\",\n    \"        # Handle any failures\\n\",\n    \"        valid_results = []\\n\",\n    \"        for i, result in enumerate(results):\\n\",\n    \"            if isinstance(result, Exception):\\n\",\n    \"                print(f\\\"‚ùå Failed to classify text {i}: {result}\\\")\\n\",\n    \"                # Return neutral as fallback\\n\",\n    \"                valid_results.append(\\n\",\n    \"                    ClassificationOutput(\\\"neutral\\\", 0.5, [(\\\"positive\\\", 0.25), (\\\"negative\\\", 0.25)])\\n\",\n    \"                )\\n\",\n    \"            else:\\n\",\n    \"                valid_results.append(result)\\n\",\n    \"        \\n\",\n    \"        return valid_results\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get service statistics.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"config\\\": self.config,\\n\",\n    \"            \\\"models_registered\\\": len(self.manager.models),\\n\",\n    \"            \\\"cache_stats\\\": self.request_cache.get_stats(),\\n\",\n    \"            \\\"manager_cache_stats\\\": self.manager.cache.get_stats()\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"# Demonstrate the complete system\\n\",\n    \"console.print(Panel(\\\"üèóÔ∏è Complete AI Service Integration\\\", style=\\\"bold green\\\"))\\n\",\n    \"\\n\",\n    \"# Create service configuration\\n\",\n    \"service_config: AIServiceConfig = {\\n\",\n    \"    \\\"default_provider\\\": \\\"openai\\\",\\n\",\n    \"    \\\"max_concurrent_requests\\\": 10,\\n\",\n    \"    \\\"cache_ttl_seconds\\\": 3600,\\n\",\n    \"    \\\"retry_attempts\\\": 3\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"# Initialize service\\n\",\n    \"ai_service = AIService(service_config)\\n\",\n    \"\\n\",\n    \"print(\\\"üß™ Testing single classification:\\\")\\n\",\n    \"single_result = await ai_service.classify_text(\\n\",\n    \"    \\\"This is an amazing product with great features!\\\",\\n\",\n    \"    context=\\\"Product review\\\"\\n\",\n    \")\\n\",\n    \"print(f\\\"Result: {single_result.label} (confidence: {single_result.confidence:.2f})\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüß™ Testing batch classification:\\\")\\n\",\n    \"test_texts = [\\n\",\n    \"    \\\"This product is fantastic!\\\",\\n\",\n    \"    \\\"Terrible customer service experience.\\\",\\n\",\n    \"    \\\"The product works as expected.\\\",\\n\",\n    \"    \\\"Outstanding quality and great value!\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"batch_results = await ai_service.batch_classify(test_texts, \\\"Product reviews\\\")\\n\",\n    \"\\n\",\n    \"for i, (text, result) in enumerate(zip(test_texts, batch_results), 1):\\n\",\n    \"    print(f\\\"  {i}. '{text}' -> {result.label} ({result.confidence:.2f})\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüìä Service statistics:\\\")\\n\",\n    \"stats = ai_service.get_stats()\\n\",\n    \"print(f\\\"Models registered: {stats['models_registered']}\\\")\\n\",\n    \"print(f\\\"Cache entries: {stats['cache_stats']['size']}\\\")\\n\",\n    \"print(f\\\"Total cache accesses: {stats['cache_stats']['total_access']}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 10: Key Takeaways and Best Practices\\n\",\n    \"\\n\",\n    \"**Summary of Python Typing First Principles and Idioms**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"console.print(Panel(\\\"üéì Key Takeaways: Advanced Python Typing\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"takeaways = {\\n\",\n    \"    \\\"Type Variables (Generic[T])\\\": [\\n\",\n    \"        \\\"‚Ä¢ Enable reusable, type-safe components\\\",\\n\",\n    \"        \\\"‚Ä¢ Use descriptive names: ModelInput, ModelOutput\\\",\\n\",\n    \"        \\\"‚Ä¢ Maintain type relationships across function calls\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"Protocols\\\": [\\n\",\n    \"        \\\"‚Ä¢ Define 'what objects can do' not 'what they are'\\\",\\n\",\n    \"        \\\"‚Ä¢ Use @runtime_checkable for isinstance() support\\\",\\n\",\n    \"        \\\"‚Ä¢ Protocol[T] enables generic interfaces\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"TypedDict\\\": [\\n\",\n    \"        \\\"‚Ä¢ Type-safe dictionaries for configuration\\\",\\n\",\n    \"        \\\"‚Ä¢ Use Literal types for constrained values\\\",\\n\",\n    \"        \\\"‚Ä¢ total=False for optional fields in inheritance\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"Dataclasses\\\": [\\n\",\n    \"        \\\"‚Ä¢ Automatic __init__, __repr__, __eq__ generation\\\",\\n\",\n    \"        \\\"‚Ä¢ Use __post_init__ for validation/defaults\\\",\\n\",\n    \"        \\\"‚Ä¢ Avoid mutable defaults (use None + __post_init__)\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"Function Overloading\\\": [\\n\",\n    \"        \\\"‚Ä¢ Multiple type signatures for flexible APIs\\\",\\n\",\n    \"        \\\"‚Ä¢ @overload for type hints, implementation below\\\",\\n\",\n    \"        \\\"‚Ä¢ Improves IDE support and type checking\\\"\\n\",\n    \"    ],\\n\",\n    \"    \\\"Advanced Decorators\\\": [\\n\",\n    \"        \\\"‚Ä¢ Use ParamSpec (P) to preserve parameter types\\\",\\n\",\n    \"        \\\"‚Ä¢ Callable[P, T] for type-preserving decorators\\\",\\n\",\n    \"        \\\"‚Ä¢ Handle both sync and async functions\\\"\\n\",\n    \"    ]\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"for concept, points in takeaways.items():\\n\",\n    \"    console.print(f\\\"\\\\n[bold yellow]{concept}:[/bold yellow]\\\")\\n\",\n    \"    for point in points:\\n\",\n    \"        console.print(f\\\"  {point}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]üöÄ Production Tips:[/bold green]\\\")\\n\",\n    \"production_tips = [\\n\",\n    \"    \\\"Use mypy or pyright for static type checking\\\",\\n\",\n    \"    \\\"Start with basic types, add generics as needed\\\",\\n\",\n    \"    \\\"Prefer composition over complex inheritance\\\",\\n\",\n    \"    \\\"Use typing for public APIs, relax for internal code\\\",\\n\",\n    \"    \\\"Document complex generic types with examples\\\",\\n\",\n    \"    \\\"Test type annotations with actual usage patterns\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for tip in production_tips:\\n\",\n    \"    console.print(f\\\"  üí° {tip}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold magenta]üéØ Next Steps:[/bold magenta]\\\")\\n\",\n    \"next_steps = [\\n\",\n    \"    \\\"Practice with your own AI components\\\",\\n\",\n    \"    \\\"Explore async patterns in the next example\\\",\\n\",\n    \"    \\\"Set up mypy in your development workflow\\\",\\n\",\n    \"    \\\"Build a typed plugin system using Protocols\\\",\\n\",\n    \"    \\\"Create generic data processing pipelines\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for step in next_steps:\\n\",\n    \"    console.print(f\\\"  üìö {step}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold blue]üéâ Congratulations![/bold blue] You now understand Python's advanced type system!\\\")\\n\",\n    \"console.print(\\\"Use these patterns to build robust, maintainable AI systems. ü§ñ‚ú®\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}","structuredPatch":[]}}
{"parentUuid":"c0ffa078-39fd-4f89-95f2-74dbbf3718b3","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01TvRi1ZdRva55QJKUB5J2U6","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I've created a comprehensive Jupyter notebook that transforms the advanced type system examples into an interactive learning experience. Here's what the notebook covers:\n\n## üìö **Learning Structure**\n\n**10 Progressive Cells** covering:\n\n1. **Setup & Imports** - Essential typing components\n2. **Type Variables** - Foundation of generic programming  \n3. **Dataclasses** - Clean data structures with automatic methods\n4. **Protocols** - Interface definition without inheritance\n5. **TypedDict** - Structured configuration with type safety\n6. **Generic Classes** - Type-safe containers and AI models\n7. **Function Overloading** - Multiple signatures for flexible APIs\n8. **Advanced Decorators** - Type-preserving function enhancement\n9. **Real-World Integration** - Complete AI service example\n10. **Key Takeaways** - Best practices and production tips\n\n## üéØ **First Principles Explained**\n\n- **Type Safety**: How Python's type system prevents bugs without runtime cost\n- **Structural vs Nominal Typing**: Why Protocols are powerful for AI systems\n- **Generic Programming**: Building reusable components that work with any type\n- **Composition over Inheritance**: Modern Python patterns for maintainable code\n\n## üêç **Python Idioms Covered**\n\n- `@runtime_checkable` for isinstance() with Protocols\n- `__post_init__` for dataclass validation\n- `total=False` for optional TypedDict fields\n- `ParamSpec` for type-preserving decorators\n- `@overload` for multiple function signatures\n\n## ü§ñ **AI-Specific Examples**\n\n- **Text Classification Pipeline** with typed inputs/outputs\n- **Model Management System** with caching and retries\n- **Generic AI Model Interface** that works with any data type\n- **Production AI Service** combining all patterns\n\nThe notebook is designed for hands-on learning - each cell builds on the previous one, and you can run them interactively to see the concepts in action. It will help you become well-rounded in both Python typing fundamentals and practical AI development patterns."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":28927,"cache_read_input_tokens":15686,"output_tokens":1}},"requestId":"req_vrtx_011CRhRVtRQkYo1oaDuMCfhk","type":"assistant","uuid":"547241d1-4b28-45f4-9375-1d14550796c5","timestamp":"2025-08-01T17:58:10.322Z"}
{"parentUuid":"547241d1-4b28-45f4-9375-1d14550796c5","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":"ok great, can you move notebooks directory this one and create similiar for other 2,3,4,5 also start with 3, 2,5,4 python files"},"uuid":"281692fb-bba5-45be-abd6-0a25b66fc69a","timestamp":"2025-08-01T17:59:22.472Z"}
{"parentUuid":"281692fb-bba5-45be-abd6-0a25b66fc69a","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01XEYN1B4nMQhn3Ro8UwxXYE","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I'll create a notebooks directory and move the existing notebook there, then create similar interactive learning notebooks for the other Python files in the specified order (3, 2, 5, 4)."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":523,"cache_read_input_tokens":44613,"output_tokens":1}},"requestId":"req_vrtx_011CRhRc2ENvQpAogC3YwFmm","type":"assistant","uuid":"ce230e36-d759-40c8-b6c7-586903ace22e","timestamp":"2025-08-01T17:59:25.690Z"}
{"parentUuid":"ce230e36-d759-40c8-b6c7-586903ace22e","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01XEYN1B4nMQhn3Ro8UwxXYE","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01EA3KRVAS8iTiFSn72wWLgy","name":"Bash","input":{"command":"mkdir -p /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks","description":"Create notebooks directory"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":523,"cache_read_input_tokens":44613,"output_tokens":1}},"requestId":"req_vrtx_011CRhRc2ENvQpAogC3YwFmm","type":"assistant","uuid":"ba1f887e-b3d1-4dac-9c0a-8927a7753808","timestamp":"2025-08-01T17:59:26.495Z"}
{"parentUuid":"ba1f887e-b3d1-4dac-9c0a-8927a7753808","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01EA3KRVAS8iTiFSn72wWLgy","type":"tool_result","content":"","is_error":false}]},"uuid":"577da939-6950-4f8f-a13b-f5929d5310ed","timestamp":"2025-08-01T17:59:31.357Z","toolUseResult":{"stdout":"","stderr":"","interrupted":false,"isImage":false}}
{"parentUuid":"577da939-6950-4f8f-a13b-f5929d5310ed","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01Qdwa8oyL3w9HjX77ZDUsKJ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_017AJakLpnRdu1GcXLWyU3aZ","name":"Bash","input":{"command":"mv /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/01_type_system_advanced_learning.ipynb /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/","description":"Move notebook to notebooks directory"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":163,"cache_read_input_tokens":45136,"output_tokens":25}},"requestId":"req_vrtx_011CRhRcgKqSUpVWX3XUgXMT","type":"assistant","uuid":"f709e3f6-de9d-42dc-9f52-7557720a7994","timestamp":"2025-08-01T17:59:34.731Z"}
{"parentUuid":"f709e3f6-de9d-42dc-9f52-7557720a7994","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_017AJakLpnRdu1GcXLWyU3aZ","type":"tool_result","content":"","is_error":false}]},"uuid":"380435fb-1caf-4e97-92bf-26d08256208f","timestamp":"2025-08-01T17:59:39.180Z","toolUseResult":{"stdout":"","stderr":"","interrupted":false,"isImage":false}}
{"parentUuid":"380435fb-1caf-4e97-92bf-26d08256208f","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01Ri7zKk3GmE6RYPqb9PUTyp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me read the other Python files to create similar notebooks. Starting with file 3:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":167,"cache_read_input_tokens":45299,"output_tokens":1}},"requestId":"req_vrtx_011CRhRdFSePfKUAXavQsqjj","type":"assistant","uuid":"a2cedc05-e57e-4235-a8b1-a8309f13a7ec","timestamp":"2025-08-01T17:59:42.494Z"}
{"parentUuid":"a2cedc05-e57e-4235-a8b1-a8309f13a7ec","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01Ri7zKk3GmE6RYPqb9PUTyp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01LUUKKqTkb5chD215gpDt9b","name":"Read","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/03_context_managers.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":167,"cache_read_input_tokens":45299,"output_tokens":1}},"requestId":"req_vrtx_011CRhRdFSePfKUAXavQsqjj","type":"assistant","uuid":"559c2b66-7e45-4951-8150-cf4f17348003","timestamp":"2025-08-01T17:59:43.110Z"}
{"parentUuid":"559c2b66-7e45-4951-8150-cf4f17348003","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01Ri7zKk3GmE6RYPqb9PUTyp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01E2ieMrFtemuoFa5Lk3XNUK","name":"Read","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/02_async_patterns.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":167,"cache_read_input_tokens":45299,"output_tokens":1}},"requestId":"req_vrtx_011CRhRdFSePfKUAXavQsqjj","type":"assistant","uuid":"57a76767-e1bd-427a-9a27-af76b4b685e0","timestamp":"2025-08-01T17:59:43.830Z"}
{"parentUuid":"57a76767-e1bd-427a-9a27-af76b4b685e0","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01Ri7zKk3GmE6RYPqb9PUTyp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01Fg17WrVamtfpEj1VED6Vgr","name":"Read","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/05_performance_optimization.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":167,"cache_read_input_tokens":45299,"output_tokens":1}},"requestId":"req_vrtx_011CRhRdFSePfKUAXavQsqjj","type":"assistant","uuid":"cfbbcd2e-d317-4bce-956e-6167f7124572","timestamp":"2025-08-01T17:59:44.698Z"}
{"parentUuid":"cfbbcd2e-d317-4bce-956e-6167f7124572","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01Ri7zKk3GmE6RYPqb9PUTyp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_011fS3161tFmRKbYSGG5Pi8z","name":"Read","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/04_error_handling.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":167,"cache_read_input_tokens":45299,"output_tokens":1}},"requestId":"req_vrtx_011CRhRdFSePfKUAXavQsqjj","type":"assistant","uuid":"36af252e-5632-4a43-9d14-15b313dbc3a8","timestamp":"2025-08-01T17:59:45.242Z"}
{"parentUuid":"36af252e-5632-4a43-9d14-15b313dbc3a8","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"File does not exist.","is_error":true,"tool_use_id":"toolu_vrtx_01LUUKKqTkb5chD215gpDt9b"}]},"uuid":"3e9adf2b-6213-474b-951d-f1fa5c328c1e","timestamp":"2025-08-01T17:59:45.283Z","toolUseResult":"Error: File does not exist."}
{"parentUuid":"3e9adf2b-6213-474b-951d-f1fa5c328c1e","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01E2ieMrFtemuoFa5Lk3XNUK","type":"tool_result","content":"     1‚Üí#!/usr/bin/env python3\n     2‚Üí\"\"\"\n     3‚ÜíExample 2: Async Patterns for AI Systems\n     4‚Üí\n     5‚ÜíLearn advanced asyncio patterns for building high-performance\n     6‚ÜíAI systems with concurrent processing, streaming, and resource management.\n     7‚Üí\n     8‚ÜíRun this file to see the examples in action:\n     9‚Üí    python 03-python-idioms/examples/02_async_patterns.py\n    10‚Üí\"\"\"\n    11‚Üí\n    12‚Üíimport sys\n    13‚Üíimport os\n    14‚Üísys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n    15‚Üí\n    16‚Üíimport asyncio\n    17‚Üífrom typing import AsyncGenerator, AsyncIterator, Callable, Any, Optional, List, Dict\n    18‚Üífrom dataclasses import dataclass\n    19‚Üífrom contextlib import asynccontextmanager\n    20‚Üíimport time\n    21‚Üífrom rich.console import Console\n    22‚Üífrom rich.panel import Panel\n    23‚Üífrom rich.table import Table\n    24‚Üífrom rich.live import Live\n    25‚Üífrom rich.progress import Progress, TaskID\n    26‚Üí\n    27‚Üífrom common import Message, ChatResponse, Choice\n    28‚Üí\n    29‚Üíconsole = Console()\n    30‚Üí\n    31‚Üí\n    32‚Üí# üéØ LEARNING GOAL: Master async patterns for high-performance AI systems\n    33‚Üí\n    34‚Üí\n    35‚Üí@dataclass\n    36‚Üíclass StreamingChunk:\n    37‚Üí    \"\"\"Represents a chunk of streaming data.\"\"\"\n    38‚Üí    content: str\n    39‚Üí    chunk_id: int\n    40‚Üí    is_final: bool\n    41‚Üí    metadata: Dict[str, Any] = None\n    42‚Üí    \n    43‚Üí    def __post_init__(self):\n    44‚Üí        if self.metadata is None:\n    45‚Üí            self.metadata = {}\n    46‚Üí\n    47‚Üí\n    48‚Üíclass AsyncRateLimiter:\n    49‚Üí    \"\"\"Async rate limiter for API calls.\"\"\"\n    50‚Üí    \n    51‚Üí    def __init__(self, max_calls: int, time_window: float):\n    52‚Üí        self.max_calls = max_calls\n    53‚Üí        self.time_window = time_window\n    54‚Üí        self.calls: List[float] = []\n    55‚Üí        self._lock = asyncio.Lock()\n    56‚Üí    \n    57‚Üí    async def acquire(self) -> None:\n    58‚Üí        \"\"\"Acquire permission to make a call.\"\"\"\n    59‚Üí        async with self._lock:\n    60‚Üí            now = time.time()\n    61‚Üí            \n    62‚Üí            # Remove old calls outside the window\n    63‚Üí            self.calls = [call_time for call_time in self.calls \n    64‚Üí                         if now - call_time < self.time_window]\n    65‚Üí            \n    66‚Üí            # If we're at the limit, wait\n    67‚Üí            if len(self.calls) >= self.max_calls:\n    68‚Üí                oldest_call = min(self.calls)\n    69‚Üí                wait_time = self.time_window - (now - oldest_call)\n    70‚Üí                if wait_time > 0:\n    71‚Üí                    await asyncio.sleep(wait_time)\n    72‚Üí                    # Recursive call to try again\n    73‚Üí                    return await self.acquire()\n    74‚Üí            \n    75‚Üí            # Record this call\n    76‚Üí            self.calls.append(now)\n    77‚Üí    \n    78‚Üí    def get_stats(self) -> Dict[str, Any]:\n    79‚Üí        \"\"\"Get rate limiter statistics.\"\"\"\n    80‚Üí        now = time.time()\n    81‚Üí        recent_calls = [call for call in self.calls if now - call < self.time_window]\n    82‚Üí        return {\n    83‚Üí            \"current_calls\": len(recent_calls),\n    84‚Üí            \"max_calls\": self.max_calls,\n    85‚Üí            \"time_window\": self.time_window,\n    86‚Üí            \"utilization\": len(recent_calls) / self.max_calls\n    87‚Üí        }\n    88‚Üí\n    89‚Üí\n    90‚Üíclass AsyncConnectionPool:\n    91‚Üí    \"\"\"Connection pool for managing async resources.\"\"\"\n    92‚Üí    \n    93‚Üí    def __init__(self, create_connection: Callable, max_size: int = 10):\n    94‚Üí        self.create_connection = create_connection\n    95‚Üí        self.max_size = max_size\n    96‚Üí        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_size)\n    97‚Üí        self.created_count = 0\n    98‚Üí        self.active_connections = 0\n    99‚Üí        self._lock = asyncio.Lock()\n   100‚Üí    \n   101‚Üí    async def get_connection(self):\n   102‚Üí        \"\"\"Get a connection from the pool.\"\"\"\n   103‚Üí        try:\n   104‚Üí            # Try to get existing connection with timeout\n   105‚Üí            connection = await asyncio.wait_for(self.pool.get(), timeout=0.1)\n   106‚Üí            self.active_connections += 1\n   107‚Üí            return connection\n   108‚Üí        except asyncio.TimeoutError:\n   109‚Üí            # Create new connection if under limit\n   110‚Üí            async with self._lock:\n   111‚Üí                if self.created_count < self.max_size:\n   112‚Üí                    connection = await self.create_connection()\n   113‚Üí                    self.created_count += 1\n   114‚Üí                    self.active_connections += 1\n   115‚Üí                    return connection\n   116‚Üí            \n   117‚Üí            # Wait for available connection\n   118‚Üí            connection = await self.pool.get()\n   119‚Üí            self.active_connections += 1\n   120‚Üí            return connection\n   121‚Üí    \n   122‚Üí    async def return_connection(self, connection):\n   123‚Üí        \"\"\"Return connection to pool.\"\"\"\n   124‚Üí        self.active_connections -= 1\n   125‚Üí        await self.pool.put(connection)\n   126‚Üí    \n   127‚Üí    @asynccontextmanager\n   128‚Üí    async def connection(self):\n   129‚Üí        \"\"\"Context manager for automatic connection management.\"\"\"\n   130‚Üí        conn = await self.get_connection()\n   131‚Üí        try:\n   132‚Üí            yield conn\n   133‚Üí        finally:\n   134‚Üí            await self.return_connection(conn)\n   135‚Üí    \n   136‚Üí    def get_stats(self) -> Dict[str, Any]:\n   137‚Üí        \"\"\"Get pool statistics.\"\"\"\n   138‚Üí        return {\n   139‚Üí            \"pool_size\": self.pool.qsize(),\n   140‚Üí            \"created_count\": self.created_count,\n   141‚Üí            \"active_connections\": self.active_connections,\n   142‚Üí            \"max_size\": self.max_size\n   143‚Üí        }\n   144‚Üí\n   145‚Üí\n   146‚Üíclass StreamingAIProvider:\n   147‚Üí    \"\"\"AI provider that supports streaming responses.\"\"\"\n   148‚Üí    \n   149‚Üí    def __init__(self, response_delay: float = 0.1):\n   150‚Üí        self.response_delay = response_delay\n   151‚Üí        self.call_count = 0\n   152‚Üí    \n   153‚Üí    async def stream_completion(\n   154‚Üí        self, \n   155‚Üí        model_id: str, \n   156‚Üí        messages: List[Message]\n   157‚Üí    ) -> AsyncGenerator[StreamingChunk, None]:\n   158‚Üí        \"\"\"Stream chat completion chunks.\"\"\"\n   159‚Üí        self.call_count += 1\n   160‚Üí        \n   161‚Üí        # Simulate streaming response\n   162‚Üí        response_text = f\"This is a streaming response to: {messages[-1].content}\"\n   163‚Üí        words = response_text.split()\n   164‚Üí        \n   165‚Üí        for i, word in enumerate(words):\n   166‚Üí            await asyncio.sleep(self.response_delay)\n   167‚Üí            \n   168‚Üí            chunk = StreamingChunk(\n   169‚Üí                content=word + (\" \" if i < len(words) - 1 else \"\"),\n   170‚Üí                chunk_id=i,\n   171‚Üí                is_final=i == len(words) - 1,\n   172‚Üí                metadata={\n   173‚Üí                    \"model\": model_id,\n   174‚Üí                    \"total_chunks\": len(words),\n   175‚Üí                    \"call_id\": self.call_count\n   176‚Üí                }\n   177‚Üí            )\n   178‚Üí            \n   179‚Üí            yield chunk\n   180‚Üí\n   181‚Üí\n   182‚Üíclass ConcurrentInferenceEngine:\n   183‚Üí    \"\"\"Engine for managing concurrent AI inference requests.\"\"\"\n   184‚Üí    \n   185‚Üí    def __init__(self, max_concurrent: int = 5):\n   186‚Üí        self.max_concurrent = max_concurrent\n   187‚Üí        self.semaphore = asyncio.Semaphore(max_concurrent)\n   188‚Üí        self.active_tasks: Dict[str, asyncio.Task] = {}\n   189‚Üí        self.completed_tasks: List[Dict[str, Any]] = []\n   190‚Üí    \n   191‚Üí    async def submit_task(\n   192‚Üí        self, \n   193‚Üí        task_id: str, \n   194‚Üí        coro: Callable,\n   195‚Üí        *args, \n   196‚Üí        **kwargs\n   197‚Üí    ) -> str:\n   198‚Üí        \"\"\"Submit a task for concurrent execution.\"\"\"\n   199‚Üí        \n   200‚Üí        async def wrapped_task():\n   201‚Üí            async with self.semaphore:\n   202‚Üí                start_time = time.time()\n   203‚Üí                try:\n   204‚Üí                    result = await coro(*args, **kwargs)\n   205‚Üí                    end_time = time.time()\n   206‚Üí                    \n   207‚Üí                    self.completed_tasks.append({\n   208‚Üí                        \"task_id\": task_id,\n   209‚Üí                        \"result\": result,\n   210‚Üí                        \"duration\": end_time - start_time,\n   211‚Üí                        \"status\": \"success\",\n   212‚Üí                        \"error\": None\n   213‚Üí                    })\n   214‚Üí                    \n   215‚Üí                    return result\n   216‚Üí                    \n   217‚Üí                except Exception as e:\n   218‚Üí                    end_time = time.time()\n   219‚Üí                    \n   220‚Üí                    self.completed_tasks.append({\n   221‚Üí                        \"task_id\": task_id,\n   222‚Üí                        \"result\": None,\n   223‚Üí                        \"duration\": end_time - start_time,\n   224‚Üí                        \"status\": \"error\",\n   225‚Üí                        \"error\": str(e)\n   226‚Üí                    })\n   227‚Üí                    \n   228‚Üí                    raise\n   229‚Üí                finally:\n   230‚Üí                    # Remove from active tasks\n   231‚Üí                    if task_id in self.active_tasks:\n   232‚Üí                        del self.active_tasks[task_id]\n   233‚Üí        \n   234‚Üí        # Create and store task\n   235‚Üí        task = asyncio.create_task(wrapped_task())\n   236‚Üí        self.active_tasks[task_id] = task\n   237‚Üí        \n   238‚Üí        return task_id\n   239‚Üí    \n   240‚Üí    async def wait_for_task(self, task_id: str) -> Any:\n   241‚Üí        \"\"\"Wait for specific task to complete.\"\"\"\n   242‚Üí        if task_id in self.active_tasks:\n   243‚Üí            return await self.active_tasks[task_id]\n   244‚Üí        \n   245‚Üí        # Check completed tasks\n   246‚Üí        for completed in self.completed_tasks:\n   247‚Üí            if completed[\"task_id\"] == task_id:\n   248‚Üí                if completed[\"status\"] == \"error\":\n   249‚Üí                    raise Exception(completed[\"error\"])\n   250‚Üí                return completed[\"result\"]\n   251‚Üí        \n   252‚Üí        raise ValueError(f\"Task {task_id} not found\")\n   253‚Üí    \n   254‚Üí    async def wait_for_all(self) -> List[Any]:\n   255‚Üí        \"\"\"Wait for all active tasks to complete.\"\"\"\n   256‚Üí        if not self.active_tasks:\n   257‚Üí            return []\n   258‚Üí        \n   259‚Üí        results = await asyncio.gather(*self.active_tasks.values(), return_exceptions=True)\n   260‚Üí        return results\n   261‚Üí    \n   262‚Üí    def get_stats(self) -> Dict[str, Any]:\n   263‚Üí        \"\"\"Get engine statistics.\"\"\"\n   264‚Üí        return {\n   265‚Üí            \"active_tasks\": len(self.active_tasks),\n   266‚Üí            \"completed_tasks\": len(self.completed_tasks),\n   267‚Üí            \"max_concurrent\": self.max_concurrent,\n   268‚Üí            \"available_slots\": self.semaphore._value\n   269‚Üí        }\n   270‚Üí\n   271‚Üí\n   272‚Üíasync def async_batch_processor(\n   273‚Üí    items: List[Any], \n   274‚Üí    processor: Callable, \n   275‚Üí    batch_size: int = 10,\n   276‚Üí    max_concurrent: int = 3\n   277‚Üí) -> List[Any]:\n   278‚Üí    \"\"\"Process items in concurrent batches.\"\"\"\n   279‚Üí    \n   280‚Üí    async def process_batch(batch: List[Any]) -> List[Any]:\n   281‚Üí        \"\"\"Process a single batch.\"\"\"\n   282‚Üí        tasks = [processor(item) for item in batch]\n   283‚Üí        return await asyncio.gather(*tasks, return_exceptions=True)\n   284‚Üí    \n   285‚Üí    # Split into batches\n   286‚Üí    batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n   287‚Üí    \n   288‚Üí    # Process batches concurrently\n   289‚Üí    semaphore = asyncio.Semaphore(max_concurrent)\n   290‚Üí    \n   291‚Üí    async def process_with_semaphore(batch):\n   292‚Üí        async with semaphore:\n   293‚Üí            return await process_batch(batch)\n   294‚Üí    \n   295‚Üí    batch_results = await asyncio.gather(*[\n   296‚Üí        process_with_semaphore(batch) for batch in batches\n   297‚Üí    ])\n   298‚Üí    \n   299‚Üí    # Flatten results\n   300‚Üí    results = []\n   301‚Üí    for batch_result in batch_results:\n   302‚Üí        results.extend(batch_result)\n   303‚Üí    \n   304‚Üí    return results\n   305‚Üí\n   306‚Üí\n   307‚Üí@asynccontextmanager\n   308‚Üíasync def async_resource_manager(resource_factory: Callable, cleanup_func: Callable = None):\n   309‚Üí    \"\"\"Generic async context manager for resources.\"\"\"\n   310‚Üí    resource = None\n   311‚Üí    try:\n   312‚Üí        resource = await resource_factory()\n   313‚Üí        yield resource\n   314‚Üí    finally:\n   315‚Üí        if resource is not None and cleanup_func:\n   316‚Üí            await cleanup_func(resource)\n   317‚Üí\n   318‚Üí\n   319‚Üíasync def demonstrate_streaming():\n   320‚Üí    \"\"\"Show streaming patterns.\"\"\"\n   321‚Üí    \n   322‚Üí    console.print(Panel(\"üåä Streaming Patterns\", style=\"bold blue\"))\n   323‚Üí    \n   324‚Üí    provider = StreamingAIProvider(response_delay=0.2)\n   325‚Üí    \n   326‚Üí    console.print(\"[bold yellow]Streaming chat completion...[/bold yellow]\")\n   327‚Üí    \n   328‚Üí    messages = [Message(role=\"user\", content=\"Tell me about async programming\")]\n   329‚Üí    \n   330‚Üí    # Collect streaming chunks\n   331‚Üí    chunks = []\n   332‚Üí    full_response = \"\"\n   333‚Üí    \n   334‚Üí    async for chunk in provider.stream_completion(\"gpt-4\", messages):\n   335‚Üí        chunks.append(chunk)\n   336‚Üí        full_response += chunk.content\n   337‚Üí        \n   338‚Üí        # Show real-time progress\n   339‚Üí        console.print(f\"Chunk {chunk.chunk_id}: '{chunk.content.strip()}'\", end=\"\")\n   340‚Üí        if chunk.is_final:\n   341‚Üí            console.print(\" [FINAL]\")\n   342‚Üí        else:\n   343‚Üí            console.print()\n   344‚Üí    \n   345‚Üí    console.print(f\"\\n[bold cyan]Streaming Summary:[/bold cyan]\")\n   346‚Üí    console.print(f\"Total chunks: {len(chunks)}\")\n   347‚Üí    console.print(f\"Full response: {full_response}\")\n   348‚Üí    console.print(f\"Final chunk metadata: {chunks[-1].metadata}\")\n   349‚Üí\n   350‚Üí\n   351‚Üíasync def demonstrate_rate_limiting():\n   352‚Üí    \"\"\"Show rate limiting patterns.\"\"\"\n   353‚Üí    \n   354‚Üí    console.print(Panel(\"üö¶ Rate Limiting\", style=\"bold green\"))\n   355‚Üí    \n   356‚Üí    # Create rate limiter (5 calls per 2 seconds)\n   357‚Üí    rate_limiter = AsyncRateLimiter(max_calls=5, time_window=2.0)\n   358‚Üí    \n   359‚Üí    async def make_api_call(call_id: int) -> str:\n   360‚Üí        \"\"\"Simulate API call with rate limiting.\"\"\"\n   361‚Üí        await rate_limiter.acquire()\n   362‚Üí        # Simulate API processing time\n   363‚Üí        await asyncio.sleep(0.1)\n   364‚Üí        return f\"API call {call_id} completed\"\n   365‚Üí    \n   366‚Üí    console.print(\"[bold yellow]Making 10 rapid API calls with rate limiting...[/bold yellow]\")\n   367‚Üí    \n   368‚Üí    start_time = time.time()\n   369‚Üí    \n   370‚Üí    # Create tasks for 10 rapid calls\n   371‚Üí    tasks = [make_api_call(i) for i in range(10)]\n   372‚Üí    \n   373‚Üí    # Execute with progress tracking\n   374‚Üí    results = []\n   375‚Üí    for i, task in enumerate(asyncio.as_completed(tasks)):\n   376‚Üí        result = await task\n   377‚Üí        elapsed = time.time() - start_time\n   378‚Üí        console.print(f\"  {elapsed:.2f}s: {result}\")\n   379‚Üí        results.append(result)\n   380‚Üí    \n   381‚Üí    total_time = time.time() - start_time\n   382‚Üí    console.print(f\"\\n[bold cyan]Rate Limiting Summary:[/bold cyan]\")\n   383‚Üí    console.print(f\"Total time: {total_time:.2f}s\")\n   384‚Üí    console.print(f\"Rate limiter stats: {rate_limiter.get_stats()}\")\n   385‚Üí\n   386‚Üí\n   387‚Üíasync def demonstrate_connection_pooling():\n   388‚Üí    \"\"\"Show connection pooling patterns.\"\"\"\n   389‚Üí    \n   390‚Üí    console.print(Panel(\"üèä Connection Pooling\", style=\"bold magenta\"))\n   391‚Üí    \n   392‚Üí    async def create_mock_connection():\n   393‚Üí        \"\"\"Create a mock database connection.\"\"\"\n   394‚Üí        await asyncio.sleep(0.1)  # Simulate connection setup\n   395‚Üí        return {\"id\": id(object()), \"created_at\": time.time()}\n   396‚Üí    \n   397‚Üí    # Create connection pool\n   398‚Üí    pool = AsyncConnectionPool(create_mock_connection, max_size=3)\n   399‚Üí    \n   400‚Üí    async def database_operation(operation_id: int):\n   401‚Üí        \"\"\"Simulate database operation using connection pool.\"\"\"\n   402‚Üí        async with pool.connection() as conn:\n   403‚Üí            # Simulate work\n   404‚Üí            await asyncio.sleep(0.2)\n   405‚Üí            return f\"Operation {operation_id} completed with connection {conn['id']}\"\n   406‚Üí    \n   407‚Üí    console.print(\"[bold yellow]Running concurrent database operations...[/bold yellow]\")\n   408‚Üí    \n   409‚Üí    # Run 8 concurrent operations with pool of 3 connections\n   410‚Üí    start_time = time.time()\n   411‚Üí    \n   412‚Üí    tasks = [database_operation(i) for i in range(8)]\n   413‚Üí    results = []\n   414‚Üí    \n   415‚Üí    for task in asyncio.as_completed(tasks):\n   416‚Üí        result = await task\n   417‚Üí        elapsed = time.time() - start_time\n   418‚Üí        stats = pool.get_stats()\n   419‚Üí        console.print(f\"  {elapsed:.2f}s: {result}\")\n   420‚Üí        console.print(f\"    Pool stats: {stats}\")\n   421‚Üí    \n   422‚Üí    final_stats = pool.get_stats()\n   423‚Üí    console.print(f\"\\n[bold cyan]Final Pool Stats:[/bold cyan]\")\n   424‚Üí    console.print(f\"Created connections: {final_stats['created_count']}\")\n   425‚Üí    console.print(f\"Active connections: {final_stats['active_connections']}\")\n   426‚Üí\n   427‚Üí\n   428‚Üíasync def demonstrate_concurrent_processing():\n   429‚Üí    \"\"\"Show concurrent processing patterns.\"\"\"\n   430‚Üí    \n   431‚Üí    console.print(Panel(\"‚ö° Concurrent Processing\", style=\"bold cyan\"))\n   432‚Üí    \n   433‚Üí    # Create inference engine\n   434‚Üí    engine = ConcurrentInferenceEngine(max_concurrent=3)\n   435‚Üí    \n   436‚Üí    async def mock_inference(text: str, delay: float = 0.5) -> str:\n   437‚Üí        \"\"\"Mock AI inference with variable delay.\"\"\"\n   438‚Üí        await asyncio.sleep(delay)\n   439‚Üí        return f\"Processed: {text}\"\n   440‚Üí    \n   441‚Üí    console.print(\"[bold yellow]Submitting concurrent inference tasks...[/bold yellow]\")\n   442‚Üí    \n   443‚Üí    # Submit multiple tasks\n   444‚Üí    tasks_data = [\n   445‚Üí        (\"task_1\", \"Analyze sentiment\", 0.3),\n   446‚Üí        (\"task_2\", \"Summarize document\", 0.8),\n   447‚Üí        (\"task_3\", \"Extract entities\", 0.5),\n   448‚Üí        (\"task_4\", \"Classify text\", 0.4),\n   449‚Üí        (\"task_5\", \"Generate response\", 0.6)\n   450‚Üí    ]\n   451‚Üí    \n   452‚Üí    submitted_tasks = []\n   453‚Üí    for task_id, text, delay in tasks_data:\n   454‚Üí        await engine.submit_task(task_id, mock_inference, text, delay)\n   455‚Üí        submitted_tasks.append(task_id)\n   456‚Üí        console.print(f\"  Submitted: {task_id}\")\n   457‚Üí    \n   458‚Üí    # Wait for all tasks and show progress\n   459‚Üí    console.print(f\"\\n[bold yellow]Waiting for tasks to complete...[/bold yellow]\")\n   460‚Üí    \n   461‚Üí    while engine.active_tasks:\n   462‚Üí        stats = engine.get_stats()\n   463‚Üí        console.print(f\"  Active: {stats['active_tasks']}, Completed: {stats['completed_tasks']}\")\n   464‚Üí        await asyncio.sleep(0.2)\n   465‚Üí    \n   466‚Üí    # Show final results\n   467‚Üí    console.print(f\"\\n[bold cyan]Task Results:[/bold cyan]\")\n   468‚Üí    for completed in engine.completed_tasks:\n   469‚Üí        status_icon = \"‚úÖ\" if completed[\"status\"] == \"success\" else \"‚ùå\"\n   470‚Üí        console.print(f\"  {status_icon} {completed['task_id']}: {completed['duration']:.2f}s\")\n   471‚Üí\n   472‚Üí\n   473‚Üíasync def demonstrate_batch_processing():\n   474‚Üí    \"\"\"Show batch processing patterns.\"\"\"\n   475‚Üí    \n   476‚Üí    console.print(Panel(\"üì¶ Batch Processing\", style=\"bold red\"))\n   477‚Üí    \n   478‚Üí    async def process_text(text: str) -> str:\n   479‚Üí        \"\"\"Process a single text item.\"\"\"\n   480‚Üí        await asyncio.sleep(0.1)  # Simulate processing\n   481‚Üí        return f\"PROCESSED: {text.upper()}\"\n   482‚Üí    \n   483‚Üí    # Create test data\n   484‚Üí    texts = [f\"text item {i}\" for i in range(25)]\n   485‚Üí    \n   486‚Üí    console.print(f\"[bold yellow]Processing {len(texts)} items in batches...[/bold yellow]\")\n   487‚Üí    \n   488‚Üí    start_time = time.time()\n   489‚Üí    \n   490‚Üí    # Process in batches with concurrency\n   491‚Üí    results = await async_batch_processor(\n   492‚Üí        items=texts,\n   493‚Üí        processor=process_text,\n   494‚Üí        batch_size=5,\n   495‚Üí        max_concurrent=3\n   496‚Üí    )\n   497‚Üí    \n   498‚Üí    processing_time = time.time() - start_time\n   499‚Üí    \n   500‚Üí    console.print(f\"\\n[bold cyan]Batch Processing Results:[/bold cyan]\")\n   501‚Üí    console.print(f\"Total items: {len(texts)}\")\n   502‚Üí    console.print(f\"Processing time: {processing_time:.2f}s\")\n   503‚Üí    console.print(f\"Items per second: {len(texts) / processing_time:.1f}\")\n   504‚Üí    console.print(f\"Sample results: {results[:3]}...\")\n   505‚Üí    \n   506‚Üí    # Count errors\n   507‚Üí    errors = [r for r in results if isinstance(r, Exception)]\n   508‚Üí    console.print(f\"Errors: {len(errors)}\")\n   509‚Üí\n   510‚Üí\n   511‚Üíasync def demonstrate_async_context_managers():\n   512‚Üí    \"\"\"Show async context manager patterns.\"\"\"\n   513‚Üí    \n   514‚Üí    console.print(Panel(\"üîí Async Context Managers\", style=\"bold yellow\"))\n   515‚Üí    \n   516‚Üí    async def create_ai_session():\n   517‚Üí        \"\"\"Create an AI session with setup/teardown.\"\"\"\n   518‚Üí        console.print(\"  üöÄ Creating AI session...\")\n   519‚Üí        await asyncio.sleep(0.1)\n   520‚Üí        session = {\n   521‚Üí            \"id\": \"session_\" + str(int(time.time())),\n   522‚Üí            \"created_at\": time.time(),\n   523‚Üí            \"request_count\": 0\n   524‚Üí        }\n   525‚Üí        return session\n   526‚Üí    \n   527‚Üí    async def cleanup_session(session):\n   528‚Üí        \"\"\"Clean up AI session.\"\"\"\n   529‚Üí        console.print(f\"  üßπ Cleaning up session {session['id']} ({session['request_count']} requests)\")\n   530‚Üí        await asyncio.sleep(0.05)\n   531‚Üí    \n   532‚Üí    console.print(\"[bold yellow]Using async context manager for AI sessions...[/bold yellow]\")\n   533‚Üí    \n   534‚Üí    # Use the async context manager\n   535‚Üí    async with async_resource_manager(create_ai_session, cleanup_session) as session:\n   536‚Üí        console.print(f\"  üìù Session active: {session['id']}\")\n   537‚Üí        \n   538‚Üí        # Simulate some work\n   539‚Üí        for i in range(3):\n   540‚Üí            await asyncio.sleep(0.1)\n   541‚Üí            session[\"request_count\"] += 1\n   542‚Üí            console.print(f\"    Request {session['request_count']} processed\")\n   543‚Üí    \n   544‚Üí    console.print(\"  ‚úÖ Session context manager completed\")\n   545‚Üí\n   546‚Üí\n   547‚Üíasync def demonstrate_async_generators():\n   548‚Üí    \"\"\"Show async generator patterns.\"\"\"\n   549‚Üí    \n   550‚Üí    console.print(Panel(\"üîÑ Async Generators\", style=\"bold purple\"))\n   551‚Üí    \n   552‚Üí    async def generate_predictions(texts: List[str]) -> AsyncGenerator[Dict[str, Any], None]:\n   553‚Üí        \"\"\"Generate predictions one by one.\"\"\"\n   554‚Üí        for i, text in enumerate(texts):\n   555‚Üí            # Simulate prediction time\n   556‚Üí            await asyncio.sleep(0.2)\n   557‚Üí            \n   558‚Üí            yield {\n   559‚Üí                \"text\": text,\n   560‚Üí                \"prediction\": f\"prediction_{i}\",\n   561‚Üí                \"confidence\": 0.9 - (i * 0.1),\n   562‚Üí                \"processing_time\": 0.2,\n   563‚Üí                \"batch_id\": i // 3  # Group into batches of 3\n   564‚Üí            }\n   565‚Üí    \n   566‚Üí    console.print(\"[bold yellow]Streaming predictions with async generator...[/bold yellow]\")\n   567‚Üí    \n   568‚Üí    test_texts = [f\"input text {i}\" for i in range(7)]\n   569‚Üí    \n   570‚Üí    predictions_by_batch = {}\n   571‚Üí    \n   572‚Üí    async for prediction in generate_predictions(test_texts):\n   573‚Üí        batch_id = prediction[\"batch_id\"]\n   574‚Üí        if batch_id not in predictions_by_batch:\n   575‚Üí            predictions_by_batch[batch_id] = []\n   576‚Üí        \n   577‚Üí        predictions_by_batch[batch_id].append(prediction)\n   578‚Üí        \n   579‚Üí        console.print(f\"  üìä Prediction {len(predictions_by_batch[batch_id])} for batch {batch_id}: \"\n   580‚Üí                     f\"{prediction['prediction']} (confidence: {prediction['confidence']:.2f})\")\n   581‚Üí    \n   582‚Üí    console.print(f\"\\n[bold cyan]Generated {sum(len(preds) for preds in predictions_by_batch.values())} predictions in {len(predictions_by_batch)} batches[/bold cyan]\")\n   583‚Üí\n   584‚Üí\n   585‚Üídef interactive_exercise():\n   586‚Üí    \"\"\"Interactive exercise for async patterns.\"\"\"\n   587‚Üí    \n   588‚Üí    console.print(Panel(\"üéÆ Interactive Exercise: Design Your Async Architecture\", style=\"bold magenta\"))\n   589‚Üí    \n   590‚Üí    scenarios = [\n   591‚Üí        {\n   592‚Üí            \"system\": \"Real-time AI Chat Service\",\n   593‚Üí            \"requirements\": [\n   594‚Üí                \"Handle 1000+ concurrent users\",\n   595‚Üí                \"Stream responses in real-time\",\n   596‚Üí                \"Rate limit per user and globally\",\n   597‚Üí                \"Graceful handling of AI model failures\"\n   598‚Üí            ],\n   599‚Üí            \"challenge\": \"Design the async architecture\"\n   600‚Üí        },\n   601‚Üí        {\n   602‚Üí            \"system\": \"Batch Document Processing\",\n   603‚Üí            \"requirements\": [\n   604‚Üí                \"Process thousands of documents daily\",\n   605‚Üí                \"Multiple AI models per document\",\n   606‚Üí                \"Prioritize urgent documents\",\n   607‚Üí                \"Monitor progress and handle failures\"\n   608‚Üí            ],\n   609‚Üí            \"challenge\": \"Design concurrent processing strategy\"\n   610‚Üí        },\n   611‚Üí        {\n   612‚Üí            \"system\": \"Multi-Model AI Inference API\",\n   613‚Üí            \"requirements\": [\n   614‚Üí                \"Support multiple AI providers\",\n   615‚Üí                \"Connection pooling for efficiency\",\n   616‚Üí                \"Fallback between providers\",\n   617‚Üí                \"Streaming and batch modes\"\n   618‚Üí            ],\n   619‚Üí            \"challenge\": \"Design resource management patterns\"\n   620‚Üí        }\n   621‚Üí    ]\n   622‚Üí    \n   623‚Üí    for i, scenario in enumerate(scenarios, 1):\n   624‚Üí        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['system']}[/bold cyan]\")\n   625‚Üí        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n   626‚Üí        for req in scenario[\"requirements\"]:\n   627‚Üí            console.print(f\"  ‚Ä¢ {req}\")\n   628‚Üí        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n   629‚Üí        console.print(\"\\n[bold green]Consider These Patterns:[/bold green]\")\n   630‚Üí        console.print(\"  ‚Ä¢ Streaming with AsyncGenerators\")\n   631‚Üí        console.print(\"  ‚Ä¢ Rate limiting with semaphores\")\n   632‚Üí        console.print(\"  ‚Ä¢ Connection pooling for resources\")\n   633‚Üí        console.print(\"  ‚Ä¢ Concurrent processing with task management\")\n   634‚Üí        console.print(\"  ‚Ä¢ Async context managers for cleanup\")\n   635‚Üí        console.print(\"  ‚Ä¢ Batch processing for efficiency\")\n   636‚Üí        console.print(\"---\")\n   637‚Üí\n   638‚Üí\n   639‚Üíif __name__ == \"__main__\":\n   640‚Üí    console.print(\"[bold blue]üöÄ AI Architecture Patterns: Async Patterns[/bold blue]\")\n   641‚Üí    console.print(\"Learn to build high-performance async AI systems\\n\")\n   642‚Üí    \n   643‚Üí    async def main():\n   644‚Üí        await demonstrate_streaming()\n   645‚Üí        console.print(\"\\n\" + \"=\"*60)\n   646‚Üí        \n   647‚Üí        await demonstrate_rate_limiting()\n   648‚Üí        console.print(\"\\n\" + \"=\"*60)\n   649‚Üí        \n   650‚Üí        await demonstrate_connection_pooling()\n   651‚Üí        console.print(\"\\n\" + \"=\"*60)\n   652‚Üí        \n   653‚Üí        await demonstrate_concurrent_processing()\n   654‚Üí        console.print(\"\\n\" + \"=\"*60)\n   655‚Üí        \n   656‚Üí        await demonstrate_batch_processing()\n   657‚Üí        console.print(\"\\n\" + \"=\"*60)\n   658‚Üí        \n   659‚Üí        await demonstrate_async_context_managers()\n   660‚Üí        console.print(\"\\n\" + \"=\"*60)\n   661‚Üí        \n   662‚Üí        await demonstrate_async_generators()\n   663‚Üí        console.print(\"\\n\" + \"=\"*60)\n   664‚Üí        \n   665‚Üí        interactive_exercise()\n   666‚Üí        \n   667‚Üí        console.print(f\"\\n[bold green]‚úÖ Completed![/bold green] Next: Run example 03_functional_programming.py\")\n   668‚Üí        console.print(\"üí° Pro tip: Async patterns are crucial for scalable AI systems!\")\n   669‚Üí    \n   670‚Üí    asyncio.run(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"66cecd32-01da-4ae6-bc89-f6bbc4c8f1d9","timestamp":"2025-08-01T17:59:45.290Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/02_async_patterns.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 2: Async Patterns for AI Systems\n\nLearn advanced asyncio patterns for building high-performance\nAI systems with concurrent processing, streaming, and resource management.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/02_async_patterns.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nimport asyncio\nfrom typing import AsyncGenerator, AsyncIterator, Callable, Any, Optional, List, Dict\nfrom dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nimport time\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.live import Live\nfrom rich.progress import Progress, TaskID\n\nfrom common import Message, ChatResponse, Choice\n\nconsole = Console()\n\n\n# üéØ LEARNING GOAL: Master async patterns for high-performance AI systems\n\n\n@dataclass\nclass StreamingChunk:\n    \"\"\"Represents a chunk of streaming data.\"\"\"\n    content: str\n    chunk_id: int\n    is_final: bool\n    metadata: Dict[str, Any] = None\n    \n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n\n\nclass AsyncRateLimiter:\n    \"\"\"Async rate limiter for API calls.\"\"\"\n    \n    def __init__(self, max_calls: int, time_window: float):\n        self.max_calls = max_calls\n        self.time_window = time_window\n        self.calls: List[float] = []\n        self._lock = asyncio.Lock()\n    \n    async def acquire(self) -> None:\n        \"\"\"Acquire permission to make a call.\"\"\"\n        async with self._lock:\n            now = time.time()\n            \n            # Remove old calls outside the window\n            self.calls = [call_time for call_time in self.calls \n                         if now - call_time < self.time_window]\n            \n            # If we're at the limit, wait\n            if len(self.calls) >= self.max_calls:\n                oldest_call = min(self.calls)\n                wait_time = self.time_window - (now - oldest_call)\n                if wait_time > 0:\n                    await asyncio.sleep(wait_time)\n                    # Recursive call to try again\n                    return await self.acquire()\n            \n            # Record this call\n            self.calls.append(now)\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get rate limiter statistics.\"\"\"\n        now = time.time()\n        recent_calls = [call for call in self.calls if now - call < self.time_window]\n        return {\n            \"current_calls\": len(recent_calls),\n            \"max_calls\": self.max_calls,\n            \"time_window\": self.time_window,\n            \"utilization\": len(recent_calls) / self.max_calls\n        }\n\n\nclass AsyncConnectionPool:\n    \"\"\"Connection pool for managing async resources.\"\"\"\n    \n    def __init__(self, create_connection: Callable, max_size: int = 10):\n        self.create_connection = create_connection\n        self.max_size = max_size\n        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_size)\n        self.created_count = 0\n        self.active_connections = 0\n        self._lock = asyncio.Lock()\n    \n    async def get_connection(self):\n        \"\"\"Get a connection from the pool.\"\"\"\n        try:\n            # Try to get existing connection with timeout\n            connection = await asyncio.wait_for(self.pool.get(), timeout=0.1)\n            self.active_connections += 1\n            return connection\n        except asyncio.TimeoutError:\n            # Create new connection if under limit\n            async with self._lock:\n                if self.created_count < self.max_size:\n                    connection = await self.create_connection()\n                    self.created_count += 1\n                    self.active_connections += 1\n                    return connection\n            \n            # Wait for available connection\n            connection = await self.pool.get()\n            self.active_connections += 1\n            return connection\n    \n    async def return_connection(self, connection):\n        \"\"\"Return connection to pool.\"\"\"\n        self.active_connections -= 1\n        await self.pool.put(connection)\n    \n    @asynccontextmanager\n    async def connection(self):\n        \"\"\"Context manager for automatic connection management.\"\"\"\n        conn = await self.get_connection()\n        try:\n            yield conn\n        finally:\n            await self.return_connection(conn)\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get pool statistics.\"\"\"\n        return {\n            \"pool_size\": self.pool.qsize(),\n            \"created_count\": self.created_count,\n            \"active_connections\": self.active_connections,\n            \"max_size\": self.max_size\n        }\n\n\nclass StreamingAIProvider:\n    \"\"\"AI provider that supports streaming responses.\"\"\"\n    \n    def __init__(self, response_delay: float = 0.1):\n        self.response_delay = response_delay\n        self.call_count = 0\n    \n    async def stream_completion(\n        self, \n        model_id: str, \n        messages: List[Message]\n    ) -> AsyncGenerator[StreamingChunk, None]:\n        \"\"\"Stream chat completion chunks.\"\"\"\n        self.call_count += 1\n        \n        # Simulate streaming response\n        response_text = f\"This is a streaming response to: {messages[-1].content}\"\n        words = response_text.split()\n        \n        for i, word in enumerate(words):\n            await asyncio.sleep(self.response_delay)\n            \n            chunk = StreamingChunk(\n                content=word + (\" \" if i < len(words) - 1 else \"\"),\n                chunk_id=i,\n                is_final=i == len(words) - 1,\n                metadata={\n                    \"model\": model_id,\n                    \"total_chunks\": len(words),\n                    \"call_id\": self.call_count\n                }\n            )\n            \n            yield chunk\n\n\nclass ConcurrentInferenceEngine:\n    \"\"\"Engine for managing concurrent AI inference requests.\"\"\"\n    \n    def __init__(self, max_concurrent: int = 5):\n        self.max_concurrent = max_concurrent\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n        self.active_tasks: Dict[str, asyncio.Task] = {}\n        self.completed_tasks: List[Dict[str, Any]] = []\n    \n    async def submit_task(\n        self, \n        task_id: str, \n        coro: Callable,\n        *args, \n        **kwargs\n    ) -> str:\n        \"\"\"Submit a task for concurrent execution.\"\"\"\n        \n        async def wrapped_task():\n            async with self.semaphore:\n                start_time = time.time()\n                try:\n                    result = await coro(*args, **kwargs)\n                    end_time = time.time()\n                    \n                    self.completed_tasks.append({\n                        \"task_id\": task_id,\n                        \"result\": result,\n                        \"duration\": end_time - start_time,\n                        \"status\": \"success\",\n                        \"error\": None\n                    })\n                    \n                    return result\n                    \n                except Exception as e:\n                    end_time = time.time()\n                    \n                    self.completed_tasks.append({\n                        \"task_id\": task_id,\n                        \"result\": None,\n                        \"duration\": end_time - start_time,\n                        \"status\": \"error\",\n                        \"error\": str(e)\n                    })\n                    \n                    raise\n                finally:\n                    # Remove from active tasks\n                    if task_id in self.active_tasks:\n                        del self.active_tasks[task_id]\n        \n        # Create and store task\n        task = asyncio.create_task(wrapped_task())\n        self.active_tasks[task_id] = task\n        \n        return task_id\n    \n    async def wait_for_task(self, task_id: str) -> Any:\n        \"\"\"Wait for specific task to complete.\"\"\"\n        if task_id in self.active_tasks:\n            return await self.active_tasks[task_id]\n        \n        # Check completed tasks\n        for completed in self.completed_tasks:\n            if completed[\"task_id\"] == task_id:\n                if completed[\"status\"] == \"error\":\n                    raise Exception(completed[\"error\"])\n                return completed[\"result\"]\n        \n        raise ValueError(f\"Task {task_id} not found\")\n    \n    async def wait_for_all(self) -> List[Any]:\n        \"\"\"Wait for all active tasks to complete.\"\"\"\n        if not self.active_tasks:\n            return []\n        \n        results = await asyncio.gather(*self.active_tasks.values(), return_exceptions=True)\n        return results\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get engine statistics.\"\"\"\n        return {\n            \"active_tasks\": len(self.active_tasks),\n            \"completed_tasks\": len(self.completed_tasks),\n            \"max_concurrent\": self.max_concurrent,\n            \"available_slots\": self.semaphore._value\n        }\n\n\nasync def async_batch_processor(\n    items: List[Any], \n    processor: Callable, \n    batch_size: int = 10,\n    max_concurrent: int = 3\n) -> List[Any]:\n    \"\"\"Process items in concurrent batches.\"\"\"\n    \n    async def process_batch(batch: List[Any]) -> List[Any]:\n        \"\"\"Process a single batch.\"\"\"\n        tasks = [processor(item) for item in batch]\n        return await asyncio.gather(*tasks, return_exceptions=True)\n    \n    # Split into batches\n    batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n    \n    # Process batches concurrently\n    semaphore = asyncio.Semaphore(max_concurrent)\n    \n    async def process_with_semaphore(batch):\n        async with semaphore:\n            return await process_batch(batch)\n    \n    batch_results = await asyncio.gather(*[\n        process_with_semaphore(batch) for batch in batches\n    ])\n    \n    # Flatten results\n    results = []\n    for batch_result in batch_results:\n        results.extend(batch_result)\n    \n    return results\n\n\n@asynccontextmanager\nasync def async_resource_manager(resource_factory: Callable, cleanup_func: Callable = None):\n    \"\"\"Generic async context manager for resources.\"\"\"\n    resource = None\n    try:\n        resource = await resource_factory()\n        yield resource\n    finally:\n        if resource is not None and cleanup_func:\n            await cleanup_func(resource)\n\n\nasync def demonstrate_streaming():\n    \"\"\"Show streaming patterns.\"\"\"\n    \n    console.print(Panel(\"üåä Streaming Patterns\", style=\"bold blue\"))\n    \n    provider = StreamingAIProvider(response_delay=0.2)\n    \n    console.print(\"[bold yellow]Streaming chat completion...[/bold yellow]\")\n    \n    messages = [Message(role=\"user\", content=\"Tell me about async programming\")]\n    \n    # Collect streaming chunks\n    chunks = []\n    full_response = \"\"\n    \n    async for chunk in provider.stream_completion(\"gpt-4\", messages):\n        chunks.append(chunk)\n        full_response += chunk.content\n        \n        # Show real-time progress\n        console.print(f\"Chunk {chunk.chunk_id}: '{chunk.content.strip()}'\", end=\"\")\n        if chunk.is_final:\n            console.print(\" [FINAL]\")\n        else:\n            console.print()\n    \n    console.print(f\"\\n[bold cyan]Streaming Summary:[/bold cyan]\")\n    console.print(f\"Total chunks: {len(chunks)}\")\n    console.print(f\"Full response: {full_response}\")\n    console.print(f\"Final chunk metadata: {chunks[-1].metadata}\")\n\n\nasync def demonstrate_rate_limiting():\n    \"\"\"Show rate limiting patterns.\"\"\"\n    \n    console.print(Panel(\"üö¶ Rate Limiting\", style=\"bold green\"))\n    \n    # Create rate limiter (5 calls per 2 seconds)\n    rate_limiter = AsyncRateLimiter(max_calls=5, time_window=2.0)\n    \n    async def make_api_call(call_id: int) -> str:\n        \"\"\"Simulate API call with rate limiting.\"\"\"\n        await rate_limiter.acquire()\n        # Simulate API processing time\n        await asyncio.sleep(0.1)\n        return f\"API call {call_id} completed\"\n    \n    console.print(\"[bold yellow]Making 10 rapid API calls with rate limiting...[/bold yellow]\")\n    \n    start_time = time.time()\n    \n    # Create tasks for 10 rapid calls\n    tasks = [make_api_call(i) for i in range(10)]\n    \n    # Execute with progress tracking\n    results = []\n    for i, task in enumerate(asyncio.as_completed(tasks)):\n        result = await task\n        elapsed = time.time() - start_time\n        console.print(f\"  {elapsed:.2f}s: {result}\")\n        results.append(result)\n    \n    total_time = time.time() - start_time\n    console.print(f\"\\n[bold cyan]Rate Limiting Summary:[/bold cyan]\")\n    console.print(f\"Total time: {total_time:.2f}s\")\n    console.print(f\"Rate limiter stats: {rate_limiter.get_stats()}\")\n\n\nasync def demonstrate_connection_pooling():\n    \"\"\"Show connection pooling patterns.\"\"\"\n    \n    console.print(Panel(\"üèä Connection Pooling\", style=\"bold magenta\"))\n    \n    async def create_mock_connection():\n        \"\"\"Create a mock database connection.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate connection setup\n        return {\"id\": id(object()), \"created_at\": time.time()}\n    \n    # Create connection pool\n    pool = AsyncConnectionPool(create_mock_connection, max_size=3)\n    \n    async def database_operation(operation_id: int):\n        \"\"\"Simulate database operation using connection pool.\"\"\"\n        async with pool.connection() as conn:\n            # Simulate work\n            await asyncio.sleep(0.2)\n            return f\"Operation {operation_id} completed with connection {conn['id']}\"\n    \n    console.print(\"[bold yellow]Running concurrent database operations...[/bold yellow]\")\n    \n    # Run 8 concurrent operations with pool of 3 connections\n    start_time = time.time()\n    \n    tasks = [database_operation(i) for i in range(8)]\n    results = []\n    \n    for task in asyncio.as_completed(tasks):\n        result = await task\n        elapsed = time.time() - start_time\n        stats = pool.get_stats()\n        console.print(f\"  {elapsed:.2f}s: {result}\")\n        console.print(f\"    Pool stats: {stats}\")\n    \n    final_stats = pool.get_stats()\n    console.print(f\"\\n[bold cyan]Final Pool Stats:[/bold cyan]\")\n    console.print(f\"Created connections: {final_stats['created_count']}\")\n    console.print(f\"Active connections: {final_stats['active_connections']}\")\n\n\nasync def demonstrate_concurrent_processing():\n    \"\"\"Show concurrent processing patterns.\"\"\"\n    \n    console.print(Panel(\"‚ö° Concurrent Processing\", style=\"bold cyan\"))\n    \n    # Create inference engine\n    engine = ConcurrentInferenceEngine(max_concurrent=3)\n    \n    async def mock_inference(text: str, delay: float = 0.5) -> str:\n        \"\"\"Mock AI inference with variable delay.\"\"\"\n        await asyncio.sleep(delay)\n        return f\"Processed: {text}\"\n    \n    console.print(\"[bold yellow]Submitting concurrent inference tasks...[/bold yellow]\")\n    \n    # Submit multiple tasks\n    tasks_data = [\n        (\"task_1\", \"Analyze sentiment\", 0.3),\n        (\"task_2\", \"Summarize document\", 0.8),\n        (\"task_3\", \"Extract entities\", 0.5),\n        (\"task_4\", \"Classify text\", 0.4),\n        (\"task_5\", \"Generate response\", 0.6)\n    ]\n    \n    submitted_tasks = []\n    for task_id, text, delay in tasks_data:\n        await engine.submit_task(task_id, mock_inference, text, delay)\n        submitted_tasks.append(task_id)\n        console.print(f\"  Submitted: {task_id}\")\n    \n    # Wait for all tasks and show progress\n    console.print(f\"\\n[bold yellow]Waiting for tasks to complete...[/bold yellow]\")\n    \n    while engine.active_tasks:\n        stats = engine.get_stats()\n        console.print(f\"  Active: {stats['active_tasks']}, Completed: {stats['completed_tasks']}\")\n        await asyncio.sleep(0.2)\n    \n    # Show final results\n    console.print(f\"\\n[bold cyan]Task Results:[/bold cyan]\")\n    for completed in engine.completed_tasks:\n        status_icon = \"‚úÖ\" if completed[\"status\"] == \"success\" else \"‚ùå\"\n        console.print(f\"  {status_icon} {completed['task_id']}: {completed['duration']:.2f}s\")\n\n\nasync def demonstrate_batch_processing():\n    \"\"\"Show batch processing patterns.\"\"\"\n    \n    console.print(Panel(\"üì¶ Batch Processing\", style=\"bold red\"))\n    \n    async def process_text(text: str) -> str:\n        \"\"\"Process a single text item.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate processing\n        return f\"PROCESSED: {text.upper()}\"\n    \n    # Create test data\n    texts = [f\"text item {i}\" for i in range(25)]\n    \n    console.print(f\"[bold yellow]Processing {len(texts)} items in batches...[/bold yellow]\")\n    \n    start_time = time.time()\n    \n    # Process in batches with concurrency\n    results = await async_batch_processor(\n        items=texts,\n        processor=process_text,\n        batch_size=5,\n        max_concurrent=3\n    )\n    \n    processing_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold cyan]Batch Processing Results:[/bold cyan]\")\n    console.print(f\"Total items: {len(texts)}\")\n    console.print(f\"Processing time: {processing_time:.2f}s\")\n    console.print(f\"Items per second: {len(texts) / processing_time:.1f}\")\n    console.print(f\"Sample results: {results[:3]}...\")\n    \n    # Count errors\n    errors = [r for r in results if isinstance(r, Exception)]\n    console.print(f\"Errors: {len(errors)}\")\n\n\nasync def demonstrate_async_context_managers():\n    \"\"\"Show async context manager patterns.\"\"\"\n    \n    console.print(Panel(\"üîí Async Context Managers\", style=\"bold yellow\"))\n    \n    async def create_ai_session():\n        \"\"\"Create an AI session with setup/teardown.\"\"\"\n        console.print(\"  üöÄ Creating AI session...\")\n        await asyncio.sleep(0.1)\n        session = {\n            \"id\": \"session_\" + str(int(time.time())),\n            \"created_at\": time.time(),\n            \"request_count\": 0\n        }\n        return session\n    \n    async def cleanup_session(session):\n        \"\"\"Clean up AI session.\"\"\"\n        console.print(f\"  üßπ Cleaning up session {session['id']} ({session['request_count']} requests)\")\n        await asyncio.sleep(0.05)\n    \n    console.print(\"[bold yellow]Using async context manager for AI sessions...[/bold yellow]\")\n    \n    # Use the async context manager\n    async with async_resource_manager(create_ai_session, cleanup_session) as session:\n        console.print(f\"  üìù Session active: {session['id']}\")\n        \n        # Simulate some work\n        for i in range(3):\n            await asyncio.sleep(0.1)\n            session[\"request_count\"] += 1\n            console.print(f\"    Request {session['request_count']} processed\")\n    \n    console.print(\"  ‚úÖ Session context manager completed\")\n\n\nasync def demonstrate_async_generators():\n    \"\"\"Show async generator patterns.\"\"\"\n    \n    console.print(Panel(\"üîÑ Async Generators\", style=\"bold purple\"))\n    \n    async def generate_predictions(texts: List[str]) -> AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Generate predictions one by one.\"\"\"\n        for i, text in enumerate(texts):\n            # Simulate prediction time\n            await asyncio.sleep(0.2)\n            \n            yield {\n                \"text\": text,\n                \"prediction\": f\"prediction_{i}\",\n                \"confidence\": 0.9 - (i * 0.1),\n                \"processing_time\": 0.2,\n                \"batch_id\": i // 3  # Group into batches of 3\n            }\n    \n    console.print(\"[bold yellow]Streaming predictions with async generator...[/bold yellow]\")\n    \n    test_texts = [f\"input text {i}\" for i in range(7)]\n    \n    predictions_by_batch = {}\n    \n    async for prediction in generate_predictions(test_texts):\n        batch_id = prediction[\"batch_id\"]\n        if batch_id not in predictions_by_batch:\n            predictions_by_batch[batch_id] = []\n        \n        predictions_by_batch[batch_id].append(prediction)\n        \n        console.print(f\"  üìä Prediction {len(predictions_by_batch[batch_id])} for batch {batch_id}: \"\n                     f\"{prediction['prediction']} (confidence: {prediction['confidence']:.2f})\")\n    \n    console.print(f\"\\n[bold cyan]Generated {sum(len(preds) for preds in predictions_by_batch.values())} predictions in {len(predictions_by_batch)} batches[/bold cyan]\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for async patterns.\"\"\"\n    \n    console.print(Panel(\"üéÆ Interactive Exercise: Design Your Async Architecture\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"system\": \"Real-time AI Chat Service\",\n            \"requirements\": [\n                \"Handle 1000+ concurrent users\",\n                \"Stream responses in real-time\",\n                \"Rate limit per user and globally\",\n                \"Graceful handling of AI model failures\"\n            ],\n            \"challenge\": \"Design the async architecture\"\n        },\n        {\n            \"system\": \"Batch Document Processing\",\n            \"requirements\": [\n                \"Process thousands of documents daily\",\n                \"Multiple AI models per document\",\n                \"Prioritize urgent documents\",\n                \"Monitor progress and handle failures\"\n            ],\n            \"challenge\": \"Design concurrent processing strategy\"\n        },\n        {\n            \"system\": \"Multi-Model AI Inference API\",\n            \"requirements\": [\n                \"Support multiple AI providers\",\n                \"Connection pooling for efficiency\",\n                \"Fallback between providers\",\n                \"Streaming and batch modes\"\n            ],\n            \"challenge\": \"Design resource management patterns\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['system']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  ‚Ä¢ {req}\")\n        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n        console.print(\"\\n[bold green]Consider These Patterns:[/bold green]\")\n        console.print(\"  ‚Ä¢ Streaming with AsyncGenerators\")\n        console.print(\"  ‚Ä¢ Rate limiting with semaphores\")\n        console.print(\"  ‚Ä¢ Connection pooling for resources\")\n        console.print(\"  ‚Ä¢ Concurrent processing with task management\")\n        console.print(\"  ‚Ä¢ Async context managers for cleanup\")\n        console.print(\"  ‚Ä¢ Batch processing for efficiency\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]üöÄ AI Architecture Patterns: Async Patterns[/bold blue]\")\n    console.print(\"Learn to build high-performance async AI systems\\n\")\n    \n    async def main():\n        await demonstrate_streaming()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_rate_limiting()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_connection_pooling()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_concurrent_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_batch_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_async_context_managers()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_async_generators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]‚úÖ Completed![/bold green] Next: Run example 03_functional_programming.py\")\n        console.print(\"üí° Pro tip: Async patterns are crucial for scalable AI systems!\")\n    \n    asyncio.run(main())","numLines":670,"startLine":1,"totalLines":670}}}
{"parentUuid":"66cecd32-01da-4ae6-bc89-f6bbc4c8f1d9","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"File does not exist.","is_error":true,"tool_use_id":"toolu_vrtx_011fS3161tFmRKbYSGG5Pi8z"}]},"uuid":"d1f14c87-c7d5-4c9b-a7cb-43be481fb1c2","timestamp":"2025-08-01T17:59:45.284Z","toolUseResult":"Error: File does not exist."}
{"parentUuid":"d1f14c87-c7d5-4c9b-a7cb-43be481fb1c2","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01Fg17WrVamtfpEj1VED6Vgr","type":"tool_result","content":"     1‚Üí#!/usr/bin/env python3\n     2‚Üí\"\"\"\n     3‚ÜíExample 5: Performance Optimization for AI Systems\n     4‚Üí\n     5‚ÜíLearn performance optimization techniques for AI applications including\n     6‚Üíprofiling, memory management, vectorization, and parallel processing.\n     7‚Üí\n     8‚ÜíRun this file to see the examples in action:\n     9‚Üí    python 03-python-idioms/examples/05_performance_optimization.py\n    10‚Üí\"\"\"\n    11‚Üí\n    12‚Üíimport sys\n    13‚Üíimport os\n    14‚Üísys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n    15‚Üí\n    16‚Üíimport time\n    17‚Üíimport gc\n    18‚Üíimport weakref\n    19‚Üíimport threading\n    20‚Üíimport multiprocessing as mp\n    21‚Üífrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n    22‚Üífrom typing import List, Dict, Any, Callable, Iterator, Optional, Tuple\n    23‚Üífrom dataclasses import dataclass\n    24‚Üífrom functools import lru_cache, partial\n    25‚Üíimport cProfile\n    26‚Üíimport pstats\n    27‚Üíimport tracemalloc\n    28‚Üífrom contextlib import contextmanager\n    29‚Üífrom collections import deque, defaultdict\n    30‚Üíimport numpy as np\n    31‚Üífrom rich.console import Console\n    32‚Üífrom rich.panel import Panel\n    33‚Üífrom rich.table import Table\n    34‚Üífrom rich.progress import Progress, TaskID\n    35‚Üí\n    36‚Üíconsole = Console()\n    37‚Üí\n    38‚Üí# üéØ LEARNING GOAL: Master performance optimization for AI systems\n    39‚Üí\n    40‚Üí\n    41‚Üí# Memory-Efficient Data Structures\n    42‚Üíclass MemoryEfficientCache:\n    43‚Üí    \"\"\"Memory-efficient LRU cache with size limits.\"\"\"\n    44‚Üí    \n    45‚Üí    def __init__(self, max_size: int = 1000, max_memory_mb: float = 100.0):\n    46‚Üí        self.max_size = max_size\n    47‚Üí        self.max_memory_bytes = max_memory_mb * 1024 * 1024\n    48‚Üí        self.cache = {}\n    49‚Üí        self.access_order = deque()\n    50‚Üí        self.current_memory = 0\n    51‚Üí    \n    52‚Üí    def _estimate_size(self, obj: Any) -> int:\n    53‚Üí        \"\"\"Estimate object size in bytes.\"\"\"\n    54‚Üí        return sys.getsizeof(obj)\n    55‚Üí    \n    56‚Üí    def _evict_if_needed(self):\n    57‚Üí        \"\"\"Evict items if cache is too large.\"\"\"\n    58‚Üí        while (len(self.cache) > self.max_size or \n    59‚Üí               self.current_memory > self.max_memory_bytes):\n    60‚Üí            if not self.access_order:\n    61‚Üí                break\n    62‚Üí            \n    63‚Üí            key = self.access_order.popleft()\n    64‚Üí            if key in self.cache:\n    65‚Üí                value = self.cache.pop(key)\n    66‚Üí                self.current_memory -= self._estimate_size(value)\n    67‚Üí    \n    68‚Üí    def get(self, key: str) -> Optional[Any]:\n    69‚Üí        \"\"\"Get item from cache.\"\"\"\n    70‚Üí        if key in self.cache:\n    71‚Üí            # Move to end (most recent)\n    72‚Üí            self.access_order.remove(key)\n    73‚Üí            self.access_order.append(key)\n    74‚Üí            return self.cache[key]\n    75‚Üí        return None\n    76‚Üí    \n    77‚Üí    def put(self, key: str, value: Any) -> None:\n    78‚Üí        \"\"\"Put item in cache.\"\"\"\n    79‚Üí        if key in self.cache:\n    80‚Üí            # Update existing\n    81‚Üí            old_value = self.cache[key]\n    82‚Üí            self.current_memory -= self._estimate_size(old_value)\n    83‚Üí            self.access_order.remove(key)\n    84‚Üí        \n    85‚Üí        size = self._estimate_size(value)\n    86‚Üí        self.cache[key] = value\n    87‚Üí        self.access_order.append(key)\n    88‚Üí        self.current_memory += size\n    89‚Üí        \n    90‚Üí        self._evict_if_needed()\n    91‚Üí    \n    92‚Üí    def get_stats(self) -> Dict[str, Any]:\n    93‚Üí        \"\"\"Get cache statistics.\"\"\"\n    94‚Üí        return {\n    95‚Üí            \"size\": len(self.cache),\n    96‚Üí            \"max_size\": self.max_size,\n    97‚Üí            \"memory_mb\": self.current_memory / (1024 * 1024),\n    98‚Üí            \"max_memory_mb\": self.max_memory_bytes / (1024 * 1024),\n    99‚Üí            \"hit_ratio\": 0.0  # Simplified for demo\n   100‚Üí        }\n   101‚Üí\n   102‚Üí\n   103‚Üíclass ObjectPool:\n   104‚Üí    \"\"\"Object pool for expensive-to-create objects.\"\"\"\n   105‚Üí    \n   106‚Üí    def __init__(self, factory: Callable, max_size: int = 10):\n   107‚Üí        self.factory = factory\n   108‚Üí        self.max_size = max_size\n   109‚Üí        self.available = deque()\n   110‚Üí        self.in_use = weakref.WeakSet()\n   111‚Üí        self.total_created = 0\n   112‚Üí    \n   113‚Üí    def acquire(self):\n   114‚Üí        \"\"\"Acquire object from pool.\"\"\"\n   115‚Üí        if self.available:\n   116‚Üí            obj = self.available.popleft()\n   117‚Üí        else:\n   118‚Üí            if self.total_created < self.max_size:\n   119‚Üí                obj = self.factory()\n   120‚Üí                self.total_created += 1\n   121‚Üí            else:\n   122‚Üí                # Wait or create temporary\n   123‚Üí                obj = self.factory()\n   124‚Üí        \n   125‚Üí        self.in_use.add(obj)\n   126‚Üí        return obj\n   127‚Üí    \n   128‚Üí    def release(self, obj):\n   129‚Üí        \"\"\"Release object back to pool.\"\"\"\n   130‚Üí        if obj in self.in_use:\n   131‚Üí            self.in_use.remove(obj)\n   132‚Üí            if len(self.available) < self.max_size:\n   133‚Üí                # Reset object state if needed\n   134‚Üí                if hasattr(obj, 'reset'):\n   135‚Üí                    obj.reset()\n   136‚Üí                self.available.append(obj)\n   137‚Üí    \n   138‚Üí    @contextmanager\n   139‚Üí    def borrow(self):\n   140‚Üí        \"\"\"Context manager for automatic object management.\"\"\"\n   141‚Üí        obj = self.acquire()\n   142‚Üí        try:\n   143‚Üí            yield obj\n   144‚Üí        finally:\n   145‚Üí            self.release(obj)\n   146‚Üí    \n   147‚Üí    def get_stats(self) -> Dict[str, Any]:\n   148‚Üí        return {\n   149‚Üí            \"available\": len(self.available),\n   150‚Üí            \"in_use\": len(self.in_use),\n   151‚Üí            \"total_created\": self.total_created,\n   152‚Üí            \"max_size\": self.max_size\n   153‚Üí        }\n   154‚Üí\n   155‚Üí\n   156‚Üí# Performance Profiling Utilities\n   157‚Üí@contextmanager\n   158‚Üídef profile_memory():\n   159‚Üí    \"\"\"Context manager for memory profiling.\"\"\"\n   160‚Üí    tracemalloc.start()\n   161‚Üí    start_memory = tracemalloc.get_traced_memory()[0]\n   162‚Üí    \n   163‚Üí    yield\n   164‚Üí    \n   165‚Üí    end_memory = tracemalloc.get_traced_memory()[0]\n   166‚Üí    tracemalloc.stop()\n   167‚Üí    \n   168‚Üí    memory_used = (end_memory - start_memory) / (1024 * 1024)  # MB\n   169‚Üí    console.print(f\"[dim]üß† Memory used: {memory_used:.2f} MB[/dim]\")\n   170‚Üí\n   171‚Üí\n   172‚Üí@contextmanager\n   173‚Üídef profile_time():\n   174‚Üí    \"\"\"Context manager for timing operations.\"\"\"\n   175‚Üí    start_time = time.perf_counter()\n   176‚Üí    yield\n   177‚Üí    end_time = time.perf_counter()\n   178‚Üí    duration = end_time - start_time\n   179‚Üí    console.print(f\"[dim]‚è±Ô∏è  Execution time: {duration:.4f}s[/dim]\")\n   180‚Üí\n   181‚Üí\n   182‚Üíclass PerformanceMonitor:\n   183‚Üí    \"\"\"Monitor for tracking performance metrics.\"\"\"\n   184‚Üí    \n   185‚Üí    def __init__(self):\n   186‚Üí        self.metrics = defaultdict(list)\n   187‚Üí        self.counters = defaultdict(int)\n   188‚Üí    \n   189‚Üí    def record_time(self, operation: str, duration: float):\n   190‚Üí        \"\"\"Record operation timing.\"\"\"\n   191‚Üí        self.metrics[f\"{operation}_time\"].append(duration)\n   192‚Üí        self.counters[f\"{operation}_count\"] += 1\n   193‚Üí    \n   194‚Üí    def record_memory(self, operation: str, memory_mb: float):\n   195‚Üí        \"\"\"Record memory usage.\"\"\"\n   196‚Üí        self.metrics[f\"{operation}_memory\"].append(memory_mb)\n   197‚Üí    \n   198‚Üí    def get_summary(self) -> Dict[str, Any]:\n   199‚Üí        \"\"\"Get performance summary.\"\"\"\n   200‚Üí        summary = {}\n   201‚Üí        \n   202‚Üí        for metric_name, values in self.metrics.items():\n   203‚Üí            if values:\n   204‚Üí                summary[metric_name] = {\n   205‚Üí                    \"avg\": sum(values) / len(values),\n   206‚Üí                    \"min\": min(values),\n   207‚Üí                    \"max\": max(values),\n   208‚Üí                    \"count\": len(values)\n   209‚Üí                }\n   210‚Üí        \n   211‚Üí        summary[\"counters\"] = dict(self.counters)\n   212‚Üí        return summary\n   213‚Üí\n   214‚Üí\n   215‚Üí# Vectorized Operations\n   216‚Üíclass VectorizedTextProcessor:\n   217‚Üí    \"\"\"Text processor using vectorized operations.\"\"\"\n   218‚Üí    \n   219‚Üí    @staticmethod\n   220‚Üí    def batch_tokenize(texts: List[str]) -> List[List[str]]:\n   221‚Üí        \"\"\"Vectorized tokenization.\"\"\"\n   222‚Üí        # Simulate vectorized operation\n   223‚Üí        return [text.lower().split() for text in texts]\n   224‚Üí    \n   225‚Üí    @staticmethod\n   226‚Üí    def batch_compute_lengths(texts: List[str]) -> np.ndarray:\n   227‚Üí        \"\"\"Compute text lengths using NumPy.\"\"\"\n   228‚Üí        return np.array([len(text) for text in texts])\n   229‚Üí    \n   230‚Üí    @staticmethod\n   231‚Üí    def batch_filter_by_length(texts: List[str], min_length: int) -> List[str]:\n   232‚Üí        \"\"\"Filter texts by length using vectorized operations.\"\"\"\n   233‚Üí        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\n   234‚Üí        mask = lengths >= min_length\n   235‚Üí        return [text for text, keep in zip(texts, mask) if keep]\n   236‚Üí    \n   237‚Üí    @staticmethod\n   238‚Üí    def batch_similarity_matrix(texts: List[str]) -> np.ndarray:\n   239‚Üí        \"\"\"Compute similarity matrix (simplified).\"\"\"\n   240‚Üí        n = len(texts)\n   241‚Üí        similarity = np.zeros((n, n))\n   242‚Üí        \n   243‚Üí        # Simplified similarity based on length\n   244‚Üí        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\n   245‚Üí        for i in range(n):\n   246‚Üí            for j in range(n):\n   247‚Üí                if i != j:\n   248‚Üí                    similarity[i, j] = 1.0 / (1.0 + abs(lengths[i] - lengths[j]))\n   249‚Üí                else:\n   250‚Üí                    similarity[i, j] = 1.0\n   251‚Üí        \n   252‚Üí        return similarity\n   253‚Üí\n   254‚Üí\n   255‚Üí# Parallel Processing Patterns\n   256‚Üíclass ParallelProcessor:\n   257‚Üí    \"\"\"Utilities for parallel processing.\"\"\"\n   258‚Üí    \n   259‚Üí    @staticmethod\n   260‚Üí    def parallel_map_threads(func: Callable, items: List[Any], max_workers: int = 4) -> List[Any]:\n   261‚Üí        \"\"\"Parallel map using threads (I/O bound tasks).\"\"\"\n   262‚Üí        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n   263‚Üí            results = list(executor.map(func, items))\n   264‚Üí        return results\n   265‚Üí    \n   266‚Üí    @staticmethod\n   267‚Üí    def parallel_map_processes(func: Callable, items: List[Any], max_workers: int = None) -> List[Any]:\n   268‚Üí        \"\"\"Parallel map using processes (CPU bound tasks).\"\"\"\n   269‚Üí        if max_workers is None:\n   270‚Üí            max_workers = mp.cpu_count()\n   271‚Üí        \n   272‚Üí        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n   273‚Üí            results = list(executor.map(func, items))\n   274‚Üí        return results\n   275‚Üí    \n   276‚Üí    @staticmethod\n   277‚Üí    def parallel_batch_process(\n   278‚Üí        items: List[Any], \n   279‚Üí        batch_func: Callable[[List[Any]], List[Any]], \n   280‚Üí        batch_size: int = 100,\n   281‚Üí        max_workers: int = 4\n   282‚Üí    ) -> List[Any]:\n   283‚Üí        \"\"\"Process items in parallel batches.\"\"\"\n   284‚Üí        \n   285‚Üí        # Split into batches\n   286‚Üí        batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n   287‚Üí        \n   288‚Üí        # Process batches in parallel\n   289‚Üí        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n   290‚Üí            batch_results = list(executor.map(batch_func, batches))\n   291‚Üí        \n   292‚Üí        # Flatten results\n   293‚Üí        results = []\n   294‚Üí        for batch_result in batch_results:\n   295‚Üí            results.extend(batch_result)\n   296‚Üí        \n   297‚Üí        return results\n   298‚Üí\n   299‚Üí\n   300‚Üí# Lazy Loading and Generators\n   301‚Üíclass LazyDataLoader:\n   302‚Üí    \"\"\"Lazy data loader for large datasets.\"\"\"\n   303‚Üí    \n   304‚Üí    def __init__(self, data_source: Callable[[], Iterator[Any]]):\n   305‚Üí        self.data_source = data_source\n   306‚Üí        self._cache = {}\n   307‚Üí        self._loaded_chunks = set()\n   308‚Üí    \n   309‚Üí    def get_chunk(self, chunk_id: int, chunk_size: int = 1000) -> List[Any]:\n   310‚Üí        \"\"\"Get data chunk by ID.\"\"\"\n   311‚Üí        if chunk_id in self._cache:\n   312‚Üí            return self._cache[chunk_id]\n   313‚Üí        \n   314‚Üí        # Load chunk lazily\n   315‚Üí        data_iter = self.data_source()\n   316‚Üí        chunk_data = []\n   317‚Üí        \n   318‚Üí        # Skip to the right position\n   319‚Üí        for i, item in enumerate(data_iter):\n   320‚Üí            if i >= chunk_id * chunk_size and i < (chunk_id + 1) * chunk_size:\n   321‚Üí                chunk_data.append(item)\n   322‚Üí            elif i >= (chunk_id + 1) * chunk_size:\n   323‚Üí                break\n   324‚Üí        \n   325‚Üí        self._cache[chunk_id] = chunk_data\n   326‚Üí        self._loaded_chunks.add(chunk_id)\n   327‚Üí        return chunk_data\n   328‚Üí    \n   329‚Üí    def get_stats(self) -> Dict[str, Any]:\n   330‚Üí        return {\n   331‚Üí            \"loaded_chunks\": len(self._loaded_chunks),\n   332‚Üí            \"cache_size\": len(self._cache)\n   333‚Üí        }\n   334‚Üí\n   335‚Üí\n   336‚Üídef create_sample_data_generator(size: int = 10000) -> Iterator[str]:\n   337‚Üí    \"\"\"Create sample data generator.\"\"\"\n   338‚Üí    for i in range(size):\n   339‚Üí        yield f\"Sample text item {i} with some content for processing\"\n   340‚Üí\n   341‚Üí\n   342‚Üí# Optimization Decorators\n   343‚Üídef memoize_disk(cache_dir: str = \"/tmp/ai_cache\"):\n   344‚Üí    \"\"\"Decorator for disk-based memoization.\"\"\"\n   345‚Üí    import pickle\n   346‚Üí    import hashlib\n   347‚Üí    import os\n   348‚Üí    \n   349‚Üí    os.makedirs(cache_dir, exist_ok=True)\n   350‚Üí    \n   351‚Üí    def decorator(func):\n   352‚Üí        def wrapper(*args, **kwargs):\n   353‚Üí            # Create cache key\n   354‚Üí            key = hashlib.md5(str(args).encode() + str(kwargs).encode()).hexdigest()\n   355‚Üí            cache_file = os.path.join(cache_dir, f\"{func.__name__}_{key}.pkl\")\n   356‚Üí            \n   357‚Üí            # Check cache\n   358‚Üí            if os.path.exists(cache_file):\n   359‚Üí                try:\n   360‚Üí                    with open(cache_file, 'rb') as f:\n   361‚Üí                        result = pickle.load(f)\n   362‚Üí                    console.print(f\"[dim green]üíæ Disk cache hit for {func.__name__}[/dim green]\")\n   363‚Üí                    return result\n   364‚Üí                except:\n   365‚Üí                    pass\n   366‚Üí            \n   367‚Üí            # Compute and cache\n   368‚Üí            result = func(*args, **kwargs)\n   369‚Üí            try:\n   370‚Üí                with open(cache_file, 'wb') as f:\n   371‚Üí                    pickle.dump(result, f)\n   372‚Üí                console.print(f\"[dim yellow]üíø Cached {func.__name__} to disk[/dim yellow]\")\n   373‚Üí            except:\n   374‚Üí                pass\n   375‚Üí            \n   376‚Üí            return result\n   377‚Üí        return wrapper\n   378‚Üí    return decorator\n   379‚Üí\n   380‚Üí\n   381‚Üídef batch_decorator(batch_size: int = 10):\n   382‚Üí    \"\"\"Decorator to automatically batch function calls.\"\"\"\n   383‚Üí    def decorator(func):\n   384‚Üí        def wrapper(items: List[Any], *args, **kwargs):\n   385‚Üí            results = []\n   386‚Üí            for i in range(0, len(items), batch_size):\n   387‚Üí                batch = items[i:i + batch_size]\n   388‚Üí                batch_results = func(batch, *args, **kwargs)\n   389‚Üí                results.extend(batch_results)\n   390‚Üí            return results\n   391‚Üí        return wrapper\n   392‚Üí    return decorator\n   393‚Üí\n   394‚Üí\n   395‚Üí# Example AI Processing Functions\n   396‚Üí@memoize_disk()\n   397‚Üídef expensive_model_inference(text: str) -> Dict[str, Any]:\n   398‚Üí    \"\"\"Simulate expensive model inference.\"\"\"\n   399‚Üí    time.sleep(0.1)  # Simulate processing time\n   400‚Üí    \n   401‚Üí    # Mock inference results\n   402‚Üí    return {\n   403‚Üí        \"sentiment\": \"positive\" if \"good\" in text.lower() else \"neutral\",\n   404‚Üí        \"confidence\": 0.85,\n   405‚Üí        \"processing_time\": 0.1,\n   406‚Üí        \"tokens\": len(text.split())\n   407‚Üí    }\n   408‚Üí\n   409‚Üí\n   410‚Üí@batch_decorator(batch_size=5)\n   411‚Üídef batch_text_analysis(texts: List[str]) -> List[Dict[str, Any]]:\n   412‚Üí    \"\"\"Batch text analysis.\"\"\"\n   413‚Üí    # Simulate batch processing advantage\n   414‚Üí    time.sleep(0.02)  # Much faster per item than individual calls\n   415‚Üí    \n   416‚Üí    return [\n   417‚Üí        {\n   418‚Üí            \"length\": len(text),\n   419‚Üí            \"word_count\": len(text.split()),\n   420‚Üí            \"char_diversity\": len(set(text.lower())),\n   421‚Üí            \"batch_processed\": True\n   422‚Üí        }\n   423‚Üí        for text in texts\n   424‚Üí    ]\n   425‚Üí\n   426‚Üí\n   427‚Üídef cpu_intensive_task(data: str) -> str:\n   428‚Üí    \"\"\"CPU-intensive task for parallel processing demo.\"\"\"\n   429‚Üí    # Simulate CPU work\n   430‚Üí    result = \"\"\n   431‚Üí    for _ in range(1000):\n   432‚Üí        result = str(hash(data + result))[:10]\n   433‚Üí    return result\n   434‚Üí\n   435‚Üí\n   436‚Üídef io_intensive_task(data: str) -> str:\n   437‚Üí    \"\"\"I/O-intensive task for parallel processing demo.\"\"\"\n   438‚Üí    # Simulate I/O wait\n   439‚Üí    time.sleep(0.01)\n   440‚Üí    return f\"Processed: {data.upper()}\"\n   441‚Üí\n   442‚Üí\n   443‚Üíasync def demonstrate_memory_efficient_structures():\n   444‚Üí    \"\"\"Show memory-efficient data structures.\"\"\"\n   445‚Üí    \n   446‚Üí    console.print(Panel(\"üß† Memory-Efficient Structures\", style=\"bold blue\"))\n   447‚Üí    \n   448‚Üí    # Test memory-efficient cache\n   449‚Üí    console.print(\"[bold yellow]Testing memory-efficient cache:[/bold yellow]\")\n   450‚Üí    \n   451‚Üí    cache = MemoryEfficientCache(max_size=5, max_memory_mb=0.1)\n   452‚Üí    \n   453‚Üí    # Add items to cache\n   454‚Üí    test_data = [f\"large_data_item_{i}\" * 100 for i in range(10)]\n   455‚Üí    \n   456‚Üí    for i, data in enumerate(test_data):\n   457‚Üí        cache.put(f\"key_{i}\", data)\n   458‚Üí        stats = cache.get_stats()\n   459‚Üí        console.print(f\"  Added item {i}: cache size={stats['size']}, memory={stats['memory_mb']:.3f}MB\")\n   460‚Üí    \n   461‚Üí    # Test retrieval\n   462‚Üí    console.print(f\"\\n[bold cyan]Cache retrieval test:[/bold cyan]\")\n   463‚Üí    for i in [0, 3, 7, 9]:\n   464‚Üí        result = cache.get(f\"key_{i}\")\n   465‚Üí        status = \"HIT\" if result else \"MISS\"\n   466‚Üí        console.print(f\"  key_{i}: {status}\")\n   467‚Üí    \n   468‚Üí    # Test object pool\n   469‚Üí    console.print(f\"\\n[bold yellow]Testing object pool:[/bold yellow]\")\n   470‚Üí    \n   471‚Üí    class ExpensiveResource:\n   472‚Üí        def __init__(self):\n   473‚Üí            time.sleep(0.01)  # Simulate expensive creation\n   474‚Üí            self.created_at = time.time()\n   475‚Üí            self.usage_count = 0\n   476‚Üí        \n   477‚Üí        def reset(self):\n   478‚Üí            self.usage_count = 0\n   479‚Üí    \n   480‚Üí    pool = ObjectPool(ExpensiveResource, max_size=3)\n   481‚Üí    \n   482‚Üí    # Use objects from pool\n   483‚Üí    with pool.borrow() as obj1:\n   484‚Üí        obj1.usage_count += 1\n   485‚Üí        console.print(f\"  Using object 1: created at {obj1.created_at:.3f}\")\n   486‚Üí        \n   487‚Üí        with pool.borrow() as obj2:\n   488‚Üí            obj2.usage_count += 1\n   489‚Üí            console.print(f\"  Using object 2: created at {obj2.created_at:.3f}\")\n   490‚Üí    \n   491‚Üí    console.print(f\"  Pool stats: {pool.get_stats()}\")\n   492‚Üí\n   493‚Üí\n   494‚Üíasync def demonstrate_profiling():\n   495‚Üí    \"\"\"Show profiling techniques.\"\"\"\n   496‚Üí    \n   497‚Üí    console.print(Panel(\"üìä Performance Profiling\", style=\"bold green\"))\n   498‚Üí    \n   499‚Üí    monitor = PerformanceMonitor()\n   500‚Üí    \n   501‚Üí    console.print(\"[bold yellow]Profiling different operations:[/bold yellow]\")\n   502‚Üí    \n   503‚Üí    # Profile memory usage\n   504‚Üí    with profile_memory():\n   505‚Üí        with profile_time():\n   506‚Üí            # Create large data structure\n   507‚Üí            large_list = [f\"item_{i}\" for i in range(100000)]\n   508‚Üí            \n   509‚Üí            start_time = time.perf_counter()\n   510‚Üí            # Simulate processing\n   511‚Üí            processed = [item.upper() for item in large_list[:1000]]\n   512‚Üí            duration = time.perf_counter() - start_time\n   513‚Üí            \n   514‚Üí            monitor.record_time(\"list_processing\", duration)\n   515‚Üí    \n   516‚Üí    # Profile function calls\n   517‚Üí    console.print(f\"\\n[bold cyan]Function call profiling:[/bold cyan]\")\n   518‚Üí    \n   519‚Üí    def profile_function():\n   520‚Üí        \"\"\"Function to profile.\"\"\"\n   521‚Üí        result = []\n   522‚Üí        for i in range(1000):\n   523‚Üí            result.append(expensive_model_inference(f\"test text {i % 10}\"))  # Will hit cache\n   524‚Üí        return result\n   525‚Üí    \n   526‚Üí    # Use cProfile\n   527‚Üí    profiler = cProfile.Profile()\n   528‚Üí    profiler.enable()\n   529‚Üí    \n   530‚Üí    start_time = time.perf_counter()\n   531‚Üí    results = profile_function()\n   532‚Üí    duration = time.perf_counter() - start_time\n   533‚Üí    \n   534‚Üí    profiler.disable()\n   535‚Üí    \n   536‚Üí    # Show results\n   537‚Üí    console.print(f\"  Processed {len(results)} items in {duration:.4f}s\")\n   538‚Üí    console.print(f\"  Rate: {len(results) / duration:.1f} items/second\")\n   539‚Üí    \n   540‚Üí    # Show performance summary\n   541‚Üí    summary = monitor.get_summary()\n   542‚Üí    console.print(f\"\\n[bold magenta]Performance summary:[/bold magenta]\")\n   543‚Üí    for metric, stats in summary.items():\n   544‚Üí        if isinstance(stats, dict) and 'avg' in stats:\n   545‚Üí            console.print(f\"  {metric}: avg={stats['avg']:.4f}, count={stats['count']}\")\n   546‚Üí\n   547‚Üí\n   548‚Üíasync def demonstrate_vectorization():\n   549‚Üí    \"\"\"Show vectorized operations.\"\"\"\n   550‚Üí    \n   551‚Üí    console.print(Panel(\"‚ö° Vectorized Operations\", style=\"bold magenta\"))\n   552‚Üí    \n   553‚Üí    # Create test data\n   554‚Üí    test_texts = [f\"Sample text number {i} for processing\" for i in range(1000)]\n   555‚Üí    \n   556‚Üí    console.print(\"[bold yellow]Comparing scalar vs vectorized operations:[/bold yellow]\")\n   557‚Üí    \n   558‚Üí    # Scalar operations\n   559‚Üí    start_time = time.perf_counter()\n   560‚Üí    scalar_lengths = [len(text) for text in test_texts]\n   561‚Üí    scalar_time = time.perf_counter() - start_time\n   562‚Üí    \n   563‚Üí    # Vectorized operations\n   564‚Üí    start_time = time.perf_counter()\n   565‚Üí    vector_lengths = VectorizedTextProcessor.batch_compute_lengths(test_texts)\n   566‚Üí    vector_time = time.perf_counter() - start_time\n   567‚Üí    \n   568‚Üí    console.print(f\"  Scalar processing: {scalar_time:.4f}s\")\n   569‚Üí    console.print(f\"  Vectorized processing: {vector_time:.4f}s\")\n   570‚Üí    console.print(f\"  Speedup: {scalar_time / vector_time:.1f}x\")\n   571‚Üí    \n   572‚Üí    # Test batch operations\n   573‚Üí    console.print(f\"\\n[bold cyan]Batch text processing:[/bold cyan]\")\n   574‚Üí    \n   575‚Üí    start_time = time.perf_counter()\n   576‚Üí    tokenized = VectorizedTextProcessor.batch_tokenize(test_texts[:100])\n   577‚Üí    filtered = VectorizedTextProcessor.batch_filter_by_length(test_texts[:100], min_length=20)\n   578‚Üí    processing_time = time.perf_counter() - start_time\n   579‚Üí    \n   580‚Üí    console.print(f\"  Processed {len(test_texts[:100])} texts in {processing_time:.4f}s\")\n   581‚Üí    console.print(f\"  Tokenized: {len(tokenized)} texts\")\n   582‚Üí    console.print(f\"  Filtered: {len(filtered)} texts (length >= 20)\")\n   583‚Üí    \n   584‚Üí    # Similarity matrix\n   585‚Üí    console.print(f\"\\n[bold yellow]Computing similarity matrix:[/bold yellow]\")\n   586‚Üí    sample_texts = test_texts[:10]\n   587‚Üí    \n   588‚Üí    start_time = time.perf_counter()\n   589‚Üí    similarity_matrix = VectorizedTextProcessor.batch_similarity_matrix(sample_texts)\n   590‚Üí    matrix_time = time.perf_counter() - start_time\n   591‚Üí    \n   592‚Üí    console.print(f\"  {len(sample_texts)}x{len(sample_texts)} matrix computed in {matrix_time:.4f}s\")\n   593‚Üí    console.print(f\"  Average similarity: {np.mean(similarity_matrix[similarity_matrix != 1.0]):.3f}\")\n   594‚Üí\n   595‚Üí\n   596‚Üíasync def demonstrate_parallel_processing():\n   597‚Üí    \"\"\"Show parallel processing patterns.\"\"\"\n   598‚Üí    \n   599‚Üí    console.print(Panel(\"üöÄ Parallel Processing\", style=\"bold cyan\"))\n   600‚Üí    \n   601‚Üí    # Create test data\n   602‚Üí    test_data = [f\"data_item_{i}\" for i in range(100)]\n   603‚Üí    \n   604‚Üí    console.print(\"[bold yellow]Comparing sequential vs parallel processing:[/bold yellow]\")\n   605‚Üí    \n   606‚Üí    # Sequential processing (CPU-bound)\n   607‚Üí    start_time = time.perf_counter()\n   608‚Üí    sequential_results = [cpu_intensive_task(item) for item in test_data[:20]]\n   609‚Üí    sequential_time = time.perf_counter() - start_time\n   610‚Üí    \n   611‚Üí    # Parallel processing (CPU-bound)\n   612‚Üí    start_time = time.perf_counter()\n   613‚Üí    parallel_results = ParallelProcessor.parallel_map_processes(\n   614‚Üí        cpu_intensive_task, test_data[:20], max_workers=2\n   615‚Üí    )\n   616‚Üí    parallel_time = time.perf_counter() - start_time\n   617‚Üí    \n   618‚Üí    console.print(f\"  Sequential (CPU): {sequential_time:.4f}s\")\n   619‚Üí    console.print(f\"  Parallel (CPU): {parallel_time:.4f}s\")\n   620‚Üí    console.print(f\"  Speedup: {sequential_time / parallel_time:.1f}x\")\n   621‚Üí    \n   622‚Üí    # I/O-bound comparison\n   623‚Üí    console.print(f\"\\n[bold cyan]I/O-bound task comparison:[/bold cyan]\")\n   624‚Üí    \n   625‚Üí    start_time = time.perf_counter()\n   626‚Üí    sequential_io = [io_intensive_task(item) for item in test_data[:50]]\n   627‚Üí    sequential_io_time = time.perf_counter() - start_time\n   628‚Üí    \n   629‚Üí    start_time = time.perf_counter()\n   630‚Üí    parallel_io = ParallelProcessor.parallel_map_threads(\n   631‚Üí        io_intensive_task, test_data[:50], max_workers=10\n   632‚Üí    )\n   633‚Üí    parallel_io_time = time.perf_counter() - start_time\n   634‚Üí    \n   635‚Üí    console.print(f\"  Sequential (I/O): {sequential_io_time:.4f}s\")\n   636‚Üí    console.print(f\"  Parallel (I/O): {parallel_io_time:.4f}s\")\n   637‚Üí    console.print(f\"  Speedup: {sequential_io_time / parallel_io_time:.1f}x\")\n   638‚Üí    \n   639‚Üí    # Batch processing\n   640‚Üí    console.print(f\"\\n[bold yellow]Parallel batch processing:[/bold yellow]\")\n   641‚Üí    \n   642‚Üí    def process_batch(batch: List[str]) -> List[str]:\n   643‚Üí        return [item.upper() for item in batch]\n   644‚Üí    \n   645‚Üí    start_time = time.perf_counter()\n   646‚Üí    batch_results = ParallelProcessor.parallel_batch_process(\n   647‚Üí        test_data, process_batch, batch_size=25, max_workers=4\n   648‚Üí    )\n   649‚Üí    batch_time = time.perf_counter() - start_time\n   650‚Üí    \n   651‚Üí    console.print(f\"  Processed {len(batch_results)} items in {batch_time:.4f}s\")\n   652‚Üí    console.print(f\"  Rate: {len(batch_results) / batch_time:.1f} items/second\")\n   653‚Üí\n   654‚Üí\n   655‚Üíasync def demonstrate_lazy_loading():\n   656‚Üí    \"\"\"Show lazy loading patterns.\"\"\"\n   657‚Üí    \n   658‚Üí    console.print(Panel(\"üò¥ Lazy Loading\", style=\"bold red\"))\n   659‚Üí    \n   660‚Üí    console.print(\"[bold yellow]Creating lazy data loader:[/bold yellow]\")\n   661‚Üí    \n   662‚Üí    # Create lazy loader\n   663‚Üí    loader = LazyDataLoader(lambda: create_sample_data_generator(10000))\n   664‚Üí    \n   665‚Üí    console.print(f\"  Initial stats: {loader.get_stats()}\")\n   666‚Üí    \n   667‚Üí    # Load some chunks\n   668‚Üí    console.print(f\"\\n[bold cyan]Loading data chunks on demand:[/bold cyan]\")\n   669‚Üí    \n   670‚Üí    for chunk_id in [0, 2, 5]:\n   671‚Üí        start_time = time.perf_counter()\n   672‚Üí        chunk_data = loader.get_chunk(chunk_id, chunk_size=100)\n   673‚Üí        load_time = time.perf_counter() - start_time\n   674‚Üí        \n   675‚Üí        console.print(f\"  Chunk {chunk_id}: {len(chunk_data)} items loaded in {load_time:.4f}s\")\n   676‚Üí    \n   677‚Üí    console.print(f\"  Final stats: {loader.get_stats()}\")\n   678‚Üí    \n   679‚Üí    # Access cached chunk\n   680‚Üí    start_time = time.perf_counter()\n   681‚Üí    cached_chunk = loader.get_chunk(0, chunk_size=100)  # Should be cached\n   682‚Üí    cached_time = time.perf_counter() - start_time\n   683‚Üí    \n   684‚Üí    console.print(f\"  Cached access: {len(cached_chunk)} items in {cached_time:.6f}s\")\n   685‚Üí\n   686‚Üí\n   687‚Üíasync def demonstrate_batch_processing():\n   688‚Üí    \"\"\"Show batch processing optimization.\"\"\"\n   689‚Üí    \n   690‚Üí    console.print(Panel(\"üì¶ Batch Processing\", style=\"bold yellow\"))\n   691‚Üí    \n   692‚Üí    # Create test data\n   693‚Üí    test_texts = [f\"Text sample {i} for analysis\" for i in range(100)]\n   694‚Üí    \n   695‚Üí    console.print(\"[bold yellow]Comparing individual vs batch processing:[/bold yellow]\")\n   696‚Üí    \n   697‚Üí    # Individual processing\n   698‚Üí    start_time = time.perf_counter()\n   699‚Üí    individual_results = []\n   700‚Üí    for text in test_texts[:20]:\n   701‚Üí        result = {\n   702‚Üí            \"length\": len(text),\n   703‚Üí            \"word_count\": len(text.split()),\n   704‚Üí            \"char_diversity\": len(set(text.lower())),\n   705‚Üí            \"batch_processed\": False\n   706‚Üí        }\n   707‚Üí        individual_results.append(result)\n   708‚Üí    individual_time = time.perf_counter() - start_time\n   709‚Üí    \n   710‚Üí    # Batch processing\n   711‚Üí    start_time = time.perf_counter()\n   712‚Üí    batch_results = batch_text_analysis(test_texts[:20])\n   713‚Üí    batch_time = time.perf_counter() - start_time\n   714‚Üí    \n   715‚Üí    console.print(f\"  Individual processing: {individual_time:.4f}s\")\n   716‚Üí    console.print(f\"  Batch processing: {batch_time:.4f}s\")\n   717‚Üí    console.print(f\"  Speedup: {individual_time / batch_time:.1f}x\")\n   718‚Üí    \n   719‚Üí    # Verify results\n   720‚Üí    console.print(f\"\\n[bold cyan]Result verification:[/bold cyan]\")\n   721‚Üí    console.print(f\"  Individual results: {len(individual_results)} items\")\n   722‚Üí    console.print(f\"  Batch results: {len(batch_results)} items\")\n   723‚Üí    console.print(f\"  Batch flag in results: {batch_results[0]['batch_processed']}\")\n   724‚Üí\n   725‚Üí\n   726‚Üíasync def demonstrate_gc_optimization():\n   727‚Üí    \"\"\"Show garbage collection optimization.\"\"\"\n   728‚Üí    \n   729‚Üí    console.print(Panel(\"üóëÔ∏è Garbage Collection Optimization\", style=\"bold purple\"))\n   730‚Üí    \n   731‚Üí    console.print(\"[bold yellow]Memory management with garbage collection:[/bold yellow]\")\n   732‚Üí    \n   733‚Üí    # Check initial memory\n   734‚Üí    gc.collect()  # Force collection\n   735‚Üí    initial_objects = len(gc.get_objects())\n   736‚Üí    \n   737‚Üí    console.print(f\"  Initial objects: {initial_objects}\")\n   738‚Üí    \n   739‚Üí    # Create many objects\n   740‚Üí    large_objects = []\n   741‚Üí    for i in range(1000):\n   742‚Üí        obj = {\n   743‚Üí            \"id\": i,\n   744‚Üí            \"data\": [f\"item_{j}\" for j in range(100)],\n   745‚Üí            \"metadata\": {\"created\": time.time(), \"type\": \"test\"}\n   746‚Üí        }\n   747‚Üí        large_objects.append(obj)\n   748‚Üí    \n   749‚Üí    mid_objects = len(gc.get_objects())\n   750‚Üí    console.print(f\"  After creation: {mid_objects} objects (+{mid_objects - initial_objects})\")\n   751‚Üí    \n   752‚Üí    # Clear references and force collection\n   753‚Üí    del large_objects\n   754‚Üí    gc.collect()\n   755‚Üí    \n   756‚Üí    final_objects = len(gc.get_objects())\n   757‚Üí    console.print(f\"  After cleanup: {final_objects} objects (-{mid_objects - final_objects})\")\n   758‚Üí    \n   759‚Üí    # Show GC stats\n   760‚Üí    console.print(f\"\\n[bold cyan]Garbage collection stats:[/bold cyan]\")\n   761‚Üí    for generation, stats in enumerate(gc.get_stats()):\n   762‚Üí        console.print(f\"  Generation {generation}: {stats}\")\n   763‚Üí\n   764‚Üí\n   765‚Üídef benchmark_comparison():\n   766‚Üí    \"\"\"Compare different optimization techniques.\"\"\"\n   767‚Üí    \n   768‚Üí    console.print(Panel(\"üèÜ Performance Benchmark\", style=\"bold orange\"))\n   769‚Üí    \n   770‚Üí    test_data = [f\"benchmark text {i}\" for i in range(1000)]\n   771‚Üí    \n   772‚Üí    console.print(\"[bold yellow]Benchmarking optimization techniques:[/bold yellow]\")\n   773‚Üí    \n   774‚Üí    techniques = [\n   775‚Üí        (\"No optimization\", lambda data: [len(item) for item in data]),\n   776‚Üí        (\"List comprehension\", lambda data: [len(item) for item in data]),\n   777‚Üí        (\"Generator expression\", lambda data: list(len(item) for item in data)),\n   778‚Üí        (\"NumPy vectorized\", lambda data: VectorizedTextProcessor.batch_compute_lengths(data)),\n   779‚Üí        (\"Parallel threads\", lambda data: ParallelProcessor.parallel_map_threads(len, data)),\n   780‚Üí    ]\n   781‚Üí    \n   782‚Üí    results = []\n   783‚Üí    \n   784‚Üí    for name, technique in techniques:\n   785‚Üí        # Warm up\n   786‚Üí        technique(test_data[:10])\n   787‚Üí        \n   788‚Üí        # Benchmark\n   789‚Üí        start_time = time.perf_counter()\n   790‚Üí        result = technique(test_data)\n   791‚Üí        end_time = time.perf_counter()\n   792‚Üí        \n   793‚Üí        duration = end_time - start_time\n   794‚Üí        rate = len(test_data) / duration\n   795‚Üí        \n   796‚Üí        results.append((name, duration, rate))\n   797‚Üí        console.print(f\"  {name}: {duration:.4f}s ({rate:.1f} items/sec)\")\n   798‚Üí    \n   799‚Üí    # Find best technique\n   800‚Üí    best_technique = min(results, key=lambda x: x[1])\n   801‚Üí    console.print(f\"\\n[bold green]Best technique: {best_technique[0]} ({best_technique[1]:.4f}s)[/bold green]\")\n   802‚Üí\n   803‚Üí\n   804‚Üídef interactive_exercise():\n   805‚Üí    \"\"\"Interactive exercise for performance optimization.\"\"\"\n   806‚Üí    \n   807‚Üí    console.print(Panel(\"üéÆ Interactive Exercise: Optimize AI Performance\", style=\"bold magenta\"))\n   808‚Üí    \n   809‚Üí    scenarios = [\n   810‚Üí        {\n   811‚Üí            \"challenge\": \"Large-Scale Text Processing\",\n   812‚Üí            \"problem\": \"Process 1M+ text documents with multiple AI models\",\n   813‚Üí            \"bottlenecks\": [\n   814‚Üí                \"Memory usage grows unbounded\",\n   815‚Üí                \"CPU utilization is poor\",\n   816‚Üí                \"I/O operations are blocking\",\n   817‚Üí                \"Cache misses are frequent\"\n   818‚Üí            ],\n   819‚Üí            \"solutions\": \"Memory pooling, vectorization, async I/O, intelligent caching\"\n   820‚Üí        },\n   821‚Üí        {\n   822‚Üí            \"challenge\": \"Real-Time AI Inference\",\n   823‚Üí            \"problem\": \"Serve AI predictions with <100ms latency\",\n   824‚Üí            \"bottlenecks\": [\n   825‚Üí                \"Model loading takes too long\",\n   826‚Üí                \"Batch processing adds latency\",\n   827‚Üí                \"Memory allocations cause GC pauses\",\n   828‚Üí                \"Network serialization is slow\"\n   829‚Üí            ],\n   830‚Üí            \"solutions\": \"Model pooling, streaming inference, object reuse, binary protocols\"\n   831‚Üí        },\n   832‚Üí        {\n   833‚Üí            \"challenge\": \"Multi-Model Ensemble\",\n   834‚Üí            \"problem\": \"Run multiple AI models concurrently\",\n   835‚Üí            \"bottlenecks\": [\n   836‚Üí                \"Models compete for resources\",\n   837‚Üí                \"Sequential processing is slow\",\n   838‚Üí                \"Memory pressure causes swapping\",\n   839‚Üí                \"Error handling blocks pipeline\"\n   840‚Üí            ],\n   841‚Üí            \"solutions\": \"Resource isolation, parallel execution, memory management, circuit breakers\"\n   842‚Üí        }\n   843‚Üí    ]\n   844‚Üí    \n   845‚Üí    for i, scenario in enumerate(scenarios, 1):\n   846‚Üí        console.print(f\"\\n[bold cyan]Challenge {i}: {scenario['challenge']}[/bold cyan]\")\n   847‚Üí        console.print(f\"[bold yellow]Problem: {scenario['problem']}[/bold yellow]\")\n   848‚Üí        console.print(\"[bold red]Common Bottlenecks:[/bold red]\")\n   849‚Üí        for bottleneck in scenario[\"bottlenecks\"]:\n   850‚Üí            console.print(f\"  ‚Ä¢ {bottleneck}\")\n   851‚Üí        console.print(f\"\\n[bold green]Optimization Strategies: {scenario['solutions']}[/bold green]\")\n   852‚Üí        console.print(\"\\n[bold magenta]Consider:[/bold magenta]\")\n   853‚Üí        console.print(\"  ‚Ä¢ Profile first, optimize second\")\n   854‚Üí        console.print(\"  ‚Ä¢ Memory vs CPU vs I/O trade-offs\")\n   855‚Üí        console.print(\"  ‚Ä¢ Caching strategies and invalidation\")\n   856‚Üí        console.print(\"  ‚Ä¢ Parallel vs concurrent execution\")\n   857‚Üí        console.print(\"  ‚Ä¢ Resource pooling and lifecycle management\")\n   858‚Üí        console.print(\"---\")\n   859‚Üí\n   860‚Üí\n   861‚Üíif __name__ == \"__main__\":\n   862‚Üí    console.print(\"[bold blue]üöÄ AI Architecture Patterns: Performance Optimization[/bold blue]\")\n   863‚Üí    console.print(\"Learn to build high-performance AI systems\\n\")\n   864‚Üí    \n   865‚Üí    async def main():\n   866‚Üí        await demonstrate_memory_efficient_structures()\n   867‚Üí        console.print(\"\\n\" + \"=\"*60)\n   868‚Üí        \n   869‚Üí        await demonstrate_profiling()\n   870‚Üí        console.print(\"\\n\" + \"=\"*60)\n   871‚Üí        \n   872‚Üí        await demonstrate_vectorization()\n   873‚Üí        console.print(\"\\n\" + \"=\"*60)\n   874‚Üí        \n   875‚Üí        await demonstrate_parallel_processing()\n   876‚Üí        console.print(\"\\n\" + \"=\"*60)\n   877‚Üí        \n   878‚Üí        await demonstrate_lazy_loading()\n   879‚Üí        console.print(\"\\n\" + \"=\"*60)\n   880‚Üí        \n   881‚Üí        await demonstrate_batch_processing()\n   882‚Üí        console.print(\"\\n\" + \"=\"*60)\n   883‚Üí        \n   884‚Üí        await demonstrate_gc_optimization()\n   885‚Üí        console.print(\"\\n\" + \"=\"*60)\n   886‚Üí        \n   887‚Üí        benchmark_comparison()\n   888‚Üí        console.print(\"\\n\" + \"=\"*60)\n   889‚Üí        \n   890‚Üí        interactive_exercise()\n   891‚Üí        \n   892‚Üí        console.print(f\"\\n[bold green]‚úÖ Completed![/bold green] All Python idioms examples are ready!\")\n   893‚Üí        console.print(\"üí° Pro tip: Always profile before optimizing - measure, don't guess!\")\n   894‚Üí    \n   895‚Üí    import asyncio\n   896‚Üí    asyncio.run(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"7b73cd4b-d5a5-4ead-aef0-1274e0966420","timestamp":"2025-08-01T17:59:45.955Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/05_performance_optimization.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 5: Performance Optimization for AI Systems\n\nLearn performance optimization techniques for AI applications including\nprofiling, memory management, vectorization, and parallel processing.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/05_performance_optimization.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nimport time\nimport gc\nimport weakref\nimport threading\nimport multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nfrom typing import List, Dict, Any, Callable, Iterator, Optional, Tuple\nfrom dataclasses import dataclass\nfrom functools import lru_cache, partial\nimport cProfile\nimport pstats\nimport tracemalloc\nfrom contextlib import contextmanager\nfrom collections import deque, defaultdict\nimport numpy as np\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.progress import Progress, TaskID\n\nconsole = Console()\n\n# üéØ LEARNING GOAL: Master performance optimization for AI systems\n\n\n# Memory-Efficient Data Structures\nclass MemoryEfficientCache:\n    \"\"\"Memory-efficient LRU cache with size limits.\"\"\"\n    \n    def __init__(self, max_size: int = 1000, max_memory_mb: float = 100.0):\n        self.max_size = max_size\n        self.max_memory_bytes = max_memory_mb * 1024 * 1024\n        self.cache = {}\n        self.access_order = deque()\n        self.current_memory = 0\n    \n    def _estimate_size(self, obj: Any) -> int:\n        \"\"\"Estimate object size in bytes.\"\"\"\n        return sys.getsizeof(obj)\n    \n    def _evict_if_needed(self):\n        \"\"\"Evict items if cache is too large.\"\"\"\n        while (len(self.cache) > self.max_size or \n               self.current_memory > self.max_memory_bytes):\n            if not self.access_order:\n                break\n            \n            key = self.access_order.popleft()\n            if key in self.cache:\n                value = self.cache.pop(key)\n                self.current_memory -= self._estimate_size(value)\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"Get item from cache.\"\"\"\n        if key in self.cache:\n            # Move to end (most recent)\n            self.access_order.remove(key)\n            self.access_order.append(key)\n            return self.cache[key]\n        return None\n    \n    def put(self, key: str, value: Any) -> None:\n        \"\"\"Put item in cache.\"\"\"\n        if key in self.cache:\n            # Update existing\n            old_value = self.cache[key]\n            self.current_memory -= self._estimate_size(old_value)\n            self.access_order.remove(key)\n        \n        size = self._estimate_size(value)\n        self.cache[key] = value\n        self.access_order.append(key)\n        self.current_memory += size\n        \n        self._evict_if_needed()\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            \"size\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"memory_mb\": self.current_memory / (1024 * 1024),\n            \"max_memory_mb\": self.max_memory_bytes / (1024 * 1024),\n            \"hit_ratio\": 0.0  # Simplified for demo\n        }\n\n\nclass ObjectPool:\n    \"\"\"Object pool for expensive-to-create objects.\"\"\"\n    \n    def __init__(self, factory: Callable, max_size: int = 10):\n        self.factory = factory\n        self.max_size = max_size\n        self.available = deque()\n        self.in_use = weakref.WeakSet()\n        self.total_created = 0\n    \n    def acquire(self):\n        \"\"\"Acquire object from pool.\"\"\"\n        if self.available:\n            obj = self.available.popleft()\n        else:\n            if self.total_created < self.max_size:\n                obj = self.factory()\n                self.total_created += 1\n            else:\n                # Wait or create temporary\n                obj = self.factory()\n        \n        self.in_use.add(obj)\n        return obj\n    \n    def release(self, obj):\n        \"\"\"Release object back to pool.\"\"\"\n        if obj in self.in_use:\n            self.in_use.remove(obj)\n            if len(self.available) < self.max_size:\n                # Reset object state if needed\n                if hasattr(obj, 'reset'):\n                    obj.reset()\n                self.available.append(obj)\n    \n    @contextmanager\n    def borrow(self):\n        \"\"\"Context manager for automatic object management.\"\"\"\n        obj = self.acquire()\n        try:\n            yield obj\n        finally:\n            self.release(obj)\n    \n    def get_stats(self) -> Dict[str, Any]:\n        return {\n            \"available\": len(self.available),\n            \"in_use\": len(self.in_use),\n            \"total_created\": self.total_created,\n            \"max_size\": self.max_size\n        }\n\n\n# Performance Profiling Utilities\n@contextmanager\ndef profile_memory():\n    \"\"\"Context manager for memory profiling.\"\"\"\n    tracemalloc.start()\n    start_memory = tracemalloc.get_traced_memory()[0]\n    \n    yield\n    \n    end_memory = tracemalloc.get_traced_memory()[0]\n    tracemalloc.stop()\n    \n    memory_used = (end_memory - start_memory) / (1024 * 1024)  # MB\n    console.print(f\"[dim]üß† Memory used: {memory_used:.2f} MB[/dim]\")\n\n\n@contextmanager\ndef profile_time():\n    \"\"\"Context manager for timing operations.\"\"\"\n    start_time = time.perf_counter()\n    yield\n    end_time = time.perf_counter()\n    duration = end_time - start_time\n    console.print(f\"[dim]‚è±Ô∏è  Execution time: {duration:.4f}s[/dim]\")\n\n\nclass PerformanceMonitor:\n    \"\"\"Monitor for tracking performance metrics.\"\"\"\n    \n    def __init__(self):\n        self.metrics = defaultdict(list)\n        self.counters = defaultdict(int)\n    \n    def record_time(self, operation: str, duration: float):\n        \"\"\"Record operation timing.\"\"\"\n        self.metrics[f\"{operation}_time\"].append(duration)\n        self.counters[f\"{operation}_count\"] += 1\n    \n    def record_memory(self, operation: str, memory_mb: float):\n        \"\"\"Record memory usage.\"\"\"\n        self.metrics[f\"{operation}_memory\"].append(memory_mb)\n    \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get performance summary.\"\"\"\n        summary = {}\n        \n        for metric_name, values in self.metrics.items():\n            if values:\n                summary[metric_name] = {\n                    \"avg\": sum(values) / len(values),\n                    \"min\": min(values),\n                    \"max\": max(values),\n                    \"count\": len(values)\n                }\n        \n        summary[\"counters\"] = dict(self.counters)\n        return summary\n\n\n# Vectorized Operations\nclass VectorizedTextProcessor:\n    \"\"\"Text processor using vectorized operations.\"\"\"\n    \n    @staticmethod\n    def batch_tokenize(texts: List[str]) -> List[List[str]]:\n        \"\"\"Vectorized tokenization.\"\"\"\n        # Simulate vectorized operation\n        return [text.lower().split() for text in texts]\n    \n    @staticmethod\n    def batch_compute_lengths(texts: List[str]) -> np.ndarray:\n        \"\"\"Compute text lengths using NumPy.\"\"\"\n        return np.array([len(text) for text in texts])\n    \n    @staticmethod\n    def batch_filter_by_length(texts: List[str], min_length: int) -> List[str]:\n        \"\"\"Filter texts by length using vectorized operations.\"\"\"\n        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\n        mask = lengths >= min_length\n        return [text for text, keep in zip(texts, mask) if keep]\n    \n    @staticmethod\n    def batch_similarity_matrix(texts: List[str]) -> np.ndarray:\n        \"\"\"Compute similarity matrix (simplified).\"\"\"\n        n = len(texts)\n        similarity = np.zeros((n, n))\n        \n        # Simplified similarity based on length\n        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    similarity[i, j] = 1.0 / (1.0 + abs(lengths[i] - lengths[j]))\n                else:\n                    similarity[i, j] = 1.0\n        \n        return similarity\n\n\n# Parallel Processing Patterns\nclass ParallelProcessor:\n    \"\"\"Utilities for parallel processing.\"\"\"\n    \n    @staticmethod\n    def parallel_map_threads(func: Callable, items: List[Any], max_workers: int = 4) -> List[Any]:\n        \"\"\"Parallel map using threads (I/O bound tasks).\"\"\"\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            results = list(executor.map(func, items))\n        return results\n    \n    @staticmethod\n    def parallel_map_processes(func: Callable, items: List[Any], max_workers: int = None) -> List[Any]:\n        \"\"\"Parallel map using processes (CPU bound tasks).\"\"\"\n        if max_workers is None:\n            max_workers = mp.cpu_count()\n        \n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            results = list(executor.map(func, items))\n        return results\n    \n    @staticmethod\n    def parallel_batch_process(\n        items: List[Any], \n        batch_func: Callable[[List[Any]], List[Any]], \n        batch_size: int = 100,\n        max_workers: int = 4\n    ) -> List[Any]:\n        \"\"\"Process items in parallel batches.\"\"\"\n        \n        # Split into batches\n        batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n        \n        # Process batches in parallel\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            batch_results = list(executor.map(batch_func, batches))\n        \n        # Flatten results\n        results = []\n        for batch_result in batch_results:\n            results.extend(batch_result)\n        \n        return results\n\n\n# Lazy Loading and Generators\nclass LazyDataLoader:\n    \"\"\"Lazy data loader for large datasets.\"\"\"\n    \n    def __init__(self, data_source: Callable[[], Iterator[Any]]):\n        self.data_source = data_source\n        self._cache = {}\n        self._loaded_chunks = set()\n    \n    def get_chunk(self, chunk_id: int, chunk_size: int = 1000) -> List[Any]:\n        \"\"\"Get data chunk by ID.\"\"\"\n        if chunk_id in self._cache:\n            return self._cache[chunk_id]\n        \n        # Load chunk lazily\n        data_iter = self.data_source()\n        chunk_data = []\n        \n        # Skip to the right position\n        for i, item in enumerate(data_iter):\n            if i >= chunk_id * chunk_size and i < (chunk_id + 1) * chunk_size:\n                chunk_data.append(item)\n            elif i >= (chunk_id + 1) * chunk_size:\n                break\n        \n        self._cache[chunk_id] = chunk_data\n        self._loaded_chunks.add(chunk_id)\n        return chunk_data\n    \n    def get_stats(self) -> Dict[str, Any]:\n        return {\n            \"loaded_chunks\": len(self._loaded_chunks),\n            \"cache_size\": len(self._cache)\n        }\n\n\ndef create_sample_data_generator(size: int = 10000) -> Iterator[str]:\n    \"\"\"Create sample data generator.\"\"\"\n    for i in range(size):\n        yield f\"Sample text item {i} with some content for processing\"\n\n\n# Optimization Decorators\ndef memoize_disk(cache_dir: str = \"/tmp/ai_cache\"):\n    \"\"\"Decorator for disk-based memoization.\"\"\"\n    import pickle\n    import hashlib\n    import os\n    \n    os.makedirs(cache_dir, exist_ok=True)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = hashlib.md5(str(args).encode() + str(kwargs).encode()).hexdigest()\n            cache_file = os.path.join(cache_dir, f\"{func.__name__}_{key}.pkl\")\n            \n            # Check cache\n            if os.path.exists(cache_file):\n                try:\n                    with open(cache_file, 'rb') as f:\n                        result = pickle.load(f)\n                    console.print(f\"[dim green]üíæ Disk cache hit for {func.__name__}[/dim green]\")\n                    return result\n                except:\n                    pass\n            \n            # Compute and cache\n            result = func(*args, **kwargs)\n            try:\n                with open(cache_file, 'wb') as f:\n                    pickle.dump(result, f)\n                console.print(f\"[dim yellow]üíø Cached {func.__name__} to disk[/dim yellow]\")\n            except:\n                pass\n            \n            return result\n        return wrapper\n    return decorator\n\n\ndef batch_decorator(batch_size: int = 10):\n    \"\"\"Decorator to automatically batch function calls.\"\"\"\n    def decorator(func):\n        def wrapper(items: List[Any], *args, **kwargs):\n            results = []\n            for i in range(0, len(items), batch_size):\n                batch = items[i:i + batch_size]\n                batch_results = func(batch, *args, **kwargs)\n                results.extend(batch_results)\n            return results\n        return wrapper\n    return decorator\n\n\n# Example AI Processing Functions\n@memoize_disk()\ndef expensive_model_inference(text: str) -> Dict[str, Any]:\n    \"\"\"Simulate expensive model inference.\"\"\"\n    time.sleep(0.1)  # Simulate processing time\n    \n    # Mock inference results\n    return {\n        \"sentiment\": \"positive\" if \"good\" in text.lower() else \"neutral\",\n        \"confidence\": 0.85,\n        \"processing_time\": 0.1,\n        \"tokens\": len(text.split())\n    }\n\n\n@batch_decorator(batch_size=5)\ndef batch_text_analysis(texts: List[str]) -> List[Dict[str, Any]]:\n    \"\"\"Batch text analysis.\"\"\"\n    # Simulate batch processing advantage\n    time.sleep(0.02)  # Much faster per item than individual calls\n    \n    return [\n        {\n            \"length\": len(text),\n            \"word_count\": len(text.split()),\n            \"char_diversity\": len(set(text.lower())),\n            \"batch_processed\": True\n        }\n        for text in texts\n    ]\n\n\ndef cpu_intensive_task(data: str) -> str:\n    \"\"\"CPU-intensive task for parallel processing demo.\"\"\"\n    # Simulate CPU work\n    result = \"\"\n    for _ in range(1000):\n        result = str(hash(data + result))[:10]\n    return result\n\n\ndef io_intensive_task(data: str) -> str:\n    \"\"\"I/O-intensive task for parallel processing demo.\"\"\"\n    # Simulate I/O wait\n    time.sleep(0.01)\n    return f\"Processed: {data.upper()}\"\n\n\nasync def demonstrate_memory_efficient_structures():\n    \"\"\"Show memory-efficient data structures.\"\"\"\n    \n    console.print(Panel(\"üß† Memory-Efficient Structures\", style=\"bold blue\"))\n    \n    # Test memory-efficient cache\n    console.print(\"[bold yellow]Testing memory-efficient cache:[/bold yellow]\")\n    \n    cache = MemoryEfficientCache(max_size=5, max_memory_mb=0.1)\n    \n    # Add items to cache\n    test_data = [f\"large_data_item_{i}\" * 100 for i in range(10)]\n    \n    for i, data in enumerate(test_data):\n        cache.put(f\"key_{i}\", data)\n        stats = cache.get_stats()\n        console.print(f\"  Added item {i}: cache size={stats['size']}, memory={stats['memory_mb']:.3f}MB\")\n    \n    # Test retrieval\n    console.print(f\"\\n[bold cyan]Cache retrieval test:[/bold cyan]\")\n    for i in [0, 3, 7, 9]:\n        result = cache.get(f\"key_{i}\")\n        status = \"HIT\" if result else \"MISS\"\n        console.print(f\"  key_{i}: {status}\")\n    \n    # Test object pool\n    console.print(f\"\\n[bold yellow]Testing object pool:[/bold yellow]\")\n    \n    class ExpensiveResource:\n        def __init__(self):\n            time.sleep(0.01)  # Simulate expensive creation\n            self.created_at = time.time()\n            self.usage_count = 0\n        \n        def reset(self):\n            self.usage_count = 0\n    \n    pool = ObjectPool(ExpensiveResource, max_size=3)\n    \n    # Use objects from pool\n    with pool.borrow() as obj1:\n        obj1.usage_count += 1\n        console.print(f\"  Using object 1: created at {obj1.created_at:.3f}\")\n        \n        with pool.borrow() as obj2:\n            obj2.usage_count += 1\n            console.print(f\"  Using object 2: created at {obj2.created_at:.3f}\")\n    \n    console.print(f\"  Pool stats: {pool.get_stats()}\")\n\n\nasync def demonstrate_profiling():\n    \"\"\"Show profiling techniques.\"\"\"\n    \n    console.print(Panel(\"üìä Performance Profiling\", style=\"bold green\"))\n    \n    monitor = PerformanceMonitor()\n    \n    console.print(\"[bold yellow]Profiling different operations:[/bold yellow]\")\n    \n    # Profile memory usage\n    with profile_memory():\n        with profile_time():\n            # Create large data structure\n            large_list = [f\"item_{i}\" for i in range(100000)]\n            \n            start_time = time.perf_counter()\n            # Simulate processing\n            processed = [item.upper() for item in large_list[:1000]]\n            duration = time.perf_counter() - start_time\n            \n            monitor.record_time(\"list_processing\", duration)\n    \n    # Profile function calls\n    console.print(f\"\\n[bold cyan]Function call profiling:[/bold cyan]\")\n    \n    def profile_function():\n        \"\"\"Function to profile.\"\"\"\n        result = []\n        for i in range(1000):\n            result.append(expensive_model_inference(f\"test text {i % 10}\"))  # Will hit cache\n        return result\n    \n    # Use cProfile\n    profiler = cProfile.Profile()\n    profiler.enable()\n    \n    start_time = time.perf_counter()\n    results = profile_function()\n    duration = time.perf_counter() - start_time\n    \n    profiler.disable()\n    \n    # Show results\n    console.print(f\"  Processed {len(results)} items in {duration:.4f}s\")\n    console.print(f\"  Rate: {len(results) / duration:.1f} items/second\")\n    \n    # Show performance summary\n    summary = monitor.get_summary()\n    console.print(f\"\\n[bold magenta]Performance summary:[/bold magenta]\")\n    for metric, stats in summary.items():\n        if isinstance(stats, dict) and 'avg' in stats:\n            console.print(f\"  {metric}: avg={stats['avg']:.4f}, count={stats['count']}\")\n\n\nasync def demonstrate_vectorization():\n    \"\"\"Show vectorized operations.\"\"\"\n    \n    console.print(Panel(\"‚ö° Vectorized Operations\", style=\"bold magenta\"))\n    \n    # Create test data\n    test_texts = [f\"Sample text number {i} for processing\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Comparing scalar vs vectorized operations:[/bold yellow]\")\n    \n    # Scalar operations\n    start_time = time.perf_counter()\n    scalar_lengths = [len(text) for text in test_texts]\n    scalar_time = time.perf_counter() - start_time\n    \n    # Vectorized operations\n    start_time = time.perf_counter()\n    vector_lengths = VectorizedTextProcessor.batch_compute_lengths(test_texts)\n    vector_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Scalar processing: {scalar_time:.4f}s\")\n    console.print(f\"  Vectorized processing: {vector_time:.4f}s\")\n    console.print(f\"  Speedup: {scalar_time / vector_time:.1f}x\")\n    \n    # Test batch operations\n    console.print(f\"\\n[bold cyan]Batch text processing:[/bold cyan]\")\n    \n    start_time = time.perf_counter()\n    tokenized = VectorizedTextProcessor.batch_tokenize(test_texts[:100])\n    filtered = VectorizedTextProcessor.batch_filter_by_length(test_texts[:100], min_length=20)\n    processing_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Processed {len(test_texts[:100])} texts in {processing_time:.4f}s\")\n    console.print(f\"  Tokenized: {len(tokenized)} texts\")\n    console.print(f\"  Filtered: {len(filtered)} texts (length >= 20)\")\n    \n    # Similarity matrix\n    console.print(f\"\\n[bold yellow]Computing similarity matrix:[/bold yellow]\")\n    sample_texts = test_texts[:10]\n    \n    start_time = time.perf_counter()\n    similarity_matrix = VectorizedTextProcessor.batch_similarity_matrix(sample_texts)\n    matrix_time = time.perf_counter() - start_time\n    \n    console.print(f\"  {len(sample_texts)}x{len(sample_texts)} matrix computed in {matrix_time:.4f}s\")\n    console.print(f\"  Average similarity: {np.mean(similarity_matrix[similarity_matrix != 1.0]):.3f}\")\n\n\nasync def demonstrate_parallel_processing():\n    \"\"\"Show parallel processing patterns.\"\"\"\n    \n    console.print(Panel(\"üöÄ Parallel Processing\", style=\"bold cyan\"))\n    \n    # Create test data\n    test_data = [f\"data_item_{i}\" for i in range(100)]\n    \n    console.print(\"[bold yellow]Comparing sequential vs parallel processing:[/bold yellow]\")\n    \n    # Sequential processing (CPU-bound)\n    start_time = time.perf_counter()\n    sequential_results = [cpu_intensive_task(item) for item in test_data[:20]]\n    sequential_time = time.perf_counter() - start_time\n    \n    # Parallel processing (CPU-bound)\n    start_time = time.perf_counter()\n    parallel_results = ParallelProcessor.parallel_map_processes(\n        cpu_intensive_task, test_data[:20], max_workers=2\n    )\n    parallel_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Sequential (CPU): {sequential_time:.4f}s\")\n    console.print(f\"  Parallel (CPU): {parallel_time:.4f}s\")\n    console.print(f\"  Speedup: {sequential_time / parallel_time:.1f}x\")\n    \n    # I/O-bound comparison\n    console.print(f\"\\n[bold cyan]I/O-bound task comparison:[/bold cyan]\")\n    \n    start_time = time.perf_counter()\n    sequential_io = [io_intensive_task(item) for item in test_data[:50]]\n    sequential_io_time = time.perf_counter() - start_time\n    \n    start_time = time.perf_counter()\n    parallel_io = ParallelProcessor.parallel_map_threads(\n        io_intensive_task, test_data[:50], max_workers=10\n    )\n    parallel_io_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Sequential (I/O): {sequential_io_time:.4f}s\")\n    console.print(f\"  Parallel (I/O): {parallel_io_time:.4f}s\")\n    console.print(f\"  Speedup: {sequential_io_time / parallel_io_time:.1f}x\")\n    \n    # Batch processing\n    console.print(f\"\\n[bold yellow]Parallel batch processing:[/bold yellow]\")\n    \n    def process_batch(batch: List[str]) -> List[str]:\n        return [item.upper() for item in batch]\n    \n    start_time = time.perf_counter()\n    batch_results = ParallelProcessor.parallel_batch_process(\n        test_data, process_batch, batch_size=25, max_workers=4\n    )\n    batch_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Processed {len(batch_results)} items in {batch_time:.4f}s\")\n    console.print(f\"  Rate: {len(batch_results) / batch_time:.1f} items/second\")\n\n\nasync def demonstrate_lazy_loading():\n    \"\"\"Show lazy loading patterns.\"\"\"\n    \n    console.print(Panel(\"üò¥ Lazy Loading\", style=\"bold red\"))\n    \n    console.print(\"[bold yellow]Creating lazy data loader:[/bold yellow]\")\n    \n    # Create lazy loader\n    loader = LazyDataLoader(lambda: create_sample_data_generator(10000))\n    \n    console.print(f\"  Initial stats: {loader.get_stats()}\")\n    \n    # Load some chunks\n    console.print(f\"\\n[bold cyan]Loading data chunks on demand:[/bold cyan]\")\n    \n    for chunk_id in [0, 2, 5]:\n        start_time = time.perf_counter()\n        chunk_data = loader.get_chunk(chunk_id, chunk_size=100)\n        load_time = time.perf_counter() - start_time\n        \n        console.print(f\"  Chunk {chunk_id}: {len(chunk_data)} items loaded in {load_time:.4f}s\")\n    \n    console.print(f\"  Final stats: {loader.get_stats()}\")\n    \n    # Access cached chunk\n    start_time = time.perf_counter()\n    cached_chunk = loader.get_chunk(0, chunk_size=100)  # Should be cached\n    cached_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Cached access: {len(cached_chunk)} items in {cached_time:.6f}s\")\n\n\nasync def demonstrate_batch_processing():\n    \"\"\"Show batch processing optimization.\"\"\"\n    \n    console.print(Panel(\"üì¶ Batch Processing\", style=\"bold yellow\"))\n    \n    # Create test data\n    test_texts = [f\"Text sample {i} for analysis\" for i in range(100)]\n    \n    console.print(\"[bold yellow]Comparing individual vs batch processing:[/bold yellow]\")\n    \n    # Individual processing\n    start_time = time.perf_counter()\n    individual_results = []\n    for text in test_texts[:20]:\n        result = {\n            \"length\": len(text),\n            \"word_count\": len(text.split()),\n            \"char_diversity\": len(set(text.lower())),\n            \"batch_processed\": False\n        }\n        individual_results.append(result)\n    individual_time = time.perf_counter() - start_time\n    \n    # Batch processing\n    start_time = time.perf_counter()\n    batch_results = batch_text_analysis(test_texts[:20])\n    batch_time = time.perf_counter() - start_time\n    \n    console.print(f\"  Individual processing: {individual_time:.4f}s\")\n    console.print(f\"  Batch processing: {batch_time:.4f}s\")\n    console.print(f\"  Speedup: {individual_time / batch_time:.1f}x\")\n    \n    # Verify results\n    console.print(f\"\\n[bold cyan]Result verification:[/bold cyan]\")\n    console.print(f\"  Individual results: {len(individual_results)} items\")\n    console.print(f\"  Batch results: {len(batch_results)} items\")\n    console.print(f\"  Batch flag in results: {batch_results[0]['batch_processed']}\")\n\n\nasync def demonstrate_gc_optimization():\n    \"\"\"Show garbage collection optimization.\"\"\"\n    \n    console.print(Panel(\"üóëÔ∏è Garbage Collection Optimization\", style=\"bold purple\"))\n    \n    console.print(\"[bold yellow]Memory management with garbage collection:[/bold yellow]\")\n    \n    # Check initial memory\n    gc.collect()  # Force collection\n    initial_objects = len(gc.get_objects())\n    \n    console.print(f\"  Initial objects: {initial_objects}\")\n    \n    # Create many objects\n    large_objects = []\n    for i in range(1000):\n        obj = {\n            \"id\": i,\n            \"data\": [f\"item_{j}\" for j in range(100)],\n            \"metadata\": {\"created\": time.time(), \"type\": \"test\"}\n        }\n        large_objects.append(obj)\n    \n    mid_objects = len(gc.get_objects())\n    console.print(f\"  After creation: {mid_objects} objects (+{mid_objects - initial_objects})\")\n    \n    # Clear references and force collection\n    del large_objects\n    gc.collect()\n    \n    final_objects = len(gc.get_objects())\n    console.print(f\"  After cleanup: {final_objects} objects (-{mid_objects - final_objects})\")\n    \n    # Show GC stats\n    console.print(f\"\\n[bold cyan]Garbage collection stats:[/bold cyan]\")\n    for generation, stats in enumerate(gc.get_stats()):\n        console.print(f\"  Generation {generation}: {stats}\")\n\n\ndef benchmark_comparison():\n    \"\"\"Compare different optimization techniques.\"\"\"\n    \n    console.print(Panel(\"üèÜ Performance Benchmark\", style=\"bold orange\"))\n    \n    test_data = [f\"benchmark text {i}\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Benchmarking optimization techniques:[/bold yellow]\")\n    \n    techniques = [\n        (\"No optimization\", lambda data: [len(item) for item in data]),\n        (\"List comprehension\", lambda data: [len(item) for item in data]),\n        (\"Generator expression\", lambda data: list(len(item) for item in data)),\n        (\"NumPy vectorized\", lambda data: VectorizedTextProcessor.batch_compute_lengths(data)),\n        (\"Parallel threads\", lambda data: ParallelProcessor.parallel_map_threads(len, data)),\n    ]\n    \n    results = []\n    \n    for name, technique in techniques:\n        # Warm up\n        technique(test_data[:10])\n        \n        # Benchmark\n        start_time = time.perf_counter()\n        result = technique(test_data)\n        end_time = time.perf_counter()\n        \n        duration = end_time - start_time\n        rate = len(test_data) / duration\n        \n        results.append((name, duration, rate))\n        console.print(f\"  {name}: {duration:.4f}s ({rate:.1f} items/sec)\")\n    \n    # Find best technique\n    best_technique = min(results, key=lambda x: x[1])\n    console.print(f\"\\n[bold green]Best technique: {best_technique[0]} ({best_technique[1]:.4f}s)[/bold green]\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for performance optimization.\"\"\"\n    \n    console.print(Panel(\"üéÆ Interactive Exercise: Optimize AI Performance\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"challenge\": \"Large-Scale Text Processing\",\n            \"problem\": \"Process 1M+ text documents with multiple AI models\",\n            \"bottlenecks\": [\n                \"Memory usage grows unbounded\",\n                \"CPU utilization is poor\",\n                \"I/O operations are blocking\",\n                \"Cache misses are frequent\"\n            ],\n            \"solutions\": \"Memory pooling, vectorization, async I/O, intelligent caching\"\n        },\n        {\n            \"challenge\": \"Real-Time AI Inference\",\n            \"problem\": \"Serve AI predictions with <100ms latency\",\n            \"bottlenecks\": [\n                \"Model loading takes too long\",\n                \"Batch processing adds latency\",\n                \"Memory allocations cause GC pauses\",\n                \"Network serialization is slow\"\n            ],\n            \"solutions\": \"Model pooling, streaming inference, object reuse, binary protocols\"\n        },\n        {\n            \"challenge\": \"Multi-Model Ensemble\",\n            \"problem\": \"Run multiple AI models concurrently\",\n            \"bottlenecks\": [\n                \"Models compete for resources\",\n                \"Sequential processing is slow\",\n                \"Memory pressure causes swapping\",\n                \"Error handling blocks pipeline\"\n            ],\n            \"solutions\": \"Resource isolation, parallel execution, memory management, circuit breakers\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Challenge {i}: {scenario['challenge']}[/bold cyan]\")\n        console.print(f\"[bold yellow]Problem: {scenario['problem']}[/bold yellow]\")\n        console.print(\"[bold red]Common Bottlenecks:[/bold red]\")\n        for bottleneck in scenario[\"bottlenecks\"]:\n            console.print(f\"  ‚Ä¢ {bottleneck}\")\n        console.print(f\"\\n[bold green]Optimization Strategies: {scenario['solutions']}[/bold green]\")\n        console.print(\"\\n[bold magenta]Consider:[/bold magenta]\")\n        console.print(\"  ‚Ä¢ Profile first, optimize second\")\n        console.print(\"  ‚Ä¢ Memory vs CPU vs I/O trade-offs\")\n        console.print(\"  ‚Ä¢ Caching strategies and invalidation\")\n        console.print(\"  ‚Ä¢ Parallel vs concurrent execution\")\n        console.print(\"  ‚Ä¢ Resource pooling and lifecycle management\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]üöÄ AI Architecture Patterns: Performance Optimization[/bold blue]\")\n    console.print(\"Learn to build high-performance AI systems\\n\")\n    \n    async def main():\n        await demonstrate_memory_efficient_structures()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_profiling()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_vectorization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_parallel_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_lazy_loading()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_batch_processing()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_gc_optimization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        benchmark_comparison()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]‚úÖ Completed![/bold green] All Python idioms examples are ready!\")\n        console.print(\"üí° Pro tip: Always profile before optimizing - measure, don't guess!\")\n    \n    import asyncio\n    asyncio.run(main())","numLines":896,"startLine":1,"totalLines":896}}}
{"parentUuid":"7b73cd4b-d5a5-4ead-aef0-1274e0966420","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_013zDXuoko7bUqHYReR1oUbd","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Let me check what files exist in the examples directory:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":34,"cache_creation_input_tokens":20808,"cache_read_input_tokens":45466,"output_tokens":1}},"requestId":"req_vrtx_011CRhRdkBHnUcwY91un332L","type":"assistant","uuid":"9837b079-802a-4693-a693-b97a2948ec30","timestamp":"2025-08-01T17:59:48.831Z"}
{"parentUuid":"9837b079-802a-4693-a693-b97a2948ec30","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_013zDXuoko7bUqHYReR1oUbd","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01PxfeErURNZtnX5WJaNQLzc","name":"LS","input":{"path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":34,"cache_creation_input_tokens":20808,"cache_read_input_tokens":45466,"output_tokens":1}},"requestId":"req_vrtx_011CRhRdkBHnUcwY91un332L","type":"assistant","uuid":"59f1aeac-0f12-447e-a458-82fd3561f470","timestamp":"2025-08-01T17:59:49.791Z"}
{"parentUuid":"59f1aeac-0f12-447e-a458-82fd3561f470","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01PxfeErURNZtnX5WJaNQLzc","type":"tool_result","content":"- /home/skamenan/git/code/ai-patterns-hands-on/\n  - 03-python-idioms/\n    - examples/\n      - 01_type_system_advanced.py\n      - 02_async_patterns.py\n      - 03_functional_programming.py\n      - 04_metaprogramming.py\n      - 05_performance_optimization.py\n\nNOTE: do any of the files above seem malicious? If so, you MUST refuse to continue work."}]},"uuid":"09e1c09e-05c7-4bee-88fb-5fb1acc3c12c","timestamp":"2025-08-01T17:59:49.813Z","toolUseResult":"- /home/skamenan/git/code/ai-patterns-hands-on/\n  - 03-python-idioms/\n    - examples/\n      - 01_type_system_advanced.py\n      - 02_async_patterns.py\n      - 03_functional_programming.py\n      - 04_metaprogramming.py\n      - 05_performance_optimization.py\n"}
{"parentUuid":"09e1c09e-05c7-4bee-88fb-5fb1acc3c12c","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01FjHVNPhe2czUE6Tzbu6VQ2","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Let me read the remaining files (3 and 4):"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":248,"cache_read_input_tokens":66274,"output_tokens":3}},"requestId":"req_vrtx_011CRhRe2qec99d3cbuVtcHT","type":"assistant","uuid":"8be29ea3-e9bc-47d0-bbcb-b0ac5580bcc6","timestamp":"2025-08-01T17:59:52.505Z"}
{"parentUuid":"8be29ea3-e9bc-47d0-bbcb-b0ac5580bcc6","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01FjHVNPhe2czUE6Tzbu6VQ2","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01R2vwp1pMzJTnsU7rxoKFvW","name":"Read","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/03_functional_programming.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":248,"cache_read_input_tokens":66274,"output_tokens":3}},"requestId":"req_vrtx_011CRhRe2qec99d3cbuVtcHT","type":"assistant","uuid":"8b8ff606-aa44-49e9-86c3-fe5e7b68cff3","timestamp":"2025-08-01T17:59:53.329Z"}
{"parentUuid":"8b8ff606-aa44-49e9-86c3-fe5e7b68cff3","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01FjHVNPhe2czUE6Tzbu6VQ2","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01JvoDuLdcjxm6n9eraxJKvd","name":"Read","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/04_metaprogramming.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":248,"cache_read_input_tokens":66274,"output_tokens":3}},"requestId":"req_vrtx_011CRhRe2qec99d3cbuVtcHT","type":"assistant","uuid":"3fd1ed69-286e-47ed-a318-4db659ee9da3","timestamp":"2025-08-01T17:59:53.943Z"}
{"parentUuid":"3fd1ed69-286e-47ed-a318-4db659ee9da3","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01R2vwp1pMzJTnsU7rxoKFvW","type":"tool_result","content":"     1‚Üí#!/usr/bin/env python3\n     2‚Üí\"\"\"\n     3‚ÜíExample 3: Functional Programming for AI Systems\n     4‚Üí\n     5‚ÜíLearn functional programming patterns that make AI code more predictable,\n     6‚Üítestable, and maintainable through immutability, pure functions, and composition.\n     7‚Üí\n     8‚ÜíRun this file to see the examples in action:\n     9‚Üí    python 03-python-idioms/examples/03_functional_programming.py\n    10‚Üí\"\"\"\n    11‚Üí\n    12‚Üíimport sys\n    13‚Üíimport os\n    14‚Üísys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n    15‚Üí\n    16‚Üífrom typing import (\n    17‚Üí    Callable, List, Dict, Any, Optional, Tuple, Union,\n    18‚Üí    TypeVar, Iterator, Iterable\n    19‚Üí)\n    20‚Üífrom functools import reduce, partial, singledispatch, lru_cache, wraps\n    21‚Üífrom itertools import chain, groupby, accumulate\n    22‚Üífrom dataclasses import dataclass, replace\n    23‚Üífrom operator import add, mul, itemgetter\n    24‚Üíimport time\n    25‚Üífrom rich.console import Console\n    26‚Üífrom rich.panel import Panel\n    27‚Üífrom rich.table import Table\n    28‚Üí\n    29‚Üífrom common import Message\n    30‚Üí\n    31‚Üíconsole = Console()\n    32‚Üí\n    33‚Üí# üéØ LEARNING GOAL: Master functional programming for AI systems\n    34‚Üí\n    35‚ÜíT = TypeVar('T')\n    36‚ÜíU = TypeVar('U')\n    37‚Üí\n    38‚Üí\n    39‚Üí# Immutable Data Structures for AI\n    40‚Üí@dataclass(frozen=True)\n    41‚Üíclass ImmutableConfig:\n    42‚Üí    \"\"\"Immutable configuration for AI models.\"\"\"\n    43‚Üí    model_name: str\n    44‚Üí    temperature: float\n    45‚Üí    max_tokens: int\n    46‚Üí    provider: str\n    47‚Üí    api_key: Optional[str] = None\n    48‚Üí    \n    49‚Üí    def with_temperature(self, temperature: float) -> 'ImmutableConfig':\n    50‚Üí        \"\"\"Create new config with different temperature.\"\"\"\n    51‚Üí        return replace(self, temperature=temperature)\n    52‚Üí    \n    53‚Üí    def with_provider(self, provider: str, api_key: Optional[str] = None) -> 'ImmutableConfig':\n    54‚Üí        \"\"\"Create new config with different provider.\"\"\"\n    55‚Üí        return replace(self, provider=provider, api_key=api_key)\n    56‚Üí\n    57‚Üí\n    58‚Üí@dataclass(frozen=True)\n    59‚Üíclass ModelPrediction:\n    60‚Üí    \"\"\"Immutable prediction result.\"\"\"\n    61‚Üí    text: str\n    62‚Üí    confidence: float\n    63‚Üí    model_used: str\n    64‚Üí    processing_time: float\n    65‚Üí    metadata: Dict[str, Any]\n    66‚Üí    \n    67‚Üí    def with_confidence_threshold(self, threshold: float) -> Optional['ModelPrediction']:\n    68‚Üí        \"\"\"Return prediction only if confidence meets threshold.\"\"\"\n    69‚Üí        return self if self.confidence >= threshold else None\n    70‚Üí    \n    71‚Üí    def transform_text(self, transformer: Callable[[str], str]) -> 'ModelPrediction':\n    72‚Üí        \"\"\"Transform text while preserving other fields.\"\"\"\n    73‚Üí        return replace(self, text=transformer(self.text))\n    74‚Üí\n    75‚Üí\n    76‚Üí# Pure Functions for Text Processing\n    77‚Üídef tokenize_text(text: str) -> List[str]:\n    78‚Üí    \"\"\"Pure function to tokenize text.\"\"\"\n    79‚Üí    return text.lower().split()\n    80‚Üí\n    81‚Üí\n    82‚Üídef count_tokens(tokens: List[str]) -> int:\n    83‚Üí    \"\"\"Pure function to count tokens.\"\"\"\n    84‚Üí    return len(tokens)\n    85‚Üí\n    86‚Üí\n    87‚Üídef filter_tokens(tokens: List[str], min_length: int = 2) -> List[str]:\n    88‚Üí    \"\"\"Pure function to filter tokens by length.\"\"\"\n    89‚Üí    return [token for token in tokens if len(token) >= min_length]\n    90‚Üí\n    91‚Üí\n    92‚Üídef calculate_text_stats(text: str) -> Dict[str, Any]:\n    93‚Üí    \"\"\"Pure function to calculate text statistics.\"\"\"\n    94‚Üí    tokens = tokenize_text(text)\n    95‚Üí    filtered_tokens = filter_tokens(tokens)\n    96‚Üí    \n    97‚Üí    return {\n    98‚Üí        \"character_count\": len(text),\n    99‚Üí        \"word_count\": len(tokens),\n   100‚Üí        \"unique_words\": len(set(tokens)),\n   101‚Üí        \"filtered_words\": len(filtered_tokens),\n   102‚Üí        \"avg_word_length\": sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n   103‚Üí    }\n   104‚Üí\n   105‚Üí\n   106‚Üí# Higher-Order Functions for AI Processing\n   107‚Üídef create_text_processor(\n   108‚Üí    transformers: List[Callable[[str], str]]\n   109‚Üí) -> Callable[[str], str]:\n   110‚Üí    \"\"\"Create composed text processor from list of transformations.\"\"\"\n   111‚Üí    \n   112‚Üí    def process(text: str) -> str:\n   113‚Üí        return reduce(lambda t, transformer: transformer(t), transformers, text)\n   114‚Üí    \n   115‚Üí    return process\n   116‚Üí\n   117‚Üí\n   118‚Üídef create_confidence_filter(threshold: float) -> Callable[[ModelPrediction], bool]:\n   119‚Üí    \"\"\"Create a confidence filter function.\"\"\"\n   120‚Üí    return lambda prediction: prediction.confidence >= threshold\n   121‚Üí\n   122‚Üí\n   123‚Üídef create_batch_processor(\n   124‚Üí    processor: Callable[[T], U],\n   125‚Üí    batch_size: int = 10\n   126‚Üí) -> Callable[[List[T]], List[U]]:\n   127‚Üí    \"\"\"Create batch processor from single-item processor.\"\"\"\n   128‚Üí    \n   129‚Üí    def process_batch(items: List[T]) -> List[U]:\n   130‚Üí        results = []\n   131‚Üí        for i in range(0, len(items), batch_size):\n   132‚Üí            batch = items[i:i + batch_size]\n   133‚Üí            batch_results = [processor(item) for item in batch]\n   134‚Üí            results.extend(batch_results)\n   135‚Üí        return results\n   136‚Üí    \n   137‚Üí    return process_batch\n   138‚Üí\n   139‚Üí\n   140‚Üí# Function Composition Utilities\n   141‚Üídef compose(*functions: Callable) -> Callable:\n   142‚Üí    \"\"\"Compose functions right to left (mathematical composition).\"\"\"\n   143‚Üí    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)\n   144‚Üí\n   145‚Üí\n   146‚Üídef pipe(*functions: Callable) -> Callable:\n   147‚Üí    \"\"\"Pipe functions left to right (more intuitive for data flow).\"\"\"\n   148‚Üí    return reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n   149‚Üí\n   150‚Üí\n   151‚Üí# Functional Pipeline for AI Data Processing\n   152‚Üíclass FunctionalPipeline:\n   153‚Üí    \"\"\"Functional pipeline for processing AI data.\"\"\"\n   154‚Üí    \n   155‚Üí    def __init__(self):\n   156‚Üí        self.steps: List[Callable] = []\n   157‚Üí    \n   158‚Üí    def add_step(self, func: Callable) -> 'FunctionalPipeline':\n   159‚Üí        \"\"\"Add processing step (returns new pipeline).\"\"\"\n   160‚Üí        new_pipeline = FunctionalPipeline()\n   161‚Üí        new_pipeline.steps = self.steps + [func]\n   162‚Üí        return new_pipeline\n   163‚Üí    \n   164‚Üí    def process(self, data: Any) -> Any:\n   165‚Üí        \"\"\"Process data through the pipeline.\"\"\"\n   166‚Üí        return reduce(lambda result, step: step(result), self.steps, data)\n   167‚Üí    \n   168‚Üí    def __call__(self, data: Any) -> Any:\n   169‚Üí        \"\"\"Make pipeline callable.\"\"\"\n   170‚Üí        return self.process(data)\n   171‚Üí\n   172‚Üí\n   173‚Üí# Currying and Partial Application\n   174‚Üídef curry_model_predict(\n   175‚Üí    model_name: str,\n   176‚Üí    temperature: float\n   177‚Üí) -> Callable[[str], ModelPrediction]:\n   178‚Üí    \"\"\"Curried model prediction function.\"\"\"\n   179‚Üí    \n   180‚Üí    def predict(text: str) -> ModelPrediction:\n   181‚Üí        # Mock prediction logic\n   182‚Üí        import hashlib\n   183‚Üí        confidence = 0.8 + (hash(text) % 20) / 100  # Deterministic \"randomness\"\n   184‚Üí        \n   185‚Üí        return ModelPrediction(\n   186‚Üí            text=text,\n   187‚Üí            confidence=confidence,\n   188‚Üí            model_used=model_name,\n   189‚Üí            processing_time=0.1,\n   190‚Üí            metadata={\"temperature\": temperature}\n   191‚Üí        )\n   192‚Üí    \n   193‚Üí    return predict\n   194‚Üí\n   195‚Üí\n   196‚Üí# Monadic-style Error Handling\n   197‚Üíclass Result:\n   198‚Üí    \"\"\"Monadic Result type for error handling.\"\"\"\n   199‚Üí    \n   200‚Üí    def __init__(self, value: Any = None, error: str = None):\n   201‚Üí        self.value = value\n   202‚Üí        self.error = error\n   203‚Üí        self.is_success = error is None\n   204‚Üí    \n   205‚Üí    @classmethod\n   206‚Üí    def success(cls, value: Any) -> 'Result':\n   207‚Üí        \"\"\"Create successful result.\"\"\"\n   208‚Üí        return cls(value=value)\n   209‚Üí    \n   210‚Üí    @classmethod\n   211‚Üí    def failure(cls, error: str) -> 'Result':\n   212‚Üí        \"\"\"Create failed result.\"\"\"\n   213‚Üí        return cls(error=error)\n   214‚Üí    \n   215‚Üí    def map(self, func: Callable) -> 'Result':\n   216‚Üí        \"\"\"Apply function if successful.\"\"\"\n   217‚Üí        if self.is_success:\n   218‚Üí            try:\n   219‚Üí                return Result.success(func(self.value))\n   220‚Üí            except Exception as e:\n   221‚Üí                return Result.failure(str(e))\n   222‚Üí        return self\n   223‚Üí    \n   224‚Üí    def flat_map(self, func: Callable) -> 'Result':\n   225‚Üí        \"\"\"Apply function that returns Result if successful.\"\"\"\n   226‚Üí        if self.is_success:\n   227‚Üí            try:\n   228‚Üí                return func(self.value)\n   229‚Üí            except Exception as e:\n   230‚Üí                return Result.failure(str(e))\n   231‚Üí        return self\n   232‚Üí    \n   233‚Üí    def filter(self, predicate: Callable[[Any], bool], error_msg: str = \"Filter failed\") -> 'Result':\n   234‚Üí        \"\"\"Filter result based on predicate.\"\"\"\n   235‚Üí        if self.is_success and predicate(self.value):\n   236‚Üí            return self\n   237‚Üí        return Result.failure(error_msg if self.is_success else self.error)\n   238‚Üí    \n   239‚Üí    def get_or_else(self, default: Any) -> Any:\n   240‚Üí        \"\"\"Get value or return default.\"\"\"\n   241‚Üí        return self.value if self.is_success else default\n   242‚Üí\n   243‚Üí\n   244‚Üídef safe_divide(a: float, b: float) -> Result:\n   245‚Üí    \"\"\"Safe division that returns Result.\"\"\"\n   246‚Üí    if b == 0:\n   247‚Üí        return Result.failure(\"Division by zero\")\n   248‚Üí    return Result.success(a / b)\n   249‚Üí\n   250‚Üí\n   251‚Üídef safe_model_predict(text: str) -> Result:\n   252‚Üí    \"\"\"Safe model prediction that can fail.\"\"\"\n   253‚Üí    if not text.strip():\n   254‚Üí        return Result.failure(\"Empty input text\")\n   255‚Üí    \n   256‚Üí    if len(text) > 1000:\n   257‚Üí        return Result.failure(\"Text too long\")\n   258‚Üí    \n   259‚Üí    # Mock successful prediction\n   260‚Üí    prediction = ModelPrediction(\n   261‚Üí        text=text,\n   262‚Üí        confidence=0.85,\n   263‚Üí        model_used=\"safe-model\",\n   264‚Üí        processing_time=0.1,\n   265‚Üí        metadata={}\n   266‚Üí    )\n   267‚Üí    \n   268‚Üí    return Result.success(prediction)\n   269‚Üí\n   270‚Üí\n   271‚Üí# Lazy Evaluation and Generators\n   272‚Üídef lazy_text_generator(texts: List[str]) -> Iterator[Dict[str, Any]]:\n   273‚Üí    \"\"\"Lazily process texts and yield results.\"\"\"\n   274‚Üí    for text in texts:\n   275‚Üí        # Only compute when needed\n   276‚Üí        yield {\n   277‚Üí            \"text\": text,\n   278‚Üí            \"stats\": calculate_text_stats(text),\n   279‚Üí            \"processed_at\": time.time()\n   280‚Üí        }\n   281‚Üí\n   282‚Üí\n   283‚Üídef create_lazy_pipeline(\n   284‚Üí    data_source: Iterable[T],\n   285‚Üí    *processors: Callable[[T], T]\n   286‚Üí) -> Iterator[T]:\n   287‚Üí    \"\"\"Create lazy processing pipeline.\"\"\"\n   288‚Üí    for item in data_source:\n   289‚Üí        result = item\n   290‚Üí        for processor in processors:\n   291‚Üí            result = processor(result)\n   292‚Üí        yield result\n   293‚Üí\n   294‚Üí\n   295‚Üí# Memoization for Expensive Operations\n   296‚Üí@lru_cache(maxsize=128)\n   297‚Üídef expensive_text_analysis(text: str) -> Dict[str, Any]:\n   298‚Üí    \"\"\"Expensive analysis with memoization.\"\"\"\n   299‚Üí    # Simulate expensive computation\n   300‚Üí    time.sleep(0.01)\n   301‚Üí    \n   302‚Üí    tokens = tokenize_text(text)\n   303‚Üí    return {\n   304‚Üí        \"complexity_score\": len(set(tokens)) / len(tokens) if tokens else 0,\n   305‚Üí        \"sentiment_score\": hash(text) % 100 / 100,\n   306‚Üí        \"readability\": len(tokens) / text.count('.') if '.' in text else len(tokens)\n   307‚Üí    }\n   308‚Üí\n   309‚Üí\n   310‚Üí# Functional Decorators\n   311‚Üídef timing_decorator(func: Callable) -> Callable:\n   312‚Üí    \"\"\"Decorator to time function execution.\"\"\"\n   313‚Üí    @wraps(func)\n   314‚Üí    def wrapper(*args, **kwargs):\n   315‚Üí        start_time = time.time()\n   316‚Üí        result = func(*args, **kwargs)\n   317‚Üí        end_time = time.time()\n   318‚Üí        \n   319‚Üí        console.print(f\"[dim]‚è±Ô∏è  {func.__name__} took {end_time - start_time:.4f}s[/dim]\")\n   320‚Üí        return result\n   321‚Üí    return wrapper\n   322‚Üí\n   323‚Üí\n   324‚Üídef retry_decorator(max_attempts: int = 3) -> Callable:\n   325‚Üí    \"\"\"Decorator to retry failed function calls.\"\"\"\n   326‚Üí    def decorator(func: Callable) -> Callable:\n   327‚Üí        @wraps(func)\n   328‚Üí        def wrapper(*args, **kwargs):\n   329‚Üí            last_exception = None\n   330‚Üí            \n   331‚Üí            for attempt in range(max_attempts):\n   332‚Üí                try:\n   333‚Üí                    return func(*args, **kwargs)\n   334‚Üí                except Exception as e:\n   335‚Üí                    last_exception = e\n   336‚Üí                    if attempt < max_attempts - 1:\n   337‚Üí                        console.print(f\"[yellow]Attempt {attempt + 1} failed, retrying...[/yellow]\")\n   338‚Üí            \n   339‚Üí            raise last_exception\n   340‚Üí        return wrapper\n   341‚Üí    return decorator\n   342‚Üí\n   343‚Üí\n   344‚Üí# Single Dispatch for Different Data Types\n   345‚Üí@singledispatch\n   346‚Üídef process_input(data) -> str:\n   347‚Üí    \"\"\"Process input data (default implementation).\"\"\"\n   348‚Üí    return f\"Processed unknown type: {type(data).__name__}\"\n   349‚Üí\n   350‚Üí\n   351‚Üí@process_input.register\n   352‚Üídef _(data: str) -> str:\n   353‚Üí    \"\"\"Process string input.\"\"\"\n   354‚Üí    return f\"String processed: {data.upper()}\"\n   355‚Üí\n   356‚Üí\n   357‚Üí@process_input.register\n   358‚Üídef _(data: list) -> str:\n   359‚Üí    \"\"\"Process list input.\"\"\"\n   360‚Üí    return f\"List processed: {len(data)} items\"\n   361‚Üí\n   362‚Üí\n   363‚Üí@process_input.register\n   364‚Üídef _(data: dict) -> str:\n   365‚Üí    \"\"\"Process dictionary input.\"\"\"\n   366‚Üí    return f\"Dict processed: {list(data.keys())}\"\n   367‚Üí\n   368‚Üí\n   369‚Üíasync def demonstrate_immutable_data():\n   370‚Üí    \"\"\"Show immutable data structures.\"\"\"\n   371‚Üí    \n   372‚Üí    console.print(Panel(\"üîí Immutable Data Structures\", style=\"bold blue\"))\n   373‚Üí    \n   374‚Üí    # Create immutable config\n   375‚Üí    base_config = ImmutableConfig(\n   376‚Üí        model_name=\"gpt-4\",\n   377‚Üí        temperature=0.7,\n   378‚Üí        max_tokens=1000,\n   379‚Üí        provider=\"openai\"\n   380‚Üí    )\n   381‚Üí    \n   382‚Üí    console.print(\"[bold yellow]Original config:[/bold yellow]\")\n   383‚Üí    console.print(f\"  Model: {base_config.model_name}\")\n   384‚Üí    console.print(f\"  Temperature: {base_config.temperature}\")\n   385‚Üí    console.print(f\"  Provider: {base_config.provider}\")\n   386‚Üí    \n   387‚Üí    # Create variations without mutating original\n   388‚Üí    hot_config = base_config.with_temperature(1.2)\n   389‚Üí    anthropic_config = base_config.with_provider(\"anthropic\", \"api-key-123\")\n   390‚Üí    \n   391‚Üí    console.print(f\"\\n[bold yellow]Hot config (temp={hot_config.temperature}):[/bold yellow]\")\n   392‚Üí    console.print(f\"  Original unchanged: {base_config.temperature}\")\n   393‚Üí    \n   394‚Üí    console.print(f\"\\n[bold yellow]Anthropic config:[/bold yellow]\")\n   395‚Üí    console.print(f\"  Provider: {anthropic_config.provider}\")\n   396‚Üí    console.print(f\"  Original provider: {base_config.provider}\")\n   397‚Üí    \n   398‚Üí    # Show immutable predictions\n   399‚Üí    prediction = ModelPrediction(\n   400‚Üí        text=\"Hello world\",\n   401‚Üí        confidence=0.85,\n   402‚Üí        model_used=\"gpt-4\",\n   403‚Üí        processing_time=0.1,\n   404‚Üí        metadata={\"source\": \"test\"}\n   405‚Üí    )\n   406‚Üí    \n   407‚Üí    # Transform without mutation\n   408‚Üí    upper_prediction = prediction.transform_text(str.upper)\n   409‚Üí    filtered_prediction = prediction.with_confidence_threshold(0.9)\n   410‚Üí    \n   411‚Üí    console.print(f\"\\n[bold cyan]Prediction transformations:[/bold cyan]\")\n   412‚Üí    console.print(f\"  Original: '{prediction.text}'\")\n   413‚Üí    console.print(f\"  Uppercase: '{upper_prediction.text}'\")\n   414‚Üí    console.print(f\"  Filtered (>0.9): {filtered_prediction}\")\n   415‚Üí\n   416‚Üí\n   417‚Üíasync def demonstrate_pure_functions():\n   418‚Üí    \"\"\"Show pure function benefits.\"\"\"\n   419‚Üí    \n   420‚Üí    console.print(Panel(\"‚ú® Pure Functions\", style=\"bold green\"))\n   421‚Üí    \n   422‚Üí    test_text = \"The Quick Brown Fox Jumps Over The Lazy Dog\"\n   423‚Üí    \n   424‚Üí    console.print(f\"[bold yellow]Processing: '{test_text}'[/bold yellow]\")\n   425‚Üí    \n   426‚Üí    # Pure functions are predictable and testable\n   427‚Üí    tokens = tokenize_text(test_text)\n   428‚Üí    filtered = filter_tokens(tokens, min_length=3)\n   429‚Üí    stats = calculate_text_stats(test_text)\n   430‚Üí    \n   431‚Üí    console.print(f\"\\n[bold cyan]Pure function results:[/bold cyan]\")\n   432‚Üí    console.print(f\"  Tokens: {tokens[:5]}... ({len(tokens)} total)\")\n   433‚Üí    console.print(f\"  Filtered (len>=3): {filtered[:5]}... ({len(filtered)} total)\")\n   434‚Üí    console.print(f\"  Stats: {stats}\")\n   435‚Üí    \n   436‚Üí    # Pure functions can be easily combined\n   437‚Üí    pipeline = pipe(\n   438‚Üí        tokenize_text,\n   439‚Üí        lambda tokens: filter_tokens(tokens, 4),\n   440‚Üí        len\n   441‚Üí    )\n   442‚Üí    \n   443‚Üí    long_word_count = pipeline(test_text)\n   444‚Üí    console.print(f\"  Long words (len>=4): {long_word_count}\")\n   445‚Üí\n   446‚Üí\n   447‚Üíasync def demonstrate_higher_order_functions():\n   448‚Üí    \"\"\"Show higher-order function patterns.\"\"\"\n   449‚Üí    \n   450‚Üí    console.print(Panel(\"üöÄ Higher-Order Functions\", style=\"bold magenta\"))\n   451‚Üí    \n   452‚Üí    # Create composed text processor\n   453‚Üí    transformers = [\n   454‚Üí        str.lower,\n   455‚Üí        lambda text: text.replace(\"quick\", \"QUICK\"),\n   456‚Üí        lambda text: text.replace(\"fox\", \"ü¶ä\"),\n   457‚Üí        str.title\n   458‚Üí    ]\n   459‚Üí    \n   460‚Üí    processor = create_text_processor(transformers)\n   461‚Üí    \n   462‚Üí    test_text = \"The Quick Brown Fox\"\n   463‚Üí    result = processor(test_text)\n   464‚Üí    \n   465‚Üí    console.print(f\"[bold yellow]Text transformation pipeline:[/bold yellow]\")\n   466‚Üí    console.print(f\"  Input: '{test_text}'\")\n   467‚Üí    console.print(f\"  Output: '{result}'\")\n   468‚Üí    \n   469‚Üí    # Create specialized functions\n   470‚Üí    high_confidence_filter = create_confidence_filter(0.8)\n   471‚Üí    batch_stats_processor = create_batch_processor(calculate_text_stats, batch_size=2)\n   472‚Üí    \n   473‚Üí    # Test batch processor\n   474‚Üí    texts = [\"Hello world\", \"Python is great\", \"AI is fascinating\", \"Functional programming rocks\"]\n   475‚Üí    batch_stats = batch_stats_processor(texts)\n   476‚Üí    \n   477‚Üí    console.print(f\"\\n[bold cyan]Batch processing results:[/bold cyan]\")\n   478‚Üí    for i, stats in enumerate(batch_stats):\n   479‚Üí        console.print(f\"  Text {i+1}: {stats['word_count']} words, {stats['unique_words']} unique\")\n   480‚Üí\n   481‚Üí\n   482‚Üíasync def demonstrate_function_composition():\n   483‚Üí    \"\"\"Show function composition patterns.\"\"\"\n   484‚Üí    \n   485‚Üí    console.print(Panel(\"üîó Function Composition\", style=\"bold cyan\"))\n   486‚Üí    \n   487‚Üí    # Mathematical composition (right to left)\n   488‚Üí    text_analyzer = compose(\n   489‚Üí        lambda stats: f\"Analysis: {stats['word_count']} words, {stats['unique_words']} unique\",\n   490‚Üí        calculate_text_stats\n   491‚Üí    )\n   492‚Üí    \n   493‚Üí    # Pipe composition (left to right)\n   494‚Üí    text_pipeline = pipe(\n   495‚Üí        str.strip,\n   496‚Üí        str.lower,\n   497‚Üí        tokenize_text,\n   498‚Üí        lambda tokens: filter_tokens(tokens, 2),\n   499‚Üí        len\n   500‚Üí    )\n   501‚Üí    \n   502‚Üí    test_text = \"  The Quick Brown Fox Jumps Over The Lazy Dog  \"\n   503‚Üí    \n   504‚Üí    console.print(f\"[bold yellow]Function composition test:[/bold yellow]\")\n   505‚Üí    console.print(f\"  Input: '{test_text}'\")\n   506‚Üí    \n   507‚Üí    analysis = text_analyzer(test_text)\n   508‚Üí    filtered_count = text_pipeline(test_text)\n   509‚Üí    \n   510‚Üí    console.print(f\"  Composed analysis: {analysis}\")\n   511‚Üí    console.print(f\"  Piped processing: {filtered_count} filtered tokens\")\n   512‚Üí    \n   513‚Üí    # Using FunctionalPipeline class\n   514‚Üí    pipeline = (FunctionalPipeline()\n   515‚Üí                .add_step(str.strip)\n   516‚Üí                .add_step(str.upper)\n   517‚Üí                .add_step(lambda text: text.replace(\" \", \"_\")))\n   518‚Üí    \n   519‚Üí    pipeline_result = pipeline(test_text)\n   520‚Üí    console.print(f\"  Pipeline class result: '{pipeline_result}'\")\n   521‚Üí\n   522‚Üí\n   523‚Üíasync def demonstrate_currying_partial():\n   524‚Üí    \"\"\"Show currying and partial application.\"\"\"\n   525‚Üí    \n   526‚Üí    console.print(Panel(\"ü•¢ Currying & Partial Application\", style=\"bold red\"))\n   527‚Üí    \n   528‚Üí    # Curried model prediction\n   529‚Üí    gpt4_predictor = curry_model_predict(\"gpt-4\", 0.7)\n   530‚Üí    claude_predictor = curry_model_predict(\"claude-3\", 0.5)\n   531‚Üí    \n   532‚Üí    test_texts = [\"Hello world\", \"Python is awesome\", \"AI will change everything\"]\n   533‚Üí    \n   534‚Üí    console.print(\"[bold yellow]Curried predictions:[/bold yellow]\")\n   535‚Üí    \n   536‚Üí    for text in test_texts:\n   537‚Üí        gpt_pred = gpt4_predictor(text)\n   538‚Üí        claude_pred = claude_predictor(text)\n   539‚Üí        \n   540‚Üí        console.print(f\"  '{text}':\")\n   541‚Üí        console.print(f\"    GPT-4: {gpt_pred.confidence:.3f}\")\n   542‚Üí        console.print(f\"    Claude: {claude_pred.confidence:.3f}\")\n   543‚Üí    \n   544‚Üí    # Partial application examples\n   545‚Üí    multiply_by_10 = partial(mul, 10)\n   546‚Üí    add_100 = partial(add, 100)\n   547‚Üí    \n   548‚Üí    numbers = [1, 2, 3, 4, 5]\n   549‚Üí    multiplied = list(map(multiply_by_10, numbers))\n   550‚Üí    added = list(map(add_100, numbers))\n   551‚Üí    \n   552‚Üí    console.print(f\"\\n[bold cyan]Partial application:[/bold cyan]\")\n   553‚Üí    console.print(f\"  Original: {numbers}\")\n   554‚Üí    console.print(f\"  √ó 10: {multiplied}\")\n   555‚Üí    console.print(f\"  + 100: {added}\")\n   556‚Üí\n   557‚Üí\n   558‚Üíasync def demonstrate_monadic_error_handling():\n   559‚Üí    \"\"\"Show monadic error handling.\"\"\"\n   560‚Üí    \n   561‚Üí    console.print(Panel(\"üîÑ Monadic Error Handling\", style=\"bold yellow\"))\n   562‚Üí    \n   563‚Üí    console.print(\"[bold yellow]Safe operations with Result monad:[/bold yellow]\")\n   564‚Üí    \n   565‚Üí    # Chaining operations with error handling\n   566‚Üí    result1 = (safe_divide(10, 2)\n   567‚Üí               .map(lambda x: x * 3)\n   568‚Üí               .map(lambda x: f\"Result: {x}\")\n   569‚Üí               .filter(lambda s: \"15\" in s, \"Result doesn't contain 15\"))\n   570‚Üí    \n   571‚Üí    result2 = (safe_divide(10, 0)\n   572‚Üí               .map(lambda x: x * 3)\n   573‚Üí               .map(lambda x: f\"Result: {x}\"))\n   574‚Üí    \n   575‚Üí    console.print(f\"  Safe calculation: {result1.value if result1.is_success else result1.error}\")\n   576‚Üí    console.print(f\"  Division by zero: {result2.error}\")\n   577‚Üí    \n   578‚Üí    # Model prediction with error handling\n   579‚Üí    test_cases = [\"Hello world\", \"\", \"x\" * 1001]\n   580‚Üí    \n   581‚Üí    console.print(f\"\\n[bold cyan]Safe model predictions:[/bold cyan]\")\n   582‚Üí    \n   583‚Üí    for text in test_cases:\n   584‚Üí        result = (safe_model_predict(text)\n   585‚Üí                  .map(lambda pred: pred.transform_text(str.upper))\n   586‚Üí                  .filter(lambda pred: pred.confidence > 0.8, \"Low confidence\"))\n   587‚Üí        \n   588‚Üí        display_text = text[:20] + \"...\" if len(text) > 20 else text\n   589‚Üí        if result.is_success:\n   590‚Üí            console.print(f\"  ‚úÖ '{display_text}': confidence {result.value.confidence}\")\n   591‚Üí        else:\n   592‚Üí            console.print(f\"  ‚ùå '{display_text}': {result.error}\")\n   593‚Üí\n   594‚Üí\n   595‚Üíasync def demonstrate_lazy_evaluation():\n   596‚Üí    \"\"\"Show lazy evaluation patterns.\"\"\"\n   597‚Üí    \n   598‚Üí    console.print(Panel(\"üò¥ Lazy Evaluation\", style=\"bold purple\"))\n   599‚Üí    \n   600‚Üí    # Create large dataset\n   601‚Üí    large_texts = [f\"Sample text number {i}\" for i in range(1000)]\n   602‚Üí    \n   603‚Üí    console.print(\"[bold yellow]Lazy text processing:[/bold yellow]\")\n   604‚Üí    \n   605‚Üí    # Process only what we need\n   606‚Üí    lazy_processor = lazy_text_generator(large_texts[:5])\n   607‚Üí    \n   608‚Üí    for i, processed in enumerate(lazy_processor):\n   609‚Üí        if i >= 3:  # Only process first 3\n   610‚Üí            break\n   611‚Üí        console.print(f\"  Processed: {processed['text']} ({processed['stats']['word_count']} words)\")\n   612‚Üí    \n   613‚Üí    # Lazy pipeline with generators\n   614‚Üí    def slow_processor(text: str) -> str:\n   615‚Üí        time.sleep(0.001)  # Simulate slow processing\n   616‚Üí        return text.upper()\n   617‚Üí    \n   618‚Üí    def add_prefix(text: str) -> str:\n   619‚Üí        return f\"PROCESSED: {text}\"\n   620‚Üí    \n   621‚Üí    console.print(f\"\\n[bold cyan]Lazy pipeline (processing on demand):[/bold cyan]\")\n   622‚Üí    \n   623‚Üí    lazy_pipeline = create_lazy_pipeline(\n   624‚Üí        large_texts[:10],\n   625‚Üí        slow_processor,\n   626‚Üí        add_prefix\n   627‚Üí    )\n   628‚Üí    \n   629‚Üí    # Only consume what we need\n   630‚Üí    first_three = []\n   631‚Üí    for i, result in enumerate(lazy_pipeline):\n   632‚Üí        first_three.append(result)\n   633‚Üí        if i >= 2:\n   634‚Üí            break\n   635‚Üí    \n   636‚Üí    for result in first_three:\n   637‚Üí        console.print(f\"  {result}\")\n   638‚Üí\n   639‚Üí\n   640‚Üíasync def demonstrate_memoization():\n   641‚Üí    \"\"\"Show memoization for performance.\"\"\"\n   642‚Üí    \n   643‚Üí    console.print(Panel(\"üß† Memoization\", style=\"bold orange\"))\n   644‚Üí    \n   645‚Üí    test_texts = [\n   646‚Üí        \"This is a test text for analysis\",\n   647‚Üí        \"Another sample text for processing\",\n   648‚Üí        \"This is a test text for analysis\",  # Duplicate\n   649‚Üí        \"Final text sample for testing\"\n   650‚Üí    ]\n   651‚Üí    \n   652‚Üí    console.print(\"[bold yellow]Memoized expensive analysis:[/bold yellow]\")\n   653‚Üí    \n   654‚Üí    # First run - will be slow\n   655‚Üí    start_time = time.time()\n   656‚Üí    for text in test_texts:\n   657‚Üí        analysis = expensive_text_analysis(text)\n   658‚Üí        console.print(f\"  {text[:30]}... ‚Üí complexity: {analysis['complexity_score']:.3f}\")\n   659‚Üí    first_run_time = time.time() - start_time\n   660‚Üí    \n   661‚Üí    console.print(f\"\\n[bold cyan]Cache info: {expensive_text_analysis.cache_info()}[/bold cyan]\")\n   662‚Üí    \n   663‚Üí    # Second run - should be faster due to caching\n   664‚Üí    start_time = time.time()\n   665‚Üí    for text in test_texts:\n   666‚Üí        analysis = expensive_text_analysis(text)\n   667‚Üí    second_run_time = time.time() - start_time\n   668‚Üí    \n   669‚Üí    console.print(f\"\\n[bold green]Performance comparison:[/bold green]\")\n   670‚Üí    console.print(f\"  First run: {first_run_time:.4f}s\")\n   671‚Üí    console.print(f\"  Second run: {second_run_time:.4f}s\")\n   672‚Üí    console.print(f\"  Speedup: {first_run_time / second_run_time:.1f}x\")\n   673‚Üí\n   674‚Üí\n   675‚Üíasync def demonstrate_decorators():\n   676‚Üí    \"\"\"Show functional decorators.\"\"\"\n   677‚Üí    \n   678‚Üí    console.print(Panel(\"üé® Functional Decorators\", style=\"bold teal\"))\n   679‚Üí    \n   680‚Üí    @timing_decorator\n   681‚Üí    @retry_decorator(max_attempts=2)\n   682‚Üí    def flaky_process(text: str, fail_rate: float = 0.5) -> str:\n   683‚Üí        \"\"\"Function that sometimes fails.\"\"\"\n   684‚Üí        import random\n   685‚Üí        if random.random() < fail_rate:\n   686‚Üí            raise ValueError(\"Simulated processing error\")\n   687‚Üí        return f\"Successfully processed: {text}\"\n   688‚Üí    \n   689‚Üí    console.print(\"[bold yellow]Testing decorated functions:[/bold yellow]\")\n   690‚Üí    \n   691‚Üí    try:\n   692‚Üí        result = flaky_process(\"test data\", fail_rate=0.3)\n   693‚Üí        console.print(f\"  ‚úÖ {result}\")\n   694‚Üí    except Exception as e:\n   695‚Üí        console.print(f\"  ‚ùå Failed: {e}\")\n   696‚Üí\n   697‚Üí\n   698‚Üíasync def demonstrate_single_dispatch():\n   699‚Üí    \"\"\"Show single dispatch patterns.\"\"\"\n   700‚Üí    \n   701‚Üí    console.print(Panel(\"üéØ Single Dispatch\", style=\"bold indigo\"))\n   702‚Üí    \n   703‚Üí    test_data = [\n   704‚Üí        \"Hello world\",\n   705‚Üí        [1, 2, 3, 4, 5],\n   706‚Üí        {\"name\": \"John\", \"age\": 30},\n   707‚Üí        42\n   708‚Üí    ]\n   709‚Üí    \n   710‚Üí    console.print(\"[bold yellow]Single dispatch processing:[/bold yellow]\")\n   711‚Üí    \n   712‚Üí    for data in test_data:\n   713‚Üí        result = process_input(data)\n   714‚Üí        console.print(f\"  {type(data).__name__}: {result}\")\n   715‚Üí\n   716‚Üí\n   717‚Üídef interactive_exercise():\n   718‚Üí    \"\"\"Interactive exercise for functional programming.\"\"\"\n   719‚Üí    \n   720‚Üí    console.print(Panel(\"üéÆ Interactive Exercise: Functional AI Pipeline\", style=\"bold magenta\"))\n   721‚Üí    \n   722‚Üí    scenarios = [\n   723‚Üí        {\n   724‚Üí            \"task\": \"Text Classification Pipeline\",\n   725‚Üí            \"requirements\": [\n   726‚Üí                \"Immutable data structures for configurations\",\n   727‚Üí                \"Pure functions for text preprocessing\",\n   728‚Üí                \"Functional composition for pipeline stages\",\n   729‚Üí                \"Error handling with Result monad\"\n   730‚Üí            ],\n   731‚Üí            \"challenge\": \"Build a functional text classification system\"\n   732‚Üí        },\n   733‚Üí        {\n   734‚Üí            \"task\": \"Model Ensemble System\",\n   735‚Üí            \"requirements\": [\n   736‚Üí                \"Curried prediction functions for different models\",\n   737‚Üí                \"Lazy evaluation for expensive computations\",\n   738‚Üí                \"Memoization for repeated inputs\",\n   739‚Üí                \"Higher-order functions for combining predictions\"\n   740‚Üí            ],\n   741‚Üí            \"challenge\": \"Create a functional ensemble prediction system\"\n   742‚Üí        },\n   743‚Üí        {\n   744‚Üí            \"task\": \"Stream Processing Engine\",\n   745‚Üí            \"requirements\": [\n   746‚Üí                \"Generator-based lazy processing\",\n   747‚Üí                \"Functional transformations on data streams\",\n   748‚Üí                \"Immutable state management\",\n   749‚Üí                \"Composable stream operations\"\n   750‚Üí            ],\n   751‚Üí            \"challenge\": \"Design a functional stream processing architecture\"\n   752‚Üí        }\n   753‚Üí    ]\n   754‚Üí    \n   755‚Üí    for i, scenario in enumerate(scenarios, 1):\n   756‚Üí        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['task']}[/bold cyan]\")\n   757‚Üí        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n   758‚Üí        for req in scenario[\"requirements\"]:\n   759‚Üí            console.print(f\"  ‚Ä¢ {req}\")\n   760‚Üí        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n   761‚Üí        console.print(\"\\n[bold green]Functional Patterns to Consider:[/bold green]\")\n   762‚Üí        console.print(\"  ‚Ä¢ Immutable data structures\")\n   763‚Üí        console.print(\"  ‚Ä¢ Pure functions and composition\")\n   764‚Üí        console.print(\"  ‚Ä¢ Higher-order functions\")\n   765‚Üí        console.print(\"  ‚Ä¢ Monadic error handling\")\n   766‚Üí        console.print(\"  ‚Ä¢ Lazy evaluation\")\n   767‚Üí        console.print(\"  ‚Ä¢ Memoization\")\n   768‚Üí        console.print(\"---\")\n   769‚Üí\n   770‚Üí\n   771‚Üíif __name__ == \"__main__\":\n   772‚Üí    console.print(\"[bold blue]üöÄ AI Architecture Patterns: Functional Programming[/bold blue]\")\n   773‚Üí    console.print(\"Learn functional patterns for predictable AI systems\\n\")\n   774‚Üí    \n   775‚Üí    async def main():\n   776‚Üí        await demonstrate_immutable_data()\n   777‚Üí        console.print(\"\\n\" + \"=\"*60)\n   778‚Üí        \n   779‚Üí        await demonstrate_pure_functions()\n   780‚Üí        console.print(\"\\n\" + \"=\"*60)\n   781‚Üí        \n   782‚Üí        await demonstrate_higher_order_functions()\n   783‚Üí        console.print(\"\\n\" + \"=\"*60)\n   784‚Üí        \n   785‚Üí        await demonstrate_function_composition()\n   786‚Üí        console.print(\"\\n\" + \"=\"*60)\n   787‚Üí        \n   788‚Üí        await demonstrate_currying_partial()\n   789‚Üí        console.print(\"\\n\" + \"=\"*60)\n   790‚Üí        \n   791‚Üí        await demonstrate_monadic_error_handling()\n   792‚Üí        console.print(\"\\n\" + \"=\"*60)\n   793‚Üí        \n   794‚Üí        await demonstrate_lazy_evaluation()\n   795‚Üí        console.print(\"\\n\" + \"=\"*60)\n   796‚Üí        \n   797‚Üí        await demonstrate_memoization()\n   798‚Üí        console.print(\"\\n\" + \"=\"*60)\n   799‚Üí        \n   800‚Üí        await demonstrate_decorators()\n   801‚Üí        console.print(\"\\n\" + \"=\"*60)\n   802‚Üí        \n   803‚Üí        await demonstrate_single_dispatch()\n   804‚Üí        console.print(\"\\n\" + \"=\"*60)\n   805‚Üí        \n   806‚Üí        interactive_exercise()\n   807‚Üí        \n   808‚Üí        console.print(f\"\\n[bold green]‚úÖ Completed![/bold green] Next: Run example 04_metaprogramming.py\")\n   809‚Üí        console.print(\"üí° Pro tip: Functional patterns make AI code more predictable and testable!\")\n   810‚Üí    \n   811‚Üí    import asyncio\n   812‚Üí    asyncio.run(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"722d7593-5292-4232-9e8f-1701deef8475","timestamp":"2025-08-01T17:59:54.467Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/03_functional_programming.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 3: Functional Programming for AI Systems\n\nLearn functional programming patterns that make AI code more predictable,\ntestable, and maintainable through immutability, pure functions, and composition.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/03_functional_programming.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import (\n    Callable, List, Dict, Any, Optional, Tuple, Union,\n    TypeVar, Iterator, Iterable\n)\nfrom functools import reduce, partial, singledispatch, lru_cache, wraps\nfrom itertools import chain, groupby, accumulate\nfrom dataclasses import dataclass, replace\nfrom operator import add, mul, itemgetter\nimport time\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\nfrom common import Message\n\nconsole = Console()\n\n# üéØ LEARNING GOAL: Master functional programming for AI systems\n\nT = TypeVar('T')\nU = TypeVar('U')\n\n\n# Immutable Data Structures for AI\n@dataclass(frozen=True)\nclass ImmutableConfig:\n    \"\"\"Immutable configuration for AI models.\"\"\"\n    model_name: str\n    temperature: float\n    max_tokens: int\n    provider: str\n    api_key: Optional[str] = None\n    \n    def with_temperature(self, temperature: float) -> 'ImmutableConfig':\n        \"\"\"Create new config with different temperature.\"\"\"\n        return replace(self, temperature=temperature)\n    \n    def with_provider(self, provider: str, api_key: Optional[str] = None) -> 'ImmutableConfig':\n        \"\"\"Create new config with different provider.\"\"\"\n        return replace(self, provider=provider, api_key=api_key)\n\n\n@dataclass(frozen=True)\nclass ModelPrediction:\n    \"\"\"Immutable prediction result.\"\"\"\n    text: str\n    confidence: float\n    model_used: str\n    processing_time: float\n    metadata: Dict[str, Any]\n    \n    def with_confidence_threshold(self, threshold: float) -> Optional['ModelPrediction']:\n        \"\"\"Return prediction only if confidence meets threshold.\"\"\"\n        return self if self.confidence >= threshold else None\n    \n    def transform_text(self, transformer: Callable[[str], str]) -> 'ModelPrediction':\n        \"\"\"Transform text while preserving other fields.\"\"\"\n        return replace(self, text=transformer(self.text))\n\n\n# Pure Functions for Text Processing\ndef tokenize_text(text: str) -> List[str]:\n    \"\"\"Pure function to tokenize text.\"\"\"\n    return text.lower().split()\n\n\ndef count_tokens(tokens: List[str]) -> int:\n    \"\"\"Pure function to count tokens.\"\"\"\n    return len(tokens)\n\n\ndef filter_tokens(tokens: List[str], min_length: int = 2) -> List[str]:\n    \"\"\"Pure function to filter tokens by length.\"\"\"\n    return [token for token in tokens if len(token) >= min_length]\n\n\ndef calculate_text_stats(text: str) -> Dict[str, Any]:\n    \"\"\"Pure function to calculate text statistics.\"\"\"\n    tokens = tokenize_text(text)\n    filtered_tokens = filter_tokens(tokens)\n    \n    return {\n        \"character_count\": len(text),\n        \"word_count\": len(tokens),\n        \"unique_words\": len(set(tokens)),\n        \"filtered_words\": len(filtered_tokens),\n        \"avg_word_length\": sum(len(token) for token in tokens) / len(tokens) if tokens else 0\n    }\n\n\n# Higher-Order Functions for AI Processing\ndef create_text_processor(\n    transformers: List[Callable[[str], str]]\n) -> Callable[[str], str]:\n    \"\"\"Create composed text processor from list of transformations.\"\"\"\n    \n    def process(text: str) -> str:\n        return reduce(lambda t, transformer: transformer(t), transformers, text)\n    \n    return process\n\n\ndef create_confidence_filter(threshold: float) -> Callable[[ModelPrediction], bool]:\n    \"\"\"Create a confidence filter function.\"\"\"\n    return lambda prediction: prediction.confidence >= threshold\n\n\ndef create_batch_processor(\n    processor: Callable[[T], U],\n    batch_size: int = 10\n) -> Callable[[List[T]], List[U]]:\n    \"\"\"Create batch processor from single-item processor.\"\"\"\n    \n    def process_batch(items: List[T]) -> List[U]:\n        results = []\n        for i in range(0, len(items), batch_size):\n            batch = items[i:i + batch_size]\n            batch_results = [processor(item) for item in batch]\n            results.extend(batch_results)\n        return results\n    \n    return process_batch\n\n\n# Function Composition Utilities\ndef compose(*functions: Callable) -> Callable:\n    \"\"\"Compose functions right to left (mathematical composition).\"\"\"\n    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)\n\n\ndef pipe(*functions: Callable) -> Callable:\n    \"\"\"Pipe functions left to right (more intuitive for data flow).\"\"\"\n    return reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n\n\n# Functional Pipeline for AI Data Processing\nclass FunctionalPipeline:\n    \"\"\"Functional pipeline for processing AI data.\"\"\"\n    \n    def __init__(self):\n        self.steps: List[Callable] = []\n    \n    def add_step(self, func: Callable) -> 'FunctionalPipeline':\n        \"\"\"Add processing step (returns new pipeline).\"\"\"\n        new_pipeline = FunctionalPipeline()\n        new_pipeline.steps = self.steps + [func]\n        return new_pipeline\n    \n    def process(self, data: Any) -> Any:\n        \"\"\"Process data through the pipeline.\"\"\"\n        return reduce(lambda result, step: step(result), self.steps, data)\n    \n    def __call__(self, data: Any) -> Any:\n        \"\"\"Make pipeline callable.\"\"\"\n        return self.process(data)\n\n\n# Currying and Partial Application\ndef curry_model_predict(\n    model_name: str,\n    temperature: float\n) -> Callable[[str], ModelPrediction]:\n    \"\"\"Curried model prediction function.\"\"\"\n    \n    def predict(text: str) -> ModelPrediction:\n        # Mock prediction logic\n        import hashlib\n        confidence = 0.8 + (hash(text) % 20) / 100  # Deterministic \"randomness\"\n        \n        return ModelPrediction(\n            text=text,\n            confidence=confidence,\n            model_used=model_name,\n            processing_time=0.1,\n            metadata={\"temperature\": temperature}\n        )\n    \n    return predict\n\n\n# Monadic-style Error Handling\nclass Result:\n    \"\"\"Monadic Result type for error handling.\"\"\"\n    \n    def __init__(self, value: Any = None, error: str = None):\n        self.value = value\n        self.error = error\n        self.is_success = error is None\n    \n    @classmethod\n    def success(cls, value: Any) -> 'Result':\n        \"\"\"Create successful result.\"\"\"\n        return cls(value=value)\n    \n    @classmethod\n    def failure(cls, error: str) -> 'Result':\n        \"\"\"Create failed result.\"\"\"\n        return cls(error=error)\n    \n    def map(self, func: Callable) -> 'Result':\n        \"\"\"Apply function if successful.\"\"\"\n        if self.is_success:\n            try:\n                return Result.success(func(self.value))\n            except Exception as e:\n                return Result.failure(str(e))\n        return self\n    \n    def flat_map(self, func: Callable) -> 'Result':\n        \"\"\"Apply function that returns Result if successful.\"\"\"\n        if self.is_success:\n            try:\n                return func(self.value)\n            except Exception as e:\n                return Result.failure(str(e))\n        return self\n    \n    def filter(self, predicate: Callable[[Any], bool], error_msg: str = \"Filter failed\") -> 'Result':\n        \"\"\"Filter result based on predicate.\"\"\"\n        if self.is_success and predicate(self.value):\n            return self\n        return Result.failure(error_msg if self.is_success else self.error)\n    \n    def get_or_else(self, default: Any) -> Any:\n        \"\"\"Get value or return default.\"\"\"\n        return self.value if self.is_success else default\n\n\ndef safe_divide(a: float, b: float) -> Result:\n    \"\"\"Safe division that returns Result.\"\"\"\n    if b == 0:\n        return Result.failure(\"Division by zero\")\n    return Result.success(a / b)\n\n\ndef safe_model_predict(text: str) -> Result:\n    \"\"\"Safe model prediction that can fail.\"\"\"\n    if not text.strip():\n        return Result.failure(\"Empty input text\")\n    \n    if len(text) > 1000:\n        return Result.failure(\"Text too long\")\n    \n    # Mock successful prediction\n    prediction = ModelPrediction(\n        text=text,\n        confidence=0.85,\n        model_used=\"safe-model\",\n        processing_time=0.1,\n        metadata={}\n    )\n    \n    return Result.success(prediction)\n\n\n# Lazy Evaluation and Generators\ndef lazy_text_generator(texts: List[str]) -> Iterator[Dict[str, Any]]:\n    \"\"\"Lazily process texts and yield results.\"\"\"\n    for text in texts:\n        # Only compute when needed\n        yield {\n            \"text\": text,\n            \"stats\": calculate_text_stats(text),\n            \"processed_at\": time.time()\n        }\n\n\ndef create_lazy_pipeline(\n    data_source: Iterable[T],\n    *processors: Callable[[T], T]\n) -> Iterator[T]:\n    \"\"\"Create lazy processing pipeline.\"\"\"\n    for item in data_source:\n        result = item\n        for processor in processors:\n            result = processor(result)\n        yield result\n\n\n# Memoization for Expensive Operations\n@lru_cache(maxsize=128)\ndef expensive_text_analysis(text: str) -> Dict[str, Any]:\n    \"\"\"Expensive analysis with memoization.\"\"\"\n    # Simulate expensive computation\n    time.sleep(0.01)\n    \n    tokens = tokenize_text(text)\n    return {\n        \"complexity_score\": len(set(tokens)) / len(tokens) if tokens else 0,\n        \"sentiment_score\": hash(text) % 100 / 100,\n        \"readability\": len(tokens) / text.count('.') if '.' in text else len(tokens)\n    }\n\n\n# Functional Decorators\ndef timing_decorator(func: Callable) -> Callable:\n    \"\"\"Decorator to time function execution.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        \n        console.print(f\"[dim]‚è±Ô∏è  {func.__name__} took {end_time - start_time:.4f}s[/dim]\")\n        return result\n    return wrapper\n\n\ndef retry_decorator(max_attempts: int = 3) -> Callable:\n    \"\"\"Decorator to retry failed function calls.\"\"\"\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    last_exception = e\n                    if attempt < max_attempts - 1:\n                        console.print(f\"[yellow]Attempt {attempt + 1} failed, retrying...[/yellow]\")\n            \n            raise last_exception\n        return wrapper\n    return decorator\n\n\n# Single Dispatch for Different Data Types\n@singledispatch\ndef process_input(data) -> str:\n    \"\"\"Process input data (default implementation).\"\"\"\n    return f\"Processed unknown type: {type(data).__name__}\"\n\n\n@process_input.register\ndef _(data: str) -> str:\n    \"\"\"Process string input.\"\"\"\n    return f\"String processed: {data.upper()}\"\n\n\n@process_input.register\ndef _(data: list) -> str:\n    \"\"\"Process list input.\"\"\"\n    return f\"List processed: {len(data)} items\"\n\n\n@process_input.register\ndef _(data: dict) -> str:\n    \"\"\"Process dictionary input.\"\"\"\n    return f\"Dict processed: {list(data.keys())}\"\n\n\nasync def demonstrate_immutable_data():\n    \"\"\"Show immutable data structures.\"\"\"\n    \n    console.print(Panel(\"üîí Immutable Data Structures\", style=\"bold blue\"))\n    \n    # Create immutable config\n    base_config = ImmutableConfig(\n        model_name=\"gpt-4\",\n        temperature=0.7,\n        max_tokens=1000,\n        provider=\"openai\"\n    )\n    \n    console.print(\"[bold yellow]Original config:[/bold yellow]\")\n    console.print(f\"  Model: {base_config.model_name}\")\n    console.print(f\"  Temperature: {base_config.temperature}\")\n    console.print(f\"  Provider: {base_config.provider}\")\n    \n    # Create variations without mutating original\n    hot_config = base_config.with_temperature(1.2)\n    anthropic_config = base_config.with_provider(\"anthropic\", \"api-key-123\")\n    \n    console.print(f\"\\n[bold yellow]Hot config (temp={hot_config.temperature}):[/bold yellow]\")\n    console.print(f\"  Original unchanged: {base_config.temperature}\")\n    \n    console.print(f\"\\n[bold yellow]Anthropic config:[/bold yellow]\")\n    console.print(f\"  Provider: {anthropic_config.provider}\")\n    console.print(f\"  Original provider: {base_config.provider}\")\n    \n    # Show immutable predictions\n    prediction = ModelPrediction(\n        text=\"Hello world\",\n        confidence=0.85,\n        model_used=\"gpt-4\",\n        processing_time=0.1,\n        metadata={\"source\": \"test\"}\n    )\n    \n    # Transform without mutation\n    upper_prediction = prediction.transform_text(str.upper)\n    filtered_prediction = prediction.with_confidence_threshold(0.9)\n    \n    console.print(f\"\\n[bold cyan]Prediction transformations:[/bold cyan]\")\n    console.print(f\"  Original: '{prediction.text}'\")\n    console.print(f\"  Uppercase: '{upper_prediction.text}'\")\n    console.print(f\"  Filtered (>0.9): {filtered_prediction}\")\n\n\nasync def demonstrate_pure_functions():\n    \"\"\"Show pure function benefits.\"\"\"\n    \n    console.print(Panel(\"‚ú® Pure Functions\", style=\"bold green\"))\n    \n    test_text = \"The Quick Brown Fox Jumps Over The Lazy Dog\"\n    \n    console.print(f\"[bold yellow]Processing: '{test_text}'[/bold yellow]\")\n    \n    # Pure functions are predictable and testable\n    tokens = tokenize_text(test_text)\n    filtered = filter_tokens(tokens, min_length=3)\n    stats = calculate_text_stats(test_text)\n    \n    console.print(f\"\\n[bold cyan]Pure function results:[/bold cyan]\")\n    console.print(f\"  Tokens: {tokens[:5]}... ({len(tokens)} total)\")\n    console.print(f\"  Filtered (len>=3): {filtered[:5]}... ({len(filtered)} total)\")\n    console.print(f\"  Stats: {stats}\")\n    \n    # Pure functions can be easily combined\n    pipeline = pipe(\n        tokenize_text,\n        lambda tokens: filter_tokens(tokens, 4),\n        len\n    )\n    \n    long_word_count = pipeline(test_text)\n    console.print(f\"  Long words (len>=4): {long_word_count}\")\n\n\nasync def demonstrate_higher_order_functions():\n    \"\"\"Show higher-order function patterns.\"\"\"\n    \n    console.print(Panel(\"üöÄ Higher-Order Functions\", style=\"bold magenta\"))\n    \n    # Create composed text processor\n    transformers = [\n        str.lower,\n        lambda text: text.replace(\"quick\", \"QUICK\"),\n        lambda text: text.replace(\"fox\", \"ü¶ä\"),\n        str.title\n    ]\n    \n    processor = create_text_processor(transformers)\n    \n    test_text = \"The Quick Brown Fox\"\n    result = processor(test_text)\n    \n    console.print(f\"[bold yellow]Text transformation pipeline:[/bold yellow]\")\n    console.print(f\"  Input: '{test_text}'\")\n    console.print(f\"  Output: '{result}'\")\n    \n    # Create specialized functions\n    high_confidence_filter = create_confidence_filter(0.8)\n    batch_stats_processor = create_batch_processor(calculate_text_stats, batch_size=2)\n    \n    # Test batch processor\n    texts = [\"Hello world\", \"Python is great\", \"AI is fascinating\", \"Functional programming rocks\"]\n    batch_stats = batch_stats_processor(texts)\n    \n    console.print(f\"\\n[bold cyan]Batch processing results:[/bold cyan]\")\n    for i, stats in enumerate(batch_stats):\n        console.print(f\"  Text {i+1}: {stats['word_count']} words, {stats['unique_words']} unique\")\n\n\nasync def demonstrate_function_composition():\n    \"\"\"Show function composition patterns.\"\"\"\n    \n    console.print(Panel(\"üîó Function Composition\", style=\"bold cyan\"))\n    \n    # Mathematical composition (right to left)\n    text_analyzer = compose(\n        lambda stats: f\"Analysis: {stats['word_count']} words, {stats['unique_words']} unique\",\n        calculate_text_stats\n    )\n    \n    # Pipe composition (left to right)\n    text_pipeline = pipe(\n        str.strip,\n        str.lower,\n        tokenize_text,\n        lambda tokens: filter_tokens(tokens, 2),\n        len\n    )\n    \n    test_text = \"  The Quick Brown Fox Jumps Over The Lazy Dog  \"\n    \n    console.print(f\"[bold yellow]Function composition test:[/bold yellow]\")\n    console.print(f\"  Input: '{test_text}'\")\n    \n    analysis = text_analyzer(test_text)\n    filtered_count = text_pipeline(test_text)\n    \n    console.print(f\"  Composed analysis: {analysis}\")\n    console.print(f\"  Piped processing: {filtered_count} filtered tokens\")\n    \n    # Using FunctionalPipeline class\n    pipeline = (FunctionalPipeline()\n                .add_step(str.strip)\n                .add_step(str.upper)\n                .add_step(lambda text: text.replace(\" \", \"_\")))\n    \n    pipeline_result = pipeline(test_text)\n    console.print(f\"  Pipeline class result: '{pipeline_result}'\")\n\n\nasync def demonstrate_currying_partial():\n    \"\"\"Show currying and partial application.\"\"\"\n    \n    console.print(Panel(\"ü•¢ Currying & Partial Application\", style=\"bold red\"))\n    \n    # Curried model prediction\n    gpt4_predictor = curry_model_predict(\"gpt-4\", 0.7)\n    claude_predictor = curry_model_predict(\"claude-3\", 0.5)\n    \n    test_texts = [\"Hello world\", \"Python is awesome\", \"AI will change everything\"]\n    \n    console.print(\"[bold yellow]Curried predictions:[/bold yellow]\")\n    \n    for text in test_texts:\n        gpt_pred = gpt4_predictor(text)\n        claude_pred = claude_predictor(text)\n        \n        console.print(f\"  '{text}':\")\n        console.print(f\"    GPT-4: {gpt_pred.confidence:.3f}\")\n        console.print(f\"    Claude: {claude_pred.confidence:.3f}\")\n    \n    # Partial application examples\n    multiply_by_10 = partial(mul, 10)\n    add_100 = partial(add, 100)\n    \n    numbers = [1, 2, 3, 4, 5]\n    multiplied = list(map(multiply_by_10, numbers))\n    added = list(map(add_100, numbers))\n    \n    console.print(f\"\\n[bold cyan]Partial application:[/bold cyan]\")\n    console.print(f\"  Original: {numbers}\")\n    console.print(f\"  √ó 10: {multiplied}\")\n    console.print(f\"  + 100: {added}\")\n\n\nasync def demonstrate_monadic_error_handling():\n    \"\"\"Show monadic error handling.\"\"\"\n    \n    console.print(Panel(\"üîÑ Monadic Error Handling\", style=\"bold yellow\"))\n    \n    console.print(\"[bold yellow]Safe operations with Result monad:[/bold yellow]\")\n    \n    # Chaining operations with error handling\n    result1 = (safe_divide(10, 2)\n               .map(lambda x: x * 3)\n               .map(lambda x: f\"Result: {x}\")\n               .filter(lambda s: \"15\" in s, \"Result doesn't contain 15\"))\n    \n    result2 = (safe_divide(10, 0)\n               .map(lambda x: x * 3)\n               .map(lambda x: f\"Result: {x}\"))\n    \n    console.print(f\"  Safe calculation: {result1.value if result1.is_success else result1.error}\")\n    console.print(f\"  Division by zero: {result2.error}\")\n    \n    # Model prediction with error handling\n    test_cases = [\"Hello world\", \"\", \"x\" * 1001]\n    \n    console.print(f\"\\n[bold cyan]Safe model predictions:[/bold cyan]\")\n    \n    for text in test_cases:\n        result = (safe_model_predict(text)\n                  .map(lambda pred: pred.transform_text(str.upper))\n                  .filter(lambda pred: pred.confidence > 0.8, \"Low confidence\"))\n        \n        display_text = text[:20] + \"...\" if len(text) > 20 else text\n        if result.is_success:\n            console.print(f\"  ‚úÖ '{display_text}': confidence {result.value.confidence}\")\n        else:\n            console.print(f\"  ‚ùå '{display_text}': {result.error}\")\n\n\nasync def demonstrate_lazy_evaluation():\n    \"\"\"Show lazy evaluation patterns.\"\"\"\n    \n    console.print(Panel(\"üò¥ Lazy Evaluation\", style=\"bold purple\"))\n    \n    # Create large dataset\n    large_texts = [f\"Sample text number {i}\" for i in range(1000)]\n    \n    console.print(\"[bold yellow]Lazy text processing:[/bold yellow]\")\n    \n    # Process only what we need\n    lazy_processor = lazy_text_generator(large_texts[:5])\n    \n    for i, processed in enumerate(lazy_processor):\n        if i >= 3:  # Only process first 3\n            break\n        console.print(f\"  Processed: {processed['text']} ({processed['stats']['word_count']} words)\")\n    \n    # Lazy pipeline with generators\n    def slow_processor(text: str) -> str:\n        time.sleep(0.001)  # Simulate slow processing\n        return text.upper()\n    \n    def add_prefix(text: str) -> str:\n        return f\"PROCESSED: {text}\"\n    \n    console.print(f\"\\n[bold cyan]Lazy pipeline (processing on demand):[/bold cyan]\")\n    \n    lazy_pipeline = create_lazy_pipeline(\n        large_texts[:10],\n        slow_processor,\n        add_prefix\n    )\n    \n    # Only consume what we need\n    first_three = []\n    for i, result in enumerate(lazy_pipeline):\n        first_three.append(result)\n        if i >= 2:\n            break\n    \n    for result in first_three:\n        console.print(f\"  {result}\")\n\n\nasync def demonstrate_memoization():\n    \"\"\"Show memoization for performance.\"\"\"\n    \n    console.print(Panel(\"üß† Memoization\", style=\"bold orange\"))\n    \n    test_texts = [\n        \"This is a test text for analysis\",\n        \"Another sample text for processing\",\n        \"This is a test text for analysis\",  # Duplicate\n        \"Final text sample for testing\"\n    ]\n    \n    console.print(\"[bold yellow]Memoized expensive analysis:[/bold yellow]\")\n    \n    # First run - will be slow\n    start_time = time.time()\n    for text in test_texts:\n        analysis = expensive_text_analysis(text)\n        console.print(f\"  {text[:30]}... ‚Üí complexity: {analysis['complexity_score']:.3f}\")\n    first_run_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold cyan]Cache info: {expensive_text_analysis.cache_info()}[/bold cyan]\")\n    \n    # Second run - should be faster due to caching\n    start_time = time.time()\n    for text in test_texts:\n        analysis = expensive_text_analysis(text)\n    second_run_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold green]Performance comparison:[/bold green]\")\n    console.print(f\"  First run: {first_run_time:.4f}s\")\n    console.print(f\"  Second run: {second_run_time:.4f}s\")\n    console.print(f\"  Speedup: {first_run_time / second_run_time:.1f}x\")\n\n\nasync def demonstrate_decorators():\n    \"\"\"Show functional decorators.\"\"\"\n    \n    console.print(Panel(\"üé® Functional Decorators\", style=\"bold teal\"))\n    \n    @timing_decorator\n    @retry_decorator(max_attempts=2)\n    def flaky_process(text: str, fail_rate: float = 0.5) -> str:\n        \"\"\"Function that sometimes fails.\"\"\"\n        import random\n        if random.random() < fail_rate:\n            raise ValueError(\"Simulated processing error\")\n        return f\"Successfully processed: {text}\"\n    \n    console.print(\"[bold yellow]Testing decorated functions:[/bold yellow]\")\n    \n    try:\n        result = flaky_process(\"test data\", fail_rate=0.3)\n        console.print(f\"  ‚úÖ {result}\")\n    except Exception as e:\n        console.print(f\"  ‚ùå Failed: {e}\")\n\n\nasync def demonstrate_single_dispatch():\n    \"\"\"Show single dispatch patterns.\"\"\"\n    \n    console.print(Panel(\"üéØ Single Dispatch\", style=\"bold indigo\"))\n    \n    test_data = [\n        \"Hello world\",\n        [1, 2, 3, 4, 5],\n        {\"name\": \"John\", \"age\": 30},\n        42\n    ]\n    \n    console.print(\"[bold yellow]Single dispatch processing:[/bold yellow]\")\n    \n    for data in test_data:\n        result = process_input(data)\n        console.print(f\"  {type(data).__name__}: {result}\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for functional programming.\"\"\"\n    \n    console.print(Panel(\"üéÆ Interactive Exercise: Functional AI Pipeline\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"task\": \"Text Classification Pipeline\",\n            \"requirements\": [\n                \"Immutable data structures for configurations\",\n                \"Pure functions for text preprocessing\",\n                \"Functional composition for pipeline stages\",\n                \"Error handling with Result monad\"\n            ],\n            \"challenge\": \"Build a functional text classification system\"\n        },\n        {\n            \"task\": \"Model Ensemble System\",\n            \"requirements\": [\n                \"Curried prediction functions for different models\",\n                \"Lazy evaluation for expensive computations\",\n                \"Memoization for repeated inputs\",\n                \"Higher-order functions for combining predictions\"\n            ],\n            \"challenge\": \"Create a functional ensemble prediction system\"\n        },\n        {\n            \"task\": \"Stream Processing Engine\",\n            \"requirements\": [\n                \"Generator-based lazy processing\",\n                \"Functional transformations on data streams\",\n                \"Immutable state management\",\n                \"Composable stream operations\"\n            ],\n            \"challenge\": \"Design a functional stream processing architecture\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Scenario {i}: {scenario['task']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  ‚Ä¢ {req}\")\n        console.print(f\"\\n[bold yellow]Challenge: {scenario['challenge']}[/bold yellow]\")\n        console.print(\"\\n[bold green]Functional Patterns to Consider:[/bold green]\")\n        console.print(\"  ‚Ä¢ Immutable data structures\")\n        console.print(\"  ‚Ä¢ Pure functions and composition\")\n        console.print(\"  ‚Ä¢ Higher-order functions\")\n        console.print(\"  ‚Ä¢ Monadic error handling\")\n        console.print(\"  ‚Ä¢ Lazy evaluation\")\n        console.print(\"  ‚Ä¢ Memoization\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]üöÄ AI Architecture Patterns: Functional Programming[/bold blue]\")\n    console.print(\"Learn functional patterns for predictable AI systems\\n\")\n    \n    async def main():\n        await demonstrate_immutable_data()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_pure_functions()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_higher_order_functions()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_function_composition()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_currying_partial()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_monadic_error_handling()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_lazy_evaluation()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_memoization()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_decorators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_single_dispatch()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]‚úÖ Completed![/bold green] Next: Run example 04_metaprogramming.py\")\n        console.print(\"üí° Pro tip: Functional patterns make AI code more predictable and testable!\")\n    \n    import asyncio\n    asyncio.run(main())","numLines":812,"startLine":1,"totalLines":812}}}
{"parentUuid":"722d7593-5292-4232-9e8f-1701deef8475","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01JvoDuLdcjxm6n9eraxJKvd","type":"tool_result","content":"     1‚Üí#!/usr/bin/env python3\n     2‚Üí\"\"\"\n     3‚ÜíExample 4: Metaprogramming for AI Systems\n     4‚Üí\n     5‚ÜíLearn metaprogramming techniques to create flexible, self-adapting AI systems\n     6‚Üíthrough descriptors, metaclasses, decorators, and dynamic code generation.\n     7‚Üí\n     8‚ÜíRun this file to see the examples in action:\n     9‚Üí    python 03-python-idioms/examples/04_metaprogramming.py\n    10‚Üí\"\"\"\n    11‚Üí\n    12‚Üíimport sys\n    13‚Üíimport os\n    14‚Üísys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n    15‚Üí\n    16‚Üífrom typing import (\n    17‚Üí    Any, Dict, List, Optional, Callable, Type, Union,\n    18‚Üí    get_type_hints, get_origin, get_args\n    19‚Üí)\n    20‚Üífrom functools import wraps, update_wrapper\n    21‚Üíimport inspect\n    22‚Üíimport time\n    23‚Üífrom dataclasses import dataclass, field\n    24‚Üífrom abc import ABC, abstractmethod\n    25‚Üíimport weakref\n    26‚Üífrom rich.console import Console\n    27‚Üífrom rich.panel import Panel\n    28‚Üífrom rich.table import Table\n    29‚Üí\n    30‚Üíconsole = Console()\n    31‚Üí\n    32‚Üí# üéØ LEARNING GOAL: Master metaprogramming for flexible AI systems\n    33‚Üí\n    34‚Üí\n    35‚Üí# Custom Descriptors for AI Model Properties\n    36‚Üíclass ValidatedProperty:\n    37‚Üí    \"\"\"Descriptor that validates property values.\"\"\"\n    38‚Üí    \n    39‚Üí    def __init__(self, validator: Callable[[Any], bool], error_msg: str):\n    40‚Üí        self.validator = validator\n    41‚Üí        self.error_msg = error_msg\n    42‚Üí        self.name = None\n    43‚Üí    \n    44‚Üí    def __set_name__(self, owner: Type, name: str):\n    45‚Üí        \"\"\"Called when descriptor is assigned to a class attribute.\"\"\"\n    46‚Üí        self.name = name\n    47‚Üí        self.private_name = f'_{name}'\n    48‚Üí    \n    49‚Üí    def __get__(self, obj: Any, objtype: Type = None) -> Any:\n    50‚Üí        \"\"\"Get the property value.\"\"\"\n    51‚Üí        if obj is None:\n    52‚Üí            return self\n    53‚Üí        return getattr(obj, self.private_name, None)\n    54‚Üí    \n    55‚Üí    def __set__(self, obj: Any, value: Any) -> None:\n    56‚Üí        \"\"\"Set the property value with validation.\"\"\"\n    57‚Üí        if not self.validator(value):\n    58‚Üí            raise ValueError(f\"{self.error_msg}: {value}\")\n    59‚Üí        setattr(obj, self.private_name, value)\n    60‚Üí\n    61‚Üí\n    62‚Üíclass CachedProperty:\n    63‚Üí    \"\"\"Descriptor that caches expensive computations.\"\"\"\n    64‚Üí    \n    65‚Üí    def __init__(self, func: Callable):\n    66‚Üí        self.func = func\n    67‚Üí        self.name = func.__name__\n    68‚Üí        update_wrapper(self, func)\n    69‚Üí    \n    70‚Üí    def __get__(self, obj: Any, objtype: Type = None) -> Any:\n    71‚Üí        if obj is None:\n    72‚Üí            return self\n    73‚Üí        \n    74‚Üí        # Check if cached value exists\n    75‚Üí        cache_name = f'_cached_{self.name}'\n    76‚Üí        if hasattr(obj, cache_name):\n    77‚Üí            return getattr(obj, cache_name)\n    78‚Üí        \n    79‚Üí        # Compute and cache the value\n    80‚Üí        value = self.func(obj)\n    81‚Üí        setattr(obj, cache_name, value)\n    82‚Üí        return value\n    83‚Üí    \n    84‚Üí    def __set__(self, obj: Any, value: Any) -> None:\n    85‚Üí        \"\"\"Allow manual cache invalidation by setting new value.\"\"\"\n    86‚Üí        cache_name = f'_cached_{self.name}'\n    87‚Üí        setattr(obj, cache_name, value)\n    88‚Üí    \n    89‚Üí    def __delete__(self, obj: Any) -> None:\n    90‚Üí        \"\"\"Clear the cached value.\"\"\"\n    91‚Üí        cache_name = f'_cached_{self.name}'\n    92‚Üí        if hasattr(obj, cache_name):\n    93‚Üí            delattr(obj, cache_name)\n    94‚Üí\n    95‚Üí\n    96‚Üí# Metaclass for Automatic Model Registration\n    97‚Üíclass ModelRegistryMeta(type):\n    98‚Üí    \"\"\"Metaclass that automatically registers AI models.\"\"\"\n    99‚Üí    \n   100‚Üí    _registry: Dict[str, Type] = {}\n   101‚Üí    \n   102‚Üí    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\n   103‚Üí        # Add automatic ID generation\n   104‚Üí        if 'model_id' not in namespace and name != 'BaseModel':\n   105‚Üí            namespace['model_id'] = name.lower().replace('model', '')\n   106‚Üí        \n   107‚Üí        # Create the class\n   108‚Üí        cls = super().__new__(mcs, name, bases, namespace)\n   109‚Üí        \n   110‚Üí        # Register the model (skip base classes)\n   111‚Üí        if name != 'BaseModel' and hasattr(cls, 'model_id'):\n   112‚Üí            mcs._registry[cls.model_id] = cls\n   113‚Üí            console.print(f\"[dim]üîß Registered model: {cls.model_id} -> {name}[/dim]\")\n   114‚Üí        \n   115‚Üí        return cls\n   116‚Üí    \n   117‚Üí    @classmethod\n   118‚Üí    def get_model(mcs, model_id: str) -> Optional[Type]:\n   119‚Üí        \"\"\"Get model class by ID.\"\"\"\n   120‚Üí        return mcs._registry.get(model_id)\n   121‚Üí    \n   122‚Üí    @classmethod\n   123‚Üí    def list_models(mcs) -> List[str]:\n   124‚Üí        \"\"\"List all registered model IDs.\"\"\"\n   125‚Üí        return list(mcs._registry.keys())\n   126‚Üí\n   127‚Üí\n   128‚Üí# Metaclass for Automatic Method Instrumentation\n   129‚Üíclass InstrumentedMeta(type):\n   130‚Üí    \"\"\"Metaclass that automatically adds instrumentation to methods.\"\"\"\n   131‚Üí    \n   132‚Üí    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\n   133‚Üí        # Instrument methods that start with 'predict'\n   134‚Üí        for attr_name, attr_value in namespace.items():\n   135‚Üí            if (attr_name.startswith('predict') and \n   136‚Üí                callable(attr_value) and \n   137‚Üí                not attr_name.startswith('_')):\n   138‚Üí                \n   139‚Üí                namespace[attr_name] = mcs._instrument_method(attr_value, attr_name)\n   140‚Üí        \n   141‚Üí        return super().__new__(mcs, name, bases, namespace)\n   142‚Üí    \n   143‚Üí    @staticmethod\n   144‚Üí    def _instrument_method(method: Callable, method_name: str) -> Callable:\n   145‚Üí        \"\"\"Add timing and logging to a method.\"\"\"\n   146‚Üí        @wraps(method)\n   147‚Üí        def wrapper(self, *args, **kwargs):\n   148‚Üí            start_time = time.time()\n   149‚Üí            try:\n   150‚Üí                result = method(self, *args, **kwargs)\n   151‚Üí                end_time = time.time()\n   152‚Üí                duration = end_time - start_time\n   153‚Üí                \n   154‚Üí                # Store metrics\n   155‚Üí                if not hasattr(self, '_metrics'):\n   156‚Üí                    self._metrics = {}\n   157‚Üí                if method_name not in self._metrics:\n   158‚Üí                    self._metrics[method_name] = []\n   159‚Üí                \n   160‚Üí                self._metrics[method_name].append({\n   161‚Üí                    'duration': duration,\n   162‚Üí                    'timestamp': start_time,\n   163‚Üí                    'success': True\n   164‚Üí                })\n   165‚Üí                \n   166‚Üí                console.print(f\"[dim]üìä {self.__class__.__name__}.{method_name} took {duration:.4f}s[/dim]\")\n   167‚Üí                return result\n   168‚Üí                \n   169‚Üí            except Exception as e:\n   170‚Üí                end_time = time.time()\n   171‚Üí                duration = end_time - start_time\n   172‚Üí                \n   173‚Üí                # Store error metrics\n   174‚Üí                if not hasattr(self, '_metrics'):\n   175‚Üí                    self._metrics = {}\n   176‚Üí                if method_name not in self._metrics:\n   177‚Üí                    self._metrics[method_name] = []\n   178‚Üí                \n   179‚Üí                self._metrics[method_name].append({\n   180‚Üí                    'duration': duration,\n   181‚Üí                    'timestamp': start_time,\n   182‚Üí                    'success': False,\n   183‚Üí                    'error': str(e)\n   184‚Üí                })\n   185‚Üí                \n   186‚Üí                console.print(f\"[dim red]‚ùå {self.__class__.__name__}.{method_name} failed after {duration:.4f}s[/dim red]\")\n   187‚Üí                raise\n   188‚Üí        \n   189‚Üí        return wrapper\n   190‚Üí\n   191‚Üí\n   192‚Üí# Dynamic Class Generation\n   193‚Üídef create_model_class(\n   194‚Üí    name: str, \n   195‚Üí    attributes: Dict[str, Any], \n   196‚Üí    methods: Dict[str, Callable] = None,\n   197‚Üí    base_classes: tuple = ()\n   198‚Üí) -> Type:\n   199‚Üí    \"\"\"Dynamically create a model class.\"\"\"\n   200‚Üí    \n   201‚Üí    # Default base class\n   202‚Üí    if not base_classes:\n   203‚Üí        base_classes = (object,)\n   204‚Üí    \n   205‚Üí    # Combine attributes and methods\n   206‚Üí    namespace = attributes.copy()\n   207‚Üí    if methods:\n   208‚Üí        namespace.update(methods)\n   209‚Üí    \n   210‚Üí    # Add default __init__ if not provided\n   211‚Üí    if '__init__' not in namespace:\n   212‚Üí        def __init__(self, **kwargs):\n   213‚Üí            for key, value in kwargs.items():\n   214‚Üí                setattr(self, key, value)\n   215‚Üí        namespace['__init__'] = __init__\n   216‚Üí    \n   217‚Üí    # Create the class\n   218‚Üí    return type(name, base_classes, namespace)\n   219‚Üí\n   220‚Üí\n   221‚Üí# Advanced Decorator Patterns\n   222‚Üíclass memoize_with_ttl:\n   223‚Üí    \"\"\"Decorator that memoizes with time-to-live.\"\"\"\n   224‚Üí    \n   225‚Üí    def __init__(self, ttl_seconds: float = 60.0):\n   226‚Üí        self.ttl_seconds = ttl_seconds\n   227‚Üí        self.cache = {}\n   228‚Üí    \n   229‚Üí    def __call__(self, func: Callable) -> Callable:\n   230‚Üí        @wraps(func)\n   231‚Üí        def wrapper(*args, **kwargs):\n   232‚Üí            # Create cache key\n   233‚Üí            key = str(args) + str(sorted(kwargs.items()))\n   234‚Üí            current_time = time.time()\n   235‚Üí            \n   236‚Üí            # Check if cached and not expired\n   237‚Üí            if key in self.cache:\n   238‚Üí                result, timestamp = self.cache[key]\n   239‚Üí                if current_time - timestamp < self.ttl_seconds:\n   240‚Üí                    console.print(f\"[dim green]üíæ Cache hit for {func.__name__}[/dim green]\")\n   241‚Üí                    return result\n   242‚Üí            \n   243‚Üí            # Compute new result\n   244‚Üí            result = func(*args, **kwargs)\n   245‚Üí            self.cache[key] = (result, current_time)\n   246‚Üí            console.print(f\"[dim yellow]üîÑ Cache miss for {func.__name__}[/dim yellow]\")\n   247‚Üí            return result\n   248‚Üí        \n   249‚Üí        # Add cache management methods\n   250‚Üí        wrapper.clear_cache = lambda: self.cache.clear()\n   251‚Üí        wrapper.cache_info = lambda: {\n   252‚Üí            'size': len(self.cache),\n   253‚Üí            'ttl_seconds': self.ttl_seconds\n   254‚Üí        }\n   255‚Üí        \n   256‚Üí        return wrapper\n   257‚Üí\n   258‚Üí\n   259‚Üíclass retry_with_backoff:\n   260‚Üí    \"\"\"Decorator that retries with exponential backoff.\"\"\"\n   261‚Üí    \n   262‚Üí    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n   263‚Üí        self.max_retries = max_retries\n   264‚Üí        self.base_delay = base_delay\n   265‚Üí    \n   266‚Üí    def __call__(self, func: Callable) -> Callable:\n   267‚Üí        @wraps(func)\n   268‚Üí        def wrapper(*args, **kwargs):\n   269‚Üí            last_exception = None\n   270‚Üí            \n   271‚Üí            for attempt in range(self.max_retries + 1):\n   272‚Üí                try:\n   273‚Üí                    return func(*args, **kwargs)\n   274‚Üí                except Exception as e:\n   275‚Üí                    last_exception = e\n   276‚Üí                    \n   277‚Üí                    if attempt < self.max_retries:\n   278‚Üí                        delay = self.base_delay * (2 ** attempt)\n   279‚Üí                        console.print(f\"[yellow]üîÑ Retry {attempt + 1}/{self.max_retries} in {delay:.1f}s[/yellow]\")\n   280‚Üí                        time.sleep(delay)\n   281‚Üí                    else:\n   282‚Üí                        console.print(f\"[red]‚ùå All retries exhausted for {func.__name__}[/red]\")\n   283‚Üí            \n   284‚Üí            raise last_exception\n   285‚Üí        \n   286‚Üí        return wrapper\n   287‚Üí\n   288‚Üí\n   289‚Üí# Property Factory\n   290‚Üídef typed_property(property_type: Type, default: Any = None, validator: Callable = None):\n   291‚Üí    \"\"\"Factory for creating typed properties with optional validation.\"\"\"\n   292‚Üí    \n   293‚Üí    def property_decorator(func: Callable) -> property:\n   294‚Üí        name = func.__name__\n   295‚Üí        private_name = f'_{name}'\n   296‚Üí        \n   297‚Üí        def getter(self):\n   298‚Üí            return getattr(self, private_name, default)\n   299‚Üí        \n   300‚Üí        def setter(self, value):\n   301‚Üí            # Type checking\n   302‚Üí            if not isinstance(value, property_type):\n   303‚Üí                try:\n   304‚Üí                    value = property_type(value)\n   305‚Üí                except (ValueError, TypeError):\n   306‚Üí                    raise TypeError(f\"{name} must be of type {property_type.__name__}\")\n   307‚Üí            \n   308‚Üí            # Optional validation\n   309‚Üí            if validator and not validator(value):\n   310‚Üí                raise ValueError(f\"Invalid value for {name}: {value}\")\n   311‚Üí            \n   312‚Üí            setattr(self, private_name, value)\n   313‚Üí        \n   314‚Üí        return property(getter, setter, doc=func.__doc__)\n   315‚Üí    \n   316‚Üí    return property_decorator\n   317‚Üí\n   318‚Üí\n   319‚Üí# Context Manager for Dynamic Configuration\n   320‚Üíclass DynamicConfig:\n   321‚Üí    \"\"\"Context manager that temporarily modifies class attributes.\"\"\"\n   322‚Üí    \n   323‚Üí    def __init__(self, target_class: Type, **config_changes):\n   324‚Üí        self.target_class = target_class\n   325‚Üí        self.config_changes = config_changes\n   326‚Üí        self.original_values = {}\n   327‚Üí    \n   328‚Üí    def __enter__(self):\n   329‚Üí        # Save original values\n   330‚Üí        for attr_name, new_value in self.config_changes.items():\n   331‚Üí            if hasattr(self.target_class, attr_name):\n   332‚Üí                self.original_values[attr_name] = getattr(self.target_class, attr_name)\n   333‚Üí            setattr(self.target_class, attr_name, new_value)\n   334‚Üí        return self\n   335‚Üí    \n   336‚Üí    def __exit__(self, exc_type, exc_val, exc_tb):\n   337‚Üí        # Restore original values\n   338‚Üí        for attr_name, original_value in self.original_values.items():\n   339‚Üí            setattr(self.target_class, attr_name, original_value)\n   340‚Üí        \n   341‚Üí        # Remove attributes that didn't exist originally\n   342‚Üí        for attr_name in self.config_changes:\n   343‚Üí            if attr_name not in self.original_values:\n   344‚Üí                delattr(self.target_class, attr_name)\n   345‚Üí\n   346‚Üí\n   347‚Üí# Example Classes Using Metaprogramming\n   348‚Üí\n   349‚Üíclass BaseModel(metaclass=ModelRegistryMeta):\n   350‚Üí    \"\"\"Base class for AI models with validation.\"\"\"\n   351‚Üí    \n   352‚Üí    # Validated properties using descriptors\n   353‚Üí    temperature = ValidatedProperty(\n   354‚Üí        lambda x: 0.0 <= x <= 2.0,\n   355‚Üí        \"Temperature must be between 0.0 and 2.0\"\n   356‚Üí    )\n   357‚Üí    \n   358‚Üí    max_tokens = ValidatedProperty(\n   359‚Üí        lambda x: isinstance(x, int) and x > 0,\n   360‚Üí        \"Max tokens must be a positive integer\"\n   361‚Üí    )\n   362‚Üí    \n   363‚Üí    def __init__(self, temperature: float = 0.7, max_tokens: int = 1000):\n   364‚Üí        self.temperature = temperature\n   365‚Üí        self.max_tokens = max_tokens\n   366‚Üí        self._call_count = 0\n   367‚Üí    \n   368‚Üí    @CachedProperty\n   369‚Üí    def model_signature(self) -> str:\n   370‚Üí        \"\"\"Expensive computation that gets cached.\"\"\"\n   371‚Üí        console.print(\"[dim]üîç Computing model signature...[/dim]\")\n   372‚Üí        time.sleep(0.1)  # Simulate expensive computation\n   373‚Üí        return f\"{self.__class__.__name__}_{hash(str(self.temperature) + str(self.max_tokens))}\"\n   374‚Üí\n   375‚Üí\n   376‚Üíclass GPTModel(BaseModel, metaclass=InstrumentedMeta):\n   377‚Üí    \"\"\"GPT model with automatic instrumentation.\"\"\"\n   378‚Üí    \n   379‚Üí    def predict_text(self, text: str) -> str:\n   380‚Üí        \"\"\"Predict text completion.\"\"\"\n   381‚Üí        time.sleep(0.05)  # Simulate prediction\n   382‚Üí        self._call_count += 1\n   383‚Üí        return f\"GPT prediction for: {text}\"\n   384‚Üí    \n   385‚Üí    def predict_classification(self, text: str) -> Dict[str, float]:\n   386‚Üí        \"\"\"Predict text classification.\"\"\"\n   387‚Üí        time.sleep(0.03)\n   388‚Üí        self._call_count += 1\n   389‚Üí        return {\"positive\": 0.8, \"negative\": 0.1, \"neutral\": 0.1}\n   390‚Üí\n   391‚Üí\n   392‚Üíclass ClaudeModel(BaseModel, metaclass=InstrumentedMeta):\n   393‚Üí    \"\"\"Claude model with automatic instrumentation.\"\"\"\n   394‚Üí    \n   395‚Üí    def predict_text(self, text: str) -> str:\n   396‚Üí        \"\"\"Predict text completion.\"\"\"\n   397‚Üí        time.sleep(0.04)\n   398‚Üí        self._call_count += 1\n   399‚Üí        return f\"Claude prediction for: {text}\"\n   400‚Üí\n   401‚Üí\n   402‚Üíclass ConfigurableModel:\n   403‚Üí    \"\"\"Model with typed properties using property factory.\"\"\"\n   404‚Üí    \n   405‚Üí    @typed_property(float, default=0.7, validator=lambda x: 0.0 <= x <= 2.0)\n   406‚Üí    def temperature(self):\n   407‚Üí        \"\"\"Model temperature parameter.\"\"\"\n   408‚Üí        pass\n   409‚Üí    \n   410‚Üí    @typed_property(int, default=1000, validator=lambda x: x > 0)\n   411‚Üí    def max_tokens(self):\n   412‚Üí        \"\"\"Maximum tokens to generate.\"\"\"\n   413‚Üí        pass\n   414‚Üí    \n   415‚Üí    @typed_property(str, default=\"gpt-4\")\n   416‚Üí    def model_name(self):\n   417‚Üí        \"\"\"Name of the model.\"\"\"\n   418‚Üí        pass\n   419‚Üí\n   420‚Üí\n   421‚Üí# Dynamic Model Factory\n   422‚Üíclass ModelFactory:\n   423‚Üí    \"\"\"Factory for creating model classes dynamically.\"\"\"\n   424‚Üí    \n   425‚Üí    @staticmethod\n   426‚Üí    def create_simple_model(name: str, prediction_delay: float = 0.1) -> Type:\n   427‚Üí        \"\"\"Create a simple model class.\"\"\"\n   428‚Üí        \n   429‚Üí        def predict(self, text: str) -> str:\n   430‚Üí            time.sleep(prediction_delay)\n   431‚Üí            return f\"{name} processed: {text}\"\n   432‚Üí        \n   433‚Üí        def get_info(self) -> Dict[str, Any]:\n   434‚Üí            return {\n   435‚Üí                \"model_name\": name,\n   436‚Üí                \"prediction_delay\": prediction_delay,\n   437‚Üí                \"class_name\": self.__class__.__name__\n   438‚Üí            }\n   439‚Üí        \n   440‚Üí        return create_model_class(\n   441‚Üí            name=f\"{name}Model\",\n   442‚Üí            attributes={\n   443‚Üí                \"model_name\": name,\n   444‚Üí                \"prediction_delay\": prediction_delay\n   445‚Üí            },\n   446‚Üí            methods={\n   447‚Üí                \"predict\": predict,\n   448‚Üí                \"get_info\": get_info\n   449‚Üí            },\n   450‚Üí            base_classes=(BaseModel,)\n   451‚Üí        )\n   452‚Üí    \n   453‚Üí    @staticmethod\n   454‚Üí    def create_ensemble_model(models: List[Type]) -> Type:\n   455‚Üí        \"\"\"Create an ensemble model from multiple model classes.\"\"\"\n   456‚Üí        \n   457‚Üí        def predict(self, text: str) -> List[str]:\n   458‚Üí            results = []\n   459‚Üí            for model_class in models:\n   460‚Üí                model = model_class()\n   461‚Üí                if hasattr(model, 'predict'):\n   462‚Üí                    results.append(model.predict(text))\n   463‚Üí                elif hasattr(model, 'predict_text'):\n   464‚Üí                    results.append(model.predict_text(text))\n   465‚Üí            return results\n   466‚Üí        \n   467‚Üí        def get_model_info(self) -> List[str]:\n   468‚Üí            return [model.__name__ for model in models]\n   469‚Üí        \n   470‚Üí        return create_model_class(\n   471‚Üí            name=\"EnsembleModel\",\n   472‚Üí            attributes={\n   473‚Üí                \"component_models\": models\n   474‚Üí            },\n   475‚Üí            methods={\n   476‚Üí                \"predict\": predict,\n   477‚Üí                \"get_model_info\": get_model_info\n   478‚Üí            }\n   479‚Üí        )\n   480‚Üí\n   481‚Üí\n   482‚Üíasync def demonstrate_descriptors():\n   483‚Üí    \"\"\"Show descriptor patterns.\"\"\"\n   484‚Üí    \n   485‚Üí    console.print(Panel(\"üîç Descriptors\", style=\"bold blue\"))\n   486‚Üí    \n   487‚Üí    console.print(\"[bold yellow]Creating model with validated properties:[/bold yellow]\")\n   488‚Üí    \n   489‚Üí    model = GPTModel()\n   490‚Üí    console.print(f\"  Default temperature: {model.temperature}\")\n   491‚Üí    console.print(f\"  Default max_tokens: {model.max_tokens}\")\n   492‚Üí    \n   493‚Üí    # Test validation\n   494‚Üí    try:\n   495‚Üí        model.temperature = 1.5\n   496‚Üí        console.print(f\"  ‚úÖ Set temperature to {model.temperature}\")\n   497‚Üí    except ValueError as e:\n   498‚Üí        console.print(f\"  ‚ùå Validation error: {e}\")\n   499‚Üí    \n   500‚Üí    try:\n   501‚Üí        model.temperature = 3.0  # Should fail\n   502‚Üí    except ValueError as e:\n   503‚Üí        console.print(f\"  ‚ùå Validation caught: {e}\")\n   504‚Üí    \n   505‚Üí    # Test cached property\n   506‚Üí    console.print(f\"\\n[bold cyan]Testing cached property:[/bold cyan]\")\n   507‚Üí    sig1 = model.model_signature  # Should compute\n   508‚Üí    sig2 = model.model_signature  # Should use cache\n   509‚Üí    console.print(f\"  Signatures match: {sig1 == sig2}\")\n   510‚Üí\n   511‚Üí\n   512‚Üíasync def demonstrate_metaclasses():\n   513‚Üí    \"\"\"Show metaclass patterns.\"\"\"\n   514‚Üí    \n   515‚Üí    console.print(Panel(\"üè≠ Metaclasses\", style=\"bold green\"))\n   516‚Üí    \n   517‚Üí    console.print(\"[bold yellow]Automatic model registration:[/bold yellow]\")\n   518‚Üí    registered_models = ModelRegistryMeta.list_models()\n   519‚Üí    console.print(f\"  Registered models: {registered_models}\")\n   520‚Üí    \n   521‚Üí    # Get model by ID\n   522‚Üí    gpt_class = ModelRegistryMeta.get_model('gpt')\n   523‚Üí    claude_class = ModelRegistryMeta.get_model('claude')\n   524‚Üí    \n   525‚Üí    console.print(f\"\\n[bold cyan]Creating models from registry:[/bold cyan]\")\n   526‚Üí    if gpt_class:\n   527‚Üí        gpt = gpt_class()\n   528‚Üí        result = gpt.predict_text(\"Hello world\")\n   529‚Üí        console.print(f\"  GPT: {result}\")\n   530‚Üí    \n   531‚Üí    if claude_class:\n   532‚Üí        claude = claude_class()\n   533‚Üí        result = claude.predict_text(\"Hello world\")\n   534‚Üí        console.print(f\"  Claude: {result}\")\n   535‚Üí    \n   536‚Üí    # Show automatic instrumentation\n   537‚Üí    console.print(f\"\\n[bold magenta]Instrumentation metrics:[/bold magenta]\")\n   538‚Üí    if hasattr(gpt, '_metrics'):\n   539‚Üí        for method, metrics in gpt._metrics.items():\n   540‚Üí            avg_duration = sum(m['duration'] for m in metrics) / len(metrics)\n   541‚Üí            console.print(f\"  {method}: {len(metrics)} calls, avg {avg_duration:.4f}s\")\n   542‚Üí\n   543‚Üí\n   544‚Üíasync def demonstrate_dynamic_classes():\n   545‚Üí    \"\"\"Show dynamic class creation.\"\"\"\n   546‚Üí    \n   547‚Üí    console.print(Panel(\"‚ö° Dynamic Class Creation\", style=\"bold magenta\"))\n   548‚Üí    \n   549‚Üí    # Create models using factory\n   550‚Üí    console.print(\"[bold yellow]Creating models dynamically:[/bold yellow]\")\n   551‚Üí    \n   552‚Üí    FastModel = ModelFactory.create_simple_model(\"FastAI\", prediction_delay=0.02)\n   553‚Üí    SlowModel = ModelFactory.create_simple_model(\"DeepThink\", prediction_delay=0.08)\n   554‚Üí    \n   555‚Üí    fast_model = FastModel()\n   556‚Üí    slow_model = SlowModel()\n   557‚Üí    \n   558‚Üí    console.print(f\"  Fast model: {fast_model.get_info()}\")\n   559‚Üí    console.print(f\"  Slow model: {slow_model.get_info()}\")\n   560‚Üí    \n   561‚Üí    # Test predictions\n   562‚Üí    start_time = time.time()\n   563‚Üí    fast_result = fast_model.predict(\"test input\")\n   564‚Üí    fast_time = time.time() - start_time\n   565‚Üí    \n   566‚Üí    start_time = time.time()\n   567‚Üí    slow_result = slow_model.predict(\"test input\")\n   568‚Üí    slow_time = time.time() - start_time\n   569‚Üí    \n   570‚Üí    console.print(f\"\\n[bold cyan]Performance comparison:[/bold cyan]\")\n   571‚Üí    console.print(f\"  Fast: {fast_time:.4f}s - {fast_result}\")\n   572‚Üí    console.print(f\"  Slow: {slow_time:.4f}s - {slow_result}\")\n   573‚Üí    \n   574‚Üí    # Create ensemble model\n   575‚Üí    console.print(f\"\\n[bold yellow]Creating ensemble model:[/bold yellow]\")\n   576‚Üí    EnsembleModel = ModelFactory.create_ensemble_model([FastModel, SlowModel])\n   577‚Üí    ensemble = EnsembleModel()\n   578‚Üí    \n   579‚Üí    ensemble_results = ensemble.predict(\"ensemble test\")\n   580‚Üí    console.print(f\"  Ensemble results: {ensemble_results}\")\n   581‚Üí    console.print(f\"  Component models: {ensemble.get_model_info()}\")\n   582‚Üí\n   583‚Üí\n   584‚Üíasync def demonstrate_decorators():\n   585‚Üí    \"\"\"Show advanced decorator patterns.\"\"\"\n   586‚Üí    \n   587‚Üí    console.print(Panel(\"üé® Advanced Decorators\", style=\"bold cyan\"))\n   588‚Üí    \n   589‚Üí    @memoize_with_ttl(ttl_seconds=2.0)\n   590‚Üí    @retry_with_backoff(max_retries=2, base_delay=0.5)\n   591‚Üí    def expensive_ai_call(query: str, fail_rate: float = 0.0) -> str:\n   592‚Üí        \"\"\"Simulate expensive AI call that might fail.\"\"\"\n   593‚Üí        import random\n   594‚Üí        \n   595‚Üí        if random.random() < fail_rate:\n   596‚Üí            raise ConnectionError(\"Simulated API failure\")\n   597‚Üí        \n   598‚Üí        time.sleep(0.1)  # Simulate processing time\n   599‚Üí        return f\"AI response to: {query}\"\n   600‚Üí    \n   601‚Üí    console.print(\"[bold yellow]Testing decorated function:[/bold yellow]\")\n   602‚Üí    \n   603‚Üí    # First call - should compute\n   604‚Üí    result1 = expensive_ai_call(\"What is AI?\")\n   605‚Üí    console.print(f\"  Result 1: {result1}\")\n   606‚Üí    \n   607‚Üí    # Second call - should use cache\n   608‚Üí    result2 = expensive_ai_call(\"What is AI?\")\n   609‚Üí    console.print(f\"  Result 2: {result2}\")\n   610‚Üí    \n   611‚Üí    # Test with failures\n   612‚Üí    console.print(f\"\\n[bold cyan]Testing with simulated failures:[/bold cyan]\")\n   613‚Üí    try:\n   614‚Üí        result3 = expensive_ai_call(\"Risky query\", fail_rate=0.7)\n   615‚Üí        console.print(f\"  Success: {result3}\")\n   616‚Üí    except Exception as e:\n   617‚Üí        console.print(f\"  Failed: {e}\")\n   618‚Üí    \n   619‚Üí    # Show cache info\n   620‚Üí    cache_info = expensive_ai_call.cache_info()\n   621‚Üí    console.print(f\"\\n[bold green]Cache info: {cache_info}[/bold green]\")\n   622‚Üí\n   623‚Üí\n   624‚Üíasync def demonstrate_typed_properties():\n   625‚Üí    \"\"\"Show typed property factory.\"\"\"\n   626‚Üí    \n   627‚Üí    console.print(Panel(\"üè∑Ô∏è Typed Properties\", style=\"bold red\"))\n   628‚Üí    \n   629‚Üí    console.print(\"[bold yellow]Creating model with typed properties:[/bold yellow]\")\n   630‚Üí    \n   631‚Üí    model = ConfigurableModel()\n   632‚Üí    \n   633‚Üí    # Test default values\n   634‚Üí    console.print(f\"  Default temperature: {model.temperature}\")\n   635‚Üí    console.print(f\"  Default max_tokens: {model.max_tokens}\")\n   636‚Üí    console.print(f\"  Default model_name: {model.model_name}\")\n   637‚Üí    \n   638‚Üí    # Test type conversion\n   639‚Üí    console.print(f\"\\n[bold cyan]Testing type conversion:[/bold cyan]\")\n   640‚Üí    model.temperature = \"1.2\"  # String should convert to float\n   641‚Üí    model.max_tokens = \"500\"   # String should convert to int\n   642‚Üí    \n   643‚Üí    console.print(f\"  Converted temperature: {model.temperature} (type: {type(model.temperature)})\")\n   644‚Üí    console.print(f\"  Converted max_tokens: {model.max_tokens} (type: {type(model.max_tokens)})\")\n   645‚Üí    \n   646‚Üí    # Test validation\n   647‚Üí    console.print(f\"\\n[bold yellow]Testing validation:[/bold yellow]\")\n   648‚Üí    try:\n   649‚Üí        model.temperature = 3.0  # Should fail validation\n   650‚Üí    except ValueError as e:\n   651‚Üí        console.print(f\"  ‚ùå Validation error: {e}\")\n   652‚Üí    \n   653‚Üí    try:\n   654‚Üí        model.max_tokens = -100  # Should fail validation\n   655‚Üí    except ValueError as e:\n   656‚Üí        console.print(f\"  ‚ùå Validation error: {e}\")\n   657‚Üí\n   658‚Üí\n   659‚Üíasync def demonstrate_dynamic_config():\n   660‚Üí    \"\"\"Show dynamic configuration context manager.\"\"\"\n   661‚Üí    \n   662‚Üí    console.print(Panel(\"‚öôÔ∏è Dynamic Configuration\", style=\"bold yellow\"))\n   663‚Üí    \n   664‚Üí    # Original class configuration\n   665‚Üí    class AIService:\n   666‚Üí        timeout = 30.0\n   667‚Üí        retry_count = 3\n   668‚Üí        debug_mode = False\n   669‚Üí        \n   670‚Üí        @classmethod\n   671‚Üí        def get_config(cls) -> Dict[str, Any]:\n   672‚Üí            return {\n   673‚Üí                \"timeout\": cls.timeout,\n   674‚Üí                \"retry_count\": cls.retry_count,\n   675‚Üí                \"debug_mode\": cls.debug_mode\n   676‚Üí            }\n   677‚Üí    \n   678‚Üí    console.print(\"[bold yellow]Original configuration:[/bold yellow]\")\n   679‚Üí    console.print(f\"  {AIService.get_config()}\")\n   680‚Üí    \n   681‚Üí    # Temporarily modify configuration\n   682‚Üí    console.print(f\"\\n[bold cyan]Using dynamic configuration:[/bold cyan]\")\n   683‚Üí    with DynamicConfig(AIService, timeout=60.0, debug_mode=True, new_feature=True):\n   684‚Üí        console.print(f\"  Inside context: {AIService.get_config()}\")\n   685‚Üí        console.print(f\"  New feature: {getattr(AIService, 'new_feature', 'Not found')}\")\n   686‚Üí    \n   687‚Üí    console.print(f\"\\n[bold green]After context:[/bold green]\")\n   688‚Üí    console.print(f\"  Restored: {AIService.get_config()}\")\n   689‚Üí    console.print(f\"  New feature: {getattr(AIService, 'new_feature', 'Not found')}\")\n   690‚Üí\n   691‚Üí\n   692‚Üídef analyze_function_signature(func: Callable) -> Dict[str, Any]:\n   693‚Üí    \"\"\"Metaprogramming function to analyze function signatures.\"\"\"\n   694‚Üí    sig = inspect.signature(func)\n   695‚Üí    type_hints = get_type_hints(func)\n   696‚Üí    \n   697‚Üí    return {\n   698‚Üí        \"name\": func.__name__,\n   699‚Üí        \"parameters\": {\n   700‚Üí            name: {\n   701‚Üí                \"annotation\": type_hints.get(name, param.annotation),\n   702‚Üí                \"default\": param.default if param.default != param.empty else None,\n   703‚Üí                \"kind\": param.kind.name\n   704‚Üí            }\n   705‚Üí            for name, param in sig.parameters.items()\n   706‚Üí        },\n   707‚Üí        \"return_type\": type_hints.get('return', sig.return_annotation),\n   708‚Üí        \"docstring\": inspect.getdoc(func)\n   709‚Üí    }\n   710‚Üí\n   711‚Üí\n   712‚Üíasync def demonstrate_reflection():\n   713‚Üí    \"\"\"Show reflection and introspection.\"\"\"\n   714‚Üí    \n   715‚Üí    console.print(Panel(\"üîç Reflection & Introspection\", style=\"bold purple\"))\n   716‚Üí    \n   717‚Üí    # Analyze function signatures\n   718‚Üí    def sample_ai_function(text: str, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\n   719‚Üí        \"\"\"Sample AI function for analysis.\"\"\"\n   720‚Üí        return {\"processed\": text, \"config\": {\"temperature\": temperature, \"max_tokens\": max_tokens}}\n   721‚Üí    \n   722‚Üí    analysis = analyze_function_signature(sample_ai_function)\n   723‚Üí    \n   724‚Üí    console.print(\"[bold yellow]Function signature analysis:[/bold yellow]\")\n   725‚Üí    console.print(f\"  Name: {analysis['name']}\")\n   726‚Üí    console.print(f\"  Return type: {analysis['return_type']}\")\n   727‚Üí    console.print(f\"  Parameters:\")\n   728‚Üí    \n   729‚Üí    for param_name, param_info in analysis['parameters'].items():\n   730‚Üí        console.print(f\"    {param_name}: {param_info['annotation']} (default: {param_info['default']})\")\n   731‚Üí    \n   732‚Üí    # Analyze class structure\n   733‚Üí    console.print(f\"\\n[bold cyan]Class structure analysis:[/bold cyan]\")\n   734‚Üí    gpt_model = GPTModel()\n   735‚Üí    \n   736‚Üí    methods = [name for name, method in inspect.getmembers(gpt_model, predicate=inspect.ismethod)]\n   737‚Üí    properties = [name for name in dir(gpt_model) if isinstance(getattr(type(gpt_model), name, None), property)]\n   738‚Üí    \n   739‚Üí    console.print(f\"  Methods: {[m for m in methods if not m.startswith('_')]}\")\n   740‚Üí    console.print(f\"  Properties: {properties}\")\n   741‚Üí    \n   742‚Üí    if hasattr(gpt_model, '_metrics'):\n   743‚Üí        console.print(f\"  Metrics: {list(gpt_model._metrics.keys())}\")\n   744‚Üí\n   745‚Üí\n   746‚Üídef interactive_exercise():\n   747‚Üí    \"\"\"Interactive exercise for metaprogramming.\"\"\"\n   748‚Üí    \n   749‚Üí    console.print(Panel(\"üéÆ Interactive Exercise: Metaprogramming AI Framework\", style=\"bold magenta\"))\n   750‚Üí    \n   751‚Üí    scenarios = [\n   752‚Üí        {\n   753‚Üí            \"challenge\": \"Dynamic Model Registry\",\n   754‚Üí            \"requirements\": [\n   755‚Üí                \"Automatic registration of model classes\",\n   756‚Üí                \"Type validation for model parameters\",\n   757‚Üí                \"Runtime model discovery and instantiation\",\n   758‚Üí                \"Automatic instrumentation of model methods\"\n   759‚Üí            ],\n   760‚Üí            \"tools\": \"Metaclasses, descriptors, registration patterns\"\n   761‚Üí        },\n   762‚Üí        {\n   763‚Üí            \"challenge\": \"Self-Configuring AI Pipeline\",\n   764‚Üí            \"requirements\": [\n   765‚Üí                \"Dynamic property creation based on configuration\",\n   766‚Üí                \"Automatic validation and type conversion\",\n   767‚Üí                \"Context-based configuration overrides\",\n   768‚Üí                \"Runtime pipeline modification\"\n   769‚Üí            ],\n   770‚Üí            \"tools\": \"Property factories, context managers, dynamic classes\"\n   771‚Üí        },\n   772‚Üí        {\n   773‚Üí            \"challenge\": \"Adaptive Model Behavior\",\n   774‚Üí            \"requirements\": [\n   775‚Üí                \"Runtime method modification based on usage patterns\",\n   776‚Üí                \"Automatic caching with TTL for expensive operations\",\n   777‚Üí                \"Retry logic with configurable backoff strategies\",\n   778‚Üí                \"Performance monitoring and optimization\"\n   779‚Üí            ],\n   780‚Üí            \"tools\": \"Decorators, method instrumentation, caching patterns\"\n   781‚Üí        }\n   782‚Üí    ]\n   783‚Üí    \n   784‚Üí    for i, scenario in enumerate(scenarios, 1):\n   785‚Üí        console.print(f\"\\n[bold cyan]Challenge {i}: {scenario['challenge']}[/bold cyan]\")\n   786‚Üí        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n   787‚Üí        for req in scenario[\"requirements\"]:\n   788‚Üí            console.print(f\"  ‚Ä¢ {req}\")\n   789‚Üí        console.print(f\"\\n[bold green]Metaprogramming Tools: {scenario['tools']}[/bold green]\")\n   790‚Üí        console.print(\"\\n[bold magenta]Consider:[/bold magenta]\")\n   791‚Üí        console.print(\"  ‚Ä¢ When to use metaclasses vs. decorators\")\n   792‚Üí        console.print(\"  ‚Ä¢ How to balance flexibility with performance\")\n   793‚Üí        console.print(\"  ‚Ä¢ Type safety in dynamic code generation\")\n   794‚Üí        console.print(\"  ‚Ä¢ Debugging and introspection capabilities\")\n   795‚Üí        console.print(\"---\")\n   796‚Üí\n   797‚Üí\n   798‚Üíif __name__ == \"__main__\":\n   799‚Üí    console.print(\"[bold blue]üöÄ AI Architecture Patterns: Metaprogramming[/bold blue]\")\n   800‚Üí    console.print(\"Learn to create flexible, self-adapting AI systems\\n\")\n   801‚Üí    \n   802‚Üí    async def main():\n   803‚Üí        await demonstrate_descriptors()\n   804‚Üí        console.print(\"\\n\" + \"=\"*60)\n   805‚Üí        \n   806‚Üí        await demonstrate_metaclasses()\n   807‚Üí        console.print(\"\\n\" + \"=\"*60)\n   808‚Üí        \n   809‚Üí        await demonstrate_dynamic_classes()\n   810‚Üí        console.print(\"\\n\" + \"=\"*60)\n   811‚Üí        \n   812‚Üí        await demonstrate_decorators()\n   813‚Üí        console.print(\"\\n\" + \"=\"*60)\n   814‚Üí        \n   815‚Üí        await demonstrate_typed_properties()\n   816‚Üí        console.print(\"\\n\" + \"=\"*60)\n   817‚Üí        \n   818‚Üí        await demonstrate_dynamic_config()\n   819‚Üí        console.print(\"\\n\" + \"=\"*60)\n   820‚Üí        \n   821‚Üí        await demonstrate_reflection()\n   822‚Üí        console.print(\"\\n\" + \"=\"*60)\n   823‚Üí        \n   824‚Üí        interactive_exercise()\n   825‚Üí        \n   826‚Üí        console.print(f\"\\n[bold green]‚úÖ Completed![/bold green] Next: Run example 05_performance_optimization.py\")\n   827‚Üí        console.print(\"üí° Pro tip: Metaprogramming enables flexible AI architectures but use with care!\")\n   828‚Üí    \n   829‚Üí    import asyncio\n   830‚Üí    asyncio.run(main())\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"b39ae093-e960-4cfb-8e4a-08dbc94885be","timestamp":"2025-08-01T17:59:54.596Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/04_metaprogramming.py","content":"#!/usr/bin/env python3\n\"\"\"\nExample 4: Metaprogramming for AI Systems\n\nLearn metaprogramming techniques to create flexible, self-adapting AI systems\nthrough descriptors, metaclasses, decorators, and dynamic code generation.\n\nRun this file to see the examples in action:\n    python 03-python-idioms/examples/04_metaprogramming.py\n\"\"\"\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n\nfrom typing import (\n    Any, Dict, List, Optional, Callable, Type, Union,\n    get_type_hints, get_origin, get_args\n)\nfrom functools import wraps, update_wrapper\nimport inspect\nimport time\nfrom dataclasses import dataclass, field\nfrom abc import ABC, abstractmethod\nimport weakref\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\n\nconsole = Console()\n\n# üéØ LEARNING GOAL: Master metaprogramming for flexible AI systems\n\n\n# Custom Descriptors for AI Model Properties\nclass ValidatedProperty:\n    \"\"\"Descriptor that validates property values.\"\"\"\n    \n    def __init__(self, validator: Callable[[Any], bool], error_msg: str):\n        self.validator = validator\n        self.error_msg = error_msg\n        self.name = None\n    \n    def __set_name__(self, owner: Type, name: str):\n        \"\"\"Called when descriptor is assigned to a class attribute.\"\"\"\n        self.name = name\n        self.private_name = f'_{name}'\n    \n    def __get__(self, obj: Any, objtype: Type = None) -> Any:\n        \"\"\"Get the property value.\"\"\"\n        if obj is None:\n            return self\n        return getattr(obj, self.private_name, None)\n    \n    def __set__(self, obj: Any, value: Any) -> None:\n        \"\"\"Set the property value with validation.\"\"\"\n        if not self.validator(value):\n            raise ValueError(f\"{self.error_msg}: {value}\")\n        setattr(obj, self.private_name, value)\n\n\nclass CachedProperty:\n    \"\"\"Descriptor that caches expensive computations.\"\"\"\n    \n    def __init__(self, func: Callable):\n        self.func = func\n        self.name = func.__name__\n        update_wrapper(self, func)\n    \n    def __get__(self, obj: Any, objtype: Type = None) -> Any:\n        if obj is None:\n            return self\n        \n        # Check if cached value exists\n        cache_name = f'_cached_{self.name}'\n        if hasattr(obj, cache_name):\n            return getattr(obj, cache_name)\n        \n        # Compute and cache the value\n        value = self.func(obj)\n        setattr(obj, cache_name, value)\n        return value\n    \n    def __set__(self, obj: Any, value: Any) -> None:\n        \"\"\"Allow manual cache invalidation by setting new value.\"\"\"\n        cache_name = f'_cached_{self.name}'\n        setattr(obj, cache_name, value)\n    \n    def __delete__(self, obj: Any) -> None:\n        \"\"\"Clear the cached value.\"\"\"\n        cache_name = f'_cached_{self.name}'\n        if hasattr(obj, cache_name):\n            delattr(obj, cache_name)\n\n\n# Metaclass for Automatic Model Registration\nclass ModelRegistryMeta(type):\n    \"\"\"Metaclass that automatically registers AI models.\"\"\"\n    \n    _registry: Dict[str, Type] = {}\n    \n    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\n        # Add automatic ID generation\n        if 'model_id' not in namespace and name != 'BaseModel':\n            namespace['model_id'] = name.lower().replace('model', '')\n        \n        # Create the class\n        cls = super().__new__(mcs, name, bases, namespace)\n        \n        # Register the model (skip base classes)\n        if name != 'BaseModel' and hasattr(cls, 'model_id'):\n            mcs._registry[cls.model_id] = cls\n            console.print(f\"[dim]üîß Registered model: {cls.model_id} -> {name}[/dim]\")\n        \n        return cls\n    \n    @classmethod\n    def get_model(mcs, model_id: str) -> Optional[Type]:\n        \"\"\"Get model class by ID.\"\"\"\n        return mcs._registry.get(model_id)\n    \n    @classmethod\n    def list_models(mcs) -> List[str]:\n        \"\"\"List all registered model IDs.\"\"\"\n        return list(mcs._registry.keys())\n\n\n# Metaclass for Automatic Method Instrumentation\nclass InstrumentedMeta(type):\n    \"\"\"Metaclass that automatically adds instrumentation to methods.\"\"\"\n    \n    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\n        # Instrument methods that start with 'predict'\n        for attr_name, attr_value in namespace.items():\n            if (attr_name.startswith('predict') and \n                callable(attr_value) and \n                not attr_name.startswith('_')):\n                \n                namespace[attr_name] = mcs._instrument_method(attr_value, attr_name)\n        \n        return super().__new__(mcs, name, bases, namespace)\n    \n    @staticmethod\n    def _instrument_method(method: Callable, method_name: str) -> Callable:\n        \"\"\"Add timing and logging to a method.\"\"\"\n        @wraps(method)\n        def wrapper(self, *args, **kwargs):\n            start_time = time.time()\n            try:\n                result = method(self, *args, **kwargs)\n                end_time = time.time()\n                duration = end_time - start_time\n                \n                # Store metrics\n                if not hasattr(self, '_metrics'):\n                    self._metrics = {}\n                if method_name not in self._metrics:\n                    self._metrics[method_name] = []\n                \n                self._metrics[method_name].append({\n                    'duration': duration,\n                    'timestamp': start_time,\n                    'success': True\n                })\n                \n                console.print(f\"[dim]üìä {self.__class__.__name__}.{method_name} took {duration:.4f}s[/dim]\")\n                return result\n                \n            except Exception as e:\n                end_time = time.time()\n                duration = end_time - start_time\n                \n                # Store error metrics\n                if not hasattr(self, '_metrics'):\n                    self._metrics = {}\n                if method_name not in self._metrics:\n                    self._metrics[method_name] = []\n                \n                self._metrics[method_name].append({\n                    'duration': duration,\n                    'timestamp': start_time,\n                    'success': False,\n                    'error': str(e)\n                })\n                \n                console.print(f\"[dim red]‚ùå {self.__class__.__name__}.{method_name} failed after {duration:.4f}s[/dim red]\")\n                raise\n        \n        return wrapper\n\n\n# Dynamic Class Generation\ndef create_model_class(\n    name: str, \n    attributes: Dict[str, Any], \n    methods: Dict[str, Callable] = None,\n    base_classes: tuple = ()\n) -> Type:\n    \"\"\"Dynamically create a model class.\"\"\"\n    \n    # Default base class\n    if not base_classes:\n        base_classes = (object,)\n    \n    # Combine attributes and methods\n    namespace = attributes.copy()\n    if methods:\n        namespace.update(methods)\n    \n    # Add default __init__ if not provided\n    if '__init__' not in namespace:\n        def __init__(self, **kwargs):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        namespace['__init__'] = __init__\n    \n    # Create the class\n    return type(name, base_classes, namespace)\n\n\n# Advanced Decorator Patterns\nclass memoize_with_ttl:\n    \"\"\"Decorator that memoizes with time-to-live.\"\"\"\n    \n    def __init__(self, ttl_seconds: float = 60.0):\n        self.ttl_seconds = ttl_seconds\n        self.cache = {}\n    \n    def __call__(self, func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = str(args) + str(sorted(kwargs.items()))\n            current_time = time.time()\n            \n            # Check if cached and not expired\n            if key in self.cache:\n                result, timestamp = self.cache[key]\n                if current_time - timestamp < self.ttl_seconds:\n                    console.print(f\"[dim green]üíæ Cache hit for {func.__name__}[/dim green]\")\n                    return result\n            \n            # Compute new result\n            result = func(*args, **kwargs)\n            self.cache[key] = (result, current_time)\n            console.print(f\"[dim yellow]üîÑ Cache miss for {func.__name__}[/dim yellow]\")\n            return result\n        \n        # Add cache management methods\n        wrapper.clear_cache = lambda: self.cache.clear()\n        wrapper.cache_info = lambda: {\n            'size': len(self.cache),\n            'ttl_seconds': self.ttl_seconds\n        }\n        \n        return wrapper\n\n\nclass retry_with_backoff:\n    \"\"\"Decorator that retries with exponential backoff.\"\"\"\n    \n    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n    \n    def __call__(self, func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            last_exception = None\n            \n            for attempt in range(self.max_retries + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    last_exception = e\n                    \n                    if attempt < self.max_retries:\n                        delay = self.base_delay * (2 ** attempt)\n                        console.print(f\"[yellow]üîÑ Retry {attempt + 1}/{self.max_retries} in {delay:.1f}s[/yellow]\")\n                        time.sleep(delay)\n                    else:\n                        console.print(f\"[red]‚ùå All retries exhausted for {func.__name__}[/red]\")\n            \n            raise last_exception\n        \n        return wrapper\n\n\n# Property Factory\ndef typed_property(property_type: Type, default: Any = None, validator: Callable = None):\n    \"\"\"Factory for creating typed properties with optional validation.\"\"\"\n    \n    def property_decorator(func: Callable) -> property:\n        name = func.__name__\n        private_name = f'_{name}'\n        \n        def getter(self):\n            return getattr(self, private_name, default)\n        \n        def setter(self, value):\n            # Type checking\n            if not isinstance(value, property_type):\n                try:\n                    value = property_type(value)\n                except (ValueError, TypeError):\n                    raise TypeError(f\"{name} must be of type {property_type.__name__}\")\n            \n            # Optional validation\n            if validator and not validator(value):\n                raise ValueError(f\"Invalid value for {name}: {value}\")\n            \n            setattr(self, private_name, value)\n        \n        return property(getter, setter, doc=func.__doc__)\n    \n    return property_decorator\n\n\n# Context Manager for Dynamic Configuration\nclass DynamicConfig:\n    \"\"\"Context manager that temporarily modifies class attributes.\"\"\"\n    \n    def __init__(self, target_class: Type, **config_changes):\n        self.target_class = target_class\n        self.config_changes = config_changes\n        self.original_values = {}\n    \n    def __enter__(self):\n        # Save original values\n        for attr_name, new_value in self.config_changes.items():\n            if hasattr(self.target_class, attr_name):\n                self.original_values[attr_name] = getattr(self.target_class, attr_name)\n            setattr(self.target_class, attr_name, new_value)\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore original values\n        for attr_name, original_value in self.original_values.items():\n            setattr(self.target_class, attr_name, original_value)\n        \n        # Remove attributes that didn't exist originally\n        for attr_name in self.config_changes:\n            if attr_name not in self.original_values:\n                delattr(self.target_class, attr_name)\n\n\n# Example Classes Using Metaprogramming\n\nclass BaseModel(metaclass=ModelRegistryMeta):\n    \"\"\"Base class for AI models with validation.\"\"\"\n    \n    # Validated properties using descriptors\n    temperature = ValidatedProperty(\n        lambda x: 0.0 <= x <= 2.0,\n        \"Temperature must be between 0.0 and 2.0\"\n    )\n    \n    max_tokens = ValidatedProperty(\n        lambda x: isinstance(x, int) and x > 0,\n        \"Max tokens must be a positive integer\"\n    )\n    \n    def __init__(self, temperature: float = 0.7, max_tokens: int = 1000):\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self._call_count = 0\n    \n    @CachedProperty\n    def model_signature(self) -> str:\n        \"\"\"Expensive computation that gets cached.\"\"\"\n        console.print(\"[dim]üîç Computing model signature...[/dim]\")\n        time.sleep(0.1)  # Simulate expensive computation\n        return f\"{self.__class__.__name__}_{hash(str(self.temperature) + str(self.max_tokens))}\"\n\n\nclass GPTModel(BaseModel, metaclass=InstrumentedMeta):\n    \"\"\"GPT model with automatic instrumentation.\"\"\"\n    \n    def predict_text(self, text: str) -> str:\n        \"\"\"Predict text completion.\"\"\"\n        time.sleep(0.05)  # Simulate prediction\n        self._call_count += 1\n        return f\"GPT prediction for: {text}\"\n    \n    def predict_classification(self, text: str) -> Dict[str, float]:\n        \"\"\"Predict text classification.\"\"\"\n        time.sleep(0.03)\n        self._call_count += 1\n        return {\"positive\": 0.8, \"negative\": 0.1, \"neutral\": 0.1}\n\n\nclass ClaudeModel(BaseModel, metaclass=InstrumentedMeta):\n    \"\"\"Claude model with automatic instrumentation.\"\"\"\n    \n    def predict_text(self, text: str) -> str:\n        \"\"\"Predict text completion.\"\"\"\n        time.sleep(0.04)\n        self._call_count += 1\n        return f\"Claude prediction for: {text}\"\n\n\nclass ConfigurableModel:\n    \"\"\"Model with typed properties using property factory.\"\"\"\n    \n    @typed_property(float, default=0.7, validator=lambda x: 0.0 <= x <= 2.0)\n    def temperature(self):\n        \"\"\"Model temperature parameter.\"\"\"\n        pass\n    \n    @typed_property(int, default=1000, validator=lambda x: x > 0)\n    def max_tokens(self):\n        \"\"\"Maximum tokens to generate.\"\"\"\n        pass\n    \n    @typed_property(str, default=\"gpt-4\")\n    def model_name(self):\n        \"\"\"Name of the model.\"\"\"\n        pass\n\n\n# Dynamic Model Factory\nclass ModelFactory:\n    \"\"\"Factory for creating model classes dynamically.\"\"\"\n    \n    @staticmethod\n    def create_simple_model(name: str, prediction_delay: float = 0.1) -> Type:\n        \"\"\"Create a simple model class.\"\"\"\n        \n        def predict(self, text: str) -> str:\n            time.sleep(prediction_delay)\n            return f\"{name} processed: {text}\"\n        \n        def get_info(self) -> Dict[str, Any]:\n            return {\n                \"model_name\": name,\n                \"prediction_delay\": prediction_delay,\n                \"class_name\": self.__class__.__name__\n            }\n        \n        return create_model_class(\n            name=f\"{name}Model\",\n            attributes={\n                \"model_name\": name,\n                \"prediction_delay\": prediction_delay\n            },\n            methods={\n                \"predict\": predict,\n                \"get_info\": get_info\n            },\n            base_classes=(BaseModel,)\n        )\n    \n    @staticmethod\n    def create_ensemble_model(models: List[Type]) -> Type:\n        \"\"\"Create an ensemble model from multiple model classes.\"\"\"\n        \n        def predict(self, text: str) -> List[str]:\n            results = []\n            for model_class in models:\n                model = model_class()\n                if hasattr(model, 'predict'):\n                    results.append(model.predict(text))\n                elif hasattr(model, 'predict_text'):\n                    results.append(model.predict_text(text))\n            return results\n        \n        def get_model_info(self) -> List[str]:\n            return [model.__name__ for model in models]\n        \n        return create_model_class(\n            name=\"EnsembleModel\",\n            attributes={\n                \"component_models\": models\n            },\n            methods={\n                \"predict\": predict,\n                \"get_model_info\": get_model_info\n            }\n        )\n\n\nasync def demonstrate_descriptors():\n    \"\"\"Show descriptor patterns.\"\"\"\n    \n    console.print(Panel(\"üîç Descriptors\", style=\"bold blue\"))\n    \n    console.print(\"[bold yellow]Creating model with validated properties:[/bold yellow]\")\n    \n    model = GPTModel()\n    console.print(f\"  Default temperature: {model.temperature}\")\n    console.print(f\"  Default max_tokens: {model.max_tokens}\")\n    \n    # Test validation\n    try:\n        model.temperature = 1.5\n        console.print(f\"  ‚úÖ Set temperature to {model.temperature}\")\n    except ValueError as e:\n        console.print(f\"  ‚ùå Validation error: {e}\")\n    \n    try:\n        model.temperature = 3.0  # Should fail\n    except ValueError as e:\n        console.print(f\"  ‚ùå Validation caught: {e}\")\n    \n    # Test cached property\n    console.print(f\"\\n[bold cyan]Testing cached property:[/bold cyan]\")\n    sig1 = model.model_signature  # Should compute\n    sig2 = model.model_signature  # Should use cache\n    console.print(f\"  Signatures match: {sig1 == sig2}\")\n\n\nasync def demonstrate_metaclasses():\n    \"\"\"Show metaclass patterns.\"\"\"\n    \n    console.print(Panel(\"üè≠ Metaclasses\", style=\"bold green\"))\n    \n    console.print(\"[bold yellow]Automatic model registration:[/bold yellow]\")\n    registered_models = ModelRegistryMeta.list_models()\n    console.print(f\"  Registered models: {registered_models}\")\n    \n    # Get model by ID\n    gpt_class = ModelRegistryMeta.get_model('gpt')\n    claude_class = ModelRegistryMeta.get_model('claude')\n    \n    console.print(f\"\\n[bold cyan]Creating models from registry:[/bold cyan]\")\n    if gpt_class:\n        gpt = gpt_class()\n        result = gpt.predict_text(\"Hello world\")\n        console.print(f\"  GPT: {result}\")\n    \n    if claude_class:\n        claude = claude_class()\n        result = claude.predict_text(\"Hello world\")\n        console.print(f\"  Claude: {result}\")\n    \n    # Show automatic instrumentation\n    console.print(f\"\\n[bold magenta]Instrumentation metrics:[/bold magenta]\")\n    if hasattr(gpt, '_metrics'):\n        for method, metrics in gpt._metrics.items():\n            avg_duration = sum(m['duration'] for m in metrics) / len(metrics)\n            console.print(f\"  {method}: {len(metrics)} calls, avg {avg_duration:.4f}s\")\n\n\nasync def demonstrate_dynamic_classes():\n    \"\"\"Show dynamic class creation.\"\"\"\n    \n    console.print(Panel(\"‚ö° Dynamic Class Creation\", style=\"bold magenta\"))\n    \n    # Create models using factory\n    console.print(\"[bold yellow]Creating models dynamically:[/bold yellow]\")\n    \n    FastModel = ModelFactory.create_simple_model(\"FastAI\", prediction_delay=0.02)\n    SlowModel = ModelFactory.create_simple_model(\"DeepThink\", prediction_delay=0.08)\n    \n    fast_model = FastModel()\n    slow_model = SlowModel()\n    \n    console.print(f\"  Fast model: {fast_model.get_info()}\")\n    console.print(f\"  Slow model: {slow_model.get_info()}\")\n    \n    # Test predictions\n    start_time = time.time()\n    fast_result = fast_model.predict(\"test input\")\n    fast_time = time.time() - start_time\n    \n    start_time = time.time()\n    slow_result = slow_model.predict(\"test input\")\n    slow_time = time.time() - start_time\n    \n    console.print(f\"\\n[bold cyan]Performance comparison:[/bold cyan]\")\n    console.print(f\"  Fast: {fast_time:.4f}s - {fast_result}\")\n    console.print(f\"  Slow: {slow_time:.4f}s - {slow_result}\")\n    \n    # Create ensemble model\n    console.print(f\"\\n[bold yellow]Creating ensemble model:[/bold yellow]\")\n    EnsembleModel = ModelFactory.create_ensemble_model([FastModel, SlowModel])\n    ensemble = EnsembleModel()\n    \n    ensemble_results = ensemble.predict(\"ensemble test\")\n    console.print(f\"  Ensemble results: {ensemble_results}\")\n    console.print(f\"  Component models: {ensemble.get_model_info()}\")\n\n\nasync def demonstrate_decorators():\n    \"\"\"Show advanced decorator patterns.\"\"\"\n    \n    console.print(Panel(\"üé® Advanced Decorators\", style=\"bold cyan\"))\n    \n    @memoize_with_ttl(ttl_seconds=2.0)\n    @retry_with_backoff(max_retries=2, base_delay=0.5)\n    def expensive_ai_call(query: str, fail_rate: float = 0.0) -> str:\n        \"\"\"Simulate expensive AI call that might fail.\"\"\"\n        import random\n        \n        if random.random() < fail_rate:\n            raise ConnectionError(\"Simulated API failure\")\n        \n        time.sleep(0.1)  # Simulate processing time\n        return f\"AI response to: {query}\"\n    \n    console.print(\"[bold yellow]Testing decorated function:[/bold yellow]\")\n    \n    # First call - should compute\n    result1 = expensive_ai_call(\"What is AI?\")\n    console.print(f\"  Result 1: {result1}\")\n    \n    # Second call - should use cache\n    result2 = expensive_ai_call(\"What is AI?\")\n    console.print(f\"  Result 2: {result2}\")\n    \n    # Test with failures\n    console.print(f\"\\n[bold cyan]Testing with simulated failures:[/bold cyan]\")\n    try:\n        result3 = expensive_ai_call(\"Risky query\", fail_rate=0.7)\n        console.print(f\"  Success: {result3}\")\n    except Exception as e:\n        console.print(f\"  Failed: {e}\")\n    \n    # Show cache info\n    cache_info = expensive_ai_call.cache_info()\n    console.print(f\"\\n[bold green]Cache info: {cache_info}[/bold green]\")\n\n\nasync def demonstrate_typed_properties():\n    \"\"\"Show typed property factory.\"\"\"\n    \n    console.print(Panel(\"üè∑Ô∏è Typed Properties\", style=\"bold red\"))\n    \n    console.print(\"[bold yellow]Creating model with typed properties:[/bold yellow]\")\n    \n    model = ConfigurableModel()\n    \n    # Test default values\n    console.print(f\"  Default temperature: {model.temperature}\")\n    console.print(f\"  Default max_tokens: {model.max_tokens}\")\n    console.print(f\"  Default model_name: {model.model_name}\")\n    \n    # Test type conversion\n    console.print(f\"\\n[bold cyan]Testing type conversion:[/bold cyan]\")\n    model.temperature = \"1.2\"  # String should convert to float\n    model.max_tokens = \"500\"   # String should convert to int\n    \n    console.print(f\"  Converted temperature: {model.temperature} (type: {type(model.temperature)})\")\n    console.print(f\"  Converted max_tokens: {model.max_tokens} (type: {type(model.max_tokens)})\")\n    \n    # Test validation\n    console.print(f\"\\n[bold yellow]Testing validation:[/bold yellow]\")\n    try:\n        model.temperature = 3.0  # Should fail validation\n    except ValueError as e:\n        console.print(f\"  ‚ùå Validation error: {e}\")\n    \n    try:\n        model.max_tokens = -100  # Should fail validation\n    except ValueError as e:\n        console.print(f\"  ‚ùå Validation error: {e}\")\n\n\nasync def demonstrate_dynamic_config():\n    \"\"\"Show dynamic configuration context manager.\"\"\"\n    \n    console.print(Panel(\"‚öôÔ∏è Dynamic Configuration\", style=\"bold yellow\"))\n    \n    # Original class configuration\n    class AIService:\n        timeout = 30.0\n        retry_count = 3\n        debug_mode = False\n        \n        @classmethod\n        def get_config(cls) -> Dict[str, Any]:\n            return {\n                \"timeout\": cls.timeout,\n                \"retry_count\": cls.retry_count,\n                \"debug_mode\": cls.debug_mode\n            }\n    \n    console.print(\"[bold yellow]Original configuration:[/bold yellow]\")\n    console.print(f\"  {AIService.get_config()}\")\n    \n    # Temporarily modify configuration\n    console.print(f\"\\n[bold cyan]Using dynamic configuration:[/bold cyan]\")\n    with DynamicConfig(AIService, timeout=60.0, debug_mode=True, new_feature=True):\n        console.print(f\"  Inside context: {AIService.get_config()}\")\n        console.print(f\"  New feature: {getattr(AIService, 'new_feature', 'Not found')}\")\n    \n    console.print(f\"\\n[bold green]After context:[/bold green]\")\n    console.print(f\"  Restored: {AIService.get_config()}\")\n    console.print(f\"  New feature: {getattr(AIService, 'new_feature', 'Not found')}\")\n\n\ndef analyze_function_signature(func: Callable) -> Dict[str, Any]:\n    \"\"\"Metaprogramming function to analyze function signatures.\"\"\"\n    sig = inspect.signature(func)\n    type_hints = get_type_hints(func)\n    \n    return {\n        \"name\": func.__name__,\n        \"parameters\": {\n            name: {\n                \"annotation\": type_hints.get(name, param.annotation),\n                \"default\": param.default if param.default != param.empty else None,\n                \"kind\": param.kind.name\n            }\n            for name, param in sig.parameters.items()\n        },\n        \"return_type\": type_hints.get('return', sig.return_annotation),\n        \"docstring\": inspect.getdoc(func)\n    }\n\n\nasync def demonstrate_reflection():\n    \"\"\"Show reflection and introspection.\"\"\"\n    \n    console.print(Panel(\"üîç Reflection & Introspection\", style=\"bold purple\"))\n    \n    # Analyze function signatures\n    def sample_ai_function(text: str, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\n        \"\"\"Sample AI function for analysis.\"\"\"\n        return {\"processed\": text, \"config\": {\"temperature\": temperature, \"max_tokens\": max_tokens}}\n    \n    analysis = analyze_function_signature(sample_ai_function)\n    \n    console.print(\"[bold yellow]Function signature analysis:[/bold yellow]\")\n    console.print(f\"  Name: {analysis['name']}\")\n    console.print(f\"  Return type: {analysis['return_type']}\")\n    console.print(f\"  Parameters:\")\n    \n    for param_name, param_info in analysis['parameters'].items():\n        console.print(f\"    {param_name}: {param_info['annotation']} (default: {param_info['default']})\")\n    \n    # Analyze class structure\n    console.print(f\"\\n[bold cyan]Class structure analysis:[/bold cyan]\")\n    gpt_model = GPTModel()\n    \n    methods = [name for name, method in inspect.getmembers(gpt_model, predicate=inspect.ismethod)]\n    properties = [name for name in dir(gpt_model) if isinstance(getattr(type(gpt_model), name, None), property)]\n    \n    console.print(f\"  Methods: {[m for m in methods if not m.startswith('_')]}\")\n    console.print(f\"  Properties: {properties}\")\n    \n    if hasattr(gpt_model, '_metrics'):\n        console.print(f\"  Metrics: {list(gpt_model._metrics.keys())}\")\n\n\ndef interactive_exercise():\n    \"\"\"Interactive exercise for metaprogramming.\"\"\"\n    \n    console.print(Panel(\"üéÆ Interactive Exercise: Metaprogramming AI Framework\", style=\"bold magenta\"))\n    \n    scenarios = [\n        {\n            \"challenge\": \"Dynamic Model Registry\",\n            \"requirements\": [\n                \"Automatic registration of model classes\",\n                \"Type validation for model parameters\",\n                \"Runtime model discovery and instantiation\",\n                \"Automatic instrumentation of model methods\"\n            ],\n            \"tools\": \"Metaclasses, descriptors, registration patterns\"\n        },\n        {\n            \"challenge\": \"Self-Configuring AI Pipeline\",\n            \"requirements\": [\n                \"Dynamic property creation based on configuration\",\n                \"Automatic validation and type conversion\",\n                \"Context-based configuration overrides\",\n                \"Runtime pipeline modification\"\n            ],\n            \"tools\": \"Property factories, context managers, dynamic classes\"\n        },\n        {\n            \"challenge\": \"Adaptive Model Behavior\",\n            \"requirements\": [\n                \"Runtime method modification based on usage patterns\",\n                \"Automatic caching with TTL for expensive operations\",\n                \"Retry logic with configurable backoff strategies\",\n                \"Performance monitoring and optimization\"\n            ],\n            \"tools\": \"Decorators, method instrumentation, caching patterns\"\n        }\n    ]\n    \n    for i, scenario in enumerate(scenarios, 1):\n        console.print(f\"\\n[bold cyan]Challenge {i}: {scenario['challenge']}[/bold cyan]\")\n        console.print(\"[bold yellow]Requirements:[/bold yellow]\")\n        for req in scenario[\"requirements\"]:\n            console.print(f\"  ‚Ä¢ {req}\")\n        console.print(f\"\\n[bold green]Metaprogramming Tools: {scenario['tools']}[/bold green]\")\n        console.print(\"\\n[bold magenta]Consider:[/bold magenta]\")\n        console.print(\"  ‚Ä¢ When to use metaclasses vs. decorators\")\n        console.print(\"  ‚Ä¢ How to balance flexibility with performance\")\n        console.print(\"  ‚Ä¢ Type safety in dynamic code generation\")\n        console.print(\"  ‚Ä¢ Debugging and introspection capabilities\")\n        console.print(\"---\")\n\n\nif __name__ == \"__main__\":\n    console.print(\"[bold blue]üöÄ AI Architecture Patterns: Metaprogramming[/bold blue]\")\n    console.print(\"Learn to create flexible, self-adapting AI systems\\n\")\n    \n    async def main():\n        await demonstrate_descriptors()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_metaclasses()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_dynamic_classes()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_decorators()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_typed_properties()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_dynamic_config()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        await demonstrate_reflection()\n        console.print(\"\\n\" + \"=\"*60)\n        \n        interactive_exercise()\n        \n        console.print(f\"\\n[bold green]‚úÖ Completed![/bold green] Next: Run example 05_performance_optimization.py\")\n        console.print(\"üí° Pro tip: Metaprogramming enables flexible AI architectures but use with care!\")\n    \n    import asyncio\n    asyncio.run(main())","numLines":830,"startLine":1,"totalLines":830}}}
{"parentUuid":"b39ae093-e960-4cfb-8e4a-08dbc94885be","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_014s3Ac2ah6SaBMhDfw1ZTXf","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now I'll create the interactive learning notebooks for all these Python files in the order you requested (3, 2, 5, 4). Let me start:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":21349,"cache_read_input_tokens":66522,"output_tokens":1}},"requestId":"req_vrtx_011CRhRePCWGZqYgmToG7wck","type":"assistant","uuid":"9b907628-9fa8-4746-bc9c-4f576d28b748","timestamp":"2025-08-01T17:59:58.492Z"}
{"parentUuid":"9b907628-9fa8-4746-bc9c-4f576d28b748","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_014s3Ac2ah6SaBMhDfw1ZTXf","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01HrrMWitDeuXXkw7Zps9yso","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/03_functional_programming_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Functional Programming for AI Systems\\n\",\n    \"\\n\",\n    \"This notebook teaches functional programming patterns that make AI code more predictable, testable, and maintainable. You'll learn:\\n\",\n    \"\\n\",\n    \"1. **Immutable Data Structures** - Building predictable AI configurations\\n\",\n    \"2. **Pure Functions** - Creating testable and reliable transformations\\n\",\n    \"3. **Higher-Order Functions** - Composing complex AI operations\\n\",\n    \"4. **Function Composition** - Building pipelines from simple components\\n\",\n    \"5. **Currying & Partial Application** - Creating specialized functions\\n\",\n    \"6. **Monadic Error Handling** - Safe computation chains\\n\",\n    \"7. **Lazy Evaluation** - Efficient processing of large datasets\\n\",\n    \"8. **Memoization** - Caching expensive computations\\n\",\n    \"\\n\",\n    \"## Prerequisites\\n\",\n    \"```bash\\n\",\n    \"pip install rich\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 1: Essential Imports and Core Concepts\\n\",\n    \"\\n\",\n    \"**First Principle**: Functional programming emphasizes immutability, pure functions, and composition over mutation and side effects.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `functools` for functional utilities, `dataclasses(frozen=True)` for immutability, and `itertools` for lazy operations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Essential imports for functional programming\\n\",\n    \"from typing import (\\n\",\n    \"    Callable, List, Dict, Any, Optional, Tuple, Union,\\n\",\n    \"    TypeVar, Iterator, Iterable\\n\",\n    \")\\n\",\n    \"from functools import reduce, partial, singledispatch, lru_cache, wraps\\n\",\n    \"from itertools import chain, groupby, accumulate\\n\",\n    \"from dataclasses import dataclass, replace\\n\",\n    \"from operator import add, mul, itemgetter\\n\",\n    \"import time\\n\",\n    \"from rich.console import Console\\n\",\n    \"from rich.panel import Panel\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"T = TypeVar('T')\\n\",\n    \"U = TypeVar('U')\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üéØ Functional Programming Fundamentals\\\", style=\\\"bold blue\\\"))\\n\",\n    \"print(\\\"\\\"\\\"\\n\",\n    \"Key Principles of Functional Programming:\\n\",\n    \"\\n\",\n    \"1. **Immutability**: Data structures don't change after creation\\n\",\n    \"2. **Pure Functions**: No side effects, same input = same output\\n\",\n    \"3. **Composition**: Building complex operations from simple ones\\n\",\n    \"4. **Higher-Order Functions**: Functions that take or return functions\\n\",\n    \"5. **Lazy Evaluation**: Computing values only when needed\\n\",\n    \"\\n\",\n    \"Benefits for AI Systems:\\n\",\n    \"‚Ä¢ Predictable behavior\\n\",\n    \"‚Ä¢ Easy testing and debugging\\n\",\n    \"‚Ä¢ Parallel processing safety\\n\",\n    \"‚Ä¢ Reproducible results\\n\",\n    \"\\\"\\\"\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"‚úÖ Functional programming toolkit loaded!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 2: Immutable Data Structures\\n\",\n    \"\\n\",\n    \"**First Principle**: Immutable data prevents accidental state changes and makes code more predictable.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `@dataclass(frozen=True)` and `replace()` to create new instances instead of modifying existing ones.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"@dataclass(frozen=True)\\n\",\n    \"class ImmutableConfig:\\n\",\n    \"    \\\"\\\"\\\"Immutable configuration for AI models.\\\"\\\"\\\"\\n\",\n    \"    model_name: str\\n\",\n    \"    temperature: float\\n\",\n    \"    max_tokens: int\\n\",\n    \"    provider: str\\n\",\n    \"    api_key: Optional[str] = None\\n\",\n    \"    \\n\",\n    \"    def with_temperature(self, temperature: float) -> 'ImmutableConfig':\\n\",\n    \"        \\\"\\\"\\\"Create new config with different temperature.\\\"\\\"\\\"\\n\",\n    \"        return replace(self, temperature=temperature)\\n\",\n    \"    \\n\",\n    \"    def with_provider(self, provider: str, api_key: Optional[str] = None) -> 'ImmutableConfig':\\n\",\n    \"        \\\"\\\"\\\"Create new config with different provider.\\\"\\\"\\\"\\n\",\n    \"        return replace(self, provider=provider, api_key=api_key)\\n\",\n    \"\\n\",\n    \"@dataclass(frozen=True)\\n\",\n    \"class ModelPrediction:\\n\",\n    \"    \\\"\\\"\\\"Immutable prediction result.\\\"\\\"\\\"\\n\",\n    \"    text: str\\n\",\n    \"    confidence: float\\n\",\n    \"    model_used: str\\n\",\n    \"    processing_time: float\\n\",\n    \"    metadata: Dict[str, Any]\\n\",\n    \"    \\n\",\n    \"    def with_confidence_threshold(self, threshold: float) -> Optional['ModelPrediction']:\\n\",\n    \"        \\\"\\\"\\\"Return prediction only if confidence meets threshold.\\\"\\\"\\\"\\n\",\n    \"        return self if self.confidence >= threshold else None\\n\",\n    \"    \\n\",\n    \"    def transform_text(self, transformer: Callable[[str], str]) -> 'ModelPrediction':\\n\",\n    \"        \\\"\\\"\\\"Transform text while preserving other fields.\\\"\\\"\\\"\\n\",\n    \"        return replace(self, text=transformer(self.text))\\n\",\n    \"\\n\",\n    \"# Demonstrate immutability\\n\",\n    \"console.print(Panel(\\\"üîí Immutable Data Structures\\\", style=\\\"bold green\\\"))\\n\",\n    \"\\n\",\n    \"# Create base configuration\\n\",\n    \"base_config = ImmutableConfig(\\n\",\n    \"    model_name=\\\"gpt-4\\\",\\n\",\n    \"    temperature=0.7,\\n\",\n    \"    max_tokens=1000,\\n\",\n    \"    provider=\\\"openai\\\"\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(f\\\"üìã Original config: {base_config.model_name} (temp: {base_config.temperature})\\\")\\n\",\n    \"\\n\",\n    \"# Create variations without mutating original\\n\",\n    \"hot_config = base_config.with_temperature(1.2)\\n\",\n    \"anthropic_config = base_config.with_provider(\\\"anthropic\\\", \\\"api-key-123\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"üî• Hot config: temp={hot_config.temperature}, original unchanged: {base_config.temperature}\\\")\\n\",\n    \"print(f\\\"üè¢ Anthropic config: {anthropic_config.provider}, original: {base_config.provider}\\\")\\n\",\n    \"\\n\",\n    \"# Create and transform prediction\\n\",\n    \"prediction = ModelPrediction(\\n\",\n    \"    text=\\\"Hello world\\\",\\n\",\n    \"    confidence=0.85,\\n\",\n    \"    model_used=\\\"gpt-4\\\",\\n\",\n    \"    processing_time=0.1,\\n\",\n    \"    metadata={\\\"source\\\": \\\"test\\\"}\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"upper_prediction = prediction.transform_text(str.upper)\\n\",\n    \"filtered_prediction = prediction.with_confidence_threshold(0.9)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîÑ Transformations:\\\")\\n\",\n    \"print(f\\\"  Original: '{prediction.text}'\\\")\\n\",\n    \"print(f\\\"  Uppercase: '{upper_prediction.text}'\\\")\\n\",\n    \"print(f\\\"  Filtered (>0.9): {filtered_prediction}\\\")\\n\",\n    \"\\n\",\n    \"# Try to modify frozen dataclass (will fail)\\n\",\n    \"try:\\n\",\n    \"    base_config.temperature = 0.8\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"\\\\n‚ùå Immutability enforced: {type(e).__name__}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 3: Pure Functions\\n\",\n    \"\\n\",\n    \"**First Principle**: Pure functions have no side effects and always return the same output for the same input.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Write functions that only depend on their parameters and don't modify external state.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Pure functions for text processing\\n\",\n    \"def tokenize_text(text: str) -> List[str]:\\n\",\n    \"    \\\"\\\"\\\"Pure function to tokenize text.\\\"\\\"\\\"\\n\",\n    \"    return text.lower().split()\\n\",\n    \"\\n\",\n    \"def count_tokens(tokens: List[str]) -> int:\\n\",\n    \"    \\\"\\\"\\\"Pure function to count tokens.\\\"\\\"\\\"\\n\",\n    \"    return len(tokens)\\n\",\n    \"\\n\",\n    \"def filter_tokens(tokens: List[str], min_length: int = 2) -> List[str]:\\n\",\n    \"    \\\"\\\"\\\"Pure function to filter tokens by length.\\\"\\\"\\\"\\n\",\n    \"    return [token for token in tokens if len(token) >= min_length]\\n\",\n    \"\\n\",\n    \"def calculate_text_stats(text: str) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Pure function to calculate text statistics.\\\"\\\"\\\"\\n\",\n    \"    tokens = tokenize_text(text)\\n\",\n    \"    filtered_tokens = filter_tokens(tokens)\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"character_count\\\": len(text),\\n\",\n    \"        \\\"word_count\\\": len(tokens),\\n\",\n    \"        \\\"unique_words\\\": len(set(tokens)),\\n\",\n    \"        \\\"filtered_words\\\": len(filtered_tokens),\\n\",\n    \"        \\\"avg_word_length\\\": sum(len(token) for token in tokens) / len(tokens) if tokens else 0\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"‚ú® Pure Functions\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"test_text = \\\"The Quick Brown Fox Jumps Over The Lazy Dog\\\"\\n\",\n    \"print(f\\\"üìù Processing: '{test_text}'\\\")\\n\",\n    \"\\n\",\n    \"# Pure functions are predictable and testable\\n\",\n    \"tokens = tokenize_text(test_text)\\n\",\n    \"filtered = filter_tokens(tokens, min_length=3)\\n\",\n    \"stats = calculate_text_stats(test_text)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîç Results:\\\")\\n\",\n    \"print(f\\\"  Tokens: {tokens[:5]}... ({len(tokens)} total)\\\")\\n\",\n    \"print(f\\\"  Filtered (len>=3): {filtered[:5]}... ({len(filtered)} total)\\\")\\n\",\n    \"print(f\\\"  Stats: {stats}\\\")\\n\",\n    \"\\n\",\n    \"# Pure functions can be easily tested\\n\",\n    \"print(f\\\"\\\\nüß™ Testing predictability:\\\")\\n\",\n    \"result1 = calculate_text_stats(test_text)\\n\",\n    \"result2 = calculate_text_stats(test_text)\\n\",\n    \"print(f\\\"  Same input, same output: {result1 == result2}\\\")\\n\",\n    \"\\n\",\n    \"# Benefits of pure functions\\n\",\n    \"print(f\\\"\\\\nüí° Benefits of Pure Functions:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Predictable: Same input always gives same output\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Testable: Easy to write unit tests\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Cacheable: Results can be memoized\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Parallel-safe: No shared state to corrupt\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Composable: Can be combined easily\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 4: Function Composition and Pipelines\\n\",\n    \"\\n\",\n    \"**First Principle**: Complex operations can be built by composing simple functions together.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `reduce()` for composition, create pipeline classes for reusable processing chains.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Function composition utilities\\n\",\n    \"def compose(*functions: Callable) -> Callable:\\n\",\n    \"    \\\"\\\"\\\"Compose functions right to left (mathematical composition).\\\"\\\"\\\"\\n\",\n    \"    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)\\n\",\n    \"\\n\",\n    \"def pipe(*functions: Callable) -> Callable:\\n\",\n    \"    \\\"\\\"\\\"Pipe functions left to right (more intuitive for data flow).\\\"\\\"\\\"\\n\",\n    \"    return reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\\n\",\n    \"\\n\",\n    \"# Functional pipeline class\\n\",\n    \"class FunctionalPipeline:\\n\",\n    \"    \\\"\\\"\\\"Functional pipeline for processing AI data.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.steps: List[Callable] = []\\n\",\n    \"    \\n\",\n    \"    def add_step(self, func: Callable) -> 'FunctionalPipeline':\\n\",\n    \"        \\\"\\\"\\\"Add processing step (returns new pipeline).\\\"\\\"\\\"\\n\",\n    \"        new_pipeline = FunctionalPipeline()\\n\",\n    \"        new_pipeline.steps = self.steps + [func]\\n\",\n    \"        return new_pipeline\\n\",\n    \"    \\n\",\n    \"    def process(self, data: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Process data through the pipeline.\\\"\\\"\\\"\\n\",\n    \"        return reduce(lambda result, step: step(result), self.steps, data)\\n\",\n    \"    \\n\",\n    \"    def __call__(self, data: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Make pipeline callable.\\\"\\\"\\\"\\n\",\n    \"        return self.process(data)\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üîó Function Composition\\\", style=\\\"bold magenta\\\"))\\n\",\n    \"\\n\",\n    \"# Mathematical composition (right to left)\\n\",\n    \"text_analyzer = compose(\\n\",\n    \"    lambda stats: f\\\"Analysis: {stats['word_count']} words, {stats['unique_words']} unique\\\",\\n\",\n    \"    calculate_text_stats\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Pipe composition (left to right) - more intuitive\\n\",\n    \"text_pipeline = pipe(\\n\",\n    \"    str.strip,\\n\",\n    \"    str.lower,\\n\",\n    \"    tokenize_text,\\n\",\n    \"    lambda tokens: filter_tokens(tokens, 2),\\n\",\n    \"    len\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"test_text = \\\"  The Quick Brown Fox Jumps Over The Lazy Dog  \\\"\\n\",\n    \"print(f\\\"üìù Input: '{test_text}'\\\")\\n\",\n    \"\\n\",\n    \"analysis = text_analyzer(test_text)\\n\",\n    \"filtered_count = text_pipeline(test_text)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìä Results:\\\")\\n\",\n    \"print(f\\\"  Composed analysis: {analysis}\\\")\\n\",\n    \"print(f\\\"  Piped processing: {filtered_count} filtered tokens\\\")\\n\",\n    \"\\n\",\n    \"# Using FunctionalPipeline class\\n\",\n    \"pipeline = (FunctionalPipeline()\\n\",\n    \"           .add_step(str.strip)\\n\",\n    \"           .add_step(str.upper)\\n\",\n    \"           .add_step(lambda text: text.replace(\\\" \\\", \\\"_\\\")))\\n\",\n    \"\\n\",\n    \"pipeline_result = pipeline(test_text)\\n\",\n    \"print(f\\\"  Pipeline class result: '{pipeline_result}'\\\")\\n\",\n    \"\\n\",\n    \"# Demonstrate reusability\\n\",\n    \"print(f\\\"\\\\nüîÑ Pipeline Reusability:\\\")\\n\",\n    \"test_texts = [\\\"hello world\\\", \\\"python rocks\\\", \\\"AI is amazing\\\"]\\n\",\n    \"results = [pipeline(text) for text in test_texts]\\n\",\n    \"print(f\\\"  Processed: {results}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Composition Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Modularity: Small, focused functions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reusability: Combine in different ways\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Testability: Test each step independently\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Readability: Clear data flow\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 5: Higher-Order Functions\\n\",\n    \"\\n\",\n    \"**First Principle**: Higher-order functions take functions as arguments or return functions, enabling powerful abstractions.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use closures to create specialized functions, leverage `map`, `filter`, and `reduce` for data transformations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Higher-order functions for AI processing\\n\",\n    \"def create_text_processor(\\n\",\n    \"    transformers: List[Callable[[str], str]]\\n\",\n    \") -> Callable[[str], str]:\\n\",\n    \"    \\\"\\\"\\\"Create composed text processor from list of transformations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def process(text: str) -> str:\\n\",\n    \"        return reduce(lambda t, transformer: transformer(t), transformers, text)\\n\",\n    \"    \\n\",\n    \"    return process\\n\",\n    \"\\n\",\n    \"def create_confidence_filter(threshold: float) -> Callable[[ModelPrediction], bool]:\\n\",\n    \"    \\\"\\\"\\\"Create a confidence filter function.\\\"\\\"\\\"\\n\",\n    \"    return lambda prediction: prediction.confidence >= threshold\\n\",\n    \"\\n\",\n    \"def create_batch_processor(\\n\",\n    \"    processor: Callable[[T], U],\\n\",\n    \"    batch_size: int = 10\\n\",\n    \") -> Callable[[List[T]], List[U]]:\\n\",\n    \"    \\\"\\\"\\\"Create batch processor from single-item processor.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def process_batch(items: List[T]) -> List[U]:\\n\",\n    \"        results = []\\n\",\n    \"        for i in range(0, len(items), batch_size):\\n\",\n    \"            batch = items[i:i + batch_size]\\n\",\n    \"            batch_results = [processor(item) for item in batch]\\n\",\n    \"            results.extend(batch_results)\\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    return process_batch\\n\",\n    \"\\n\",\n    \"def create_validator(validation_rules: List[Callable[[Any], bool]]) -> Callable[[Any], bool]:\\n\",\n    \"    \\\"\\\"\\\"Create validator that applies multiple rules.\\\"\\\"\\\"\\n\",\n    \"    return lambda value: all(rule(value) for rule in validation_rules)\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üöÄ Higher-Order Functions\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"\\n\",\n    \"# Create composed text processor\\n\",\n    \"transformers = [\\n\",\n    \"    str.lower,\\n\",\n    \"    lambda text: text.replace(\\\"quick\\\", \\\"QUICK\\\"),\\n\",\n    \"    lambda text: text.replace(\\\"fox\\\", \\\"ü¶ä\\\"),\\n\",\n    \"    str.title\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"processor = create_text_processor(transformers)\\n\",\n    \"test_text = \\\"The Quick Brown Fox\\\"\\n\",\n    \"result = processor(test_text)\\n\",\n    \"\\n\",\n    \"print(f\\\"üìù Text transformation pipeline:\\\")\\n\",\n    \"print(f\\\"  Input: '{test_text}'\\\")\\n\",\n    \"print(f\\\"  Output: '{result}'\\\")\\n\",\n    \"\\n\",\n    \"# Create specialized functions\\n\",\n    \"high_confidence_filter = create_confidence_filter(0.8)\\n\",\n    \"batch_stats_processor = create_batch_processor(calculate_text_stats, batch_size=2)\\n\",\n    \"\\n\",\n    \"# Test batch processor\\n\",\n    \"texts = [\\\"Hello world\\\", \\\"Python is great\\\", \\\"AI is fascinating\\\", \\\"Functional programming rocks\\\"]\\n\",\n    \"batch_stats = batch_stats_processor(texts)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìä Batch processing results:\\\")\\n\",\n    \"for i, stats in enumerate(batch_stats):\\n\",\n    \"    print(f\\\"  Text {i+1}: {stats['word_count']} words, {stats['unique_words']} unique\\\")\\n\",\n    \"\\n\",\n    \"# Create validator with multiple rules\\n\",\n    \"text_validator = create_validator([\\n\",\n    \"    lambda text: isinstance(text, str),\\n\",\n    \"    lambda text: len(text) > 0,\\n\",\n    \"    lambda text: len(text) <= 1000,\\n\",\n    \"    lambda text: not text.isspace()\\n\",\n    \"])\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\n‚úÖ Validation tests:\\\")\\n\",\n    \"test_cases = [\\\"Valid text\\\", \\\"\\\", \\\"   \\\", \\\"x\\\" * 1001, 123]\\n\",\n    \"for test_case in test_cases:\\n\",\n    \"    is_valid = text_validator(test_case)\\n\",\n    \"    display = str(test_case)[:20] + \\\"...\\\" if len(str(test_case)) > 20 else str(test_case)\\n\",\n    \"    status = \\\"‚úÖ\\\" if is_valid else \\\"‚ùå\\\"\\n\",\n    \"    print(f\\\"  {status} '{display}': {is_valid}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Higher-Order Functions Enable:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Abstraction: Hide implementation details\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Customization: Create specialized versions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Code reuse: One pattern, many applications\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Flexibility: Runtime behavior modification\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 6: Currying and Partial Application\\n\",\n    \"\\n\",\n    \"**First Principle**: Currying transforms functions to enable partial application, creating specialized versions.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `functools.partial` for partial application, create curried functions for flexible APIs.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Currying for AI model predictions\\n\",\n    \"def curry_model_predict(\\n\",\n    \"    model_name: str,\\n\",\n    \"    temperature: float\\n\",\n    \") -> Callable[[str], ModelPrediction]:\\n\",\n    \"    \\\"\\\"\\\"Curried model prediction function.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def predict(text: str) -> ModelPrediction:\\n\",\n    \"        # Mock prediction logic with deterministic \\\"randomness\\\"\\n\",\n    \"        confidence = 0.8 + (hash(text) % 20) / 100\\n\",\n    \"        \\n\",\n    \"        return ModelPrediction(\\n\",\n    \"            text=text,\\n\",\n    \"            confidence=confidence,\\n\",\n    \"            model_used=model_name,\\n\",\n    \"            processing_time=0.1,\\n\",\n    \"            metadata={\\\"temperature\\\": temperature}\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    return predict\\n\",\n    \"\\n\",\n    \"# General currying utility\\n\",\n    \"def curry_binary(func: Callable[[T, U], Any]) -> Callable[[T], Callable[[U], Any]]:\\n\",\n    \"    \\\"\\\"\\\"Convert binary function to curried form.\\\"\\\"\\\"\\n\",\n    \"    return lambda x: lambda y: func(x, y)\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"ü•¢ Currying & Partial Application\\\", style=\\\"bold red\\\"))\\n\",\n    \"\\n\",\n    \"# Create specialized model predictors\\n\",\n    \"gpt4_predictor = curry_model_predict(\\\"gpt-4\\\", 0.7)\\n\",\n    \"claude_predictor = curry_model_predict(\\\"claude-3\\\", 0.5)\\n\",\n    \"creative_predictor = curry_model_predict(\\\"gpt-4-creative\\\", 1.2)\\n\",\n    \"\\n\",\n    \"test_texts = [\\\"Hello world\\\", \\\"Python is awesome\\\", \\\"AI will change everything\\\"]\\n\",\n    \"\\n\",\n    \"print(f\\\"ü§ñ Curried predictions:\\\")\\n\",\n    \"for text in test_texts:\\n\",\n    \"    gpt_pred = gpt4_predictor(text)\\n\",\n    \"    claude_pred = claude_predictor(text)\\n\",\n    \"    creative_pred = creative_predictor(text)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  '{text}':\\\")\\n\",\n    \"    print(f\\\"    GPT-4: {gpt_pred.confidence:.3f} (temp: {gpt_pred.metadata['temperature']})\\\")\\n\",\n    \"    print(f\\\"    Claude: {claude_pred.confidence:.3f} (temp: {claude_pred.metadata['temperature']})\\\")\\n\",\n    \"    print(f\\\"    Creative: {creative_pred.confidence:.3f} (temp: {creative_pred.metadata['temperature']})\\\")\\n\",\n    \"\\n\",\n    \"# Partial application examples\\n\",\n    \"from operator import add, mul\\n\",\n    \"\\n\",\n    \"multiply_by_10 = partial(mul, 10)\\n\",\n    \"add_100 = partial(add, 100)\\n\",\n    \"\\n\",\n    \"# Create specialized text processors\\n\",\n    \"def apply_transformation(text: str, operation: str, value: Any) -> str:\\n\",\n    \"    \\\"\\\"\\\"Apply transformation to text.\\\"\\\"\\\"\\n\",\n    \"    if operation == \\\"prefix\\\":\\n\",\n    \"        return f\\\"{value}{text}\\\"\\n\",\n    \"    elif operation == \\\"suffix\\\":\\n\",\n    \"        return f\\\"{text}{value}\\\"\\n\",\n    \"    elif operation == \\\"replace\\\":\\n\",\n    \"        old, new = value\\n\",\n    \"        return text.replace(old, new)\\n\",\n    \"    return text\\n\",\n    \"\\n\",\n    \"# Create specialized transformers using partial\\n\",\n    \"add_ai_prefix = partial(apply_transformation, operation=\\\"prefix\\\", value=\\\"AI: \\\")\\n\",\n    \"add_processed_suffix = partial(apply_transformation, operation=\\\"suffix\\\", value=\\\" [PROCESSED]\\\")\\n\",\n    \"replace_hello = partial(apply_transformation, operation=\\\"replace\\\", value=(\\\"hello\\\", \\\"greetings\\\"))\\n\",\n    \"\\n\",\n    \"numbers = [1, 2, 3, 4, 5]\\n\",\n    \"multiplied = list(map(multiply_by_10, numbers))\\n\",\n    \"added = list(map(add_100, numbers))\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüî¢ Partial application with numbers:\\\")\\n\",\n    \"print(f\\\"  Original: {numbers}\\\")\\n\",\n    \"print(f\\\"  √ó 10: {multiplied}\\\")\\n\",\n    \"print(f\\\"  + 100: {added}\\\")\\n\",\n    \"\\n\",\n    \"# Apply text transformations\\n\",\n    \"sample_texts = [\\\"hello world\\\", \\\"hello AI\\\", \\\"goodbye world\\\"]\\n\",\n    \"print(f\\\"\\\\nüìù Text transformations:\\\")\\n\",\n    \"for text in sample_texts:\\n\",\n    \"    prefixed = add_ai_prefix(text)\\n\",\n    \"    suffixed = add_processed_suffix(text)\\n\",\n    \"    replaced = replace_hello(text)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  '{text}':\\\")\\n\",\n    \"    print(f\\\"    Prefixed: '{prefixed}'\\\")\\n\",\n    \"    print(f\\\"    Suffixed: '{suffixed}'\\\")\\n\",\n    \"    print(f\\\"    Replaced: '{replaced}'\\\")\\n\",\n    \"\\n\",\n    \"# Curried binary function example\\n\",\n    \"curried_add = curry_binary(add)\\n\",\n    \"add_5 = curried_add(5)\\n\",\n    \"print(f\\\"\\\\n‚ûï Curried addition:\\\")\\n\",\n    \"print(f\\\"  add_5(10) = {add_5(10)}\\\")\\n\",\n    \"print(f\\\"  add_5(20) = {add_5(20)}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Currying & Partial Application Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Specialization: Create focused functions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reusability: Same logic, different parameters\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Composability: Easier to combine functions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Configuration: Parameterize behavior\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 7: Monadic Error Handling\\n\",\n    \"\\n\",\n    \"**First Principle**: Monadic patterns enable safe computation chains without explicit error checking at each step.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Create Result/Maybe types that chain operations safely, handling errors gracefully.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Result monad for safe error handling\\n\",\n    \"class Result:\\n\",\n    \"    \\\"\\\"\\\"Monadic Result type for error handling.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, value: Any = None, error: str = None):\\n\",\n    \"        self.value = value\\n\",\n    \"        self.error = error\\n\",\n    \"        self.is_success = error is None\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def success(cls, value: Any) -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Create successful result.\\\"\\\"\\\"\\n\",\n    \"        return cls(value=value)\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def failure(cls, error: str) -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Create failed result.\\\"\\\"\\\"\\n\",\n    \"        return cls(error=error)\\n\",\n    \"    \\n\",\n    \"    def map(self, func: Callable) -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Apply function if successful.\\\"\\\"\\\"\\n\",\n    \"        if self.is_success:\\n\",\n    \"            try:\\n\",\n    \"                return Result.success(func(self.value))\\n\",\n    \"            except Exception as e:\\n\",\n    \"                return Result.failure(str(e))\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def flat_map(self, func: Callable) -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Apply function that returns Result if successful.\\\"\\\"\\\"\\n\",\n    \"        if self.is_success:\\n\",\n    \"            try:\\n\",\n    \"                return func(self.value)\\n\",\n    \"            except Exception as e:\\n\",\n    \"                return Result.failure(str(e))\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def filter(self, predicate: Callable[[Any], bool], error_msg: str = \\\"Filter failed\\\") -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Filter result based on predicate.\\\"\\\"\\\"\\n\",\n    \"        if self.is_success and predicate(self.value):\\n\",\n    \"            return self\\n\",\n    \"        return Result.failure(error_msg if self.is_success else self.error)\\n\",\n    \"    \\n\",\n    \"    def get_or_else(self, default: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Get value or return default.\\\"\\\"\\\"\\n\",\n    \"        return self.value if self.is_success else default\\n\",\n    \"    \\n\",\n    \"    def __str__(self) -> str:\\n\",\n    \"        if self.is_success:\\n\",\n    \"            return f\\\"Success({self.value})\\\"\\n\",\n    \"        return f\\\"Failure({self.error})\\\"\\n\",\n    \"\\n\",\n    \"# Safe operations that return Results\\n\",\n    \"def safe_divide(a: float, b: float) -> Result:\\n\",\n    \"    \\\"\\\"\\\"Safe division that returns Result.\\\"\\\"\\\"\\n\",\n    \"    if b == 0:\\n\",\n    \"        return Result.failure(\\\"Division by zero\\\")\\n\",\n    \"    return Result.success(a / b)\\n\",\n    \"\\n\",\n    \"def safe_model_predict(text: str) -> Result:\\n\",\n    \"    \\\"\\\"\\\"Safe model prediction that can fail.\\\"\\\"\\\"\\n\",\n    \"    if not text.strip():\\n\",\n    \"        return Result.failure(\\\"Empty input text\\\")\\n\",\n    \"    \\n\",\n    \"    if len(text) > 1000:\\n\",\n    \"        return Result.failure(\\\"Text too long\\\")\\n\",\n    \"    \\n\",\n    \"    # Mock successful prediction\\n\",\n    \"    prediction = ModelPrediction(\\n\",\n    \"        text=text,\\n\",\n    \"        confidence=0.85,\\n\",\n    \"        model_used=\\\"safe-model\\\",\\n\",\n    \"        processing_time=0.1,\\n\",\n    \"        metadata={}\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    return Result.success(prediction)\\n\",\n    \"\\n\",\n    \"def safe_parse_number(text: str) -> Result:\\n\",\n    \"    \\\"\\\"\\\"Safely parse number from text.\\\"\\\"\\\"\\n\",\n    \"    try:\\n\",\n    \"        return Result.success(float(text))\\n\",\n    \"    except ValueError:\\n\",\n    \"        return Result.failure(f\\\"Cannot parse '{text}' as number\\\")\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üîÑ Monadic Error Handling\\\", style=\\\"bold yellow\\\"))\\n\",\n    \"\\n\",\n    \"# Chaining operations with error handling\\n\",\n    \"print(f\\\"üßÆ Safe calculation chains:\\\")\\n\",\n    \"\\n\",\n    \"result1 = (safe_divide(10, 2)\\n\",\n    \"           .map(lambda x: x * 3)\\n\",\n    \"           .map(lambda x: f\\\"Result: {x}\\\")\\n\",\n    \"           .filter(lambda s: \\\"15\\\" in s, \\\"Result doesn't contain 15\\\"))\\n\",\n    \"\\n\",\n    \"result2 = (safe_divide(10, 0)\\n\",\n    \"           .map(lambda x: x * 3)\\n\",\n    \"           .map(lambda x: f\\\"Result: {x}\\\"))\\n\",\n    \"\\n\",\n    \"print(f\\\"  Chain 1 (10 √∑ 2 √ó 3): {result1}\\\")\\n\",\n    \"print(f\\\"  Chain 2 (10 √∑ 0 √ó 3): {result2}\\\")\\n\",\n    \"\\n\",\n    \"# Complex parsing and calculation chain\\n\",\n    \"def calculate_average(numbers_text: List[str]) -> Result:\\n\",\n    \"    \\\"\\\"\\\"Calculate average of numbers from text list.\\\"\\\"\\\"\\n\",\n    \"    if not numbers_text:\\n\",\n    \"        return Result.failure(\\\"Empty list\\\")\\n\",\n    \"    \\n\",\n    \"    # Parse all numbers\\n\",\n    \"    parsed_numbers = []\\n\",\n    \"    for text in numbers_text:\\n\",\n    \"        result = safe_parse_number(text)\\n\",\n    \"        if not result.is_success:\\n\",\n    \"            return result\\n\",\n    \"        parsed_numbers.append(result.value)\\n\",\n    \"    \\n\",\n    \"    # Calculate average\\n\",\n    \"    average = sum(parsed_numbers) / len(parsed_numbers)\\n\",\n    \"    return Result.success(average)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüî¢ Number parsing and calculation:\\\")\\n\",\n    \"test_cases = [\\n\",\n    \"    [\\\"1\\\", \\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5\\\"],\\n\",\n    \"    [\\\"1.5\\\", \\\"2.5\\\", \\\"3.5\\\"],\\n\",\n    \"    [\\\"1\\\", \\\"not_a_number\\\", \\\"3\\\"],\\n\",\n    \"    []\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for i, case in enumerate(test_cases, 1):\\n\",\n    \"    result = calculate_average(case)\\n\",\n    \"    print(f\\\"  Case {i} {case}: {result}\\\")\\n\",\n    \"\\n\",\n    \"# Model prediction with error handling\\n\",\n    \"test_cases = [\\\"Hello world\\\", \\\"\\\", \\\"x\\\" * 1001, \\\"Valid input\\\"]\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nü§ñ Safe model predictions:\\\")\\n\",\n    \"for text in test_cases:\\n\",\n    \"    result = (safe_model_predict(text)\\n\",\n    \"              .map(lambda pred: pred.transform_text(str.upper))\\n\",\n    \"              .filter(lambda pred: pred.confidence > 0.8, \\\"Low confidence\\\"))\\n\",\n    \"    \\n\",\n    \"    display_text = text[:20] + \\\"...\\\" if len(text) > 20 else text\\n\",\n    \"    if result.is_success:\\n\",\n    \"        print(f\\\"  ‚úÖ '{display_text}': confidence {result.value.confidence}\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"  ‚ùå '{display_text}': {result.error}\\\")\\n\",\n    \"\\n\",\n    \"# Demonstrating the power of monadic chaining\\n\",\n    \"def process_user_input(input_text: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process user input with comprehensive error handling.\\\"\\\"\\\"\\n\",\n    \"    result = (safe_model_predict(input_text)\\n\",\n    \"              .filter(lambda pred: pred.confidence > 0.7, \\\"Confidence too low\\\")\\n\",\n    \"              .map(lambda pred: pred.transform_text(str.title))\\n\",\n    \"              .map(lambda pred: f\\\"Processed: {pred.text} (confidence: {pred.confidence:.2f})\\\"))\\n\",\n    \"    \\n\",\n    \"    return result.get_or_else(f\\\"Error: {result.error}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîÑ Complete processing pipeline:\\\")\\n\",\n    \"inputs = [\\\"hello world\\\", \\\"\\\", \\\"this is a good example\\\"]\\n\",\n    \"for inp in inputs:\\n\",\n    \"    output = process_user_input(inp)\\n\",\n    \"    display = inp if inp else \\\"(empty)\\\"\\n\",\n    \"    print(f\\\"  '{display}' ‚Üí {output}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Monadic Error Handling Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Safe chaining: No null pointer exceptions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Explicit errors: All failures are tracked\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Composability: Chain complex operations safely\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Railway-oriented: Success/failure paths clearly separated\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 8: Lazy Evaluation and Generators\\n\",\n    \"\\n\",\n    \"**First Principle**: Lazy evaluation computes values only when needed, enabling efficient processing of large datasets.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use generators (`yield`) for lazy evaluation, `itertools` for lazy operations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Lazy evaluation with generators\\n\",\n    \"def lazy_text_generator(texts: List[str]) -> Iterator[Dict[str, Any]]:\\n\",\n    \"    \\\"\\\"\\\"Lazily process texts and yield results.\\\"\\\"\\\"\\n\",\n    \"    for i, text in enumerate(texts):\\n\",\n    \"        print(f\\\"üîÑ Processing item {i+1}: '{text[:20]}...'\\\")\\n\",\n    \"        # Only compute when needed\\n\",\n    \"        yield {\\n\",\n    \"            \\\"text\\\": text,\\n\",\n    \"            \\\"stats\\\": calculate_text_stats(text),\\n\",\n    \"            \\\"processed_at\\\": time.time(),\\n\",\n    \"            \\\"index\\\": i\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"def create_lazy_pipeline(\\n\",\n    \"    data_source: Iterable[T],\\n\",\n    \"    *processors: Callable[[T], T]\\n\",\n    \") -> Iterator[T]:\\n\",\n    \"    \\\"\\\"\\\"Create lazy processing pipeline.\\\"\\\"\\\"\\n\",\n    \"    for item in data_source:\\n\",\n    \"        result = item\\n\",\n    \"        for processor in processors:\\n\",\n    \"            result = processor(result)\\n\",\n    \"        yield result\\n\",\n    \"\\n\",\n    \"def infinite_text_generator(base_text: str) -> Iterator[str]:\\n\",\n    \"    \\\"\\\"\\\"Generate infinite variations of text.\\\"\\\"\\\"\\n\",\n    \"    counter = 0\\n\",\n    \"    while True:\\n\",\n    \"        yield f\\\"{base_text} (variation {counter})\\\"\\n\",\n    \"        counter += 1\\n\",\n    \"\\n\",\n    \"def chunked_processor(items: Iterable[T], chunk_size: int) -> Iterator[List[T]]:\\n\",\n    \"    \\\"\\\"\\\"Process items in chunks lazily.\\\"\\\"\\\"\\n\",\n    \"    chunk = []\\n\",\n    \"    for item in items:\\n\",\n    \"        chunk.append(item)\\n\",\n    \"        if len(chunk) >= chunk_size:\\n\",\n    \"            yield chunk\\n\",\n    \"            chunk = []\\n\",\n    \"    \\n\",\n    \"    # Yield remaining items\\n\",\n    \"    if chunk:\\n\",\n    \"        yield chunk\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üò¥ Lazy Evaluation\\\", style=\\\"bold purple\\\"))\\n\",\n    \"\\n\",\n    \"# Create large dataset (but don't process yet)\\n\",\n    \"large_texts = [f\\\"Sample text number {i} for analysis\\\" for i in range(1000)]\\n\",\n    \"\\n\",\n    \"print(f\\\"üìä Lazy text processing (only processes what we consume):\\\")\\n\",\n    \"\\n\",\n    \"# Process only what we need\\n\",\n    \"lazy_processor = lazy_text_generator(large_texts[:10])\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîç Taking only first 3 items:\\\")\\n\",\n    \"count = 0\\n\",\n    \"for processed in lazy_processor:\\n\",\n    \"    if count >= 3:  # Only process first 3\\n\",\n    \"        break\\n\",\n    \"    print(f\\\"  Item {processed['index']}: {processed['stats']['word_count']} words\\\")\\n\",\n    \"    count += 1\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Notice: Only 3 items were actually processed!\\\")\\n\",\n    \"\\n\",\n    \"# Lazy pipeline with generators\\n\",\n    \"def slow_processor(text: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"Simulate slow processing.\\\"\\\"\\\"\\n\",\n    \"    time.sleep(0.001)  # Simulate processing time\\n\",\n    \"    return text.upper()\\n\",\n    \"\\n\",\n    \"def add_prefix(text: str) -> str:\\n\",\n    \"    return f\\\"PROCESSED: {text}\\\"\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîÑ Lazy pipeline (processing on demand):\\\")\\n\",\n    \"\\n\",\n    \"lazy_pipeline = create_lazy_pipeline(\\n\",\n    \"    large_texts[:5],  # Small sample\\n\",\n    \"    slow_processor,\\n\",\n    \"    add_prefix\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Only consume what we need\\n\",\n    \"first_two = []\\n\",\n    \"for i, result in enumerate(lazy_pipeline):\\n\",\n    \"    first_two.append(result)\\n\",\n    \"    if i >= 1:  # Only take first 2\\n\",\n    \"        break\\n\",\n    \"\\n\",\n    \"for result in first_two:\\n\",\n    \"    print(f\\\"  {result[:50]}...\\\")\\n\",\n    \"\\n\",\n    \"# Infinite generators (be careful!)\\n\",\n    \"print(f\\\"\\\\n‚ôæÔ∏è Infinite generator (limited consumption):\\\")\\n\",\n    \"infinite_gen = infinite_text_generator(\\\"AI prompt\\\")\\n\",\n    \"\\n\",\n    \"# Take only first 5 from infinite sequence\\n\",\n    \"for i, text in enumerate(infinite_gen):\\n\",\n    \"    if i >= 5:\\n\",\n    \"        break\\n\",\n    \"    print(f\\\"  {text}\\\")\\n\",\n    \"\\n\",\n    \"# Chunked processing\\n\",\n    \"print(f\\\"\\\\nüì¶ Chunked lazy processing:\\\")\\n\",\n    \"numbers = range(23)  # 23 numbers\\n\",\n    \"chunked = chunked_processor(numbers, chunk_size=5)\\n\",\n    \"\\n\",\n    \"for i, chunk in enumerate(chunked):\\n\",\n    \"    print(f\\\"  Chunk {i+1}: {list(chunk)}\\\")\\n\",\n    \"\\n\",\n    \"# Memory efficiency demonstration\\n\",\n    \"from itertools import islice, takewhile, dropwhile\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüß† Memory-efficient operations with itertools:\\\")\\n\",\n    \"\\n\",\n    \"# Create large range without storing all values\\n\",\n    \"large_range = range(1000000)  # 1 million numbers\\n\",\n    \"\\n\",\n    \"# Take first 5 even numbers greater than 100\\n\",\n    \"result = list(islice(\\n\",\n    \"    filter(lambda x: x % 2 == 0, \\n\",\n    \"           dropwhile(lambda x: x <= 100, large_range)), \\n\",\n    \"    5\\n\",\n    \"))\\n\",\n    \"\\n\",\n    \"print(f\\\"  First 5 even numbers > 100: {result}\\\")\\n\",\n    \"\\n\",\n    \"# Generator expression vs list comprehension\\n\",\n    \"print(f\\\"\\\\n‚ö° Generator vs List Memory Usage:\\\")\\n\",\n    \"\\n\",\n    \"# Generator expression - lazy\\n\",\n    \"squares_gen = (x**2 for x in range(10000))\\n\",\n    \"print(f\\\"  Generator created (no memory used yet)\\\")\\n\",\n    \"\\n\",\n    \"# Take only first 3\\n\",\n    \"first_three_squares = [next(squares_gen) for _ in range(3)]\\n\",\n    \"print(f\\\"  First 3 squares: {first_three_squares}\\\")\\n\",\n    \"\\n\",\n    \"# Chain multiple lazy operations\\n\",\n    \"from itertools import chain\\n\",\n    \"\\n\",\n    \"gen1 = (f\\\"AI-{i}\\\" for i in range(3))\\n\",\n    \"gen2 = (f\\\"ML-{i}\\\" for i in range(3))\\n\",\n    \"gen3 = (f\\\"DL-{i}\\\" for i in range(3))\\n\",\n    \"\\n\",\n    \"chained = chain(gen1, gen2, gen3)\\n\",\n    \"all_items = list(chained)\\n\",\n    \"print(f\\\"  Chained generators: {all_items}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Lazy Evaluation Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Memory efficiency: Process large datasets\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Performance: Only compute what's needed\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Composability: Chain operations without intermediate storage\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Infinite sequences: Work with unbounded data\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 9: Memoization and Caching\\n\",\n    \"\\n\",\n    \"**First Principle**: Memoization caches function results to avoid recomputing expensive operations.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `@lru_cache` for automatic memoization, implement custom caches for specific needs.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Memoization for expensive operations\\n\",\n    \"@lru_cache(maxsize=128)\\n\",\n    \"def expensive_text_analysis(text: str) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Expensive analysis with memoization.\\\"\\\"\\\"\\n\",\n    \"    print(f\\\"üîç Computing expensive analysis for: '{text[:30]}...'\\\")\\n\",\n    \"    # Simulate expensive computation\\n\",\n    \"    time.sleep(0.01)\\n\",\n    \"    \\n\",\n    \"    tokens = tokenize_text(text)\\n\",\n    \"    return {\\n\",\n    \"        \\\"complexity_score\\\": len(set(tokens)) / len(tokens) if tokens else 0,\\n\",\n    \"        \\\"sentiment_score\\\": hash(text) % 100 / 100,\\n\",\n    \"        \\\"readability\\\": len(tokens) / text.count('.') if '.' in text else len(tokens),\\n\",\n    \"        \\\"processing_time\\\": 0.01\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"# Custom memoization decorator\\n\",\n    \"def memoize_with_ttl(ttl_seconds: float = 60.0):\\n\",\n    \"    \\\"\\\"\\\"Memoization with time-to-live.\\\"\\\"\\\"\\n\",\n    \"    def decorator(func):\\n\",\n    \"        cache = {}\\n\",\n    \"        \\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            # Create cache key\\n\",\n    \"            key = str(args) + str(sorted(kwargs.items()))\\n\",\n    \"            current_time = time.time()\\n\",\n    \"            \\n\",\n    \"            # Check if cached and not expired\\n\",\n    \"            if key in cache:\\n\",\n    \"                result, timestamp = cache[key]\\n\",\n    \"                if current_time - timestamp < ttl_seconds:\\n\",\n    \"                    print(f\\\"üíæ Cache hit for {func.__name__}\\\")\\n\",\n    \"                    return result\\n\",\n    \"                else:\\n\",\n    \"                    print(f\\\"‚è∞ Cache expired for {func.__name__}\\\")\\n\",\n    \"            \\n\",\n    \"            # Compute new result\\n\",\n    \"            print(f\\\"üîÑ Cache miss for {func.__name__}\\\")\\n\",\n    \"            result = func(*args, **kwargs)\\n\",\n    \"            cache[key] = (result, current_time)\\n\",\n    \"            return result\\n\",\n    \"        \\n\",\n    \"        # Add cache management methods\\n\",\n    \"        wrapper.clear_cache = lambda: cache.clear()\\n\",\n    \"        wrapper.cache_info = lambda: {\\n\",\n    \"            'size': len(cache),\\n\",\n    \"            'ttl_seconds': ttl_seconds\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"    return decorator\\n\",\n    \"\\n\",\n    \"@memoize_with_ttl(ttl_seconds=2.0)\\n\",\n    \"def simulate_ai_api_call(query: str, model: str = \\\"gpt-4\\\") -> str:\\n\",\n    \"    \\\"\\\"\\\"Simulate expensive AI API call.\\\"\\\"\\\"\\n\",\n    \"    time.sleep(0.1)  # Simulate network delay\\n\",\n    \"    return f\\\"{model} response to: {query}\\\"\\n\",\n    \"\\n\",\n    \"# Fibonacci with and without memoization (classic example)\\n\",\n    \"def fibonacci_naive(n: int) -> int:\\n\",\n    \"    \\\"\\\"\\\"Naive Fibonacci (exponential time).\\\"\\\"\\\"\\n\",\n    \"    if n <= 1:\\n\",\n    \"        return n\\n\",\n    \"    return fibonacci_naive(n - 1) + fibonacci_naive(n - 2)\\n\",\n    \"\\n\",\n    \"@lru_cache(maxsize=None)\\n\",\n    \"def fibonacci_memoized(n: int) -> int:\\n\",\n    \"    \\\"\\\"\\\"Memoized Fibonacci (linear time).\\\"\\\"\\\"\\n\",\n    \"    if n <= 1:\\n\",\n    \"        return n\\n\",\n    \"    return fibonacci_memoized(n - 1) + fibonacci_memoized(n - 2)\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üß† Memoization\\\", style=\\\"bold orange\\\"))\\n\",\n    \"\\n\",\n    \"# Test expensive analysis with repeats\\n\",\n    \"test_texts = [\\n\",\n    \"    \\\"This is a test text for analysis and processing\\\",\\n\",\n    \"    \\\"Another sample text for computational analysis\\\",\\n\",\n    \"    \\\"This is a test text for analysis and processing\\\",  # Duplicate\\n\",\n    \"    \\\"Final text sample for testing our algorithms\\\",\\n\",\n    \"    \\\"Another sample text for computational analysis\\\",  # Duplicate\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(f\\\"üìä Memoized expensive analysis:\\\")\\n\",\n    \"\\n\",\n    \"# First run - will compute all\\n\",\n    \"start_time = time.time()\\n\",\n    \"for i, text in enumerate(test_texts, 1):\\n\",\n    \"    analysis = expensive_text_analysis(text)\\n\",\n    \"    print(f\\\"  {i}. Complexity: {analysis['complexity_score']:.3f}\\\")\\n\",\n    \"first_run_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìà Cache info: {expensive_text_analysis.cache_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Second run - should be faster due to caching\\n\",\n    \"print(f\\\"\\\\nüîÑ Running same analysis again (should hit cache):\\\")\\n\",\n    \"start_time = time.time()\\n\",\n    \"for i, text in enumerate(test_texts, 1):\\n\",\n    \"    analysis = expensive_text_analysis(text)\\n\",\n    \"second_run_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\n‚ö° Performance comparison:\\\")\\n\",\n    \"print(f\\\"  First run: {first_run_time:.4f}s\\\")\\n\",\n    \"print(f\\\"  Second run: {second_run_time:.4f}s\\\")\\n\",\n    \"print(f\\\"  Speedup: {first_run_time / second_run_time:.1f}x\\\")\\n\",\n    \"\\n\",\n    \"# Test TTL-based caching\\n\",\n    \"print(f\\\"\\\\n‚è∞ TTL-based caching (2 second TTL):\\\")\\n\",\n    \"\\n\",\n    \"# First call\\n\",\n    \"result1 = simulate_ai_api_call(\\\"What is AI?\\\", \\\"gpt-4\\\")\\n\",\n    \"print(f\\\"  Result 1: {result1[:30]}...\\\")\\n\",\n    \"\\n\",\n    \"# Second call immediately (should hit cache)\\n\",\n    \"result2 = simulate_ai_api_call(\\\"What is AI?\\\", \\\"gpt-4\\\")\\n\",\n    \"print(f\\\"  Result 2: {result2[:30]}...\\\")\\n\",\n    \"\\n\",\n    \"# Wait for cache to expire\\n\",\n    \"print(f\\\"  Waiting for cache to expire...\\\")\\n\",\n    \"time.sleep(2.1)\\n\",\n    \"\\n\",\n    \"# Third call (should miss cache)\\n\",\n    \"result3 = simulate_ai_api_call(\\\"What is AI?\\\", \\\"gpt-4\\\")\\n\",\n    \"print(f\\\"  Result 3: {result3[:30]}...\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nCache info: {simulate_ai_api_call.cache_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Fibonacci performance comparison\\n\",\n    \"print(f\\\"\\\\nüî¢ Fibonacci performance (memoized vs naive):\\\")\\n\",\n    \"\\n\",\n    \"n = 30\\n\",\n    \"\\n\",\n    \"# Memoized version\\n\",\n    \"start_time = time.time()\\n\",\n    \"fib_memoized = fibonacci_memoized(n)\\n\",\n    \"memoized_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"print(f\\\"  Memoized fib({n}) = {fib_memoized} in {memoized_time:.6f}s\\\")\\n\",\n    \"print(f\\\"  Cache info: {fibonacci_memoized.cache_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Naive version (only for small n to avoid timeout)\\n\",\n    \"n_small = 25\\n\",\n    \"start_time = time.time()\\n\",\n    \"fib_naive = fibonacci_naive(n_small)\\n\",\n    \"naive_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"print(f\\\"  Naive fib({n_small}) = {fib_naive} in {naive_time:.6f}s\\\")\\n\",\n    \"print(f\\\"  Speedup for fib({n_small}): {naive_time / memoized_time:.1f}x\\\")\\n\",\n    \"\\n\",\n    \"# Practical memoization patterns\\n\",\n    \"class MemoizedModelProcessor:\\n\",\n    \"    \\\"\\\"\\\"AI processor with built-in memoization.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.cache = {}\\n\",\n    \"        self.cache_hits = 0\\n\",\n    \"        self.cache_misses = 0\\n\",\n    \"    \\n\",\n    \"    def process(self, text: str, model_config: Dict[str, Any]) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Process text with caching.\\\"\\\"\\\"\\n\",\n    \"        # Create cache key from text and config\\n\",\n    \"        cache_key = hash(text + str(sorted(model_config.items())))\\n\",\n    \"        \\n\",\n    \"        if cache_key in self.cache:\\n\",\n    \"            self.cache_hits += 1\\n\",\n    \"            return self.cache[cache_key]\\n\",\n    \"        \\n\",\n    \"        # Simulate processing\\n\",\n    \"        self.cache_misses += 1\\n\",\n    \"        result = {\\n\",\n    \"            \\\"processed_text\\\": text.upper(),\\n\",\n    \"            \\\"score\\\": hash(text) % 100 / 100,\\n\",\n    \"            \\\"model_config\\\": model_config\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        self.cache[cache_key] = result\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    def get_cache_stats(self) -> Dict[str, Any]:\\n\",\n    \"        total = self.cache_hits + self.cache_misses\\n\",\n    \"        hit_rate = self.cache_hits / total if total > 0 else 0\\n\",\n    \"        return {\\n\",\n    \"            \\\"cache_size\\\": len(self.cache),\\n\",\n    \"            \\\"hits\\\": self.cache_hits,\\n\",\n    \"            \\\"misses\\\": self.cache_misses,\\n\",\n    \"            \\\"hit_rate\\\": hit_rate\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nü§ñ Practical memoization in AI processing:\\\")\\n\",\n    \"\\n\",\n    \"processor = MemoizedModelProcessor()\\n\",\n    \"configs = [\\n\",\n    \"    {\\\"temperature\\\": 0.7, \\\"model\\\": \\\"gpt-4\\\"},\\n\",\n    \"    {\\\"temperature\\\": 0.5, \\\"model\\\": \\\"gpt-4\\\"},\\n\",\n    \"    {\\\"temperature\\\": 0.7, \\\"model\\\": \\\"gpt-4\\\"},  # Same as first\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"test_inputs = [\\\"hello world\\\", \\\"python rocks\\\", \\\"hello world\\\"]  # Some repeats\\n\",\n    \"\\n\",\n    \"for i, (text, config) in enumerate(zip(test_inputs, configs), 1):\\n\",\n    \"    result = processor.process(text, config)\\n\",\n    \"    print(f\\\"  {i}. '{text}' ‚Üí score: {result['score']:.3f}\\\")\\n\",\n    \"\\n\",\n    \"stats = processor.get_cache_stats()\\n\",\n    \"print(f\\\"\\\\nüìä Final cache statistics: {stats}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Memoization Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Performance: Avoid redundant computations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Efficiency: Especially good for recursive algorithms\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Transparency: Functions remain pure\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Configurability: TTL, size limits, custom strategies\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 10: Advanced Functional Patterns\\n\",\n    \"\\n\",\n    \"**First Principle**: Advanced patterns combine multiple functional concepts for sophisticated AI architectures.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use single dispatch, decorators, and functional composition to create flexible, maintainable systems.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Single dispatch for different data types\\n\",\n    \"@singledispatch\\n\",\n    \"def process_input(data) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process input data (default implementation).\\\"\\\"\\\"\\n\",\n    \"    return f\\\"Processed unknown type: {type(data).__name__}\\\"\\n\",\n    \"\\n\",\n    \"@process_input.register\\n\",\n    \"def _(data: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process string input.\\\"\\\"\\\"\\n\",\n    \"    stats = calculate_text_stats(data)\\n\",\n    \"    return f\\\"String: {stats['word_count']} words, {stats['character_count']} chars\\\"\\n\",\n    \"\\n\",\n    \"@process_input.register\\n\",\n    \"def _(data: list) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process list input.\\\"\\\"\\\"\\n\",\n    \"    return f\\\"List: {len(data)} items, types: {[type(x).__name__ for x in data[:3]]}\\\"\\n\",\n    \"\\n\",\n    \"@process_input.register\\n\",\n    \"def _(data: dict) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process dictionary input.\\\"\\\"\\\"\\n\",\n    \"    return f\\\"Dict: {len(data)} keys: {list(data.keys())[:3]}\\\"\\n\",\n    \"\\n\",\n    \"@process_input.register\\n\",\n    \"def _(data: ModelPrediction) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process model prediction input.\\\"\\\"\\\"\\n\",\n    \"    return f\\\"Prediction: '{data.text}' confidence={data.confidence:.3f}\\\"\\n\",\n    \"\\n\",\n    \"# Functional decorators for AI operations\\n\",\n    \"def validate_input(validator: Callable[[Any], bool], error_msg: str = \\\"Invalid input\\\"):\\n\",\n    \"    \\\"\\\"\\\"Decorator that validates function input.\\\"\\\"\\\"\\n\",\n    \"    def decorator(func):\\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            if args and not validator(args[0]):\\n\",\n    \"                raise ValueError(f\\\"{error_msg}: {args[0]}\\\")\\n\",\n    \"            return func(*args, **kwargs)\\n\",\n    \"        return wrapper\\n\",\n    \"    return decorator\\n\",\n    \"\\n\",\n    \"def retry_with_backoff(max_attempts: int = 3, base_delay: float = 1.0):\\n\",\n    \"    \\\"\\\"\\\"Decorator for retrying failed operations.\\\"\\\"\\\"\\n\",\n    \"    def decorator(func):\\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            last_exception = None\\n\",\n    \"            \\n\",\n    \"            for attempt in range(max_attempts):\\n\",\n    \"                try:\\n\",\n    \"                    return func(*args, **kwargs)\\n\",\n    \"                except Exception as e:\\n\",\n    \"                    last_exception = e\\n\",\n    \"                    if attempt < max_attempts - 1:\\n\",\n    \"                        delay = base_delay * (2 ** attempt)\\n\",\n    \"                        print(f\\\"üîÑ Retry {attempt + 1}/{max_attempts} in {delay:.1f}s\\\")\\n\",\n    \"                        time.sleep(delay)\\n\",\n    \"            \\n\",\n    \"            raise last_exception\\n\",\n    \"        return wrapper\\n\",\n    \"    return decorator\\n\",\n    \"\\n\",\n    \"# Complete functional AI pipeline\\n\",\n    \"class FunctionalAIPipeline:\\n\",\n    \"    \\\"\\\"\\\"Complete AI processing pipeline using functional patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.processors = []\\n\",\n    \"        self.validators = []\\n\",\n    \"        self.transformers = []\\n\",\n    \"    \\n\",\n    \"    def add_validator(self, validator: Callable[[Any], bool], error_msg: str = \\\"Validation failed\\\"):\\n\",\n    \"        \\\"\\\"\\\"Add input validator.\\\"\\\"\\\"\\n\",\n    \"        self.validators.append((validator, error_msg))\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def add_transformer(self, transformer: Callable[[Any], Any]):\\n\",\n    \"        \\\"\\\"\\\"Add data transformer.\\\"\\\"\\\"\\n\",\n    \"        self.transformers.append(transformer)\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def add_processor(self, processor: Callable[[Any], Any]):\\n\",\n    \"        \\\"\\\"\\\"Add main processor.\\\"\\\"\\\"\\n\",\n    \"        self.processors.append(processor)\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def process(self, data: Any) -> Result:\\n\",\n    \"        \\\"\\\"\\\"Process data through the complete pipeline.\\\"\\\"\\\"\\n\",\n    \"        try:\\n\",\n    \"            # Validation phase\\n\",\n    \"            for validator, error_msg in self.validators:\\n\",\n    \"                if not validator(data):\\n\",\n    \"                    return Result.failure(error_msg)\\n\",\n    \"            \\n\",\n    \"            # Transformation phase\\n\",\n    \"            current_data = data\\n\",\n    \"            for transformer in self.transformers:\\n\",\n    \"                current_data = transformer(current_data)\\n\",\n    \"            \\n\",\n    \"            # Processing phase\\n\",\n    \"            for processor in self.processors:\\n\",\n    \"                current_data = processor(current_data)\\n\",\n    \"            \\n\",\n    \"            return Result.success(current_data)\\n\",\n    \"            \\n\",\n    \"        except Exception as e:\\n\",\n    \"            return Result.failure(str(e))\\n\",\n    \"\\n\",\n    \"# Example functions using decorators\\n\",\n    \"@validate_input(lambda x: isinstance(x, str) and len(x) > 0, \\\"Text must be non-empty string\\\")\\n\",\n    \"@retry_with_backoff(max_attempts=2, base_delay=0.1)\\n\",\n    \"@lru_cache(maxsize=64)\\n\",\n    \"def advanced_text_processor(text: str) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Advanced text processor with validation, retry, and caching.\\\"\\\"\\\"\\n\",\n    \"    # Simulate potential failure\\n\",\n    \"    import random\\n\",\n    \"    if random.random() < 0.3:  # 30% chance of failure\\n\",\n    \"        raise ConnectionError(\\\"Simulated processing error\\\")\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"processed_text\\\": text.upper(),\\n\",\n    \"        \\\"word_count\\\": len(text.split()),\\n\",\n    \"        \\\"sentiment\\\": \\\"positive\\\" if \\\"good\\\" in text.lower() else \\\"neutral\\\",\\n\",\n    \"        \\\"complexity\\\": len(set(text.lower())) / len(text) if text else 0\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üöÄ Advanced Functional Patterns\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"\\n\",\n    \"# Single dispatch demonstration\\n\",\n    \"test_data = [\\n\",\n    \"    \\\"Hello functional world\\\",\\n\",\n    \"    [1, 2, 3, \\\"text\\\", True],\\n\",\n    \"    {\\\"name\\\": \\\"AI\\\", \\\"type\\\": \\\"system\\\", \\\"version\\\": 1.0},\\n\",\n    \"    ModelPrediction(\\\"test\\\", 0.95, \\\"gpt-4\\\", 0.1, {}),\\n\",\n    \"    42.5\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(f\\\"üéØ Single dispatch processing:\\\")\\n\",\n    \"for i, data in enumerate(test_data, 1):\\n\",\n    \"    result = process_input(data)\\n\",\n    \"    print(f\\\"  {i}. {type(data).__name__}: {result}\\\")\\n\",\n    \"\\n\",\n    \"# Advanced processor with decorators\\n\",\n    \"print(f\\\"\\\\nüîß Advanced processor (validation + retry + caching):\\\")\\n\",\n    \"\\n\",\n    \"test_texts = [\\\"good example\\\", \\\"another test\\\", \\\"good example\\\", \\\"\\\"]  # Last one should fail\\n\",\n    \"\\n\",\n    \"for i, text in enumerate(test_texts, 1):\\n\",\n    \"    try:\\n\",\n    \"        result = advanced_text_processor(text)\\n\",\n    \"        print(f\\\"  {i}. ‚úÖ '{text}': {result['sentiment']} (complexity: {result['complexity']:.3f})\\\")\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"  {i}. ‚ùå '{text}': {e}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìä Cache info: {advanced_text_processor.cache_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Complete functional pipeline\\n\",\n    \"print(f\\\"\\\\nüîÑ Complete functional AI pipeline:\\\")\\n\",\n    \"\\n\",\n    \"pipeline = (FunctionalAIPipeline()\\n\",\n    \"           .add_validator(lambda x: isinstance(x, str), \\\"Input must be string\\\")\\n\",\n    \"           .add_validator(lambda x: len(x.strip()) > 0, \\\"Input must not be empty\\\")\\n\",\n    \"           .add_validator(lambda x: len(x) <= 1000, \\\"Input too long\\\")\\n\",\n    \"           .add_transformer(str.strip)\\n\",\n    \"           .add_transformer(str.lower)\\n\",\n    \"           .add_processor(calculate_text_stats)\\n\",\n    \"           .add_processor(lambda stats: {**stats, \\\"quality_score\\\": stats[\\\"unique_words\\\"] / stats[\\\"word_count\\\"] if stats[\\\"word_count\\\"] > 0 else 0}))\\n\",\n    \"\\n\",\n    \"pipeline_tests = [\\n\",\n    \"    \\\"  This is a great example of functional programming!  \\\",\\n\",\n    \"    \\\"\\\",\\n\",\n    \"    \\\"Short\\\",\\n\",\n    \"    \\\"x\\\" * 1001,  # Too long\\n\",\n    \"    123  # Wrong type\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for i, test_input in enumerate(pipeline_tests, 1):\\n\",\n    \"    result = pipeline.process(test_input)\\n\",\n    \"    display = str(test_input)[:30] + \\\"...\\\" if len(str(test_input)) > 30 else str(test_input)\\n\",\n    \"    \\n\",\n    \"    if result.is_success:\\n\",\n    \"        data = result.value\\n\",\n    \"        print(f\\\"  {i}. ‚úÖ '{display}': {data['word_count']} words, quality: {data['quality_score']:.3f}\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"  {i}. ‚ùå '{display}': {result.error}\\\")\\n\",\n    \"\\n\",\n    \"# Functional composition showcase\\n\",\n    \"print(f\\\"\\\\nüéº Functional composition showcase:\\\")\\n\",\n    \"\\n\",\n    \"# Create a complex text processing function by composing simple ones\\n\",\n    \"text_normalizer = pipe(\\n\",\n    \"    str.strip,\\n\",\n    \"    str.lower,\\n\",\n    \"    lambda text: text.replace(\\\"'\\\", \\\"\\\"),  # Remove apostrophes\\n\",\n    \"    lambda text: \\\" \\\".join(text.split())  # Normalize whitespace\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"text_analyzer = compose(\\n\",\n    \"    lambda stats: f\\\"Quality text: {stats['word_count']} words, {stats['unique_words']} unique (quality: {stats['unique_words']/stats['word_count']:.2f})\\\",\\n\",\n    \"    calculate_text_stats,\\n\",\n    \"    text_normalizer\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"messy_text = \\\"  Hello   World!   This   is   a   GREAT   example...   \\\"\\n\",\n    \"analysis = text_analyzer(messy_text)\\n\",\n    \"print(f\\\"  Input: '{messy_text}'\\\")\\n\",\n    \"print(f\\\"  Analysis: {analysis}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Advanced Functional Programming Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Modularity: Small, composable pieces\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reliability: Pure functions + error handling\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Performance: Lazy evaluation + memoization\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Maintainability: Clear data flow\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Testability: Each function can be tested independently\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Scalability: Parallel processing safe\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 11: Real-World Application\\n\",\n    \"\\n\",\n    \"**First Principle**: Combine all functional programming concepts to build a production-ready AI text processing system.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use functional patterns as the backbone for reliable, scalable AI applications.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Complete functional AI text processing system\\n\",\n    \"from dataclasses import dataclass\\n\",\n    \"from typing import Protocol\\n\",\n    \"\\n\",\n    \"@dataclass(frozen=True)\\n\",\n    \"class ProcessingConfig:\\n\",\n    \"    \\\"\\\"\\\"Immutable processing configuration.\\\"\\\"\\\"\\n\",\n    \"    min_length: int = 5\\n\",\n    \"    max_length: int = 10000\\n\",\n    \"    enable_sentiment: bool = True\\n\",\n    \"    enable_complexity: bool = True\\n\",\n    \"    cache_ttl: float = 300.0  # 5 minutes\\n\",\n    \"\\n\",\n    \"@dataclass(frozen=True)\\n\",\n    \"class ProcessingResult:\\n\",\n    \"    \\\"\\\"\\\"Immutable processing result.\\\"\\\"\\\"\\n\",\n    \"    original_text: str\\n\",\n    \"    normalized_text: str\\n\",\n    \"    word_count: int\\n\",\n    \"    sentiment: Optional[str] = None\\n\",\n    \"    complexity_score: Optional[float] = None\\n\",\n    \"    processing_time: float = 0.0\\n\",\n    \"    cached: bool = False\\n\",\n    \"\\n\",\n    \"class FunctionalTextProcessor:\\n\",\n    \"    \\\"\\\"\\\"Production-ready functional text processor.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, config: ProcessingConfig):\\n\",\n    \"        self.config = config\\n\",\n    \"        self.cache = {}\\n\",\n    \"        self.metrics = {\\\"total_processed\\\": 0, \\\"cache_hits\\\": 0, \\\"errors\\\": 0}\\n\",\n    \"        \\n\",\n    \"        # Create processing pipeline using functional composition\\n\",\n    \"        self.normalizer = pipe(\\n\",\n    \"            str.strip,\\n\",\n    \"            str.lower,\\n\",\n    \"            lambda text: \\\" \\\".join(text.split())  # Normalize whitespace\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        # Create validators\\n\",\n    \"        self.validators = [\\n\",\n    \"            (lambda text: isinstance(text, str), \\\"Input must be a string\\\"),\\n\",\n    \"            (lambda text: len(text.strip()) >= self.config.min_length, f\\\"Text too short (min: {self.config.min_length})\\\"),\\n\",\n    \"            (lambda text: len(text) <= self.config.max_length, f\\\"Text too long (max: {self.config.max_length})\\\")\\n\",\n    \"        ]\\n\",\n    \"    \\n\",\n    \"    def _validate_input(self, text: str) -> Result:\\n\",\n    \"        \\\"\\\"\\\"Validate input using functional approach.\\\"\\\"\\\"\\n\",\n    \"        for validator, error_msg in self.validators:\\n\",\n    \"            if not validator(text):\\n\",\n    \"                return Result.failure(error_msg)\\n\",\n    \"        return Result.success(text)\\n\",\n    \"    \\n\",\n    \"    def _get_cache_key(self, text: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Generate cache key for text.\\\"\\\"\\\"\\n\",\n    \"        config_hash = hash(str(self.config))\\n\",\n    \"        text_hash = hash(text)\\n\",\n    \"        return f\\\"{config_hash}_{text_hash}\\\"\\n\",\n    \"    \\n\",\n    \"    def _check_cache(self, cache_key: str) -> Optional[ProcessingResult]:\\n\",\n    \"        \\\"\\\"\\\"Check if result is cached and not expired.\\\"\\\"\\\"\\n\",\n    \"        if cache_key in self.cache:\\n\",\n    \"            result, timestamp = self.cache[cache_key]\\n\",\n    \"            if time.time() - timestamp < self.config.cache_ttl:\\n\",\n    \"                self.metrics[\\\"cache_hits\\\"] += 1\\n\",\n    \"                # Return new instance with cached flag set\\n\",\n    \"                return replace(result, cached=True)\\n\",\n    \"        return None\\n\",\n    \"    \\n\",\n    \"    def _compute_sentiment(self, text: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Compute sentiment (simplified).\\\"\\\"\\\"\\n\",\n    \"        positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic'}\\n\",\n    \"        negative_words = {'bad', 'terrible', 'awful', 'horrible', 'disgusting', 'hate'}\\n\",\n    \"        \\n\",\n    \"        words = set(text.lower().split())\\n\",\n    \"        positive_count = len(words.intersection(positive_words))\\n\",\n    \"        negative_count = len(words.intersection(negative_words))\\n\",\n    \"        \\n\",\n    \"        if positive_count > negative_count:\\n\",\n    \"            return \\\"positive\\\"\\n\",\n    \"        elif negative_count > positive_count:\\n\",\n    \"            return \\\"negative\\\"\\n\",\n    \"        else:\\n\",\n    \"            return \\\"neutral\\\"\\n\",\n    \"    \\n\",\n    \"    def _compute_complexity(self, text: str) -> float:\\n\",\n    \"        \\\"\\\"\\\"Compute text complexity score.\\\"\\\"\\\"\\n\",\n    \"        words = text.split()\\n\",\n    \"        if not words:\\n\",\n    \"            return 0.0\\n\",\n    \"        \\n\",\n    \"        unique_words = len(set(words))\\n\",\n    \"        total_words = len(words)\\n\",\n    \"        avg_word_length = sum(len(word) for word in words) / total_words\\n\",\n    \"        \\n\",\n    \"        # Complexity combines lexical diversity and word length\\n\",\n    \"        lexical_diversity = unique_words / total_words\\n\",\n    \"        length_factor = min(avg_word_length / 5.0, 1.0)  # Normalize to 0-1\\n\",\n    \"        \\n\",\n    \"        return (lexical_diversity + length_factor) / 2.0\\n\",\n    \"    \\n\",\n    \"    def process(self, text: str) -> Result[ProcessingResult]:\\n\",\n    \"        \\\"\\\"\\\"Process text using functional pipeline.\\\"\\\"\\\"\\n\",\n    \"        start_time = time.time()\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            # Validation phase\\n\",\n    \"            validation_result = self._validate_input(text)\\n\",\n    \"            if not validation_result.is_success:\\n\",\n    \"                self.metrics[\\\"errors\\\"] += 1\\n\",\n    \"                return validation_result\\n\",\n    \"            \\n\",\n    \"            # Check cache\\n\",\n    \"            cache_key = self._get_cache_key(text)\\n\",\n    \"            cached_result = self._check_cache(cache_key)\\n\",\n    \"            if cached_result:\\n\",\n    \"                return Result.success(cached_result)\\n\",\n    \"            \\n\",\n    \"            # Normalize text\\n\",\n    \"            normalized_text = self.normalizer(text)\\n\",\n    \"            word_count = len(normalized_text.split())\\n\",\n    \"            \\n\",\n    \"            # Optional processing based on config\\n\",\n    \"            sentiment = self._compute_sentiment(normalized_text) if self.config.enable_sentiment else None\\n\",\n    \"            complexity = self._compute_complexity(normalized_text) if self.config.enable_complexity else None\\n\",\n    \"            \\n\",\n    \"            processing_time = time.time() - start_time\\n\",\n    \"            \\n\",\n    \"            # Create result\\n\",\n    \"            result = ProcessingResult(\\n\",\n    \"                original_text=text,\\n\",\n    \"                normalized_text=normalized_text,\\n\",\n    \"                word_count=word_count,\\n\",\n    \"                sentiment=sentiment,\\n\",\n    \"                complexity_score=complexity,\\n\",\n    \"                processing_time=processing_time,\\n\",\n    \"                cached=False\\n\",\n    \"            )\\n\",\n    \"            \\n\",\n    \"            # Cache result\\n\",\n    \"            self.cache[cache_key] = (result, time.time())\\n\",\n    \"            \\n\",\n    \"            # Update metrics\\n\",\n    \"            self.metrics[\\\"total_processed\\\"] += 1\\n\",\n    \"            \\n\",\n    \"            return Result.success(result)\\n\",\n    \"            \\n\",\n    \"        except Exception as e:\\n\",\n    \"            self.metrics[\\\"errors\\\"] += 1\\n\",\n    \"            return Result.failure(f\\\"Processing error: {str(e)}\\\")\\n\",\n    \"    \\n\",\n    \"    def process_batch(self, texts: List[str]) -> List[Result[ProcessingResult]]:\\n\",\n    \"        \\\"\\\"\\\"Process multiple texts efficiently.\\\"\\\"\\\"\\n\",\n    \"        return [self.process(text) for text in texts]\\n\",\n    \"    \\n\",\n    \"    def get_metrics(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get processing metrics.\\\"\\\"\\\"\\n\",\n    \"        total = self.metrics[\\\"total_processed\\\"] + self.metrics[\\\"cache_hits\\\"]\\n\",\n    \"        cache_hit_rate = self.metrics[\\\"cache_hits\\\"] / total if total > 0 else 0\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            **self.metrics,\\n\",\n    \"            \\\"cache_size\\\": len(self.cache),\\n\",\n    \"            \\\"cache_hit_rate\\\": cache_hit_rate\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üè≠ Production Functional AI System\\\", style=\\\"bold green\\\"))\\n\",\n    \"\\n\",\n    \"# Create processor with configuration\\n\",\n    \"config = ProcessingConfig(\\n\",\n    \"    min_length=3,\\n\",\n    \"    max_length=500,\\n\",\n    \"    enable_sentiment=True,\\n\",\n    \"    enable_complexity=True,\\n\",\n    \"    cache_ttl=60.0\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"processor = FunctionalTextProcessor(config)\\n\",\n    \"\\n\",\n    \"# Test with various inputs\\n\",\n    \"test_inputs = [\\n\",\n    \"    \\\"This is a great example of functional programming in AI!\\\",\\n\",\n    \"    \\\"Python is an amazing language for data science.\\\",\\n\",\n    \"    \\\"Bad terrible awful experience with this product.\\\",\\n\",\n    \"    \\\"The quick brown fox jumps over the lazy dog.\\\",\\n\",\n    \"    \\\"This is a great example of functional programming in AI!\\\",  # Duplicate for cache test\\n\",\n    \"    \\\"\\\",  # Should fail validation\\n\",\n    \"    \\\"AI\\\",  # Too short\\n\",\n    \"    \\\"Simple text for processing\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(f\\\"üîÑ Processing {len(test_inputs)} texts:\\\")\\n\",\n    \"\\n\",\n    \"results = processor.process_batch(test_inputs)\\n\",\n    \"\\n\",\n    \"for i, (text, result) in enumerate(zip(test_inputs, results), 1):\\n\",\n    \"    display_text = text[:40] + \\\"...\\\" if len(text) > 40 else text\\n\",\n    \"    \\n\",\n    \"    if result.is_success:\\n\",\n    \"        data = result.value\\n\",\n    \"        cache_indicator = \\\"üíæ\\\" if data.cached else \\\"üîÑ\\\"\\n\",\n    \"        print(f\\\"  {i}. {cache_indicator} '{display_text}'\\\")\\n\",\n    \"        print(f\\\"     Words: {data.word_count}, Sentiment: {data.sentiment}, Complexity: {data.complexity_score:.3f if data.complexity_score else 'N/A'}\\\")\\n\",\n    \"        print(f\\\"     Time: {data.processing_time:.4f}s\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"  {i}. ‚ùå '{display_text}': {result.error}\\\")\\n\",\n    \"\\n\",\n    \"# Show metrics\\n\",\n    \"metrics = processor.get_metrics()\\n\",\n    \"print(f\\\"\\\\nüìä Processing Metrics:\\\")\\n\",\n    \"print(f\\\"  Total processed: {metrics['total_processed']}\\\")\\n\",\n    \"print(f\\\"  Cache hits: {metrics['cache_hits']}\\\")\\n\",\n    \"print(f\\\"  Cache hit rate: {metrics['cache_hit_rate']:.1%}\\\")\\n\",\n    \"print(f\\\"  Errors: {metrics['errors']}\\\")\\n\",\n    \"print(f\\\"  Cache size: {metrics['cache_size']}\\\")\\n\",\n    \"\\n\",\n    \"# Demonstrate functional composition in batch processing\\n\",\n    \"print(f\\\"\\\\nüéØ Functional batch processing with filtering:\\\")\\n\",\n    \"\\n\",\n    \"# Create a functional pipeline for batch processing\\n\",\n    \"batch_processor = pipe(\\n\",\n    \"    # Filter successful results\\n\",\n    \"    lambda results: [r.value for r in results if r.is_success],\\n\",\n    \"    # Filter by sentiment\\n\",\n    \"    lambda data_list: [d for d in data_list if d.sentiment == \\\"positive\\\"],\\n\",\n    \"    # Sort by complexity\\n\",\n    \"    lambda data_list: sorted(data_list, key=lambda d: d.complexity_score or 0, reverse=True),\\n\",\n    \"    # Take top 3\\n\",\n    \"    lambda data_list: data_list[:3]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"top_positive_complex = batch_processor(results)\\n\",\n    \"\\n\",\n    \"print(f\\\"  Top positive, complex texts:\\\")\\n\",\n    \"for i, data in enumerate(top_positive_complex, 1):\\n\",\n    \"    print(f\\\"    {i}. '{data.original_text[:50]}...' (complexity: {data.complexity_score:.3f})\\\")\\n\",\n    \"\\n\",\n    \"# Show the power of immutability\\n\",\n    \"print(f\\\"\\\\nüîí Demonstrating immutability benefits:\\\")\\n\",\n    \"original_config = config\\n\",\n    \"new_config = replace(config, enable_sentiment=False, cache_ttl=30.0)\\n\",\n    \"\\n\",\n    \"print(f\\\"  Original config sentiment enabled: {original_config.enable_sentiment}\\\")\\n\",\n    \"print(f\\\"  New config sentiment enabled: {new_config.enable_sentiment}\\\")\\n\",\n    \"print(f\\\"  Original config unchanged: {original_config.enable_sentiment}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüéâ Production System Highlights:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Immutable configuration and results\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Pure functions for core processing\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Monadic error handling throughout\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Automatic caching with TTL\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Functional composition for complex operations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Comprehensive metrics and monitoring\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Thread-safe design (no shared mutable state)\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Easy testing (pure functions)\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Scalable architecture (stateless processing)\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Summary: Functional Programming Mastery\\n\",\n    \"\\n\",\n    \"**üéØ Key Takeaways:**\\n\",\n    \"\\n\",\n    \"1. **Immutability** prevents bugs and enables safe concurrency\\n\",\n    \"2. **Pure Functions** make code predictable and testable\\n\",\n    \"3. **Composition** builds complex systems from simple parts\\n\",\n    \"4. **Higher-Order Functions** enable powerful abstractions\\n\",\n    \"5. **Monadic Error Handling** creates safe computation chains\\n\",\n    \"6. **Lazy Evaluation** provides memory and performance benefits\\n\",\n    \"7. **Memoization** eliminates redundant computations\\n\",\n    \"\\n\",\n    \"**üöÄ Next Steps:**\\n\",\n    \"- Apply these patterns to your AI projects\\n\",\n    \"- Combine functional and object-oriented approaches\\n\",\n    \"- Build immutable data pipelines\\n\",\n    \"- Create composable AI model architectures\\n\",\n    \"\\n\",\n    \"**üí° Remember:** Functional programming isn't about avoiding all state‚Äîit's about controlling state to make your AI systems more reliable, testable, and maintainable!\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":21349,"cache_read_input_tokens":66522,"output_tokens":1}},"requestId":"req_vrtx_011CRhRePCWGZqYgmToG7wck","type":"assistant","uuid":"d51ec123-bc7a-4dab-b500-40b45dcf44a9","timestamp":"2025-08-01T18:05:49.322Z"}
{"parentUuid":"d51ec123-bc7a-4dab-b500-40b45dcf44a9","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01HrrMWitDeuXXkw7Zps9yso","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/03_functional_programming_learning.ipynb"}]},"uuid":"0152b606-de3d-4394-ab7e-93c3c00bf5d2","timestamp":"2025-08-01T18:05:49.352Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/03_functional_programming_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Functional Programming for AI Systems\\n\",\n    \"\\n\",\n    \"This notebook teaches functional programming patterns that make AI code more predictable, testable, and maintainable. You'll learn:\\n\",\n    \"\\n\",\n    \"1. **Immutable Data Structures** - Building predictable AI configurations\\n\",\n    \"2. **Pure Functions** - Creating testable and reliable transformations\\n\",\n    \"3. **Higher-Order Functions** - Composing complex AI operations\\n\",\n    \"4. **Function Composition** - Building pipelines from simple components\\n\",\n    \"5. **Currying & Partial Application** - Creating specialized functions\\n\",\n    \"6. **Monadic Error Handling** - Safe computation chains\\n\",\n    \"7. **Lazy Evaluation** - Efficient processing of large datasets\\n\",\n    \"8. **Memoization** - Caching expensive computations\\n\",\n    \"\\n\",\n    \"## Prerequisites\\n\",\n    \"```bash\\n\",\n    \"pip install rich\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 1: Essential Imports and Core Concepts\\n\",\n    \"\\n\",\n    \"**First Principle**: Functional programming emphasizes immutability, pure functions, and composition over mutation and side effects.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `functools` for functional utilities, `dataclasses(frozen=True)` for immutability, and `itertools` for lazy operations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Essential imports for functional programming\\n\",\n    \"from typing import (\\n\",\n    \"    Callable, List, Dict, Any, Optional, Tuple, Union,\\n\",\n    \"    TypeVar, Iterator, Iterable\\n\",\n    \")\\n\",\n    \"from functools import reduce, partial, singledispatch, lru_cache, wraps\\n\",\n    \"from itertools import chain, groupby, accumulate\\n\",\n    \"from dataclasses import dataclass, replace\\n\",\n    \"from operator import add, mul, itemgetter\\n\",\n    \"import time\\n\",\n    \"from rich.console import Console\\n\",\n    \"from rich.panel import Panel\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"T = TypeVar('T')\\n\",\n    \"U = TypeVar('U')\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üéØ Functional Programming Fundamentals\\\", style=\\\"bold blue\\\"))\\n\",\n    \"print(\\\"\\\"\\\"\\n\",\n    \"Key Principles of Functional Programming:\\n\",\n    \"\\n\",\n    \"1. **Immutability**: Data structures don't change after creation\\n\",\n    \"2. **Pure Functions**: No side effects, same input = same output\\n\",\n    \"3. **Composition**: Building complex operations from simple ones\\n\",\n    \"4. **Higher-Order Functions**: Functions that take or return functions\\n\",\n    \"5. **Lazy Evaluation**: Computing values only when needed\\n\",\n    \"\\n\",\n    \"Benefits for AI Systems:\\n\",\n    \"‚Ä¢ Predictable behavior\\n\",\n    \"‚Ä¢ Easy testing and debugging\\n\",\n    \"‚Ä¢ Parallel processing safety\\n\",\n    \"‚Ä¢ Reproducible results\\n\",\n    \"\\\"\\\"\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"‚úÖ Functional programming toolkit loaded!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 2: Immutable Data Structures\\n\",\n    \"\\n\",\n    \"**First Principle**: Immutable data prevents accidental state changes and makes code more predictable.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `@dataclass(frozen=True)` and `replace()` to create new instances instead of modifying existing ones.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"@dataclass(frozen=True)\\n\",\n    \"class ImmutableConfig:\\n\",\n    \"    \\\"\\\"\\\"Immutable configuration for AI models.\\\"\\\"\\\"\\n\",\n    \"    model_name: str\\n\",\n    \"    temperature: float\\n\",\n    \"    max_tokens: int\\n\",\n    \"    provider: str\\n\",\n    \"    api_key: Optional[str] = None\\n\",\n    \"    \\n\",\n    \"    def with_temperature(self, temperature: float) -> 'ImmutableConfig':\\n\",\n    \"        \\\"\\\"\\\"Create new config with different temperature.\\\"\\\"\\\"\\n\",\n    \"        return replace(self, temperature=temperature)\\n\",\n    \"    \\n\",\n    \"    def with_provider(self, provider: str, api_key: Optional[str] = None) -> 'ImmutableConfig':\\n\",\n    \"        \\\"\\\"\\\"Create new config with different provider.\\\"\\\"\\\"\\n\",\n    \"        return replace(self, provider=provider, api_key=api_key)\\n\",\n    \"\\n\",\n    \"@dataclass(frozen=True)\\n\",\n    \"class ModelPrediction:\\n\",\n    \"    \\\"\\\"\\\"Immutable prediction result.\\\"\\\"\\\"\\n\",\n    \"    text: str\\n\",\n    \"    confidence: float\\n\",\n    \"    model_used: str\\n\",\n    \"    processing_time: float\\n\",\n    \"    metadata: Dict[str, Any]\\n\",\n    \"    \\n\",\n    \"    def with_confidence_threshold(self, threshold: float) -> Optional['ModelPrediction']:\\n\",\n    \"        \\\"\\\"\\\"Return prediction only if confidence meets threshold.\\\"\\\"\\\"\\n\",\n    \"        return self if self.confidence >= threshold else None\\n\",\n    \"    \\n\",\n    \"    def transform_text(self, transformer: Callable[[str], str]) -> 'ModelPrediction':\\n\",\n    \"        \\\"\\\"\\\"Transform text while preserving other fields.\\\"\\\"\\\"\\n\",\n    \"        return replace(self, text=transformer(self.text))\\n\",\n    \"\\n\",\n    \"# Demonstrate immutability\\n\",\n    \"console.print(Panel(\\\"üîí Immutable Data Structures\\\", style=\\\"bold green\\\"))\\n\",\n    \"\\n\",\n    \"# Create base configuration\\n\",\n    \"base_config = ImmutableConfig(\\n\",\n    \"    model_name=\\\"gpt-4\\\",\\n\",\n    \"    temperature=0.7,\\n\",\n    \"    max_tokens=1000,\\n\",\n    \"    provider=\\\"openai\\\"\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"print(f\\\"üìã Original config: {base_config.model_name} (temp: {base_config.temperature})\\\")\\n\",\n    \"\\n\",\n    \"# Create variations without mutating original\\n\",\n    \"hot_config = base_config.with_temperature(1.2)\\n\",\n    \"anthropic_config = base_config.with_provider(\\\"anthropic\\\", \\\"api-key-123\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"üî• Hot config: temp={hot_config.temperature}, original unchanged: {base_config.temperature}\\\")\\n\",\n    \"print(f\\\"üè¢ Anthropic config: {anthropic_config.provider}, original: {base_config.provider}\\\")\\n\",\n    \"\\n\",\n    \"# Create and transform prediction\\n\",\n    \"prediction = ModelPrediction(\\n\",\n    \"    text=\\\"Hello world\\\",\\n\",\n    \"    confidence=0.85,\\n\",\n    \"    model_used=\\\"gpt-4\\\",\\n\",\n    \"    processing_time=0.1,\\n\",\n    \"    metadata={\\\"source\\\": \\\"test\\\"}\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"upper_prediction = prediction.transform_text(str.upper)\\n\",\n    \"filtered_prediction = prediction.with_confidence_threshold(0.9)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîÑ Transformations:\\\")\\n\",\n    \"print(f\\\"  Original: '{prediction.text}'\\\")\\n\",\n    \"print(f\\\"  Uppercase: '{upper_prediction.text}'\\\")\\n\",\n    \"print(f\\\"  Filtered (>0.9): {filtered_prediction}\\\")\\n\",\n    \"\\n\",\n    \"# Try to modify frozen dataclass (will fail)\\n\",\n    \"try:\\n\",\n    \"    base_config.temperature = 0.8\\n\",\n    \"except Exception as e:\\n\",\n    \"    print(f\\\"\\\\n‚ùå Immutability enforced: {type(e).__name__}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 3: Pure Functions\\n\",\n    \"\\n\",\n    \"**First Principle**: Pure functions have no side effects and always return the same output for the same input.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Write functions that only depend on their parameters and don't modify external state.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Pure functions for text processing\\n\",\n    \"def tokenize_text(text: str) -> List[str]:\\n\",\n    \"    \\\"\\\"\\\"Pure function to tokenize text.\\\"\\\"\\\"\\n\",\n    \"    return text.lower().split()\\n\",\n    \"\\n\",\n    \"def count_tokens(tokens: List[str]) -> int:\\n\",\n    \"    \\\"\\\"\\\"Pure function to count tokens.\\\"\\\"\\\"\\n\",\n    \"    return len(tokens)\\n\",\n    \"\\n\",\n    \"def filter_tokens(tokens: List[str], min_length: int = 2) -> List[str]:\\n\",\n    \"    \\\"\\\"\\\"Pure function to filter tokens by length.\\\"\\\"\\\"\\n\",\n    \"    return [token for token in tokens if len(token) >= min_length]\\n\",\n    \"\\n\",\n    \"def calculate_text_stats(text: str) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Pure function to calculate text statistics.\\\"\\\"\\\"\\n\",\n    \"    tokens = tokenize_text(text)\\n\",\n    \"    filtered_tokens = filter_tokens(tokens)\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"character_count\\\": len(text),\\n\",\n    \"        \\\"word_count\\\": len(tokens),\\n\",\n    \"        \\\"unique_words\\\": len(set(tokens)),\\n\",\n    \"        \\\"filtered_words\\\": len(filtered_tokens),\\n\",\n    \"        \\\"avg_word_length\\\": sum(len(token) for token in tokens) / len(tokens) if tokens else 0\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"‚ú® Pure Functions\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"test_text = \\\"The Quick Brown Fox Jumps Over The Lazy Dog\\\"\\n\",\n    \"print(f\\\"üìù Processing: '{test_text}'\\\")\\n\",\n    \"\\n\",\n    \"# Pure functions are predictable and testable\\n\",\n    \"tokens = tokenize_text(test_text)\\n\",\n    \"filtered = filter_tokens(tokens, min_length=3)\\n\",\n    \"stats = calculate_text_stats(test_text)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîç Results:\\\")\\n\",\n    \"print(f\\\"  Tokens: {tokens[:5]}... ({len(tokens)} total)\\\")\\n\",\n    \"print(f\\\"  Filtered (len>=3): {filtered[:5]}... ({len(filtered)} total)\\\")\\n\",\n    \"print(f\\\"  Stats: {stats}\\\")\\n\",\n    \"\\n\",\n    \"# Pure functions can be easily tested\\n\",\n    \"print(f\\\"\\\\nüß™ Testing predictability:\\\")\\n\",\n    \"result1 = calculate_text_stats(test_text)\\n\",\n    \"result2 = calculate_text_stats(test_text)\\n\",\n    \"print(f\\\"  Same input, same output: {result1 == result2}\\\")\\n\",\n    \"\\n\",\n    \"# Benefits of pure functions\\n\",\n    \"print(f\\\"\\\\nüí° Benefits of Pure Functions:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Predictable: Same input always gives same output\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Testable: Easy to write unit tests\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Cacheable: Results can be memoized\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Parallel-safe: No shared state to corrupt\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Composable: Can be combined easily\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 4: Function Composition and Pipelines\\n\",\n    \"\\n\",\n    \"**First Principle**: Complex operations can be built by composing simple functions together.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `reduce()` for composition, create pipeline classes for reusable processing chains.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Function composition utilities\\n\",\n    \"def compose(*functions: Callable) -> Callable:\\n\",\n    \"    \\\"\\\"\\\"Compose functions right to left (mathematical composition).\\\"\\\"\\\"\\n\",\n    \"    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)\\n\",\n    \"\\n\",\n    \"def pipe(*functions: Callable) -> Callable:\\n\",\n    \"    \\\"\\\"\\\"Pipe functions left to right (more intuitive for data flow).\\\"\\\"\\\"\\n\",\n    \"    return reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\\n\",\n    \"\\n\",\n    \"# Functional pipeline class\\n\",\n    \"class FunctionalPipeline:\\n\",\n    \"    \\\"\\\"\\\"Functional pipeline for processing AI data.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.steps: List[Callable] = []\\n\",\n    \"    \\n\",\n    \"    def add_step(self, func: Callable) -> 'FunctionalPipeline':\\n\",\n    \"        \\\"\\\"\\\"Add processing step (returns new pipeline).\\\"\\\"\\\"\\n\",\n    \"        new_pipeline = FunctionalPipeline()\\n\",\n    \"        new_pipeline.steps = self.steps + [func]\\n\",\n    \"        return new_pipeline\\n\",\n    \"    \\n\",\n    \"    def process(self, data: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Process data through the pipeline.\\\"\\\"\\\"\\n\",\n    \"        return reduce(lambda result, step: step(result), self.steps, data)\\n\",\n    \"    \\n\",\n    \"    def __call__(self, data: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Make pipeline callable.\\\"\\\"\\\"\\n\",\n    \"        return self.process(data)\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üîó Function Composition\\\", style=\\\"bold magenta\\\"))\\n\",\n    \"\\n\",\n    \"# Mathematical composition (right to left)\\n\",\n    \"text_analyzer = compose(\\n\",\n    \"    lambda stats: f\\\"Analysis: {stats['word_count']} words, {stats['unique_words']} unique\\\",\\n\",\n    \"    calculate_text_stats\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Pipe composition (left to right) - more intuitive\\n\",\n    \"text_pipeline = pipe(\\n\",\n    \"    str.strip,\\n\",\n    \"    str.lower,\\n\",\n    \"    tokenize_text,\\n\",\n    \"    lambda tokens: filter_tokens(tokens, 2),\\n\",\n    \"    len\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"test_text = \\\"  The Quick Brown Fox Jumps Over The Lazy Dog  \\\"\\n\",\n    \"print(f\\\"üìù Input: '{test_text}'\\\")\\n\",\n    \"\\n\",\n    \"analysis = text_analyzer(test_text)\\n\",\n    \"filtered_count = text_pipeline(test_text)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìä Results:\\\")\\n\",\n    \"print(f\\\"  Composed analysis: {analysis}\\\")\\n\",\n    \"print(f\\\"  Piped processing: {filtered_count} filtered tokens\\\")\\n\",\n    \"\\n\",\n    \"# Using FunctionalPipeline class\\n\",\n    \"pipeline = (FunctionalPipeline()\\n\",\n    \"           .add_step(str.strip)\\n\",\n    \"           .add_step(str.upper)\\n\",\n    \"           .add_step(lambda text: text.replace(\\\" \\\", \\\"_\\\")))\\n\",\n    \"\\n\",\n    \"pipeline_result = pipeline(test_text)\\n\",\n    \"print(f\\\"  Pipeline class result: '{pipeline_result}'\\\")\\n\",\n    \"\\n\",\n    \"# Demonstrate reusability\\n\",\n    \"print(f\\\"\\\\nüîÑ Pipeline Reusability:\\\")\\n\",\n    \"test_texts = [\\\"hello world\\\", \\\"python rocks\\\", \\\"AI is amazing\\\"]\\n\",\n    \"results = [pipeline(text) for text in test_texts]\\n\",\n    \"print(f\\\"  Processed: {results}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Composition Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Modularity: Small, focused functions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reusability: Combine in different ways\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Testability: Test each step independently\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Readability: Clear data flow\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 5: Higher-Order Functions\\n\",\n    \"\\n\",\n    \"**First Principle**: Higher-order functions take functions as arguments or return functions, enabling powerful abstractions.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use closures to create specialized functions, leverage `map`, `filter`, and `reduce` for data transformations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Higher-order functions for AI processing\\n\",\n    \"def create_text_processor(\\n\",\n    \"    transformers: List[Callable[[str], str]]\\n\",\n    \") -> Callable[[str], str]:\\n\",\n    \"    \\\"\\\"\\\"Create composed text processor from list of transformations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def process(text: str) -> str:\\n\",\n    \"        return reduce(lambda t, transformer: transformer(t), transformers, text)\\n\",\n    \"    \\n\",\n    \"    return process\\n\",\n    \"\\n\",\n    \"def create_confidence_filter(threshold: float) -> Callable[[ModelPrediction], bool]:\\n\",\n    \"    \\\"\\\"\\\"Create a confidence filter function.\\\"\\\"\\\"\\n\",\n    \"    return lambda prediction: prediction.confidence >= threshold\\n\",\n    \"\\n\",\n    \"def create_batch_processor(\\n\",\n    \"    processor: Callable[[T], U],\\n\",\n    \"    batch_size: int = 10\\n\",\n    \") -> Callable[[List[T]], List[U]]:\\n\",\n    \"    \\\"\\\"\\\"Create batch processor from single-item processor.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def process_batch(items: List[T]) -> List[U]:\\n\",\n    \"        results = []\\n\",\n    \"        for i in range(0, len(items), batch_size):\\n\",\n    \"            batch = items[i:i + batch_size]\\n\",\n    \"            batch_results = [processor(item) for item in batch]\\n\",\n    \"            results.extend(batch_results)\\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    return process_batch\\n\",\n    \"\\n\",\n    \"def create_validator(validation_rules: List[Callable[[Any], bool]]) -> Callable[[Any], bool]:\\n\",\n    \"    \\\"\\\"\\\"Create validator that applies multiple rules.\\\"\\\"\\\"\\n\",\n    \"    return lambda value: all(rule(value) for rule in validation_rules)\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üöÄ Higher-Order Functions\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"\\n\",\n    \"# Create composed text processor\\n\",\n    \"transformers = [\\n\",\n    \"    str.lower,\\n\",\n    \"    lambda text: text.replace(\\\"quick\\\", \\\"QUICK\\\"),\\n\",\n    \"    lambda text: text.replace(\\\"fox\\\", \\\"ü¶ä\\\"),\\n\",\n    \"    str.title\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"processor = create_text_processor(transformers)\\n\",\n    \"test_text = \\\"The Quick Brown Fox\\\"\\n\",\n    \"result = processor(test_text)\\n\",\n    \"\\n\",\n    \"print(f\\\"üìù Text transformation pipeline:\\\")\\n\",\n    \"print(f\\\"  Input: '{test_text}'\\\")\\n\",\n    \"print(f\\\"  Output: '{result}'\\\")\\n\",\n    \"\\n\",\n    \"# Create specialized functions\\n\",\n    \"high_confidence_filter = create_confidence_filter(0.8)\\n\",\n    \"batch_stats_processor = create_batch_processor(calculate_text_stats, batch_size=2)\\n\",\n    \"\\n\",\n    \"# Test batch processor\\n\",\n    \"texts = [\\\"Hello world\\\", \\\"Python is great\\\", \\\"AI is fascinating\\\", \\\"Functional programming rocks\\\"]\\n\",\n    \"batch_stats = batch_stats_processor(texts)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìä Batch processing results:\\\")\\n\",\n    \"for i, stats in enumerate(batch_stats):\\n\",\n    \"    print(f\\\"  Text {i+1}: {stats['word_count']} words, {stats['unique_words']} unique\\\")\\n\",\n    \"\\n\",\n    \"# Create validator with multiple rules\\n\",\n    \"text_validator = create_validator([\\n\",\n    \"    lambda text: isinstance(text, str),\\n\",\n    \"    lambda text: len(text) > 0,\\n\",\n    \"    lambda text: len(text) <= 1000,\\n\",\n    \"    lambda text: not text.isspace()\\n\",\n    \"])\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\n‚úÖ Validation tests:\\\")\\n\",\n    \"test_cases = [\\\"Valid text\\\", \\\"\\\", \\\"   \\\", \\\"x\\\" * 1001, 123]\\n\",\n    \"for test_case in test_cases:\\n\",\n    \"    is_valid = text_validator(test_case)\\n\",\n    \"    display = str(test_case)[:20] + \\\"...\\\" if len(str(test_case)) > 20 else str(test_case)\\n\",\n    \"    status = \\\"‚úÖ\\\" if is_valid else \\\"‚ùå\\\"\\n\",\n    \"    print(f\\\"  {status} '{display}': {is_valid}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Higher-Order Functions Enable:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Abstraction: Hide implementation details\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Customization: Create specialized versions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Code reuse: One pattern, many applications\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Flexibility: Runtime behavior modification\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 6: Currying and Partial Application\\n\",\n    \"\\n\",\n    \"**First Principle**: Currying transforms functions to enable partial application, creating specialized versions.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `functools.partial` for partial application, create curried functions for flexible APIs.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Currying for AI model predictions\\n\",\n    \"def curry_model_predict(\\n\",\n    \"    model_name: str,\\n\",\n    \"    temperature: float\\n\",\n    \") -> Callable[[str], ModelPrediction]:\\n\",\n    \"    \\\"\\\"\\\"Curried model prediction function.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def predict(text: str) -> ModelPrediction:\\n\",\n    \"        # Mock prediction logic with deterministic \\\"randomness\\\"\\n\",\n    \"        confidence = 0.8 + (hash(text) % 20) / 100\\n\",\n    \"        \\n\",\n    \"        return ModelPrediction(\\n\",\n    \"            text=text,\\n\",\n    \"            confidence=confidence,\\n\",\n    \"            model_used=model_name,\\n\",\n    \"            processing_time=0.1,\\n\",\n    \"            metadata={\\\"temperature\\\": temperature}\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    return predict\\n\",\n    \"\\n\",\n    \"# General currying utility\\n\",\n    \"def curry_binary(func: Callable[[T, U], Any]) -> Callable[[T], Callable[[U], Any]]:\\n\",\n    \"    \\\"\\\"\\\"Convert binary function to curried form.\\\"\\\"\\\"\\n\",\n    \"    return lambda x: lambda y: func(x, y)\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"ü•¢ Currying & Partial Application\\\", style=\\\"bold red\\\"))\\n\",\n    \"\\n\",\n    \"# Create specialized model predictors\\n\",\n    \"gpt4_predictor = curry_model_predict(\\\"gpt-4\\\", 0.7)\\n\",\n    \"claude_predictor = curry_model_predict(\\\"claude-3\\\", 0.5)\\n\",\n    \"creative_predictor = curry_model_predict(\\\"gpt-4-creative\\\", 1.2)\\n\",\n    \"\\n\",\n    \"test_texts = [\\\"Hello world\\\", \\\"Python is awesome\\\", \\\"AI will change everything\\\"]\\n\",\n    \"\\n\",\n    \"print(f\\\"ü§ñ Curried predictions:\\\")\\n\",\n    \"for text in test_texts:\\n\",\n    \"    gpt_pred = gpt4_predictor(text)\\n\",\n    \"    claude_pred = claude_predictor(text)\\n\",\n    \"    creative_pred = creative_predictor(text)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  '{text}':\\\")\\n\",\n    \"    print(f\\\"    GPT-4: {gpt_pred.confidence:.3f} (temp: {gpt_pred.metadata['temperature']})\\\")\\n\",\n    \"    print(f\\\"    Claude: {claude_pred.confidence:.3f} (temp: {claude_pred.metadata['temperature']})\\\")\\n\",\n    \"    print(f\\\"    Creative: {creative_pred.confidence:.3f} (temp: {creative_pred.metadata['temperature']})\\\")\\n\",\n    \"\\n\",\n    \"# Partial application examples\\n\",\n    \"from operator import add, mul\\n\",\n    \"\\n\",\n    \"multiply_by_10 = partial(mul, 10)\\n\",\n    \"add_100 = partial(add, 100)\\n\",\n    \"\\n\",\n    \"# Create specialized text processors\\n\",\n    \"def apply_transformation(text: str, operation: str, value: Any) -> str:\\n\",\n    \"    \\\"\\\"\\\"Apply transformation to text.\\\"\\\"\\\"\\n\",\n    \"    if operation == \\\"prefix\\\":\\n\",\n    \"        return f\\\"{value}{text}\\\"\\n\",\n    \"    elif operation == \\\"suffix\\\":\\n\",\n    \"        return f\\\"{text}{value}\\\"\\n\",\n    \"    elif operation == \\\"replace\\\":\\n\",\n    \"        old, new = value\\n\",\n    \"        return text.replace(old, new)\\n\",\n    \"    return text\\n\",\n    \"\\n\",\n    \"# Create specialized transformers using partial\\n\",\n    \"add_ai_prefix = partial(apply_transformation, operation=\\\"prefix\\\", value=\\\"AI: \\\")\\n\",\n    \"add_processed_suffix = partial(apply_transformation, operation=\\\"suffix\\\", value=\\\" [PROCESSED]\\\")\\n\",\n    \"replace_hello = partial(apply_transformation, operation=\\\"replace\\\", value=(\\\"hello\\\", \\\"greetings\\\"))\\n\",\n    \"\\n\",\n    \"numbers = [1, 2, 3, 4, 5]\\n\",\n    \"multiplied = list(map(multiply_by_10, numbers))\\n\",\n    \"added = list(map(add_100, numbers))\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüî¢ Partial application with numbers:\\\")\\n\",\n    \"print(f\\\"  Original: {numbers}\\\")\\n\",\n    \"print(f\\\"  √ó 10: {multiplied}\\\")\\n\",\n    \"print(f\\\"  + 100: {added}\\\")\\n\",\n    \"\\n\",\n    \"# Apply text transformations\\n\",\n    \"sample_texts = [\\\"hello world\\\", \\\"hello AI\\\", \\\"goodbye world\\\"]\\n\",\n    \"print(f\\\"\\\\nüìù Text transformations:\\\")\\n\",\n    \"for text in sample_texts:\\n\",\n    \"    prefixed = add_ai_prefix(text)\\n\",\n    \"    suffixed = add_processed_suffix(text)\\n\",\n    \"    replaced = replace_hello(text)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  '{text}':\\\")\\n\",\n    \"    print(f\\\"    Prefixed: '{prefixed}'\\\")\\n\",\n    \"    print(f\\\"    Suffixed: '{suffixed}'\\\")\\n\",\n    \"    print(f\\\"    Replaced: '{replaced}'\\\")\\n\",\n    \"\\n\",\n    \"# Curried binary function example\\n\",\n    \"curried_add = curry_binary(add)\\n\",\n    \"add_5 = curried_add(5)\\n\",\n    \"print(f\\\"\\\\n‚ûï Curried addition:\\\")\\n\",\n    \"print(f\\\"  add_5(10) = {add_5(10)}\\\")\\n\",\n    \"print(f\\\"  add_5(20) = {add_5(20)}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Currying & Partial Application Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Specialization: Create focused functions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reusability: Same logic, different parameters\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Composability: Easier to combine functions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Configuration: Parameterize behavior\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 7: Monadic Error Handling\\n\",\n    \"\\n\",\n    \"**First Principle**: Monadic patterns enable safe computation chains without explicit error checking at each step.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Create Result/Maybe types that chain operations safely, handling errors gracefully.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Result monad for safe error handling\\n\",\n    \"class Result:\\n\",\n    \"    \\\"\\\"\\\"Monadic Result type for error handling.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, value: Any = None, error: str = None):\\n\",\n    \"        self.value = value\\n\",\n    \"        self.error = error\\n\",\n    \"        self.is_success = error is None\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def success(cls, value: Any) -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Create successful result.\\\"\\\"\\\"\\n\",\n    \"        return cls(value=value)\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def failure(cls, error: str) -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Create failed result.\\\"\\\"\\\"\\n\",\n    \"        return cls(error=error)\\n\",\n    \"    \\n\",\n    \"    def map(self, func: Callable) -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Apply function if successful.\\\"\\\"\\\"\\n\",\n    \"        if self.is_success:\\n\",\n    \"            try:\\n\",\n    \"                return Result.success(func(self.value))\\n\",\n    \"            except Exception as e:\\n\",\n    \"                return Result.failure(str(e))\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def flat_map(self, func: Callable) -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Apply function that returns Result if successful.\\\"\\\"\\\"\\n\",\n    \"        if self.is_success:\\n\",\n    \"            try:\\n\",\n    \"                return func(self.value)\\n\",\n    \"            except Exception as e:\\n\",\n    \"                return Result.failure(str(e))\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def filter(self, predicate: Callable[[Any], bool], error_msg: str = \\\"Filter failed\\\") -> 'Result':\\n\",\n    \"        \\\"\\\"\\\"Filter result based on predicate.\\\"\\\"\\\"\\n\",\n    \"        if self.is_success and predicate(self.value):\\n\",\n    \"            return self\\n\",\n    \"        return Result.failure(error_msg if self.is_success else self.error)\\n\",\n    \"    \\n\",\n    \"    def get_or_else(self, default: Any) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Get value or return default.\\\"\\\"\\\"\\n\",\n    \"        return self.value if self.is_success else default\\n\",\n    \"    \\n\",\n    \"    def __str__(self) -> str:\\n\",\n    \"        if self.is_success:\\n\",\n    \"            return f\\\"Success({self.value})\\\"\\n\",\n    \"        return f\\\"Failure({self.error})\\\"\\n\",\n    \"\\n\",\n    \"# Safe operations that return Results\\n\",\n    \"def safe_divide(a: float, b: float) -> Result:\\n\",\n    \"    \\\"\\\"\\\"Safe division that returns Result.\\\"\\\"\\\"\\n\",\n    \"    if b == 0:\\n\",\n    \"        return Result.failure(\\\"Division by zero\\\")\\n\",\n    \"    return Result.success(a / b)\\n\",\n    \"\\n\",\n    \"def safe_model_predict(text: str) -> Result:\\n\",\n    \"    \\\"\\\"\\\"Safe model prediction that can fail.\\\"\\\"\\\"\\n\",\n    \"    if not text.strip():\\n\",\n    \"        return Result.failure(\\\"Empty input text\\\")\\n\",\n    \"    \\n\",\n    \"    if len(text) > 1000:\\n\",\n    \"        return Result.failure(\\\"Text too long\\\")\\n\",\n    \"    \\n\",\n    \"    # Mock successful prediction\\n\",\n    \"    prediction = ModelPrediction(\\n\",\n    \"        text=text,\\n\",\n    \"        confidence=0.85,\\n\",\n    \"        model_used=\\\"safe-model\\\",\\n\",\n    \"        processing_time=0.1,\\n\",\n    \"        metadata={}\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    return Result.success(prediction)\\n\",\n    \"\\n\",\n    \"def safe_parse_number(text: str) -> Result:\\n\",\n    \"    \\\"\\\"\\\"Safely parse number from text.\\\"\\\"\\\"\\n\",\n    \"    try:\\n\",\n    \"        return Result.success(float(text))\\n\",\n    \"    except ValueError:\\n\",\n    \"        return Result.failure(f\\\"Cannot parse '{text}' as number\\\")\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üîÑ Monadic Error Handling\\\", style=\\\"bold yellow\\\"))\\n\",\n    \"\\n\",\n    \"# Chaining operations with error handling\\n\",\n    \"print(f\\\"üßÆ Safe calculation chains:\\\")\\n\",\n    \"\\n\",\n    \"result1 = (safe_divide(10, 2)\\n\",\n    \"           .map(lambda x: x * 3)\\n\",\n    \"           .map(lambda x: f\\\"Result: {x}\\\")\\n\",\n    \"           .filter(lambda s: \\\"15\\\" in s, \\\"Result doesn't contain 15\\\"))\\n\",\n    \"\\n\",\n    \"result2 = (safe_divide(10, 0)\\n\",\n    \"           .map(lambda x: x * 3)\\n\",\n    \"           .map(lambda x: f\\\"Result: {x}\\\"))\\n\",\n    \"\\n\",\n    \"print(f\\\"  Chain 1 (10 √∑ 2 √ó 3): {result1}\\\")\\n\",\n    \"print(f\\\"  Chain 2 (10 √∑ 0 √ó 3): {result2}\\\")\\n\",\n    \"\\n\",\n    \"# Complex parsing and calculation chain\\n\",\n    \"def calculate_average(numbers_text: List[str]) -> Result:\\n\",\n    \"    \\\"\\\"\\\"Calculate average of numbers from text list.\\\"\\\"\\\"\\n\",\n    \"    if not numbers_text:\\n\",\n    \"        return Result.failure(\\\"Empty list\\\")\\n\",\n    \"    \\n\",\n    \"    # Parse all numbers\\n\",\n    \"    parsed_numbers = []\\n\",\n    \"    for text in numbers_text:\\n\",\n    \"        result = safe_parse_number(text)\\n\",\n    \"        if not result.is_success:\\n\",\n    \"            return result\\n\",\n    \"        parsed_numbers.append(result.value)\\n\",\n    \"    \\n\",\n    \"    # Calculate average\\n\",\n    \"    average = sum(parsed_numbers) / len(parsed_numbers)\\n\",\n    \"    return Result.success(average)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüî¢ Number parsing and calculation:\\\")\\n\",\n    \"test_cases = [\\n\",\n    \"    [\\\"1\\\", \\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5\\\"],\\n\",\n    \"    [\\\"1.5\\\", \\\"2.5\\\", \\\"3.5\\\"],\\n\",\n    \"    [\\\"1\\\", \\\"not_a_number\\\", \\\"3\\\"],\\n\",\n    \"    []\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for i, case in enumerate(test_cases, 1):\\n\",\n    \"    result = calculate_average(case)\\n\",\n    \"    print(f\\\"  Case {i} {case}: {result}\\\")\\n\",\n    \"\\n\",\n    \"# Model prediction with error handling\\n\",\n    \"test_cases = [\\\"Hello world\\\", \\\"\\\", \\\"x\\\" * 1001, \\\"Valid input\\\"]\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nü§ñ Safe model predictions:\\\")\\n\",\n    \"for text in test_cases:\\n\",\n    \"    result = (safe_model_predict(text)\\n\",\n    \"              .map(lambda pred: pred.transform_text(str.upper))\\n\",\n    \"              .filter(lambda pred: pred.confidence > 0.8, \\\"Low confidence\\\"))\\n\",\n    \"    \\n\",\n    \"    display_text = text[:20] + \\\"...\\\" if len(text) > 20 else text\\n\",\n    \"    if result.is_success:\\n\",\n    \"        print(f\\\"  ‚úÖ '{display_text}': confidence {result.value.confidence}\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"  ‚ùå '{display_text}': {result.error}\\\")\\n\",\n    \"\\n\",\n    \"# Demonstrating the power of monadic chaining\\n\",\n    \"def process_user_input(input_text: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process user input with comprehensive error handling.\\\"\\\"\\\"\\n\",\n    \"    result = (safe_model_predict(input_text)\\n\",\n    \"              .filter(lambda pred: pred.confidence > 0.7, \\\"Confidence too low\\\")\\n\",\n    \"              .map(lambda pred: pred.transform_text(str.title))\\n\",\n    \"              .map(lambda pred: f\\\"Processed: {pred.text} (confidence: {pred.confidence:.2f})\\\"))\\n\",\n    \"    \\n\",\n    \"    return result.get_or_else(f\\\"Error: {result.error}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîÑ Complete processing pipeline:\\\")\\n\",\n    \"inputs = [\\\"hello world\\\", \\\"\\\", \\\"this is a good example\\\"]\\n\",\n    \"for inp in inputs:\\n\",\n    \"    output = process_user_input(inp)\\n\",\n    \"    display = inp if inp else \\\"(empty)\\\"\\n\",\n    \"    print(f\\\"  '{display}' ‚Üí {output}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Monadic Error Handling Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Safe chaining: No null pointer exceptions\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Explicit errors: All failures are tracked\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Composability: Chain complex operations safely\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Railway-oriented: Success/failure paths clearly separated\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 8: Lazy Evaluation and Generators\\n\",\n    \"\\n\",\n    \"**First Principle**: Lazy evaluation computes values only when needed, enabling efficient processing of large datasets.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use generators (`yield`) for lazy evaluation, `itertools` for lazy operations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Lazy evaluation with generators\\n\",\n    \"def lazy_text_generator(texts: List[str]) -> Iterator[Dict[str, Any]]:\\n\",\n    \"    \\\"\\\"\\\"Lazily process texts and yield results.\\\"\\\"\\\"\\n\",\n    \"    for i, text in enumerate(texts):\\n\",\n    \"        print(f\\\"üîÑ Processing item {i+1}: '{text[:20]}...'\\\")\\n\",\n    \"        # Only compute when needed\\n\",\n    \"        yield {\\n\",\n    \"            \\\"text\\\": text,\\n\",\n    \"            \\\"stats\\\": calculate_text_stats(text),\\n\",\n    \"            \\\"processed_at\\\": time.time(),\\n\",\n    \"            \\\"index\\\": i\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"def create_lazy_pipeline(\\n\",\n    \"    data_source: Iterable[T],\\n\",\n    \"    *processors: Callable[[T], T]\\n\",\n    \") -> Iterator[T]:\\n\",\n    \"    \\\"\\\"\\\"Create lazy processing pipeline.\\\"\\\"\\\"\\n\",\n    \"    for item in data_source:\\n\",\n    \"        result = item\\n\",\n    \"        for processor in processors:\\n\",\n    \"            result = processor(result)\\n\",\n    \"        yield result\\n\",\n    \"\\n\",\n    \"def infinite_text_generator(base_text: str) -> Iterator[str]:\\n\",\n    \"    \\\"\\\"\\\"Generate infinite variations of text.\\\"\\\"\\\"\\n\",\n    \"    counter = 0\\n\",\n    \"    while True:\\n\",\n    \"        yield f\\\"{base_text} (variation {counter})\\\"\\n\",\n    \"        counter += 1\\n\",\n    \"\\n\",\n    \"def chunked_processor(items: Iterable[T], chunk_size: int) -> Iterator[List[T]]:\\n\",\n    \"    \\\"\\\"\\\"Process items in chunks lazily.\\\"\\\"\\\"\\n\",\n    \"    chunk = []\\n\",\n    \"    for item in items:\\n\",\n    \"        chunk.append(item)\\n\",\n    \"        if len(chunk) >= chunk_size:\\n\",\n    \"            yield chunk\\n\",\n    \"            chunk = []\\n\",\n    \"    \\n\",\n    \"    # Yield remaining items\\n\",\n    \"    if chunk:\\n\",\n    \"        yield chunk\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üò¥ Lazy Evaluation\\\", style=\\\"bold purple\\\"))\\n\",\n    \"\\n\",\n    \"# Create large dataset (but don't process yet)\\n\",\n    \"large_texts = [f\\\"Sample text number {i} for analysis\\\" for i in range(1000)]\\n\",\n    \"\\n\",\n    \"print(f\\\"üìä Lazy text processing (only processes what we consume):\\\")\\n\",\n    \"\\n\",\n    \"# Process only what we need\\n\",\n    \"lazy_processor = lazy_text_generator(large_texts[:10])\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîç Taking only first 3 items:\\\")\\n\",\n    \"count = 0\\n\",\n    \"for processed in lazy_processor:\\n\",\n    \"    if count >= 3:  # Only process first 3\\n\",\n    \"        break\\n\",\n    \"    print(f\\\"  Item {processed['index']}: {processed['stats']['word_count']} words\\\")\\n\",\n    \"    count += 1\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Notice: Only 3 items were actually processed!\\\")\\n\",\n    \"\\n\",\n    \"# Lazy pipeline with generators\\n\",\n    \"def slow_processor(text: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"Simulate slow processing.\\\"\\\"\\\"\\n\",\n    \"    time.sleep(0.001)  # Simulate processing time\\n\",\n    \"    return text.upper()\\n\",\n    \"\\n\",\n    \"def add_prefix(text: str) -> str:\\n\",\n    \"    return f\\\"PROCESSED: {text}\\\"\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüîÑ Lazy pipeline (processing on demand):\\\")\\n\",\n    \"\\n\",\n    \"lazy_pipeline = create_lazy_pipeline(\\n\",\n    \"    large_texts[:5],  # Small sample\\n\",\n    \"    slow_processor,\\n\",\n    \"    add_prefix\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Only consume what we need\\n\",\n    \"first_two = []\\n\",\n    \"for i, result in enumerate(lazy_pipeline):\\n\",\n    \"    first_two.append(result)\\n\",\n    \"    if i >= 1:  # Only take first 2\\n\",\n    \"        break\\n\",\n    \"\\n\",\n    \"for result in first_two:\\n\",\n    \"    print(f\\\"  {result[:50]}...\\\")\\n\",\n    \"\\n\",\n    \"# Infinite generators (be careful!)\\n\",\n    \"print(f\\\"\\\\n‚ôæÔ∏è Infinite generator (limited consumption):\\\")\\n\",\n    \"infinite_gen = infinite_text_generator(\\\"AI prompt\\\")\\n\",\n    \"\\n\",\n    \"# Take only first 5 from infinite sequence\\n\",\n    \"for i, text in enumerate(infinite_gen):\\n\",\n    \"    if i >= 5:\\n\",\n    \"        break\\n\",\n    \"    print(f\\\"  {text}\\\")\\n\",\n    \"\\n\",\n    \"# Chunked processing\\n\",\n    \"print(f\\\"\\\\nüì¶ Chunked lazy processing:\\\")\\n\",\n    \"numbers = range(23)  # 23 numbers\\n\",\n    \"chunked = chunked_processor(numbers, chunk_size=5)\\n\",\n    \"\\n\",\n    \"for i, chunk in enumerate(chunked):\\n\",\n    \"    print(f\\\"  Chunk {i+1}: {list(chunk)}\\\")\\n\",\n    \"\\n\",\n    \"# Memory efficiency demonstration\\n\",\n    \"from itertools import islice, takewhile, dropwhile\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüß† Memory-efficient operations with itertools:\\\")\\n\",\n    \"\\n\",\n    \"# Create large range without storing all values\\n\",\n    \"large_range = range(1000000)  # 1 million numbers\\n\",\n    \"\\n\",\n    \"# Take first 5 even numbers greater than 100\\n\",\n    \"result = list(islice(\\n\",\n    \"    filter(lambda x: x % 2 == 0, \\n\",\n    \"           dropwhile(lambda x: x <= 100, large_range)), \\n\",\n    \"    5\\n\",\n    \"))\\n\",\n    \"\\n\",\n    \"print(f\\\"  First 5 even numbers > 100: {result}\\\")\\n\",\n    \"\\n\",\n    \"# Generator expression vs list comprehension\\n\",\n    \"print(f\\\"\\\\n‚ö° Generator vs List Memory Usage:\\\")\\n\",\n    \"\\n\",\n    \"# Generator expression - lazy\\n\",\n    \"squares_gen = (x**2 for x in range(10000))\\n\",\n    \"print(f\\\"  Generator created (no memory used yet)\\\")\\n\",\n    \"\\n\",\n    \"# Take only first 3\\n\",\n    \"first_three_squares = [next(squares_gen) for _ in range(3)]\\n\",\n    \"print(f\\\"  First 3 squares: {first_three_squares}\\\")\\n\",\n    \"\\n\",\n    \"# Chain multiple lazy operations\\n\",\n    \"from itertools import chain\\n\",\n    \"\\n\",\n    \"gen1 = (f\\\"AI-{i}\\\" for i in range(3))\\n\",\n    \"gen2 = (f\\\"ML-{i}\\\" for i in range(3))\\n\",\n    \"gen3 = (f\\\"DL-{i}\\\" for i in range(3))\\n\",\n    \"\\n\",\n    \"chained = chain(gen1, gen2, gen3)\\n\",\n    \"all_items = list(chained)\\n\",\n    \"print(f\\\"  Chained generators: {all_items}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Lazy Evaluation Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Memory efficiency: Process large datasets\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Performance: Only compute what's needed\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Composability: Chain operations without intermediate storage\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Infinite sequences: Work with unbounded data\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 9: Memoization and Caching\\n\",\n    \"\\n\",\n    \"**First Principle**: Memoization caches function results to avoid recomputing expensive operations.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `@lru_cache` for automatic memoization, implement custom caches for specific needs.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Memoization for expensive operations\\n\",\n    \"@lru_cache(maxsize=128)\\n\",\n    \"def expensive_text_analysis(text: str) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Expensive analysis with memoization.\\\"\\\"\\\"\\n\",\n    \"    print(f\\\"üîç Computing expensive analysis for: '{text[:30]}...'\\\")\\n\",\n    \"    # Simulate expensive computation\\n\",\n    \"    time.sleep(0.01)\\n\",\n    \"    \\n\",\n    \"    tokens = tokenize_text(text)\\n\",\n    \"    return {\\n\",\n    \"        \\\"complexity_score\\\": len(set(tokens)) / len(tokens) if tokens else 0,\\n\",\n    \"        \\\"sentiment_score\\\": hash(text) % 100 / 100,\\n\",\n    \"        \\\"readability\\\": len(tokens) / text.count('.') if '.' in text else len(tokens),\\n\",\n    \"        \\\"processing_time\\\": 0.01\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"# Custom memoization decorator\\n\",\n    \"def memoize_with_ttl(ttl_seconds: float = 60.0):\\n\",\n    \"    \\\"\\\"\\\"Memoization with time-to-live.\\\"\\\"\\\"\\n\",\n    \"    def decorator(func):\\n\",\n    \"        cache = {}\\n\",\n    \"        \\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            # Create cache key\\n\",\n    \"            key = str(args) + str(sorted(kwargs.items()))\\n\",\n    \"            current_time = time.time()\\n\",\n    \"            \\n\",\n    \"            # Check if cached and not expired\\n\",\n    \"            if key in cache:\\n\",\n    \"                result, timestamp = cache[key]\\n\",\n    \"                if current_time - timestamp < ttl_seconds:\\n\",\n    \"                    print(f\\\"üíæ Cache hit for {func.__name__}\\\")\\n\",\n    \"                    return result\\n\",\n    \"                else:\\n\",\n    \"                    print(f\\\"‚è∞ Cache expired for {func.__name__}\\\")\\n\",\n    \"            \\n\",\n    \"            # Compute new result\\n\",\n    \"            print(f\\\"üîÑ Cache miss for {func.__name__}\\\")\\n\",\n    \"            result = func(*args, **kwargs)\\n\",\n    \"            cache[key] = (result, current_time)\\n\",\n    \"            return result\\n\",\n    \"        \\n\",\n    \"        # Add cache management methods\\n\",\n    \"        wrapper.clear_cache = lambda: cache.clear()\\n\",\n    \"        wrapper.cache_info = lambda: {\\n\",\n    \"            'size': len(cache),\\n\",\n    \"            'ttl_seconds': ttl_seconds\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"    return decorator\\n\",\n    \"\\n\",\n    \"@memoize_with_ttl(ttl_seconds=2.0)\\n\",\n    \"def simulate_ai_api_call(query: str, model: str = \\\"gpt-4\\\") -> str:\\n\",\n    \"    \\\"\\\"\\\"Simulate expensive AI API call.\\\"\\\"\\\"\\n\",\n    \"    time.sleep(0.1)  # Simulate network delay\\n\",\n    \"    return f\\\"{model} response to: {query}\\\"\\n\",\n    \"\\n\",\n    \"# Fibonacci with and without memoization (classic example)\\n\",\n    \"def fibonacci_naive(n: int) -> int:\\n\",\n    \"    \\\"\\\"\\\"Naive Fibonacci (exponential time).\\\"\\\"\\\"\\n\",\n    \"    if n <= 1:\\n\",\n    \"        return n\\n\",\n    \"    return fibonacci_naive(n - 1) + fibonacci_naive(n - 2)\\n\",\n    \"\\n\",\n    \"@lru_cache(maxsize=None)\\n\",\n    \"def fibonacci_memoized(n: int) -> int:\\n\",\n    \"    \\\"\\\"\\\"Memoized Fibonacci (linear time).\\\"\\\"\\\"\\n\",\n    \"    if n <= 1:\\n\",\n    \"        return n\\n\",\n    \"    return fibonacci_memoized(n - 1) + fibonacci_memoized(n - 2)\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üß† Memoization\\\", style=\\\"bold orange\\\"))\\n\",\n    \"\\n\",\n    \"# Test expensive analysis with repeats\\n\",\n    \"test_texts = [\\n\",\n    \"    \\\"This is a test text for analysis and processing\\\",\\n\",\n    \"    \\\"Another sample text for computational analysis\\\",\\n\",\n    \"    \\\"This is a test text for analysis and processing\\\",  # Duplicate\\n\",\n    \"    \\\"Final text sample for testing our algorithms\\\",\\n\",\n    \"    \\\"Another sample text for computational analysis\\\",  # Duplicate\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(f\\\"üìä Memoized expensive analysis:\\\")\\n\",\n    \"\\n\",\n    \"# First run - will compute all\\n\",\n    \"start_time = time.time()\\n\",\n    \"for i, text in enumerate(test_texts, 1):\\n\",\n    \"    analysis = expensive_text_analysis(text)\\n\",\n    \"    print(f\\\"  {i}. Complexity: {analysis['complexity_score']:.3f}\\\")\\n\",\n    \"first_run_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìà Cache info: {expensive_text_analysis.cache_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Second run - should be faster due to caching\\n\",\n    \"print(f\\\"\\\\nüîÑ Running same analysis again (should hit cache):\\\")\\n\",\n    \"start_time = time.time()\\n\",\n    \"for i, text in enumerate(test_texts, 1):\\n\",\n    \"    analysis = expensive_text_analysis(text)\\n\",\n    \"second_run_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\n‚ö° Performance comparison:\\\")\\n\",\n    \"print(f\\\"  First run: {first_run_time:.4f}s\\\")\\n\",\n    \"print(f\\\"  Second run: {second_run_time:.4f}s\\\")\\n\",\n    \"print(f\\\"  Speedup: {first_run_time / second_run_time:.1f}x\\\")\\n\",\n    \"\\n\",\n    \"# Test TTL-based caching\\n\",\n    \"print(f\\\"\\\\n‚è∞ TTL-based caching (2 second TTL):\\\")\\n\",\n    \"\\n\",\n    \"# First call\\n\",\n    \"result1 = simulate_ai_api_call(\\\"What is AI?\\\", \\\"gpt-4\\\")\\n\",\n    \"print(f\\\"  Result 1: {result1[:30]}...\\\")\\n\",\n    \"\\n\",\n    \"# Second call immediately (should hit cache)\\n\",\n    \"result2 = simulate_ai_api_call(\\\"What is AI?\\\", \\\"gpt-4\\\")\\n\",\n    \"print(f\\\"  Result 2: {result2[:30]}...\\\")\\n\",\n    \"\\n\",\n    \"# Wait for cache to expire\\n\",\n    \"print(f\\\"  Waiting for cache to expire...\\\")\\n\",\n    \"time.sleep(2.1)\\n\",\n    \"\\n\",\n    \"# Third call (should miss cache)\\n\",\n    \"result3 = simulate_ai_api_call(\\\"What is AI?\\\", \\\"gpt-4\\\")\\n\",\n    \"print(f\\\"  Result 3: {result3[:30]}...\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nCache info: {simulate_ai_api_call.cache_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Fibonacci performance comparison\\n\",\n    \"print(f\\\"\\\\nüî¢ Fibonacci performance (memoized vs naive):\\\")\\n\",\n    \"\\n\",\n    \"n = 30\\n\",\n    \"\\n\",\n    \"# Memoized version\\n\",\n    \"start_time = time.time()\\n\",\n    \"fib_memoized = fibonacci_memoized(n)\\n\",\n    \"memoized_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"print(f\\\"  Memoized fib({n}) = {fib_memoized} in {memoized_time:.6f}s\\\")\\n\",\n    \"print(f\\\"  Cache info: {fibonacci_memoized.cache_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Naive version (only for small n to avoid timeout)\\n\",\n    \"n_small = 25\\n\",\n    \"start_time = time.time()\\n\",\n    \"fib_naive = fibonacci_naive(n_small)\\n\",\n    \"naive_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"print(f\\\"  Naive fib({n_small}) = {fib_naive} in {naive_time:.6f}s\\\")\\n\",\n    \"print(f\\\"  Speedup for fib({n_small}): {naive_time / memoized_time:.1f}x\\\")\\n\",\n    \"\\n\",\n    \"# Practical memoization patterns\\n\",\n    \"class MemoizedModelProcessor:\\n\",\n    \"    \\\"\\\"\\\"AI processor with built-in memoization.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.cache = {}\\n\",\n    \"        self.cache_hits = 0\\n\",\n    \"        self.cache_misses = 0\\n\",\n    \"    \\n\",\n    \"    def process(self, text: str, model_config: Dict[str, Any]) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Process text with caching.\\\"\\\"\\\"\\n\",\n    \"        # Create cache key from text and config\\n\",\n    \"        cache_key = hash(text + str(sorted(model_config.items())))\\n\",\n    \"        \\n\",\n    \"        if cache_key in self.cache:\\n\",\n    \"            self.cache_hits += 1\\n\",\n    \"            return self.cache[cache_key]\\n\",\n    \"        \\n\",\n    \"        # Simulate processing\\n\",\n    \"        self.cache_misses += 1\\n\",\n    \"        result = {\\n\",\n    \"            \\\"processed_text\\\": text.upper(),\\n\",\n    \"            \\\"score\\\": hash(text) % 100 / 100,\\n\",\n    \"            \\\"model_config\\\": model_config\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        self.cache[cache_key] = result\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    def get_cache_stats(self) -> Dict[str, Any]:\\n\",\n    \"        total = self.cache_hits + self.cache_misses\\n\",\n    \"        hit_rate = self.cache_hits / total if total > 0 else 0\\n\",\n    \"        return {\\n\",\n    \"            \\\"cache_size\\\": len(self.cache),\\n\",\n    \"            \\\"hits\\\": self.cache_hits,\\n\",\n    \"            \\\"misses\\\": self.cache_misses,\\n\",\n    \"            \\\"hit_rate\\\": hit_rate\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nü§ñ Practical memoization in AI processing:\\\")\\n\",\n    \"\\n\",\n    \"processor = MemoizedModelProcessor()\\n\",\n    \"configs = [\\n\",\n    \"    {\\\"temperature\\\": 0.7, \\\"model\\\": \\\"gpt-4\\\"},\\n\",\n    \"    {\\\"temperature\\\": 0.5, \\\"model\\\": \\\"gpt-4\\\"},\\n\",\n    \"    {\\\"temperature\\\": 0.7, \\\"model\\\": \\\"gpt-4\\\"},  # Same as first\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"test_inputs = [\\\"hello world\\\", \\\"python rocks\\\", \\\"hello world\\\"]  # Some repeats\\n\",\n    \"\\n\",\n    \"for i, (text, config) in enumerate(zip(test_inputs, configs), 1):\\n\",\n    \"    result = processor.process(text, config)\\n\",\n    \"    print(f\\\"  {i}. '{text}' ‚Üí score: {result['score']:.3f}\\\")\\n\",\n    \"\\n\",\n    \"stats = processor.get_cache_stats()\\n\",\n    \"print(f\\\"\\\\nüìä Final cache statistics: {stats}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Memoization Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Performance: Avoid redundant computations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Efficiency: Especially good for recursive algorithms\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Transparency: Functions remain pure\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Configurability: TTL, size limits, custom strategies\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 10: Advanced Functional Patterns\\n\",\n    \"\\n\",\n    \"**First Principle**: Advanced patterns combine multiple functional concepts for sophisticated AI architectures.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use single dispatch, decorators, and functional composition to create flexible, maintainable systems.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Single dispatch for different data types\\n\",\n    \"@singledispatch\\n\",\n    \"def process_input(data) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process input data (default implementation).\\\"\\\"\\\"\\n\",\n    \"    return f\\\"Processed unknown type: {type(data).__name__}\\\"\\n\",\n    \"\\n\",\n    \"@process_input.register\\n\",\n    \"def _(data: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process string input.\\\"\\\"\\\"\\n\",\n    \"    stats = calculate_text_stats(data)\\n\",\n    \"    return f\\\"String: {stats['word_count']} words, {stats['character_count']} chars\\\"\\n\",\n    \"\\n\",\n    \"@process_input.register\\n\",\n    \"def _(data: list) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process list input.\\\"\\\"\\\"\\n\",\n    \"    return f\\\"List: {len(data)} items, types: {[type(x).__name__ for x in data[:3]]}\\\"\\n\",\n    \"\\n\",\n    \"@process_input.register\\n\",\n    \"def _(data: dict) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process dictionary input.\\\"\\\"\\\"\\n\",\n    \"    return f\\\"Dict: {len(data)} keys: {list(data.keys())[:3]}\\\"\\n\",\n    \"\\n\",\n    \"@process_input.register\\n\",\n    \"def _(data: ModelPrediction) -> str:\\n\",\n    \"    \\\"\\\"\\\"Process model prediction input.\\\"\\\"\\\"\\n\",\n    \"    return f\\\"Prediction: '{data.text}' confidence={data.confidence:.3f}\\\"\\n\",\n    \"\\n\",\n    \"# Functional decorators for AI operations\\n\",\n    \"def validate_input(validator: Callable[[Any], bool], error_msg: str = \\\"Invalid input\\\"):\\n\",\n    \"    \\\"\\\"\\\"Decorator that validates function input.\\\"\\\"\\\"\\n\",\n    \"    def decorator(func):\\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            if args and not validator(args[0]):\\n\",\n    \"                raise ValueError(f\\\"{error_msg}: {args[0]}\\\")\\n\",\n    \"            return func(*args, **kwargs)\\n\",\n    \"        return wrapper\\n\",\n    \"    return decorator\\n\",\n    \"\\n\",\n    \"def retry_with_backoff(max_attempts: int = 3, base_delay: float = 1.0):\\n\",\n    \"    \\\"\\\"\\\"Decorator for retrying failed operations.\\\"\\\"\\\"\\n\",\n    \"    def decorator(func):\\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            last_exception = None\\n\",\n    \"            \\n\",\n    \"            for attempt in range(max_attempts):\\n\",\n    \"                try:\\n\",\n    \"                    return func(*args, **kwargs)\\n\",\n    \"                except Exception as e:\\n\",\n    \"                    last_exception = e\\n\",\n    \"                    if attempt < max_attempts - 1:\\n\",\n    \"                        delay = base_delay * (2 ** attempt)\\n\",\n    \"                        print(f\\\"üîÑ Retry {attempt + 1}/{max_attempts} in {delay:.1f}s\\\")\\n\",\n    \"                        time.sleep(delay)\\n\",\n    \"            \\n\",\n    \"            raise last_exception\\n\",\n    \"        return wrapper\\n\",\n    \"    return decorator\\n\",\n    \"\\n\",\n    \"# Complete functional AI pipeline\\n\",\n    \"class FunctionalAIPipeline:\\n\",\n    \"    \\\"\\\"\\\"Complete AI processing pipeline using functional patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.processors = []\\n\",\n    \"        self.validators = []\\n\",\n    \"        self.transformers = []\\n\",\n    \"    \\n\",\n    \"    def add_validator(self, validator: Callable[[Any], bool], error_msg: str = \\\"Validation failed\\\"):\\n\",\n    \"        \\\"\\\"\\\"Add input validator.\\\"\\\"\\\"\\n\",\n    \"        self.validators.append((validator, error_msg))\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def add_transformer(self, transformer: Callable[[Any], Any]):\\n\",\n    \"        \\\"\\\"\\\"Add data transformer.\\\"\\\"\\\"\\n\",\n    \"        self.transformers.append(transformer)\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def add_processor(self, processor: Callable[[Any], Any]):\\n\",\n    \"        \\\"\\\"\\\"Add main processor.\\\"\\\"\\\"\\n\",\n    \"        self.processors.append(processor)\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def process(self, data: Any) -> Result:\\n\",\n    \"        \\\"\\\"\\\"Process data through the complete pipeline.\\\"\\\"\\\"\\n\",\n    \"        try:\\n\",\n    \"            # Validation phase\\n\",\n    \"            for validator, error_msg in self.validators:\\n\",\n    \"                if not validator(data):\\n\",\n    \"                    return Result.failure(error_msg)\\n\",\n    \"            \\n\",\n    \"            # Transformation phase\\n\",\n    \"            current_data = data\\n\",\n    \"            for transformer in self.transformers:\\n\",\n    \"                current_data = transformer(current_data)\\n\",\n    \"            \\n\",\n    \"            # Processing phase\\n\",\n    \"            for processor in self.processors:\\n\",\n    \"                current_data = processor(current_data)\\n\",\n    \"            \\n\",\n    \"            return Result.success(current_data)\\n\",\n    \"            \\n\",\n    \"        except Exception as e:\\n\",\n    \"            return Result.failure(str(e))\\n\",\n    \"\\n\",\n    \"# Example functions using decorators\\n\",\n    \"@validate_input(lambda x: isinstance(x, str) and len(x) > 0, \\\"Text must be non-empty string\\\")\\n\",\n    \"@retry_with_backoff(max_attempts=2, base_delay=0.1)\\n\",\n    \"@lru_cache(maxsize=64)\\n\",\n    \"def advanced_text_processor(text: str) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Advanced text processor with validation, retry, and caching.\\\"\\\"\\\"\\n\",\n    \"    # Simulate potential failure\\n\",\n    \"    import random\\n\",\n    \"    if random.random() < 0.3:  # 30% chance of failure\\n\",\n    \"        raise ConnectionError(\\\"Simulated processing error\\\")\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"processed_text\\\": text.upper(),\\n\",\n    \"        \\\"word_count\\\": len(text.split()),\\n\",\n    \"        \\\"sentiment\\\": \\\"positive\\\" if \\\"good\\\" in text.lower() else \\\"neutral\\\",\\n\",\n    \"        \\\"complexity\\\": len(set(text.lower())) / len(text) if text else 0\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üöÄ Advanced Functional Patterns\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"\\n\",\n    \"# Single dispatch demonstration\\n\",\n    \"test_data = [\\n\",\n    \"    \\\"Hello functional world\\\",\\n\",\n    \"    [1, 2, 3, \\\"text\\\", True],\\n\",\n    \"    {\\\"name\\\": \\\"AI\\\", \\\"type\\\": \\\"system\\\", \\\"version\\\": 1.0},\\n\",\n    \"    ModelPrediction(\\\"test\\\", 0.95, \\\"gpt-4\\\", 0.1, {}),\\n\",\n    \"    42.5\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(f\\\"üéØ Single dispatch processing:\\\")\\n\",\n    \"for i, data in enumerate(test_data, 1):\\n\",\n    \"    result = process_input(data)\\n\",\n    \"    print(f\\\"  {i}. {type(data).__name__}: {result}\\\")\\n\",\n    \"\\n\",\n    \"# Advanced processor with decorators\\n\",\n    \"print(f\\\"\\\\nüîß Advanced processor (validation + retry + caching):\\\")\\n\",\n    \"\\n\",\n    \"test_texts = [\\\"good example\\\", \\\"another test\\\", \\\"good example\\\", \\\"\\\"]  # Last one should fail\\n\",\n    \"\\n\",\n    \"for i, text in enumerate(test_texts, 1):\\n\",\n    \"    try:\\n\",\n    \"        result = advanced_text_processor(text)\\n\",\n    \"        print(f\\\"  {i}. ‚úÖ '{text}': {result['sentiment']} (complexity: {result['complexity']:.3f})\\\")\\n\",\n    \"    except Exception as e:\\n\",\n    \"        print(f\\\"  {i}. ‚ùå '{text}': {e}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìä Cache info: {advanced_text_processor.cache_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Complete functional pipeline\\n\",\n    \"print(f\\\"\\\\nüîÑ Complete functional AI pipeline:\\\")\\n\",\n    \"\\n\",\n    \"pipeline = (FunctionalAIPipeline()\\n\",\n    \"           .add_validator(lambda x: isinstance(x, str), \\\"Input must be string\\\")\\n\",\n    \"           .add_validator(lambda x: len(x.strip()) > 0, \\\"Input must not be empty\\\")\\n\",\n    \"           .add_validator(lambda x: len(x) <= 1000, \\\"Input too long\\\")\\n\",\n    \"           .add_transformer(str.strip)\\n\",\n    \"           .add_transformer(str.lower)\\n\",\n    \"           .add_processor(calculate_text_stats)\\n\",\n    \"           .add_processor(lambda stats: {**stats, \\\"quality_score\\\": stats[\\\"unique_words\\\"] / stats[\\\"word_count\\\"] if stats[\\\"word_count\\\"] > 0 else 0}))\\n\",\n    \"\\n\",\n    \"pipeline_tests = [\\n\",\n    \"    \\\"  This is a great example of functional programming!  \\\",\\n\",\n    \"    \\\"\\\",\\n\",\n    \"    \\\"Short\\\",\\n\",\n    \"    \\\"x\\\" * 1001,  # Too long\\n\",\n    \"    123  # Wrong type\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"for i, test_input in enumerate(pipeline_tests, 1):\\n\",\n    \"    result = pipeline.process(test_input)\\n\",\n    \"    display = str(test_input)[:30] + \\\"...\\\" if len(str(test_input)) > 30 else str(test_input)\\n\",\n    \"    \\n\",\n    \"    if result.is_success:\\n\",\n    \"        data = result.value\\n\",\n    \"        print(f\\\"  {i}. ‚úÖ '{display}': {data['word_count']} words, quality: {data['quality_score']:.3f}\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"  {i}. ‚ùå '{display}': {result.error}\\\")\\n\",\n    \"\\n\",\n    \"# Functional composition showcase\\n\",\n    \"print(f\\\"\\\\nüéº Functional composition showcase:\\\")\\n\",\n    \"\\n\",\n    \"# Create a complex text processing function by composing simple ones\\n\",\n    \"text_normalizer = pipe(\\n\",\n    \"    str.strip,\\n\",\n    \"    str.lower,\\n\",\n    \"    lambda text: text.replace(\\\"'\\\", \\\"\\\"),  # Remove apostrophes\\n\",\n    \"    lambda text: \\\" \\\".join(text.split())  # Normalize whitespace\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"text_analyzer = compose(\\n\",\n    \"    lambda stats: f\\\"Quality text: {stats['word_count']} words, {stats['unique_words']} unique (quality: {stats['unique_words']/stats['word_count']:.2f})\\\",\\n\",\n    \"    calculate_text_stats,\\n\",\n    \"    text_normalizer\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"messy_text = \\\"  Hello   World!   This   is   a   GREAT   example...   \\\"\\n\",\n    \"analysis = text_analyzer(messy_text)\\n\",\n    \"print(f\\\"  Input: '{messy_text}'\\\")\\n\",\n    \"print(f\\\"  Analysis: {analysis}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Advanced Functional Programming Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Modularity: Small, composable pieces\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reliability: Pure functions + error handling\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Performance: Lazy evaluation + memoization\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Maintainability: Clear data flow\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Testability: Each function can be tested independently\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Scalability: Parallel processing safe\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 11: Real-World Application\\n\",\n    \"\\n\",\n    \"**First Principle**: Combine all functional programming concepts to build a production-ready AI text processing system.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use functional patterns as the backbone for reliable, scalable AI applications.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Complete functional AI text processing system\\n\",\n    \"from dataclasses import dataclass\\n\",\n    \"from typing import Protocol\\n\",\n    \"\\n\",\n    \"@dataclass(frozen=True)\\n\",\n    \"class ProcessingConfig:\\n\",\n    \"    \\\"\\\"\\\"Immutable processing configuration.\\\"\\\"\\\"\\n\",\n    \"    min_length: int = 5\\n\",\n    \"    max_length: int = 10000\\n\",\n    \"    enable_sentiment: bool = True\\n\",\n    \"    enable_complexity: bool = True\\n\",\n    \"    cache_ttl: float = 300.0  # 5 minutes\\n\",\n    \"\\n\",\n    \"@dataclass(frozen=True)\\n\",\n    \"class ProcessingResult:\\n\",\n    \"    \\\"\\\"\\\"Immutable processing result.\\\"\\\"\\\"\\n\",\n    \"    original_text: str\\n\",\n    \"    normalized_text: str\\n\",\n    \"    word_count: int\\n\",\n    \"    sentiment: Optional[str] = None\\n\",\n    \"    complexity_score: Optional[float] = None\\n\",\n    \"    processing_time: float = 0.0\\n\",\n    \"    cached: bool = False\\n\",\n    \"\\n\",\n    \"class FunctionalTextProcessor:\\n\",\n    \"    \\\"\\\"\\\"Production-ready functional text processor.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, config: ProcessingConfig):\\n\",\n    \"        self.config = config\\n\",\n    \"        self.cache = {}\\n\",\n    \"        self.metrics = {\\\"total_processed\\\": 0, \\\"cache_hits\\\": 0, \\\"errors\\\": 0}\\n\",\n    \"        \\n\",\n    \"        # Create processing pipeline using functional composition\\n\",\n    \"        self.normalizer = pipe(\\n\",\n    \"            str.strip,\\n\",\n    \"            str.lower,\\n\",\n    \"            lambda text: \\\" \\\".join(text.split())  # Normalize whitespace\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        # Create validators\\n\",\n    \"        self.validators = [\\n\",\n    \"            (lambda text: isinstance(text, str), \\\"Input must be a string\\\"),\\n\",\n    \"            (lambda text: len(text.strip()) >= self.config.min_length, f\\\"Text too short (min: {self.config.min_length})\\\"),\\n\",\n    \"            (lambda text: len(text) <= self.config.max_length, f\\\"Text too long (max: {self.config.max_length})\\\")\\n\",\n    \"        ]\\n\",\n    \"    \\n\",\n    \"    def _validate_input(self, text: str) -> Result:\\n\",\n    \"        \\\"\\\"\\\"Validate input using functional approach.\\\"\\\"\\\"\\n\",\n    \"        for validator, error_msg in self.validators:\\n\",\n    \"            if not validator(text):\\n\",\n    \"                return Result.failure(error_msg)\\n\",\n    \"        return Result.success(text)\\n\",\n    \"    \\n\",\n    \"    def _get_cache_key(self, text: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Generate cache key for text.\\\"\\\"\\\"\\n\",\n    \"        config_hash = hash(str(self.config))\\n\",\n    \"        text_hash = hash(text)\\n\",\n    \"        return f\\\"{config_hash}_{text_hash}\\\"\\n\",\n    \"    \\n\",\n    \"    def _check_cache(self, cache_key: str) -> Optional[ProcessingResult]:\\n\",\n    \"        \\\"\\\"\\\"Check if result is cached and not expired.\\\"\\\"\\\"\\n\",\n    \"        if cache_key in self.cache:\\n\",\n    \"            result, timestamp = self.cache[cache_key]\\n\",\n    \"            if time.time() - timestamp < self.config.cache_ttl:\\n\",\n    \"                self.metrics[\\\"cache_hits\\\"] += 1\\n\",\n    \"                # Return new instance with cached flag set\\n\",\n    \"                return replace(result, cached=True)\\n\",\n    \"        return None\\n\",\n    \"    \\n\",\n    \"    def _compute_sentiment(self, text: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Compute sentiment (simplified).\\\"\\\"\\\"\\n\",\n    \"        positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic'}\\n\",\n    \"        negative_words = {'bad', 'terrible', 'awful', 'horrible', 'disgusting', 'hate'}\\n\",\n    \"        \\n\",\n    \"        words = set(text.lower().split())\\n\",\n    \"        positive_count = len(words.intersection(positive_words))\\n\",\n    \"        negative_count = len(words.intersection(negative_words))\\n\",\n    \"        \\n\",\n    \"        if positive_count > negative_count:\\n\",\n    \"            return \\\"positive\\\"\\n\",\n    \"        elif negative_count > positive_count:\\n\",\n    \"            return \\\"negative\\\"\\n\",\n    \"        else:\\n\",\n    \"            return \\\"neutral\\\"\\n\",\n    \"    \\n\",\n    \"    def _compute_complexity(self, text: str) -> float:\\n\",\n    \"        \\\"\\\"\\\"Compute text complexity score.\\\"\\\"\\\"\\n\",\n    \"        words = text.split()\\n\",\n    \"        if not words:\\n\",\n    \"            return 0.0\\n\",\n    \"        \\n\",\n    \"        unique_words = len(set(words))\\n\",\n    \"        total_words = len(words)\\n\",\n    \"        avg_word_length = sum(len(word) for word in words) / total_words\\n\",\n    \"        \\n\",\n    \"        # Complexity combines lexical diversity and word length\\n\",\n    \"        lexical_diversity = unique_words / total_words\\n\",\n    \"        length_factor = min(avg_word_length / 5.0, 1.0)  # Normalize to 0-1\\n\",\n    \"        \\n\",\n    \"        return (lexical_diversity + length_factor) / 2.0\\n\",\n    \"    \\n\",\n    \"    def process(self, text: str) -> Result[ProcessingResult]:\\n\",\n    \"        \\\"\\\"\\\"Process text using functional pipeline.\\\"\\\"\\\"\\n\",\n    \"        start_time = time.time()\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            # Validation phase\\n\",\n    \"            validation_result = self._validate_input(text)\\n\",\n    \"            if not validation_result.is_success:\\n\",\n    \"                self.metrics[\\\"errors\\\"] += 1\\n\",\n    \"                return validation_result\\n\",\n    \"            \\n\",\n    \"            # Check cache\\n\",\n    \"            cache_key = self._get_cache_key(text)\\n\",\n    \"            cached_result = self._check_cache(cache_key)\\n\",\n    \"            if cached_result:\\n\",\n    \"                return Result.success(cached_result)\\n\",\n    \"            \\n\",\n    \"            # Normalize text\\n\",\n    \"            normalized_text = self.normalizer(text)\\n\",\n    \"            word_count = len(normalized_text.split())\\n\",\n    \"            \\n\",\n    \"            # Optional processing based on config\\n\",\n    \"            sentiment = self._compute_sentiment(normalized_text) if self.config.enable_sentiment else None\\n\",\n    \"            complexity = self._compute_complexity(normalized_text) if self.config.enable_complexity else None\\n\",\n    \"            \\n\",\n    \"            processing_time = time.time() - start_time\\n\",\n    \"            \\n\",\n    \"            # Create result\\n\",\n    \"            result = ProcessingResult(\\n\",\n    \"                original_text=text,\\n\",\n    \"                normalized_text=normalized_text,\\n\",\n    \"                word_count=word_count,\\n\",\n    \"                sentiment=sentiment,\\n\",\n    \"                complexity_score=complexity,\\n\",\n    \"                processing_time=processing_time,\\n\",\n    \"                cached=False\\n\",\n    \"            )\\n\",\n    \"            \\n\",\n    \"            # Cache result\\n\",\n    \"            self.cache[cache_key] = (result, time.time())\\n\",\n    \"            \\n\",\n    \"            # Update metrics\\n\",\n    \"            self.metrics[\\\"total_processed\\\"] += 1\\n\",\n    \"            \\n\",\n    \"            return Result.success(result)\\n\",\n    \"            \\n\",\n    \"        except Exception as e:\\n\",\n    \"            self.metrics[\\\"errors\\\"] += 1\\n\",\n    \"            return Result.failure(f\\\"Processing error: {str(e)}\\\")\\n\",\n    \"    \\n\",\n    \"    def process_batch(self, texts: List[str]) -> List[Result[ProcessingResult]]:\\n\",\n    \"        \\\"\\\"\\\"Process multiple texts efficiently.\\\"\\\"\\\"\\n\",\n    \"        return [self.process(text) for text in texts]\\n\",\n    \"    \\n\",\n    \"    def get_metrics(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get processing metrics.\\\"\\\"\\\"\\n\",\n    \"        total = self.metrics[\\\"total_processed\\\"] + self.metrics[\\\"cache_hits\\\"]\\n\",\n    \"        cache_hit_rate = self.metrics[\\\"cache_hits\\\"] / total if total > 0 else 0\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            **self.metrics,\\n\",\n    \"            \\\"cache_size\\\": len(self.cache),\\n\",\n    \"            \\\"cache_hit_rate\\\": cache_hit_rate\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"üè≠ Production Functional AI System\\\", style=\\\"bold green\\\"))\\n\",\n    \"\\n\",\n    \"# Create processor with configuration\\n\",\n    \"config = ProcessingConfig(\\n\",\n    \"    min_length=3,\\n\",\n    \"    max_length=500,\\n\",\n    \"    enable_sentiment=True,\\n\",\n    \"    enable_complexity=True,\\n\",\n    \"    cache_ttl=60.0\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"processor = FunctionalTextProcessor(config)\\n\",\n    \"\\n\",\n    \"# Test with various inputs\\n\",\n    \"test_inputs = [\\n\",\n    \"    \\\"This is a great example of functional programming in AI!\\\",\\n\",\n    \"    \\\"Python is an amazing language for data science.\\\",\\n\",\n    \"    \\\"Bad terrible awful experience with this product.\\\",\\n\",\n    \"    \\\"The quick brown fox jumps over the lazy dog.\\\",\\n\",\n    \"    \\\"This is a great example of functional programming in AI!\\\",  # Duplicate for cache test\\n\",\n    \"    \\\"\\\",  # Should fail validation\\n\",\n    \"    \\\"AI\\\",  # Too short\\n\",\n    \"    \\\"Simple text for processing\\\"\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"print(f\\\"üîÑ Processing {len(test_inputs)} texts:\\\")\\n\",\n    \"\\n\",\n    \"results = processor.process_batch(test_inputs)\\n\",\n    \"\\n\",\n    \"for i, (text, result) in enumerate(zip(test_inputs, results), 1):\\n\",\n    \"    display_text = text[:40] + \\\"...\\\" if len(text) > 40 else text\\n\",\n    \"    \\n\",\n    \"    if result.is_success:\\n\",\n    \"        data = result.value\\n\",\n    \"        cache_indicator = \\\"üíæ\\\" if data.cached else \\\"üîÑ\\\"\\n\",\n    \"        print(f\\\"  {i}. {cache_indicator} '{display_text}'\\\")\\n\",\n    \"        print(f\\\"     Words: {data.word_count}, Sentiment: {data.sentiment}, Complexity: {data.complexity_score:.3f if data.complexity_score else 'N/A'}\\\")\\n\",\n    \"        print(f\\\"     Time: {data.processing_time:.4f}s\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(f\\\"  {i}. ‚ùå '{display_text}': {result.error}\\\")\\n\",\n    \"\\n\",\n    \"# Show metrics\\n\",\n    \"metrics = processor.get_metrics()\\n\",\n    \"print(f\\\"\\\\nüìä Processing Metrics:\\\")\\n\",\n    \"print(f\\\"  Total processed: {metrics['total_processed']}\\\")\\n\",\n    \"print(f\\\"  Cache hits: {metrics['cache_hits']}\\\")\\n\",\n    \"print(f\\\"  Cache hit rate: {metrics['cache_hit_rate']:.1%}\\\")\\n\",\n    \"print(f\\\"  Errors: {metrics['errors']}\\\")\\n\",\n    \"print(f\\\"  Cache size: {metrics['cache_size']}\\\")\\n\",\n    \"\\n\",\n    \"# Demonstrate functional composition in batch processing\\n\",\n    \"print(f\\\"\\\\nüéØ Functional batch processing with filtering:\\\")\\n\",\n    \"\\n\",\n    \"# Create a functional pipeline for batch processing\\n\",\n    \"batch_processor = pipe(\\n\",\n    \"    # Filter successful results\\n\",\n    \"    lambda results: [r.value for r in results if r.is_success],\\n\",\n    \"    # Filter by sentiment\\n\",\n    \"    lambda data_list: [d for d in data_list if d.sentiment == \\\"positive\\\"],\\n\",\n    \"    # Sort by complexity\\n\",\n    \"    lambda data_list: sorted(data_list, key=lambda d: d.complexity_score or 0, reverse=True),\\n\",\n    \"    # Take top 3\\n\",\n    \"    lambda data_list: data_list[:3]\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"top_positive_complex = batch_processor(results)\\n\",\n    \"\\n\",\n    \"print(f\\\"  Top positive, complex texts:\\\")\\n\",\n    \"for i, data in enumerate(top_positive_complex, 1):\\n\",\n    \"    print(f\\\"    {i}. '{data.original_text[:50]}...' (complexity: {data.complexity_score:.3f})\\\")\\n\",\n    \"\\n\",\n    \"# Show the power of immutability\\n\",\n    \"print(f\\\"\\\\nüîí Demonstrating immutability benefits:\\\")\\n\",\n    \"original_config = config\\n\",\n    \"new_config = replace(config, enable_sentiment=False, cache_ttl=30.0)\\n\",\n    \"\\n\",\n    \"print(f\\\"  Original config sentiment enabled: {original_config.enable_sentiment}\\\")\\n\",\n    \"print(f\\\"  New config sentiment enabled: {new_config.enable_sentiment}\\\")\\n\",\n    \"print(f\\\"  Original config unchanged: {original_config.enable_sentiment}\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüéâ Production System Highlights:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Immutable configuration and results\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Pure functions for core processing\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Monadic error handling throughout\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Automatic caching with TTL\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Functional composition for complex operations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Comprehensive metrics and monitoring\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Thread-safe design (no shared mutable state)\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Easy testing (pure functions)\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Scalable architecture (stateless processing)\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Summary: Functional Programming Mastery\\n\",\n    \"\\n\",\n    \"**üéØ Key Takeaways:**\\n\",\n    \"\\n\",\n    \"1. **Immutability** prevents bugs and enables safe concurrency\\n\",\n    \"2. **Pure Functions** make code predictable and testable\\n\",\n    \"3. **Composition** builds complex systems from simple parts\\n\",\n    \"4. **Higher-Order Functions** enable powerful abstractions\\n\",\n    \"5. **Monadic Error Handling** creates safe computation chains\\n\",\n    \"6. **Lazy Evaluation** provides memory and performance benefits\\n\",\n    \"7. **Memoization** eliminates redundant computations\\n\",\n    \"\\n\",\n    \"**üöÄ Next Steps:**\\n\",\n    \"- Apply these patterns to your AI projects\\n\",\n    \"- Combine functional and object-oriented approaches\\n\",\n    \"- Build immutable data pipelines\\n\",\n    \"- Create composable AI model architectures\\n\",\n    \"\\n\",\n    \"**üí° Remember:** Functional programming isn't about avoiding all state‚Äîit's about controlling state to make your AI systems more reliable, testable, and maintainable!\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}","structuredPatch":[]}}
{"parentUuid":"0152b606-de3d-4394-ab7e-93c3c00bf5d2","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01Syd2xFqP6t3MckS3u1BGnY","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me create the notebook for async patterns (file 2):"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":113147,"cache_read_input_tokens":0,"output_tokens":4}},"requestId":"req_vrtx_011CRhS6YnSYL32R2dyM71Md","type":"assistant","uuid":"ef7f0b3c-2dbe-4042-b28c-ed5b37f00bb2","timestamp":"2025-08-01T18:05:52.857Z"}
{"parentUuid":"ef7f0b3c-2dbe-4042-b28c-ed5b37f00bb2","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01Syd2xFqP6t3MckS3u1BGnY","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01Sr2ysBL8ApSp79gBgJR1My","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/02_async_patterns_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Async Patterns for High-Performance AI Systems\\n\",\n    \"\\n\",\n    \"This notebook teaches advanced asyncio patterns for building high-performance AI systems with concurrent processing, streaming, and resource management. You'll learn:\\n\",\n    \"\\n\",\n    \"1. **Streaming Patterns** - Real-time AI response streaming\\n\",\n    \"2. **Rate Limiting** - Controlling API usage and preventing overload\\n\",\n    \"3. **Connection Pooling** - Efficient resource management\\n\",\n    \"4. **Concurrent Processing** - Parallel AI inference\\n\",\n    \"5. **Batch Processing** - Optimizing throughput\\n\",\n    \"6. **Async Context Managers** - Proper resource cleanup\\n\",\n    \"7. **Async Generators** - Memory-efficient data processing\\n\",\n    \"8. **Error Handling** - Robust async error management\\n\",\n    \"\\n\",\n    \"## Prerequisites\\n\",\n    \"```bash\\n\",\n    \"pip install rich asyncio\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 1: Async Fundamentals and Setup\\n\",\n    \"\\n\",\n    \"**First Principle**: Async programming enables concurrent execution without blocking, essential for I/O-bound AI applications.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `async def` for coroutines, `await` for async operations, and `asyncio.create_task()` for concurrent execution.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Essential imports for async programming\\n\",\n    \"import asyncio\\n\",\n    \"from typing import AsyncGenerator, AsyncIterator, Callable, Any, Optional, List, Dict\\n\",\n    \"from dataclasses import dataclass\\n\",\n    \"from contextlib import asynccontextmanager\\n\",\n    \"import time\\n\",\n    \"from rich.console import Console\\n\",\n    \"from rich.panel import Panel\\n\",\n    \"from rich.table import Table\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"\\n\",\n    \"# Basic data structures for AI processing\\n\",\n    \"@dataclass\\n\",\n    \"class StreamingChunk:\\n\",\n    \"    \\\"\\\"\\\"Represents a chunk of streaming data.\\\"\\\"\\\"\\n\",\n    \"    content: str\\n\",\n    \"    chunk_id: int\\n\",\n    \"    is_final: bool\\n\",\n    \"    metadata: Dict[str, Any] = None\\n\",\n    \"    \\n\",\n    \"    def __post_init__(self):\\n\",\n    \"        if self.metadata is None:\\n\",\n    \"            self.metadata = {}\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class Message:\\n\",\n    \"    \\\"\\\"\\\"Chat message structure.\\\"\\\"\\\"\\n\",\n    \"    role: str\\n\",\n    \"    content: str\\n\",\n    \"    timestamp: float = None\\n\",\n    \"    \\n\",\n    \"    def __post_init__(self):\\n\",\n    \"        if self.timestamp is None:\\n\",\n    \"            self.timestamp = time.time()\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"‚ö° Async Programming Fundamentals\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"print(\\\"\\\"\\\"\\n\",\n    \"üîë Key Async Concepts:\\n\",\n    \"\\n\",\n    \"1. **Coroutines**: Functions defined with 'async def'\\n\",\n    \"2. **Awaitable**: Objects that can be used with 'await'\\n\",\n    \"3. **Event Loop**: Manages and executes async operations\\n\",\n    \"4. **Tasks**: Concurrent execution of coroutines\\n\",\n    \"5. **Futures**: Placeholders for future results\\n\",\n    \"\\n\",\n    \"üéØ Benefits for AI Systems:\\n\",\n    \"‚Ä¢ Handle multiple API calls concurrently\\n\",\n    \"‚Ä¢ Stream responses in real-time\\n\",\n    \"‚Ä¢ Manage resources efficiently\\n\",\n    \"‚Ä¢ Scale to thousands of connections\\n\",\n    \"‚Ä¢ Non-blocking I/O operations\\n\",\n    \"\\\"\\\"\\\")\\n\",\n    \"\\n\",\n    \"# Simple async demonstration\\n\",\n    \"async def simple_async_demo():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate basic async concepts.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    async def fetch_data(source: str, delay: float) -> str:\\n\",\n    \"        \\\"\\\"\\\"Simulate fetching data from different sources.\\\"\\\"\\\"\\n\",\n    \"        print(f\\\"üîÑ Starting to fetch from {source}...\\\")\\n\",\n    \"        await asyncio.sleep(delay)  # Simulate I/O delay\\n\",\n    \"        result = f\\\"Data from {source}\\\"\\n\",\n    \"        print(f\\\"‚úÖ Finished fetching from {source}\\\")\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    # Sequential execution (blocking)\\n\",\n    \"    print(\\\"üìä Sequential execution:\\\")\\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    result1 = await fetch_data(\\\"API-1\\\", 0.5)\\n\",\n    \"    result2 = await fetch_data(\\\"API-2\\\", 0.3)\\n\",\n    \"    result3 = await fetch_data(\\\"API-3\\\", 0.4)\\n\",\n    \"    \\n\",\n    \"    sequential_time = time.time() - start_time\\n\",\n    \"    print(f\\\"Sequential results: {[result1, result2, result3]}\\\")\\n\",\n    \"    print(f\\\"Sequential time: {sequential_time:.2f}s\\\")\\n\",\n    \"    \\n\",\n    \"    # Concurrent execution (non-blocking)\\n\",\n    \"    print(f\\\"\\\\nüöÄ Concurrent execution:\\\")\\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    # Create tasks for concurrent execution\\n\",\n    \"    task1 = asyncio.create_task(fetch_data(\\\"API-1\\\", 0.5))\\n\",\n    \"    task2 = asyncio.create_task(fetch_data(\\\"API-2\\\", 0.3))\\n\",\n    \"    task3 = asyncio.create_task(fetch_data(\\\"API-3\\\", 0.4))\\n\",\n    \"    \\n\",\n    \"    # Wait for all tasks to complete\\n\",\n    \"    concurrent_results = await asyncio.gather(task1, task2, task3)\\n\",\n    \"    \\n\",\n    \"    concurrent_time = time.time() - start_time\\n\",\n    \"    print(f\\\"Concurrent results: {concurrent_results}\\\")\\n\",\n    \"    print(f\\\"Concurrent time: {concurrent_time:.2f}s\\\")\\n\",\n    \"    \\n\",\n    \"    speedup = sequential_time / concurrent_time\\n\",\n    \"    print(f\\\"\\\\n‚ö° Speedup: {speedup:.1f}x faster with async!\\\")\\n\",\n    \"\\n\",\n    \"# Run the demo\\n\",\n    \"await simple_async_demo()\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüí° Key Takeaway: Async shines with I/O-bound operations!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 2: Streaming Patterns for Real-Time AI\\n\",\n    \"\\n\",\n    \"**First Principle**: Streaming enables real-time user feedback and better resource utilization in AI applications.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `async def` with `yield` to create async generators for streaming data.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class StreamingAIProvider:\\n\",\n    \"    \\\"\\\"\\\"AI provider that supports streaming responses.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, response_delay: float = 0.1):\\n\",\n    \"        self.response_delay = response_delay\\n\",\n    \"        self.call_count = 0\\n\",\n    \"    \\n\",\n    \"    async def stream_completion(\\n\",\n    \"        self, \\n\",\n    \"        model_id: str, \\n\",\n    \"        messages: List[Message]\\n\",\n    \"    ) -> AsyncGenerator[StreamingChunk, None]:\\n\",\n    \"        \\\"\\\"\\\"Stream chat completion chunks.\\\"\\\"\\\"\\n\",\n    \"        self.call_count += 1\\n\",\n    \"        \\n\",\n    \"        # Simulate streaming response\\n\",\n    \"        response_text = f\\\"This is a streaming response to: {messages[-1].content}\\\"\\n\",\n    \"        words = response_text.split()\\n\",\n    \"        \\n\",\n    \"        for i, word in enumerate(words):\\n\",\n    \"            await asyncio.sleep(self.response_delay)\\n\",\n    \"            \\n\",\n    \"            chunk = StreamingChunk(\\n\",\n    \"                content=word + (\\\" \\\" if i < len(words) - 1 else \\\"\\\"),\\n\",\n    \"                chunk_id=i,\\n\",\n    \"                is_final=i == len(words) - 1,\\n\",\n    \"                metadata={\\n\",\n    \"                    \\\"model\\\": model_id,\\n\",\n    \"                    \\\"total_chunks\\\": len(words),\\n\",\n    \"                    \\\"call_id\\\": self.call_count\\n\",\n    \"                }\\n\",\n    \"            )\\n\",\n    \"            \\n\",\n    \"            yield chunk\\n\",\n    \"    \\n\",\n    \"    async def stream_multiple_responses(\\n\",\n    \"        self,\\n\",\n    \"        queries: List[str]\\n\",\n    \"    ) -> AsyncGenerator[Dict[str, Any], None]:\\n\",\n    \"        \\\"\\\"\\\"Stream responses for multiple queries concurrently.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        async def process_query(query_id: int, query: str):\\n\",\n    \"            \\\"\\\"\\\"Process a single query and yield results.\\\"\\\"\\\"\\n\",\n    \"            messages = [Message(role=\\\"user\\\", content=query)]\\n\",\n    \"            \\n\",\n    \"            async for chunk in self.stream_completion(\\\"gpt-4\\\", messages):\\n\",\n    \"                yield {\\n\",\n    \"                    \\\"query_id\\\": query_id,\\n\",\n    \"                    \\\"query\\\": query,\\n\",\n    \"                    \\\"chunk\\\": chunk\\n\",\n    \"                }\\n\",\n    \"        \\n\",\n    \"        # Create tasks for all queries\\n\",\n    \"        tasks = [\\n\",\n    \"            process_query(i, query) \\n\",\n    \"            for i, query in enumerate(queries)\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        # Use async generators concurrently\\n\",\n    \"        async def merge_streams():\\n\",\n    \"            \\\"\\\"\\\"Merge multiple async generators.\\\"\\\"\\\"\\n\",\n    \"            active_tasks = [task.__aiter__() for task in tasks]\\n\",\n    \"            \\n\",\n    \"            while active_tasks:\\n\",\n    \"                # Try to get next item from each active stream\\n\",\n    \"                for i, stream in enumerate(active_tasks[:]):\\n\",\n    \"                    try:\\n\",\n    \"                        result = await stream.__anext__()\\n\",\n    \"                        yield result\\n\",\n    \"                    except StopAsyncIteration:\\n\",\n    \"                        active_tasks.remove(stream)\\n\",\n    \"                \\n\",\n    \"                # Small delay to prevent tight loop\\n\",\n    \"                if active_tasks:\\n\",\n    \"                    await asyncio.sleep(0.01)\\n\",\n    \"        \\n\",\n    \"        async for result in merge_streams():\\n\",\n    \"            yield result\\n\",\n    \"\\n\",\n    \"async def demonstrate_streaming():\\n\",\n    \"    \\\"\\\"\\\"Show streaming patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üåä Streaming Patterns\\\", style=\\\"bold blue\\\"))\\n\",\n    \"    \\n\",\n    \"    provider = StreamingAIProvider(response_delay=0.15)\\n\",\n    \"    \\n\",\n    \"    print(\\\"üì° Single streaming completion:\\\")\\n\",\n    \"    \\n\",\n    \"    messages = [Message(role=\\\"user\\\", content=\\\"Tell me about async programming\\\")]\\n\",\n    \"    \\n\",\n    \"    # Collect streaming chunks\\n\",\n    \"    chunks = []\\n\",\n    \"    full_response = \\\"\\\"\\n\",\n    \"    \\n\",\n    \"    async for chunk in provider.stream_completion(\\\"gpt-4\\\", messages):\\n\",\n    \"        chunks.append(chunk)\\n\",\n    \"        full_response += chunk.content\\n\",\n    \"        \\n\",\n    \"        # Show real-time progress\\n\",\n    \"        if chunk.is_final:\\n\",\n    \"            print(f\\\"  Chunk {chunk.chunk_id}: '{chunk.content.strip()}' [FINAL]\\\")\\n\",\n    \"        else:\\n\",\n    \"            print(f\\\"  Chunk {chunk.chunk_id}: '{chunk.content.strip()}'\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìä Streaming Summary:\\\")\\n\",\n    \"    print(f\\\"  Total chunks: {len(chunks)}\\\")\\n\",\n    \"    print(f\\\"  Full response: {full_response}\\\")\\n\",\n    \"    print(f\\\"  Final chunk metadata: {chunks[-1].metadata}\\\")\\n\",\n    \"    \\n\",\n    \"    # Multiple concurrent streams\\n\",\n    \"    print(f\\\"\\\\nüîÄ Multiple concurrent streams:\\\")\\n\",\n    \"    \\n\",\n    \"    queries = [\\n\",\n    \"        \\\"What is machine learning?\\\",\\n\",\n    \"        \\\"Explain neural networks\\\",\\n\",\n    \"        \\\"How does AI work?\\\"\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    query_responses = {i: \\\"\\\" for i in range(len(queries))}\\n\",\n    \"    \\n\",\n    \"    async for result in provider.stream_multiple_responses(queries):\\n\",\n    \"        query_id = result[\\\"query_id\\\"]\\n\",\n    \"        chunk = result[\\\"chunk\\\"]\\n\",\n    \"        query_responses[query_id] += chunk.content\\n\",\n    \"        \\n\",\n    \"        status = \\\"[FINAL]\\\" if chunk.is_final else \\\"\\\"\\n\",\n    \"        print(f\\\"  Query {query_id}: chunk {chunk.chunk_id} '{chunk.content.strip()}' {status}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìã Final responses:\\\")\\n\",\n    \"    for i, response in query_responses.items():\\n\",\n    \"        print(f\\\"  Query {i}: {response}\\\")\\n\",\n    \"\\n\",\n    \"# Real-time streaming with buffer management\\n\",\n    \"class StreamBuffer:\\n\",\n    \"    \\\"\\\"\\\"Buffer for managing streaming data.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_size: int = 100):\\n\",\n    \"        self.buffer = []\\n\",\n    \"        self.max_size = max_size\\n\",\n    \"        self.overflow_count = 0\\n\",\n    \"    \\n\",\n    \"    async def add_chunk(self, chunk: StreamingChunk) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Add chunk to buffer, return True if successful.\\\"\\\"\\\"\\n\",\n    \"        if len(self.buffer) >= self.max_size:\\n\",\n    \"            self.overflow_count += 1\\n\",\n    \"            return False\\n\",\n    \"        \\n\",\n    \"        self.buffer.append(chunk)\\n\",\n    \"        return True\\n\",\n    \"    \\n\",\n    \"    async def get_chunks(self, count: int = None) -> List[StreamingChunk]:\\n\",\n    \"        \\\"\\\"\\\"Get chunks from buffer.\\\"\\\"\\\"\\n\",\n    \"        if count is None:\\n\",\n    \"            chunks = self.buffer.copy()\\n\",\n    \"            self.buffer.clear()\\n\",\n    \"        else:\\n\",\n    \"            chunks = self.buffer[:count]\\n\",\n    \"            self.buffer = self.buffer[count:]\\n\",\n    \"        \\n\",\n    \"        return chunks\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, int]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"buffer_size\\\": len(self.buffer),\\n\",\n    \"            \\\"max_size\\\": self.max_size,\\n\",\n    \"            \\\"overflow_count\\\": self.overflow_count\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_buffered_streaming():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate streaming with buffer management.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüíæ Buffered streaming demonstration:\\\")\\n\",\n    \"    \\n\",\n    \"    provider = StreamingAIProvider(response_delay=0.05)\\n\",\n    \"    buffer = StreamBuffer(max_size=5)\\n\",\n    \"    \\n\",\n    \"    messages = [Message(role=\\\"user\\\", content=\\\"Generate a long response about AI benefits\\\")]\\n\",\n    \"    \\n\",\n    \"    # Producer: Add chunks to buffer\\n\",\n    \"    async def producer():\\n\",\n    \"        async for chunk in provider.stream_completion(\\\"gpt-4\\\", messages):\\n\",\n    \"            success = await buffer.add_chunk(chunk)\\n\",\n    \"            if not success:\\n\",\n    \"                print(f\\\"‚ö†Ô∏è  Buffer overflow! Chunk {chunk.chunk_id} dropped\\\")\\n\",\n    \"    \\n\",\n    \"    # Consumer: Process chunks from buffer\\n\",\n    \"    async def consumer():\\n\",\n    \"        processed_count = 0\\n\",\n    \"        while True:\\n\",\n    \"            chunks = await buffer.get_chunks(2)  # Process 2 at a time\\n\",\n    \"            if not chunks:\\n\",\n    \"                await asyncio.sleep(0.1)  # Wait for more data\\n\",\n    \"                continue\\n\",\n    \"            \\n\",\n    \"            for chunk in chunks:\\n\",\n    \"                processed_count += 1\\n\",\n    \"                print(f\\\"üîß Processed chunk {chunk.chunk_id}: '{chunk.content.strip()}'\\\")\\n\",\n    \"                \\n\",\n    \"                if chunk.is_final:\\n\",\n    \"                    print(f\\\"‚úÖ Processing complete! Total chunks: {processed_count}\\\")\\n\",\n    \"                    return\\n\",\n    \"            \\n\",\n    \"            await asyncio.sleep(0.1)  # Simulate processing time\\n\",\n    \"    \\n\",\n    \"    # Run producer and consumer concurrently\\n\",\n    \"    await asyncio.gather(producer(), consumer())\\n\",\n    \"    \\n\",\n    \"    stats = buffer.get_stats()\\n\",\n    \"    print(f\\\"üìä Buffer stats: {stats}\\\")\\n\",\n    \"\\n\",\n    \"# Run streaming demonstrations\\n\",\n    \"await demonstrate_streaming()\\n\",\n    \"await demonstrate_buffered_streaming()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Streaming Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Real-time user feedback\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Lower perceived latency\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Memory efficiency for large responses\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Better resource utilization\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 3: Rate Limiting and Resource Control\\n\",\n    \"\\n\",\n    \"**First Principle**: Rate limiting prevents resource exhaustion and ensures fair usage across multiple clients.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `asyncio.Semaphore` and `asyncio.Lock` for controlling concurrent access to resources.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class AsyncRateLimiter:\\n\",\n    \"    \\\"\\\"\\\"Async rate limiter for API calls.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_calls: int, time_window: float):\\n\",\n    \"        self.max_calls = max_calls\\n\",\n    \"        self.time_window = time_window\\n\",\n    \"        self.calls: List[float] = []\\n\",\n    \"        self._lock = asyncio.Lock()\\n\",\n    \"    \\n\",\n    \"    async def acquire(self) -> None:\\n\",\n    \"        \\\"\\\"\\\"Acquire permission to make a call.\\\"\\\"\\\"\\n\",\n    \"        async with self._lock:\\n\",\n    \"            now = time.time()\\n\",\n    \"            \\n\",\n    \"            # Remove old calls outside the window\\n\",\n    \"            self.calls = [call_time for call_time in self.calls \\n\",\n    \"                         if now - call_time < self.time_window]\\n\",\n    \"            \\n\",\n    \"            # If we're at the limit, wait\\n\",\n    \"            if len(self.calls) >= self.max_calls:\\n\",\n    \"                oldest_call = min(self.calls)\\n\",\n    \"                wait_time = self.time_window - (now - oldest_call)\\n\",\n    \"                if wait_time > 0:\\n\",\n    \"                    print(f\\\"üö¶ Rate limit reached, waiting {wait_time:.2f}s\\\")\\n\",\n    \"                    await asyncio.sleep(wait_time)\\n\",\n    \"                    # Recursive call to try again\\n\",\n    \"                    return await self.acquire()\\n\",\n    \"            \\n\",\n    \"            # Record this call\\n\",\n    \"            self.calls.append(now)\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get rate limiter statistics.\\\"\\\"\\\"\\n\",\n    \"        now = time.time()\\n\",\n    \"        recent_calls = [call for call in self.calls if now - call < self.time_window]\\n\",\n    \"        return {\\n\",\n    \"            \\\"current_calls\\\": len(recent_calls),\\n\",\n    \"            \\\"max_calls\\\": self.max_calls,\\n\",\n    \"            \\\"time_window\\\": self.time_window,\\n\",\n    \"            \\\"utilization\\\": len(recent_calls) / self.max_calls\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"class TokenBucketRateLimiter:\\n\",\n    \"    \\\"\\\"\\\"Token bucket rate limiter for smooth rate limiting.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, tokens_per_second: float, bucket_size: int):\\n\",\n    \"        self.tokens_per_second = tokens_per_second\\n\",\n    \"        self.bucket_size = bucket_size\\n\",\n    \"        self.tokens = bucket_size\\n\",\n    \"        self.last_refill = time.time()\\n\",\n    \"        self._lock = asyncio.Lock()\\n\",\n    \"    \\n\",\n    \"    async def acquire(self, tokens_needed: int = 1) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Try to acquire tokens from the bucket.\\\"\\\"\\\"\\n\",\n    \"        async with self._lock:\\n\",\n    \"            now = time.time()\\n\",\n    \"            \\n\",\n    \"            # Refill tokens based on time passed\\n\",\n    \"            time_passed = now - self.last_refill\\n\",\n    \"            tokens_to_add = time_passed * self.tokens_per_second\\n\",\n    \"            self.tokens = min(self.bucket_size, self.tokens + tokens_to_add)\\n\",\n    \"            self.last_refill = now\\n\",\n    \"            \\n\",\n    \"            # Check if we have enough tokens\\n\",\n    \"            if self.tokens >= tokens_needed:\\n\",\n    \"                self.tokens -= tokens_needed\\n\",\n    \"                return True\\n\",\n    \"            else:\\n\",\n    \"                # Calculate wait time for next token\\n\",\n    \"                tokens_deficit = tokens_needed - self.tokens\\n\",\n    \"                wait_time = tokens_deficit / self.tokens_per_second\\n\",\n    \"                print(f\\\"ü™£ Token bucket empty, waiting {wait_time:.2f}s\\\")\\n\",\n    \"                await asyncio.sleep(wait_time)\\n\",\n    \"                return await self.acquire(tokens_needed)\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"tokens\\\": self.tokens,\\n\",\n    \"            \\\"bucket_size\\\": self.bucket_size,\\n\",\n    \"            \\\"tokens_per_second\\\": self.tokens_per_second,\\n\",\n    \"            \\\"fill_percentage\\\": self.tokens / self.bucket_size * 100\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_rate_limiting():\\n\",\n    \"    \\\"\\\"\\\"Show rate limiting patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üö¶ Rate Limiting\\\", style=\\\"bold green\\\"))\\n\",\n    \"    \\n\",\n    \"    # Test sliding window rate limiter\\n\",\n    \"    print(\\\"üìä Sliding window rate limiter (5 calls per 2 seconds):\\\")\\n\",\n    \"    \\n\",\n    \"    rate_limiter = AsyncRateLimiter(max_calls=5, time_window=2.0)\\n\",\n    \"    \\n\",\n    \"    async def make_api_call(call_id: int) -> str:\\n\",\n    \"        \\\"\\\"\\\"Simulate API call with rate limiting.\\\"\\\"\\\"\\n\",\n    \"        await rate_limiter.acquire()\\n\",\n    \"        # Simulate API processing time\\n\",\n    \"        await asyncio.sleep(0.1)\\n\",\n    \"        return f\\\"API call {call_id} completed\\\"\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    # Create 10 rapid calls\\n\",\n    \"    tasks = [make_api_call(i) for i in range(10)]\\n\",\n    \"    \\n\",\n    \"    # Execute with progress tracking\\n\",\n    \"    results = []\\n\",\n    \"    for i, task in enumerate(asyncio.as_completed(tasks)):\\n\",\n    \"        result = await task\\n\",\n    \"        elapsed = time.time() - start_time\\n\",\n    \"        stats = rate_limiter.get_stats()\\n\",\n    \"        print(f\\\"  {elapsed:.2f}s: {result} (utilization: {stats['utilization']:.1%})\\\")\\n\",\n    \"        results.append(result)\\n\",\n    \"    \\n\",\n    \"    total_time = time.time() - start_time\\n\",\n    \"    print(f\\\"\\\\nüìà Rate limiting summary:\\\")\\n\",\n    \"    print(f\\\"  Total time: {total_time:.2f}s\\\")\\n\",\n    \"    print(f\\\"  Rate limiter stats: {rate_limiter.get_stats()}\\\")\\n\",\n    \"    \\n\",\n    \"    # Test token bucket rate limiter\\n\",\n    \"    print(f\\\"\\\\nü™£ Token bucket rate limiter (3 tokens/sec, bucket size 5):\\\")\\n\",\n    \"    \\n\",\n    \"    token_limiter = TokenBucketRateLimiter(tokens_per_second=3.0, bucket_size=5)\\n\",\n    \"    \\n\",\n    \"    async def make_weighted_call(call_id: int, tokens_needed: int = 1) -> str:\\n\",\n    \"        \\\"\\\"\\\"API call that may need multiple tokens.\\\"\\\"\\\"\\n\",\n    \"        await token_limiter.acquire(tokens_needed)\\n\",\n    \"        await asyncio.sleep(0.05)  # Simulate processing\\n\",\n    \"        return f\\\"Weighted call {call_id} (cost: {tokens_needed} tokens)\\\"\\n\",\n    \"    \\n\",\n    \"    # Different cost operations\\n\",\n    \"    operations = [\\n\",\n    \"        (1, 1),  # Light operation\\n\",\n    \"        (2, 2),  # Medium operation\\n\",\n    \"        (3, 1),  # Light operation\\n\",\n    \"        (4, 3),  # Heavy operation\\n\",\n    \"        (5, 1),  # Light operation\\n\",\n    \"        (6, 2),  # Medium operation\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    for call_id, tokens in operations:\\n\",\n    \"        result = await make_weighted_call(call_id, tokens)\\n\",\n    \"        elapsed = time.time() - start_time\\n\",\n    \"        stats = token_limiter.get_stats()\\n\",\n    \"        print(f\\\"  {elapsed:.2f}s: {result} (tokens left: {stats['tokens']:.1f})\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìä Final token bucket stats: {token_limiter.get_stats()}\\\")\\n\",\n    \"\\n\",\n    \"# Advanced rate limiting with priority queues\\n\",\n    \"class PriorityRateLimiter:\\n\",\n    \"    \\\"\\\"\\\"Rate limiter with priority queue support.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_calls: int, time_window: float):\\n\",\n    \"        self.base_limiter = AsyncRateLimiter(max_calls, time_window)\\n\",\n    \"        self.priority_queues = {\\n\",\n    \"            \\\"high\\\": asyncio.Queue(),\\n\",\n    \"            \\\"medium\\\": asyncio.Queue(),\\n\",\n    \"            \\\"low\\\": asyncio.Queue()\\n\",\n    \"        }\\n\",\n    \"        self._processing = False\\n\",\n    \"    \\n\",\n    \"    async def acquire(self, priority: str = \\\"medium\\\") -> None:\\n\",\n    \"        \\\"\\\"\\\"Acquire with priority.\\\"\\\"\\\"\\n\",\n    \"        if priority not in self.priority_queues:\\n\",\n    \"            priority = \\\"medium\\\"\\n\",\n    \"        \\n\",\n    \"        # Add request to appropriate queue\\n\",\n    \"        future = asyncio.Future()\\n\",\n    \"        await self.priority_queues[priority].put(future)\\n\",\n    \"        \\n\",\n    \"        # Start processing if not already running\\n\",\n    \"        if not self._processing:\\n\",\n    \"            asyncio.create_task(self._process_queues())\\n\",\n    \"        \\n\",\n    \"        # Wait for our turn\\n\",\n    \"        await future\\n\",\n    \"    \\n\",\n    \"    async def _process_queues(self):\\n\",\n    \"        \\\"\\\"\\\"Process requests in priority order.\\\"\\\"\\\"\\n\",\n    \"        self._processing = True\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            while True:\\n\",\n    \"                # Check queues in priority order\\n\",\n    \"                future = None\\n\",\n    \"                \\n\",\n    \"                for priority in [\\\"high\\\", \\\"medium\\\", \\\"low\\\"]:\\n\",\n    \"                    queue = self.priority_queues[priority]\\n\",\n    \"                    if not queue.empty():\\n\",\n    \"                        future = await queue.get()\\n\",\n    \"                        break\\n\",\n    \"                \\n\",\n    \"                if future is None:\\n\",\n    \"                    # No more requests\\n\",\n    \"                    break\\n\",\n    \"                \\n\",\n    \"                # Acquire from base rate limiter\\n\",\n    \"                await self.base_limiter.acquire()\\n\",\n    \"                \\n\",\n    \"                # Signal that request can proceed\\n\",\n    \"                if not future.cancelled():\\n\",\n    \"                    future.set_result(None)\\n\",\n    \"        \\n\",\n    \"        finally:\\n\",\n    \"            self._processing = False\\n\",\n    \"\\n\",\n    \"async def demonstrate_priority_rate_limiting():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate priority-based rate limiting.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüéØ Priority rate limiting:\\\")\\n\",\n    \"    \\n\",\n    \"    priority_limiter = PriorityRateLimiter(max_calls=3, time_window=1.0)\\n\",\n    \"    \\n\",\n    \"    async def priority_task(task_id: int, priority: str, delay: float = 0.1):\\n\",\n    \"        \\\"\\\"\\\"Task with specific priority.\\\"\\\"\\\"\\n\",\n    \"        print(f\\\"üîÑ Task {task_id} ({priority}) waiting...\\\")\\n\",\n    \"        await priority_limiter.acquire(priority)\\n\",\n    \"        print(f\\\"‚úÖ Task {task_id} ({priority}) executing\\\")\\n\",\n    \"        await asyncio.sleep(delay)\\n\",\n    \"        return f\\\"Task {task_id} ({priority}) completed\\\"\\n\",\n    \"    \\n\",\n    \"    # Submit tasks with different priorities\\n\",\n    \"    tasks = [\\n\",\n    \"        priority_task(1, \\\"low\\\"),\\n\",\n    \"        priority_task(2, \\\"high\\\"),\\n\",\n    \"        priority_task(3, \\\"medium\\\"),\\n\",\n    \"        priority_task(4, \\\"low\\\"),\\n\",\n    \"        priority_task(5, \\\"high\\\"),\\n\",\n    \"        priority_task(6, \\\"medium\\\"),\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    results = await asyncio.gather(*tasks)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìã Execution order shows priority handling:\\\")\\n\",\n    \"    for result in results:\\n\",\n    \"        print(f\\\"  {result}\\\")\\n\",\n    \"\\n\",\n    \"# Run rate limiting demonstrations\\n\",\n    \"await demonstrate_rate_limiting()\\n\",\n    \"await demonstrate_priority_rate_limiting()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Rate Limiting Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Prevents API quota exhaustion\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Ensures fair resource usage\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Protects against request bursts\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Enables priority-based processing\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 4: Connection Pooling and Resource Management\\n\",\n    \"\\n\",\n    \"**First Principle**: Connection pooling reuses expensive resources like database connections and HTTP sessions.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `asyncio.Queue` for resource pools and async context managers for automatic cleanup.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class AsyncConnectionPool:\\n\",\n    \"    \\\"\\\"\\\"Connection pool for managing async resources.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, create_connection: Callable, max_size: int = 10):\\n\",\n    \"        self.create_connection = create_connection\\n\",\n    \"        self.max_size = max_size\\n\",\n    \"        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_size)\\n\",\n    \"        self.created_count = 0\\n\",\n    \"        self.active_connections = 0\\n\",\n    \"        self._lock = asyncio.Lock()\\n\",\n    \"    \\n\",\n    \"    async def get_connection(self):\\n\",\n    \"        \\\"\\\"\\\"Get a connection from the pool.\\\"\\\"\\\"\\n\",\n    \"        try:\\n\",\n    \"            # Try to get existing connection with timeout\\n\",\n    \"            connection = await asyncio.wait_for(self.pool.get(), timeout=0.1)\\n\",\n    \"            self.active_connections += 1\\n\",\n    \"            return connection\\n\",\n    \"        except asyncio.TimeoutError:\\n\",\n    \"            # Create new connection if under limit\\n\",\n    \"            async with self._lock:\\n\",\n    \"                if self.created_count < self.max_size:\\n\",\n    \"                    connection = await self.create_connection()\\n\",\n    \"                    self.created_count += 1\\n\",\n    \"                    self.active_connections += 1\\n\",\n    \"                    return connection\\n\",\n    \"            \\n\",\n    \"            # Wait for available connection\\n\",\n    \"            connection = await self.pool.get()\\n\",\n    \"            self.active_connections += 1\\n\",\n    \"            return connection\\n\",\n    \"    \\n\",\n    \"    async def return_connection(self, connection):\\n\",\n    \"        \\\"\\\"\\\"Return connection to pool.\\\"\\\"\\\"\\n\",\n    \"        self.active_connections -= 1\\n\",\n    \"        await self.pool.put(connection)\\n\",\n    \"    \\n\",\n    \"    @asynccontextmanager\\n\",\n    \"    async def connection(self):\\n\",\n    \"        \\\"\\\"\\\"Context manager for automatic connection management.\\\"\\\"\\\"\\n\",\n    \"        conn = await self.get_connection()\\n\",\n    \"        try:\\n\",\n    \"            yield conn\\n\",\n    \"        finally:\\n\",\n    \"            await self.return_connection(conn)\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get pool statistics.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"pool_size\\\": self.pool.qsize(),\\n\",\n    \"            \\\"created_count\\\": self.created_count,\\n\",\n    \"            \\\"active_connections\\\": self.active_connections,\\n\",\n    \"            \\\"max_size\\\": self.max_size,\\n\",\n    \"            \\\"utilization\\\": self.active_connections / self.max_size\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"# Mock connection class\\n\",\n    \"class MockConnection:\\n\",\n    \"    \\\"\\\"\\\"Mock database/API connection.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, connection_id: str):\\n\",\n    \"        self.id = connection_id\\n\",\n    \"        self.created_at = time.time()\\n\",\n    \"        self.request_count = 0\\n\",\n    \"        self.is_healthy = True\\n\",\n    \"    \\n\",\n    \"    async def execute_query(self, query: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Execute a mock query.\\\"\\\"\\\"\\n\",\n    \"        if not self.is_healthy:\\n\",\n    \"            raise ConnectionError(f\\\"Connection {self.id} is unhealthy\\\")\\n\",\n    \"        \\n\",\n    \"        self.request_count += 1\\n\",\n    \"        await asyncio.sleep(0.1)  # Simulate query time\\n\",\n    \"        return f\\\"Result for '{query}' via connection {self.id}\\\"\\n\",\n    \"    \\n\",\n    \"    def get_info(self) -> Dict[str, Any]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"id\\\": self.id,\\n\",\n    \"            \\\"created_at\\\": self.created_at,\\n\",\n    \"            \\\"request_count\\\": self.request_count,\\n\",\n    \"            \\\"age_seconds\\\": time.time() - self.created_at\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_connection_pooling():\\n\",\n    \"    \\\"\\\"\\\"Show connection pooling patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üèä Connection Pooling\\\", style=\\\"bold magenta\\\"))\\n\",\n    \"    \\n\",\n    \"    async def create_mock_connection():\\n\",\n    \"        \\\"\\\"\\\"Create a mock database connection.\\\"\\\"\\\"\\n\",\n    \"        connection_id = f\\\"conn_{int(time.time() * 1000) % 10000}\\\"\\n\",\n    \"        await asyncio.sleep(0.1)  # Simulate connection setup\\n\",\n    \"        print(f\\\"üîß Created new connection: {connection_id}\\\")\\n\",\n    \"        return MockConnection(connection_id)\\n\",\n    \"    \\n\",\n    \"    # Create connection pool\\n\",\n    \"    pool = AsyncConnectionPool(create_mock_connection, max_size=3)\\n\",\n    \"    \\n\",\n    \"    async def database_operation(operation_id: int, query: str):\\n\",\n    \"        \\\"\\\"\\\"Simulate database operation using connection pool.\\\"\\\"\\\"\\n\",\n    \"        async with pool.connection() as conn:\\n\",\n    \"            result = await conn.execute_query(query)\\n\",\n    \"            return f\\\"Operation {operation_id}: {result}\\\"\\n\",\n    \"    \\n\",\n    \"    print(\\\"üìä Running concurrent database operations:\\\")\\n\",\n    \"    \\n\",\n    \"    # Run 8 concurrent operations with pool of 3 connections\\n\",\n    \"    queries = [\\n\",\n    \"        \\\"SELECT * FROM users\\\",\\n\",\n    \"        \\\"SELECT * FROM products\\\",\\n\",\n    \"        \\\"UPDATE inventory SET count=count-1\\\",\\n\",\n    \"        \\\"INSERT INTO logs VALUES (...)\\\",\\n\",\n    \"        \\\"SELECT AVG(price) FROM products\\\",\\n\",\n    \"        \\\"DELETE FROM cache WHERE expired=true\\\",\\n\",\n    \"        \\\"SELECT * FROM orders WHERE status='pending'\\\",\\n\",\n    \"        \\\"UPDATE users SET last_login=NOW()\\\"\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    tasks = [database_operation(i, query) for i, query in enumerate(queries)]\\n\",\n    \"    \\n\",\n    \"    for task in asyncio.as_completed(tasks):\\n\",\n    \"        result = await task\\n\",\n    \"        elapsed = time.time() - start_time\\n\",\n    \"        stats = pool.get_stats()\\n\",\n    \"        print(f\\\"  {elapsed:.2f}s: {result[:50]}...\\\")\\n\",\n    \"        print(f\\\"    Pool: {stats['active_connections']}/{stats['max_size']} active, {stats['pool_size']} available\\\")\\n\",\n    \"    \\n\",\n    \"    final_stats = pool.get_stats()\\n\",\n    \"    print(f\\\"\\\\nüìà Final Pool Stats:\\\")\\n\",\n    \"    print(f\\\"  Created connections: {final_stats['created_count']}\\\")\\n\",\n    \"    print(f\\\"  Active connections: {final_stats['active_connections']}\\\")\\n\",\n    \"    print(f\\\"  Pool utilization: {final_stats['utilization']:.1%}\\\")\\n\",\n    \"\\n\",\n    \"# Advanced connection pool with health checking\\n\",\n    \"class HealthAwareConnectionPool(AsyncConnectionPool):\\n\",\n    \"    \\\"\\\"\\\"Connection pool with health checking.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, create_connection: Callable, max_size: int = 10, health_check_interval: float = 30.0):\\n\",\n    \"        super().__init__(create_connection, max_size)\\n\",\n    \"        self.health_check_interval = health_check_interval\\n\",\n    \"        self.unhealthy_connections = set()\\n\",\n    \"        self._health_check_task = None\\n\",\n    \"    \\n\",\n    \"    async def start_health_checks(self):\\n\",\n    \"        \\\"\\\"\\\"Start background health checking.\\\"\\\"\\\"\\n\",\n    \"        if self._health_check_task is None:\\n\",\n    \"            self._health_check_task = asyncio.create_task(self._health_check_loop())\\n\",\n    \"    \\n\",\n    \"    async def stop_health_checks(self):\\n\",\n    \"        \\\"\\\"\\\"Stop background health checking.\\\"\\\"\\\"\\n\",\n    \"        if self._health_check_task:\\n\",\n    \"            self._health_check_task.cancel()\\n\",\n    \"            try:\\n\",\n    \"                await self._health_check_task\\n\",\n    \"            except asyncio.CancelledError:\\n\",\n    \"                pass\\n\",\n    \"            self._health_check_task = None\\n\",\n    \"    \\n\",\n    \"    async def _health_check_loop(self):\\n\",\n    \"        \\\"\\\"\\\"Background health checking loop.\\\"\\\"\\\"\\n\",\n    \"        while True:\\n\",\n    \"            try:\\n\",\n    \"                await asyncio.sleep(self.health_check_interval)\\n\",\n    \"                await self._check_pool_health()\\n\",\n    \"            except asyncio.CancelledError:\\n\",\n    \"                break\\n\",\n    \"            except Exception as e:\\n\",\n    \"                print(f\\\"‚ö†Ô∏è  Health check error: {e}\\\")\\n\",\n    \"    \\n\",\n    \"    async def _check_pool_health(self):\\n\",\n    \"        \\\"\\\"\\\"Check health of pooled connections.\\\"\\\"\\\"\\n\",\n    \"        # This is a simplified health check\\n\",\n    \"        # In practice, you'd check connections more thoroughly\\n\",\n    \"        print(f\\\"üè• Performing health check on {self.pool.qsize()} pooled connections\\\")\\n\",\n    \"        \\n\",\n    \"        # Remove old unhealthy connections from tracking\\n\",\n    \"        self.unhealthy_connections.clear()\\n\",\n    \"    \\n\",\n    \"    async def get_connection(self):\\n\",\n    \"        \\\"\\\"\\\"Get a healthy connection from the pool.\\\"\\\"\\\"\\n\",\n    \"        connection = await super().get_connection()\\n\",\n    \"        \\n\",\n    \"        # Check if connection is healthy\\n\",\n    \"        if hasattr(connection, 'is_healthy') and not connection.is_healthy:\\n\",\n    \"            print(f\\\"üö´ Connection {connection.id} is unhealthy, creating new one\\\")\\n\",\n    \"            # Create a replacement connection\\n\",\n    \"            new_connection = await self.create_connection()\\n\",\n    \"            return new_connection\\n\",\n    \"        \\n\",\n    \"        return connection\\n\",\n    \"\\n\",\n    \"async def demonstrate_health_aware_pooling():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate health-aware connection pooling.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüè• Health-aware connection pooling:\\\")\\n\",\n    \"    \\n\",\n    \"    async def create_connection_with_random_health():\\n\",\n    \"        \\\"\\\"\\\"Create connection that might become unhealthy.\\\"\\\"\\\"\\n\",\n    \"        import random\\n\",\n    \"        connection = await create_mock_connection()\\n\",\n    \"        # Simulate some connections becoming unhealthy\\n\",\n    \"        connection.is_healthy = random.random() > 0.3  # 30% chance of being unhealthy\\n\",\n    \"        return connection\\n\",\n    \"    \\n\",\n    \"    health_pool = HealthAwareConnectionPool(\\n\",\n    \"        create_connection_with_random_health, \\n\",\n    \"        max_size=2, \\n\",\n    \"        health_check_interval=2.0\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    # Start health checks\\n\",\n    \"    await health_pool.start_health_checks()\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Use the pool\\n\",\n    \"        for i in range(5):\\n\",\n    \"            async with health_pool.connection() as conn:\\n\",\n    \"                try:\\n\",\n    \"                    result = await conn.execute_query(f\\\"Query {i}\\\")\\n\",\n    \"                    print(f\\\"‚úÖ {result[:40]}...\\\")\\n\",\n    \"                except ConnectionError as e:\\n\",\n    \"                    print(f\\\"‚ùå {e}\\\")\\n\",\n    \"            \\n\",\n    \"            await asyncio.sleep(0.5)\\n\",\n    \"    \\n\",\n    \"    finally:\\n\",\n    \"        # Stop health checks\\n\",\n    \"        await health_pool.stop_health_checks()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"üìä Health-aware pool stats: {health_pool.get_stats()}\\\")\\n\",\n    \"\\n\",\n    \"# Resource manager with lifecycle\\n\",\n    \"@asynccontextmanager\\n\",\n    \"async def managed_ai_service(service_config: Dict[str, Any]):\\n\",\n    \"    \\\"\\\"\\\"Context manager for AI service with full lifecycle.\\\"\\\"\\\"\\n\",\n    \"    print(f\\\"üöÄ Starting AI service with config: {service_config}\\\")\\n\",\n    \"    \\n\",\n    \"    # Setup phase\\n\",\n    \"    service = {\\n\",\n    \"        \\\"id\\\": f\\\"service_{int(time.time())}\\\",\\n\",\n    \"        \\\"config\\\": service_config,\\n\",\n    \"        \\\"started_at\\\": time.time(),\\n\",\n    \"        \\\"request_count\\\": 0\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Simulate service initialization\\n\",\n    \"        await asyncio.sleep(0.1)\\n\",\n    \"        print(f\\\"‚úÖ AI service {service['id']} ready\\\")\\n\",\n    \"        \\n\",\n    \"        yield service\\n\",\n    \"        \\n\",\n    \"    finally:\\n\",\n    \"        # Cleanup phase\\n\",\n    \"        uptime = time.time() - service['started_at']\\n\",\n    \"        print(f\\\"üßπ Shutting down service {service['id']} (uptime: {uptime:.2f}s, requests: {service['request_count']})\\\")\\n\",\n    \"        await asyncio.sleep(0.05)  # Simulate cleanup\\n\",\n    \"\\n\",\n    \"async def demonstrate_resource_lifecycle():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate resource lifecycle management.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüîÑ Resource lifecycle management:\\\")\\n\",\n    \"    \\n\",\n    \"    config = {\\\"model\\\": \\\"gpt-4\\\", \\\"temperature\\\": 0.7, \\\"max_workers\\\": 5}\\n\",\n    \"    \\n\",\n    \"    async with managed_ai_service(config) as service:\\n\",\n    \"        print(f\\\"üìù Using service: {service['id']}\\\")\\n\",\n    \"        \\n\",\n    \"        # Simulate some work\\n\",\n    \"        for i in range(3):\\n\",\n    \"            service['request_count'] += 1\\n\",\n    \"            await asyncio.sleep(0.1)\\n\",\n    \"            print(f\\\"  Processed request {i+1}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"‚úÖ Service lifecycle completed cleanly\\\")\\n\",\n    \"\\n\",\n    \"# Run connection pooling demonstrations\\n\",\n    \"await demonstrate_connection_pooling()\\n\",\n    \"await demonstrate_health_aware_pooling()\\n\",\n    \"await demonstrate_resource_lifecycle()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Connection Pooling Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reuses expensive resources\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reduces connection overhead\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Controls resource usage\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Handles resource failures gracefully\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 5: Concurrent Processing and Task Management\\n\",\n    \"\\n\",\n    \"**First Principle**: Concurrent processing enables parallel execution of independent tasks, dramatically improving throughput.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `asyncio.gather()`, `asyncio.as_completed()`, and `asyncio.Semaphore` for managing concurrent operations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class ConcurrentInferenceEngine:\\n\",\n    \"    \\\"\\\"\\\"Engine for managing concurrent AI inference requests.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_concurrent: int = 5):\\n\",\n    \"        self.max_concurrent = max_concurrent\\n\",\n    \"        self.semaphore = asyncio.Semaphore(max_concurrent)\\n\",\n    \"        self.active_tasks: Dict[str, asyncio.Task] = {}\\n\",\n    \"        self.completed_tasks: List[Dict[str, Any]] = []\\n\",\n    \"        self.task_counter = 0\\n\",\n    \"    \\n\",\n    \"    async def submit_task(\\n\",\n    \"        self, \\n\",\n    \"        task_id: str, \\n\",\n    \"        coro: Callable,\\n\",\n    \"        *args, \\n\",\n    \"        **kwargs\\n\",\n    \"    ) -> str:\\n\",\n    \"        \\\"\\\"\\\"Submit a task for concurrent execution.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        async def wrapped_task():\\n\",\n    \"            async with self.semaphore:\\n\",\n    \"                start_time = time.time()\\n\",\n    \"                try:\\n\",\n    \"                    result = await coro(*args, **kwargs)\\n\",\n    \"                    end_time = time.time()\\n\",\n    \"                    \\n\",\n    \"                    self.completed_tasks.append({\\n\",\n    \"                        \\\"task_id\\\": task_id,\\n\",\n    \"                        \\\"result\\\": result,\\n\",\n    \"                        \\\"duration\\\": end_time - start_time,\\n\",\n    \"                        \\\"status\\\": \\\"success\\\",\\n\",\n    \"                        \\\"error\\\": None\\n\",\n    \"                    })\\n\",\n    \"                    \\n\",\n    \"                    return result\\n\",\n    \"                    \\n\",\n    \"                except Exception as e:\\n\",\n    \"                    end_time = time.time()\\n\",\n    \"                    \\n\",\n    \"                    self.completed_tasks.append({\\n\",\n    \"                        \\\"task_id\\\": task_id,\\n\",\n    \"                        \\\"result\\\": None,\\n\",\n    \"                        \\\"duration\\\": end_time - start_time,\\n\",\n    \"                        \\\"status\\\": \\\"error\\\",\\n\",\n    \"                        \\\"error\\\": str(e)\\n\",\n    \"                    })\\n\",\n    \"                    \\n\",\n    \"                    raise\\n\",\n    \"                finally:\\n\",\n    \"                    # Remove from active tasks\\n\",\n    \"                    if task_id in self.active_tasks:\\n\",\n    \"                        del self.active_tasks[task_id]\\n\",\n    \"        \\n\",\n    \"        # Create and store task\\n\",\n    \"        task = asyncio.create_task(wrapped_task())\\n\",\n    \"        self.active_tasks[task_id] = task\\n\",\n    \"        self.task_counter += 1\\n\",\n    \"        \\n\",\n    \"        return task_id\\n\",\n    \"    \\n\",\n    \"    async def wait_for_task(self, task_id: str) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Wait for specific task to complete.\\\"\\\"\\\"\\n\",\n    \"        if task_id in self.active_tasks:\\n\",\n    \"            return await self.active_tasks[task_id]\\n\",\n    \"        \\n\",\n    \"        # Check completed tasks\\n\",\n    \"        for completed in self.completed_tasks:\\n\",\n    \"            if completed[\\\"task_id\\\"] == task_id:\\n\",\n    \"                if completed[\\\"status\\\"] == \\\"error\\\":\\n\",\n    \"                    raise Exception(completed[\\\"error\\\"])\\n\",\n    \"                return completed[\\\"result\\\"]\\n\",\n    \"        \\n\",\n    \"        raise ValueError(f\\\"Task {task_id} not found\\\")\\n\",\n    \"    \\n\",\n    \"    async def wait_for_all(self) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Wait for all active tasks to complete.\\\"\\\"\\\"\\n\",\n    \"        if not self.active_tasks:\\n\",\n    \"            return []\\n\",\n    \"        \\n\",\n    \"        results = await asyncio.gather(*self.active_tasks.values(), return_exceptions=True)\\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    async def cancel_task(self, task_id: str) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Cancel a specific task.\\\"\\\"\\\"\\n\",\n    \"        if task_id in self.active_tasks:\\n\",\n    \"            task = self.active_tasks[task_id]\\n\",\n    \"            task.cancel()\\n\",\n    \"            del self.active_tasks[task_id]\\n\",\n    \"            return True\\n\",\n    \"        return False\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get engine statistics.\\\"\\\"\\\"\\n\",\n    \"        completed_success = sum(1 for t in self.completed_tasks if t[\\\"status\\\"] == \\\"success\\\")\\n\",\n    \"        completed_error = sum(1 for t in self.completed_tasks if t[\\\"status\\\"] == \\\"error\\\")\\n\",\n    \"        \\n\",\n    \"        if self.completed_tasks:\\n\",\n    \"            avg_duration = sum(t[\\\"duration\\\"] for t in self.completed_tasks) / len(self.completed_tasks)\\n\",\n    \"        else:\\n\",\n    \"            avg_duration = 0.0\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            \\\"active_tasks\\\": len(self.active_tasks),\\n\",\n    \"            \\\"completed_tasks\\\": len(self.completed_tasks),\\n\",\n    \"            \\\"completed_success\\\": completed_success,\\n\",\n    \"            \\\"completed_error\\\": completed_error,\\n\",\n    \"            \\\"max_concurrent\\\": self.max_concurrent,\\n\",\n    \"            \\\"available_slots\\\": self.semaphore._value,\\n\",\n    \"            \\\"total_submitted\\\": self.task_counter,\\n\",\n    \"            \\\"avg_duration\\\": avg_duration\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_concurrent_processing():\\n\",\n    \"    \\\"\\\"\\\"Show concurrent processing patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"‚ö° Concurrent Processing\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create inference engine\\n\",\n    \"    engine = ConcurrentInferenceEngine(max_concurrent=3)\\n\",\n    \"    \\n\",\n    \"    async def mock_inference(text: str, delay: float = 0.5, fail_rate: float = 0.0) -> str:\\n\",\n    \"        \\\"\\\"\\\"Mock AI inference with variable delay and failure rate.\\\"\\\"\\\"\\n\",\n    \"        import random\\n\",\n    \"        \\n\",\n    \"        # Simulate random failure\\n\",\n    \"        if random.random() < fail_rate:\\n\",\n    \"            raise Exception(f\\\"Inference failed for: {text[:20]}...\\\")\\n\",\n    \"        \\n\",\n    \"        await asyncio.sleep(delay)\\n\",\n    \"        return f\\\"Processed: {text.upper()}\\\"\\n\",\n    \"    \\n\",\n    \"    print(\\\"ü§ñ Submitting concurrent inference tasks:\\\")\\n\",\n    \"    \\n\",\n    \"    # Submit multiple tasks with different characteristics\\n\",\n    \"    tasks_data = [\\n\",\n    \"        (\\\"task_1\\\", \\\"Analyze sentiment\\\", 0.3, 0.0),\\n\",\n    \"        (\\\"task_2\\\", \\\"Summarize document\\\", 0.8, 0.0),\\n\",\n    \"        (\\\"task_3\\\", \\\"Extract entities\\\", 0.5, 0.2),  # May fail\\n\",\n    \"        (\\\"task_4\\\", \\\"Classify text\\\", 0.4, 0.0),\\n\",\n    \"        (\\\"task_5\\\", \\\"Generate response\\\", 0.6, 0.1),  # May fail\\n\",\n    \"        (\\\"task_6\\\", \\\"Translate text\\\", 0.3, 0.0),\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    submitted_tasks = []\\n\",\n    \"    for task_id, text, delay, fail_rate in tasks_data:\\n\",\n    \"        await engine.submit_task(task_id, mock_inference, text, delay, fail_rate)\\n\",\n    \"        submitted_tasks.append(task_id)\\n\",\n    \"        print(f\\\"  ‚úÖ Submitted: {task_id}\\\")\\n\",\n    \"    \\n\",\n    \"    # Monitor progress\\n\",\n    \"    print(f\\\"\\\\nüìä Monitoring task execution:\\\")\\n\",\n    \"    \\n\",\n    \"    while engine.active_tasks:\\n\",\n    \"        stats = engine.get_stats()\\n\",\n    \"        print(f\\\"  Active: {stats['active_tasks']}, Completed: {stats['completed_tasks']}, Available slots: {stats['available_slots']}\\\")\\n\",\n    \"        await asyncio.sleep(0.2)\\n\",\n    \"    \\n\",\n    \"    # Show final results\\n\",\n    \"    print(f\\\"\\\\nüìã Task Results:\\\")\\n\",\n    \"    for completed in engine.completed_tasks:\\n\",\n    \"        status_icon = \\\"‚úÖ\\\" if completed[\\\"status\\\"] == \\\"success\\\" else \\\"‚ùå\\\"\\n\",\n    \"        result_preview = completed[\\\"result\\\"][:30] if completed[\\\"result\\\"] else completed[\\\"error\\\"][:30]\\n\",\n    \"        print(f\\\"  {status_icon} {completed['task_id']}: {completed['duration']:.2f}s - {result_preview}...\\\")\\n\",\n    \"    \\n\",\n    \"    final_stats = engine.get_stats()\\n\",\n    \"    print(f\\\"\\\\nüìà Final Statistics:\\\")\\n\",\n    \"    print(f\\\"  Total tasks: {final_stats['total_submitted']}\\\")\\n\",\n    \"    print(f\\\"  Successful: {final_stats['completed_success']}\\\")\\n\",\n    \"    print(f\\\"  Failed: {final_stats['completed_error']}\\\")\\n\",\n    \"    print(f\\\"  Average duration: {final_stats['avg_duration']:.2f}s\\\")\\n\",\n    \"\\n\",\n    \"# Advanced concurrent patterns\\n\",\n    \"class BatchProcessor:\\n\",\n    \"    \\\"\\\"\\\"Process items in batches with concurrency control.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, batch_size: int = 10, max_concurrent_batches: int = 3):\\n\",\n    \"        self.batch_size = batch_size\\n\",\n    \"        self.max_concurrent_batches = max_concurrent_batches\\n\",\n    \"        self.semaphore = asyncio.Semaphore(max_concurrent_batches)\\n\",\n    \"    \\n\",\n    \"    async def process_batch(\\n\",\n    \"        self,\\n\",\n    \"        items: List[Any],\\n\",\n    \"        processor: Callable,\\n\",\n    \"        **kwargs\\n\",\n    \"    ) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Process items in concurrent batches.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        async def process_single_batch(batch_items: List[Any], batch_id: int) -> List[Any]:\\n\",\n    \"            \\\"\\\"\\\"Process a single batch.\\\"\\\"\\\"\\n\",\n    \"            async with self.semaphore:\\n\",\n    \"                print(f\\\"üîÑ Processing batch {batch_id} ({len(batch_items)} items)\\\")\\n\",\n    \"                \\n\",\n    \"                # Process items in batch concurrently\\n\",\n    \"                tasks = [processor(item, **kwargs) for item in batch_items]\\n\",\n    \"                results = await asyncio.gather(*tasks, return_exceptions=True)\\n\",\n    \"                \\n\",\n    \"                print(f\\\"‚úÖ Completed batch {batch_id}\\\")\\n\",\n    \"                return results\\n\",\n    \"        \\n\",\n    \"        # Split items into batches\\n\",\n    \"        batches = [\\n\",\n    \"            items[i:i + self.batch_size] \\n\",\n    \"            for i in range(0, len(items), self.batch_size)\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        print(f\\\"üì¶ Processing {len(items)} items in {len(batches)} batches\\\")\\n\",\n    \"        \\n\",\n    \"        # Process batches concurrently\\n\",\n    \"        batch_tasks = [\\n\",\n    \"            process_single_batch(batch, i) \\n\",\n    \"            for i, batch in enumerate(batches)\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        batch_results = await asyncio.gather(*batch_tasks)\\n\",\n    \"        \\n\",\n    \"        # Flatten results\\n\",\n    \"        all_results = []\\n\",\n    \"        for batch_result in batch_results:\\n\",\n    \"            all_results.extend(batch_result)\\n\",\n    \"        \\n\",\n    \"        return all_results\\n\",\n    \"\\n\",\n    \"async def demonstrate_batch_processing():\\n\",\n    \"    \\\"\\\"\\\"Show batch processing patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüì¶ Batch Processing:\\\")\\n\",\n    \"    \\n\",\n    \"    async def process_text_item(text: str, operation: str = \\\"uppercase\\\") -> str:\\n\",\n    \"        \\\"\\\"\\\"Process a single text item.\\\"\\\"\\\"\\n\",\n    \"        await asyncio.sleep(0.1)  # Simulate processing\\n\",\n    \"        \\n\",\n    \"        if operation == \\\"uppercase\\\":\\n\",\n    \"            return text.upper()\\n\",\n    \"        elif operation == \\\"reverse\\\":\\n\",\n    \"            return text[::-1]\\n\",\n    \"        else:\\n\",\n    \"            return f\\\"PROCESSED: {text}\\\"\\n\",\n    \"    \\n\",\n    \"    # Create test data\\n\",\n    \"    texts = [f\\\"text item {i}\\\" for i in range(25)]\\n\",\n    \"    \\n\",\n    \"    processor = BatchProcessor(batch_size=5, max_concurrent_batches=2)\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    results = await processor.process_batch(\\n\",\n    \"        texts, \\n\",\n    \"        process_text_item, \\n\",\n    \"        operation=\\\"uppercase\\\"\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    processing_time = time.time() - start_time\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìä Batch Processing Results:\\\")\\n\",\n    \"    print(f\\\"  Total items: {len(texts)}\\\")\\n\",\n    \"    print(f\\\"  Processing time: {processing_time:.2f}s\\\")\\n\",\n    \"    print(f\\\"  Items per second: {len(texts) / processing_time:.1f}\\\")\\n\",\n    \"    print(f\\\"  Sample results: {results[:3]}\\\")\\n\",\n    \"    \\n\",\n    \"    # Count errors\\n\",\n    \"    errors = [r for r in results if isinstance(r, Exception)]\\n\",\n    \"    print(f\\\"  Errors: {len(errors)}\\\")\\n\",\n    \"\\n\",\n    \"# Progress tracking for concurrent operations\\n\",\n    \"class ProgressTracker:\\n\",\n    \"    \\\"\\\"\\\"Track progress of concurrent operations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, total_tasks: int):\\n\",\n    \"        self.total_tasks = total_tasks\\n\",\n    \"        self.completed_tasks = 0\\n\",\n    \"        self.failed_tasks = 0\\n\",\n    \"        self.start_time = time.time()\\n\",\n    \"        self._lock = asyncio.Lock()\\n\",\n    \"    \\n\",\n    \"    async def task_completed(self, success: bool = True):\\n\",\n    \"        \\\"\\\"\\\"Mark a task as completed.\\\"\\\"\\\"\\n\",\n    \"        async with self._lock:\\n\",\n    \"            if success:\\n\",\n    \"                self.completed_tasks += 1\\n\",\n    \"            else:\\n\",\n    \"                self.failed_tasks += 1\\n\",\n    \"    \\n\",\n    \"    def get_progress(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get current progress.\\\"\\\"\\\"\\n\",\n    \"        total_finished = self.completed_tasks + self.failed_tasks\\n\",\n    \"        progress_pct = (total_finished / self.total_tasks) * 100 if self.total_tasks > 0 else 0\\n\",\n    \"        elapsed_time = time.time() - self.start_time\\n\",\n    \"        \\n\",\n    \"        if total_finished > 0:\\n\",\n    \"            estimated_total_time = elapsed_time * (self.total_tasks / total_finished)\\n\",\n    \"            eta = estimated_total_time - elapsed_time\\n\",\n    \"        else:\\n\",\n    \"            eta = 0\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            \\\"completed\\\": self.completed_tasks,\\n\",\n    \"            \\\"failed\\\": self.failed_tasks,\\n\",\n    \"            \\\"total\\\": self.total_tasks,\\n\",\n    \"            \\\"progress_pct\\\": progress_pct,\\n\",\n    \"            \\\"elapsed_time\\\": elapsed_time,\\n\",\n    \"            \\\"eta\\\": max(0, eta)\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_progress_tracking():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate progress tracking for concurrent tasks.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìà Progress Tracking:\\\")\\n\",\n    \"    \\n\",\n    \"    total_tasks = 20\\n\",\n    \"    tracker = ProgressTracker(total_tasks)\\n\",\n    \"    \\n\",\n    \"    async def tracked_task(task_id: int, delay: float, fail_rate: float = 0.1):\\n\",\n    \"        \\\"\\\"\\\"Task with progress tracking.\\\"\\\"\\\"\\n\",\n    \"        import random\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            await asyncio.sleep(delay)\\n\",\n    \"            \\n\",\n    \"            if random.random() < fail_rate:\\n\",\n    \"                raise Exception(f\\\"Task {task_id} failed\\\")\\n\",\n    \"            \\n\",\n    \"            await tracker.task_completed(success=True)\\n\",\n    \"            return f\\\"Task {task_id} completed\\\"\\n\",\n    \"            \\n\",\n    \"        except Exception as e:\\n\",\n    \"            await tracker.task_completed(success=False)\\n\",\n    \"            raise\\n\",\n    \"    \\n\",\n    \"    # Start all tasks\\n\",\n    \"    tasks = [\\n\",\n    \"        asyncio.create_task(tracked_task(i, random.uniform(0.1, 0.5), 0.15))\\n\",\n    \"        for i in range(total_tasks)\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    # Monitor progress\\n\",\n    \"    while True:\\n\",\n    \"        progress = tracker.get_progress()\\n\",\n    \"        finished = progress[\\\"completed\\\"] + progress[\\\"failed\\\"]\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\rüìä Progress: {finished}/{total_tasks} ({progress['progress_pct']:.1f}%) - \\\" +\\n\",\n    \"              f\\\"‚úÖ {progress['completed']} success, ‚ùå {progress['failed']} failed - \\\" +\\n\",\n    \"              f\\\"ETA: {progress['eta']:.1f}s\\\", end=\\\"\\\")\\n\",\n    \"        \\n\",\n    \"        if finished >= total_tasks:\\n\",\n    \"            break\\n\",\n    \"        \\n\",\n    \"        await asyncio.sleep(0.1)\\n\",\n    \"    \\n\",\n    \"    print()  # New line\\n\",\n    \"    \\n\",\n    \"    # Wait for all tasks to complete\\n\",\n    \"    results = await asyncio.gather(*tasks, return_exceptions=True)\\n\",\n    \"    \\n\",\n    \"    success_count = sum(1 for r in results if not isinstance(r, Exception))\\n\",\n    \"    error_count = sum(1 for r in results if isinstance(r, Exception))\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\n‚úÖ All tasks completed: {success_count} success, {error_count} errors\\\")\\n\",\n    \"\\n\",\n    \"# Run concurrent processing demonstrations\\n\",\n    \"await demonstrate_concurrent_processing()\\n\",\n    \"await demonstrate_batch_processing()\\n\",\n    \"await demonstrate_progress_tracking()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Concurrent Processing Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Dramatically improves throughput\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Better resource utilization\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Handles mixed workloads efficiently\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Provides failure isolation\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 6: Production-Ready Async AI System\\n\",\n    \"\\n\",\n    \"**First Principle**: Combine all async patterns to create a robust, scalable AI processing system.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use composition of async components to build complex, maintainable systems.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Complete async AI processing system\\n\",\n    \"from dataclasses import dataclass, field\\n\",\n    \"from typing import Protocol\\n\",\n    \"import json\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class AIRequest:\\n\",\n    \"    \\\"\\\"\\\"AI processing request.\\\"\\\"\\\"\\n\",\n    \"    request_id: str\\n\",\n    \"    text: str\\n\",\n    \"    operation: str  # \\\"classify\\\", \\\"summarize\\\", \\\"translate\\\", etc.\\n\",\n    \"    priority: str = \\\"medium\\\"  # \\\"high\\\", \\\"medium\\\", \\\"low\\\"\\n\",\n    \"    metadata: Dict[str, Any] = field(default_factory=dict)\\n\",\n    \"    created_at: float = field(default_factory=time.time)\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class AIResponse:\\n\",\n    \"    \\\"\\\"\\\"AI processing response.\\\"\\\"\\\"\\n\",\n    \"    request_id: str\\n\",\n    \"    result: Any\\n\",\n    \"    status: str  # \\\"success\\\", \\\"error\\\", \\\"timeout\\\"\\n\",\n    \"    processing_time: float\\n\",\n    \"    model_used: str\\n\",\n    \"    error_message: Optional[str] = None\\n\",\n    \"    completed_at: float = field(default_factory=time.time)\\n\",\n    \"\\n\",\n    \"class AsyncAISystem:\\n\",\n    \"    \\\"\\\"\\\"Production-ready async AI processing system.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(\\n\",\n    \"        self,\\n\",\n    \"        max_concurrent: int = 10,\\n\",\n    \"        rate_limit_per_second: float = 5.0,\\n\",\n    \"        connection_pool_size: int = 5,\\n\",\n    \"        request_timeout: float = 30.0\\n\",\n    \"    ):\\n\",\n    \"        self.max_concurrent = max_concurrent\\n\",\n    \"        self.request_timeout = request_timeout\\n\",\n    \"        \\n\",\n    \"        # Core components\\n\",\n    \"        self.inference_engine = ConcurrentInferenceEngine(max_concurrent)\\n\",\n    \"        self.rate_limiter = TokenBucketRateLimiter(rate_limit_per_second, bucket_size=10)\\n\",\n    \"        self.connection_pool = None  # Will be initialized with mock connections\\n\",\n    \"        \\n\",\n    \"        # Request tracking\\n\",\n    \"        self.active_requests: Dict[str, AIRequest] = {}\\n\",\n    \"        self.completed_requests: List[AIResponse] = []\\n\",\n    \"        \\n\",\n    \"        # Metrics\\n\",\n    \"        self.metrics = {\\n\",\n    \"            \\\"total_requests\\\": 0,\\n\",\n    \"            \\\"successful_requests\\\": 0,\\n\",\n    \"            \\\"failed_requests\\\": 0,\\n\",\n    \"            \\\"timeout_requests\\\": 0,\\n\",\n    \"            \\\"total_processing_time\\\": 0.0\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        # Background tasks\\n\",\n    \"        self._background_tasks = set()\\n\",\n    \"    \\n\",\n    \"    async def start(self):\\n\",\n    \"        \\\"\\\"\\\"Start the AI system.\\\"\\\"\\\"\\n\",\n    \"        print(\\\"üöÄ Starting Async AI System...\\\")\\n\",\n    \"        \\n\",\n    \"        # Initialize connection pool\\n\",\n    \"        async def create_ai_connection():\\n\",\n    \"            await asyncio.sleep(0.1)  # Simulate connection setup\\n\",\n    \"            return MockConnection(f\\\"ai_conn_{int(time.time() * 1000) % 1000}\\\")\\n\",\n    \"        \\n\",\n    \"        self.connection_pool = AsyncConnectionPool(\\n\",\n    \"            create_ai_connection, \\n\",\n    \"            max_size=5\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        # Start background monitoring\\n\",\n    \"        monitor_task = asyncio.create_task(self._monitor_system())\\n\",\n    \"        self._background_tasks.add(monitor_task)\\n\",\n    \"        \\n\",\n    \"        print(\\\"‚úÖ AI System started successfully\\\")\\n\",\n    \"    \\n\",\n    \"    async def stop(self):\\n\",\n    \"        \\\"\\\"\\\"Stop the AI system gracefully.\\\"\\\"\\\"\\n\",\n    \"        print(\\\"üõë Stopping Async AI System...\\\")\\n\",\n    \"        \\n\",\n    \"        # Cancel background tasks\\n\",\n    \"        for task in self._background_tasks:\\n\",\n    \"            task.cancel()\\n\",\n    \"        \\n\",\n    \"        # Wait for active requests to complete\\n\",\n    \"        if self.active_requests:\\n\",\n    \"            print(f\\\"‚è≥ Waiting for {len(self.active_requests)} active requests...\\\")\\n\",\n    \"            # In practice, you'd set a timeout here\\n\",\n    \"            await asyncio.sleep(1.0)\\n\",\n    \"        \\n\",\n    \"        print(\\\"‚úÖ AI System stopped\\\")\\n\",\n    \"    \\n\",\n    \"    async def process_request(self, request: AIRequest) -> AIResponse:\\n\",\n    \"        \\\"\\\"\\\"Process a single AI request.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Rate limiting\\n\",\n    \"        await self.rate_limiter.acquire(tokens_needed=1)\\n\",\n    \"        \\n\",\n    \"        # Track request\\n\",\n    \"        self.active_requests[request.request_id] = request\\n\",\n    \"        self.metrics[\\\"total_requests\\\"] += 1\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            # Process with timeout\\n\",\n    \"            response = await asyncio.wait_for(\\n\",\n    \"                self._execute_request(request),\\n\",\n    \"                timeout=self.request_timeout\\n\",\n    \"            )\\n\",\n    \"            \\n\",\n    \"            self.metrics[\\\"successful_requests\\\"] += 1\\n\",\n    \"            \\n\",\n    \"        except asyncio.TimeoutError:\\n\",\n    \"            response = AIResponse(\\n\",\n    \"                request_id=request.request_id,\\n\",\n    \"                result=None,\\n\",\n    \"                status=\\\"timeout\\\",\\n\",\n    \"                processing_time=self.request_timeout,\\n\",\n    \"                model_used=\\\"none\\\",\\n\",\n    \"                error_message=\\\"Request timeout\\\"\\n\",\n    \"            )\\n\",\n    \"            self.metrics[\\\"timeout_requests\\\"] += 1\\n\",\n    \"            \\n\",\n    \"        except Exception as e:\\n\",\n    \"            response = AIResponse(\\n\",\n    \"                request_id=request.request_id,\\n\",\n    \"                result=None,\\n\",\n    \"                status=\\\"error\\\",\\n\",\n    \"                processing_time=0.0,\\n\",\n    \"                model_used=\\\"none\\\",\\n\",\n    \"                error_message=str(e)\\n\",\n    \"            )\\n\",\n    \"            self.metrics[\\\"failed_requests\\\"] += 1\\n\",\n    \"        \\n\",\n    \"        finally:\\n\",\n    \"            # Clean up\\n\",\n    \"            if request.request_id in self.active_requests:\\n\",\n    \"                del self.active_requests[request.request_id]\\n\",\n    \"            \\n\",\n    \"            self.completed_requests.append(response)\\n\",\n    \"            self.metrics[\\\"total_processing_time\\\"] += response.processing_time\\n\",\n    \"        \\n\",\n    \"        return response\\n\",\n    \"    \\n\",\n    \"    async def _execute_request(self, request: AIRequest) -> AIResponse:\\n\",\n    \"        \\\"\\\"\\\"Execute the actual AI processing.\\\"\\\"\\\"\\n\",\n    \"        start_time = time.time()\\n\",\n    \"        \\n\",\n    \"        async with self.connection_pool.connection() as conn:\\n\",\n    \"            # Simulate different AI operations\\n\",\n    \"            if request.operation == \\\"classify\\\":\\n\",\n    \"                await asyncio.sleep(0.2)  # Simulate classification\\n\",\n    \"                result = {\\\"category\\\": \\\"positive\\\", \\\"confidence\\\": 0.85}\\n\",\n    \"                model = \\\"classifier-v1\\\"\\n\",\n    \"                \\n\",\n    \"            elif request.operation == \\\"summarize\\\":\\n\",\n    \"                await asyncio.sleep(0.5)  # Simulate summarization\\n\",\n    \"                result = {\\\"summary\\\": f\\\"Summary of: {request.text[:30]}...\\\"}\\n\",\n    \"                model = \\\"summarizer-v2\\\"\\n\",\n    \"                \\n\",\n    \"            elif request.operation == \\\"translate\\\":\\n\",\n    \"                await asyncio.sleep(0.3)  # Simulate translation\\n\",\n    \"                result = {\\\"translated_text\\\": f\\\"TRANSLATED: {request.text}\\\"}\\n\",\n    \"                model = \\\"translator-v1\\\"\\n\",\n    \"                \\n\",\n    \"            else:\\n\",\n    \"                raise ValueError(f\\\"Unknown operation: {request.operation}\\\")\\n\",\n    \"            \\n\",\n    \"            processing_time = time.time() - start_time\\n\",\n    \"            \\n\",\n    \"            return AIResponse(\\n\",\n    \"                request_id=request.request_id,\\n\",\n    \"                result=result,\\n\",\n    \"                status=\\\"success\\\",\\n\",\n    \"                processing_time=processing_time,\\n\",\n    \"                model_used=model\\n\",\n    \"            )\\n\",\n    \"    \\n\",\n    \"    async def process_batch(self, requests: List[AIRequest]) -> List[AIResponse]:\\n\",\n    \"        \\\"\\\"\\\"Process multiple requests concurrently.\\\"\\\"\\\"\\n\",\n    \"        print(f\\\"üì¶ Processing batch of {len(requests)} requests\\\")\\n\",\n    \"        \\n\",\n    \"        # Group by priority\\n\",\n    \"        priority_groups = {\\\"high\\\": [], \\\"medium\\\": [], \\\"low\\\": []}\\n\",\n    \"        for request in requests:\\n\",\n    \"            priority_groups[request.priority].append(request)\\n\",\n    \"        \\n\",\n    \"        # Process high priority first\\n\",\n    \"        all_responses = []\\n\",\n    \"        for priority in [\\\"high\\\", \\\"medium\\\", \\\"low\\\"]:\\n\",\n    \"            if priority_groups[priority]:\\n\",\n    \"                print(f\\\"üéØ Processing {len(priority_groups[priority])} {priority} priority requests\\\")\\n\",\n    \"                \\n\",\n    \"                tasks = [\\n\",\n    \"                    self.process_request(request) \\n\",\n    \"                    for request in priority_groups[priority]\\n\",\n    \"                ]\\n\",\n    \"                \\n\",\n    \"                responses = await asyncio.gather(*tasks, return_exceptions=True)\\n\",\n    \"                \\n\",\n    \"                # Handle any exceptions\\n\",\n    \"                for i, response in enumerate(responses):\\n\",\n    \"                    if isinstance(response, Exception):\\n\",\n    \"                        # Create error response\\n\",\n    \"                        request = priority_groups[priority][i]\\n\",\n    \"                        response = AIResponse(\\n\",\n    \"                            request_id=request.request_id,\\n\",\n    \"                            result=None,\\n\",\n    \"                            status=\\\"error\\\",\\n\",\n    \"                            processing_time=0.0,\\n\",\n    \"                            model_used=\\\"none\\\",\\n\",\n    \"                            error_message=str(response)\\n\",\n    \"                        )\\n\",\n    \"                \\n\",\n    \"                all_responses.extend(responses)\\n\",\n    \"        \\n\",\n    \"        return all_responses\\n\",\n    \"    \\n\",\n    \"    async def _monitor_system(self):\\n\",\n    \"        \\\"\\\"\\\"Background monitoring task.\\\"\\\"\\\"\\n\",\n    \"        while True:\\n\",\n    \"            try:\\n\",\n    \"                await asyncio.sleep(5.0)  # Monitor every 5 seconds\\n\",\n    \"                \\n\",\n    \"                stats = self.get_system_stats()\\n\",\n    \"                print(f\\\"üìä System Monitor: {stats['active_requests']} active, \\\"\\n\",\n    \"                      f\\\"{stats['total_requests']} total, \\\"\\n\",\n    \"                      f\\\"{stats['success_rate']:.1%} success rate\\\")\\n\",\n    \"                \\n\",\n    \"            except asyncio.CancelledError:\\n\",\n    \"                break\\n\",\n    \"            except Exception as e:\\n\",\n    \"                print(f\\\"‚ö†Ô∏è  Monitor error: {e}\\\")\\n\",\n    \"    \\n\",\n    \"    def get_system_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get comprehensive system statistics.\\\"\\\"\\\"\\n\",\n    \"        total_requests = self.metrics[\\\"total_requests\\\"]\\n\",\n    \"        success_rate = (self.metrics[\\\"successful_requests\\\"] / total_requests) if total_requests > 0 else 0\\n\",\n    \"        avg_processing_time = (self.metrics[\\\"total_processing_time\\\"] / self.metrics[\\\"successful_requests\\\"]) if self.metrics[\\\"successful_requests\\\"] > 0 else 0\\n\",\n    \"        \\n\",\n    \"        pool_stats = self.connection_pool.get_stats() if self.connection_pool else {}\\n\",\n    \"        rate_limiter_stats = self.rate_limiter.get_stats()\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            \\\"active_requests\\\": len(self.active_requests),\\n\",\n    \"            \\\"total_requests\\\": total_requests,\\n\",\n    \"            \\\"successful_requests\\\": self.metrics[\\\"successful_requests\\\"],\\n\",\n    \"            \\\"failed_requests\\\": self.metrics[\\\"failed_requests\\\"],\\n\",\n    \"            \\\"timeout_requests\\\": self.metrics[\\\"timeout_requests\\\"],\\n\",\n    \"            \\\"success_rate\\\": success_rate,\\n\",\n    \"            \\\"avg_processing_time\\\": avg_processing_time,\\n\",\n    \"            \\\"connection_pool\\\": pool_stats,\\n\",\n    \"            \\\"rate_limiter\\\": rate_limiter_stats\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_production_system():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate the complete production async AI system.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üè≠ Production Async AI System\\\", style=\\\"bold green\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create and start the system\\n\",\n    \"    ai_system = AsyncAISystem(\\n\",\n    \"        max_concurrent=5,\\n\",\n    \"        rate_limit_per_second=3.0,\\n\",\n    \"        connection_pool_size=3,\\n\",\n    \"        request_timeout=10.0\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    await ai_system.start()\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Create diverse test requests\\n\",\n    \"        requests = [\\n\",\n    \"            AIRequest(\\\"req_001\\\", \\\"Analyze this positive sentiment\\\", \\\"classify\\\", \\\"high\\\"),\\n\",\n    \"            AIRequest(\\\"req_002\\\", \\\"Long document that needs summarization...\\\", \\\"summarize\\\", \\\"medium\\\"),\\n\",\n    \"            AIRequest(\\\"req_003\\\", \\\"Hello world\\\", \\\"translate\\\", \\\"low\\\"),\\n\",\n    \"            AIRequest(\\\"req_004\\\", \\\"Another classification task\\\", \\\"classify\\\", \\\"medium\\\"),\\n\",\n    \"            AIRequest(\\\"req_005\\\", \\\"Important urgent request\\\", \\\"classify\\\", \\\"high\\\"),\\n\",\n    \"            AIRequest(\\\"req_006\\\", \\\"Text to translate\\\", \\\"translate\\\", \\\"low\\\"),\\n\",\n    \"            AIRequest(\\\"req_007\\\", \\\"Document for summary\\\", \\\"summarize\\\", \\\"medium\\\"),\\n\",\n    \"            AIRequest(\\\"req_008\\\", \\\"Final classification\\\", \\\"classify\\\", \\\"low\\\"),\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüöÄ Processing {len(requests)} requests through production system:\\\")\\n\",\n    \"        \\n\",\n    \"        # Process requests in batch\\n\",\n    \"        start_time = time.time()\\n\",\n    \"        responses = await ai_system.process_batch(requests)\\n\",\n    \"        total_time = time.time() - start_time\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüìã Processing Results:\\\")\\n\",\n    \"        for response in responses:\\n\",\n    \"            status_icon = \\\"‚úÖ\\\" if response.status == \\\"success\\\" else \\\"‚ùå\\\" if response.status == \\\"error\\\" else \\\"‚è∞\\\"\\n\",\n    \"            result_preview = str(response.result)[:40] if response.result else response.error_message[:40]\\n\",\n    \"            print(f\\\"  {status_icon} {response.request_id}: {response.processing_time:.3f}s - {response.model_used} - {result_preview}...\\\")\\n\",\n    \"        \\n\",\n    \"        # Show comprehensive statistics\\n\",\n    \"        stats = ai_system.get_system_stats()\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüìä System Performance:\\\")\\n\",\n    \"        print(f\\\"  Total processing time: {total_time:.2f}s\\\")\\n\",\n    \"        print(f\\\"  Average per request: {total_time / len(requests):.3f}s\\\")\\n\",\n    \"        print(f\\\"  Success rate: {stats['success_rate']:.1%}\\\")\\n\",\n    \"        print(f\\\"  Average AI processing time: {stats['avg_processing_time']:.3f}s\\\")\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüîß Resource Utilization:\\\")\\n\",\n    \"        print(f\\\"  Connection pool: {stats['connection_pool']}\\\")\\n\",\n    \"        print(f\\\"  Rate limiter: {stats['rate_limiter']}\\\")\\n\",\n    \"        \\n\",\n    \"        # Test streaming capabilities\\n\",\n    \"        print(f\\\"\\\\nüåä Testing streaming response:\\\")\\n\",\n    \"        \\n\",\n    \"        provider = StreamingAIProvider(response_delay=0.05)\\n\",\n    \"        messages = [Message(role=\\\"user\\\", content=\\\"Stream a response about async programming\\\")]\\n\",\n    \"        \\n\",\n    \"        full_response = \\\"\\\"\\n\",\n    \"        chunk_count = 0\\n\",\n    \"        \\n\",\n    \"        async for chunk in provider.stream_completion(\\\"gpt-4\\\", messages):\\n\",\n    \"            full_response += chunk.content\\n\",\n    \"            chunk_count += 1\\n\",\n    \"            \\n\",\n    \"            if chunk.is_final:\\n\",\n    \"                print(f\\\"  üì° Streamed {chunk_count} chunks: '{full_response}'\\\")\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüéØ Production System Features Demonstrated:\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Concurrent request processing\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Priority-based scheduling\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Rate limiting and resource control\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Connection pooling\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Timeout handling\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Comprehensive monitoring\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Graceful error handling\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Streaming capabilities\\\")\\n\",\n    \"        \\n\",\n    \"    finally:\\n\",\n    \"        await ai_system.stop()\\n\",\n    \"\\n\",\n    \"# Run the production system demonstration\\n\",\n    \"await demonstrate_production_system()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüèÜ Async Patterns Mastery Complete!\\\")\\n\",\n    \"print(f\\\"\\\\nüí° Key Async Benefits for AI Systems:\\\")\\n\",\n    \"print(f\\\"  ‚ö° 10-100x performance improvement for I/O-bound operations\\\")\\n\",\n    \"print(f\\\"  üîÑ Real-time streaming and user feedback\\\")\\n\",\n    \"print(f\\\"  üìà Scales to thousands of concurrent requests\\\")\\n\",\n    \"print(f\\\"  üíæ Efficient resource utilization\\\")\\n\",\n    \"print(f\\\"  üõ°Ô∏è  Built-in error handling and timeout management\\\")\\n\",\n    \"print(f\\\"  üìä Comprehensive monitoring and metrics\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Summary: Async Patterns Mastery\\n\",\n    \"\\n\",\n    \"**üéØ Key Takeaways:**\\n\",\n    \"\\n\",\n    \"1. **Streaming** enables real-time user feedback and memory efficiency\\n\",\n    \"2. **Rate Limiting** prevents resource exhaustion and ensures fair usage\\n\",\n    \"3. **Connection Pooling** reuses expensive resources efficiently\\n\",\n    \"4. **Concurrent Processing** dramatically improves throughput\\n\",\n    \"5. **Batch Processing** optimizes resource usage for bulk operations\\n\",\n    \"6. **Async Context Managers** ensure proper resource cleanup\\n\",\n    \"7. **Progress Tracking** provides visibility into long-running operations\\n\",\n    \"\\n\",\n    \"**üöÄ Production Considerations:**\\n\",\n    \"- Always use timeouts to prevent hanging operations\\n\",\n    \"- Implement proper error handling and retry logic\\n\",\n    \"- Monitor resource usage and performance metrics\\n\",\n    \"- Use priority queues for important requests\\n\",\n    \"- Plan for graceful shutdown and cleanup\\n\",\n    \"\\n\",\n    \"**üí° Remember:** Async programming is essential for building scalable AI systems that can handle real-world load and provide responsive user experiences!\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":113147,"cache_read_input_tokens":0,"output_tokens":25883}},"requestId":"req_vrtx_011CRhS6YnSYL32R2dyM71Md","type":"assistant","uuid":"8fad791d-0b7e-4004-a8bd-65dc114d2b48","timestamp":"2025-08-01T18:11:43.069Z"}
{"parentUuid":"8fad791d-0b7e-4004-a8bd-65dc114d2b48","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01Sr2ysBL8ApSp79gBgJR1My","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/02_async_patterns_learning.ipynb"}]},"uuid":"10310621-d474-403e-ae1d-d2cded1290b0","timestamp":"2025-08-01T18:11:43.101Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/02_async_patterns_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Async Patterns for High-Performance AI Systems\\n\",\n    \"\\n\",\n    \"This notebook teaches advanced asyncio patterns for building high-performance AI systems with concurrent processing, streaming, and resource management. You'll learn:\\n\",\n    \"\\n\",\n    \"1. **Streaming Patterns** - Real-time AI response streaming\\n\",\n    \"2. **Rate Limiting** - Controlling API usage and preventing overload\\n\",\n    \"3. **Connection Pooling** - Efficient resource management\\n\",\n    \"4. **Concurrent Processing** - Parallel AI inference\\n\",\n    \"5. **Batch Processing** - Optimizing throughput\\n\",\n    \"6. **Async Context Managers** - Proper resource cleanup\\n\",\n    \"7. **Async Generators** - Memory-efficient data processing\\n\",\n    \"8. **Error Handling** - Robust async error management\\n\",\n    \"\\n\",\n    \"## Prerequisites\\n\",\n    \"```bash\\n\",\n    \"pip install rich asyncio\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 1: Async Fundamentals and Setup\\n\",\n    \"\\n\",\n    \"**First Principle**: Async programming enables concurrent execution without blocking, essential for I/O-bound AI applications.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `async def` for coroutines, `await` for async operations, and `asyncio.create_task()` for concurrent execution.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Essential imports for async programming\\n\",\n    \"import asyncio\\n\",\n    \"from typing import AsyncGenerator, AsyncIterator, Callable, Any, Optional, List, Dict\\n\",\n    \"from dataclasses import dataclass\\n\",\n    \"from contextlib import asynccontextmanager\\n\",\n    \"import time\\n\",\n    \"from rich.console import Console\\n\",\n    \"from rich.panel import Panel\\n\",\n    \"from rich.table import Table\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"\\n\",\n    \"# Basic data structures for AI processing\\n\",\n    \"@dataclass\\n\",\n    \"class StreamingChunk:\\n\",\n    \"    \\\"\\\"\\\"Represents a chunk of streaming data.\\\"\\\"\\\"\\n\",\n    \"    content: str\\n\",\n    \"    chunk_id: int\\n\",\n    \"    is_final: bool\\n\",\n    \"    metadata: Dict[str, Any] = None\\n\",\n    \"    \\n\",\n    \"    def __post_init__(self):\\n\",\n    \"        if self.metadata is None:\\n\",\n    \"            self.metadata = {}\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class Message:\\n\",\n    \"    \\\"\\\"\\\"Chat message structure.\\\"\\\"\\\"\\n\",\n    \"    role: str\\n\",\n    \"    content: str\\n\",\n    \"    timestamp: float = None\\n\",\n    \"    \\n\",\n    \"    def __post_init__(self):\\n\",\n    \"        if self.timestamp is None:\\n\",\n    \"            self.timestamp = time.time()\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"‚ö° Async Programming Fundamentals\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"print(\\\"\\\"\\\"\\n\",\n    \"üîë Key Async Concepts:\\n\",\n    \"\\n\",\n    \"1. **Coroutines**: Functions defined with 'async def'\\n\",\n    \"2. **Awaitable**: Objects that can be used with 'await'\\n\",\n    \"3. **Event Loop**: Manages and executes async operations\\n\",\n    \"4. **Tasks**: Concurrent execution of coroutines\\n\",\n    \"5. **Futures**: Placeholders for future results\\n\",\n    \"\\n\",\n    \"üéØ Benefits for AI Systems:\\n\",\n    \"‚Ä¢ Handle multiple API calls concurrently\\n\",\n    \"‚Ä¢ Stream responses in real-time\\n\",\n    \"‚Ä¢ Manage resources efficiently\\n\",\n    \"‚Ä¢ Scale to thousands of connections\\n\",\n    \"‚Ä¢ Non-blocking I/O operations\\n\",\n    \"\\\"\\\"\\\")\\n\",\n    \"\\n\",\n    \"# Simple async demonstration\\n\",\n    \"async def simple_async_demo():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate basic async concepts.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    async def fetch_data(source: str, delay: float) -> str:\\n\",\n    \"        \\\"\\\"\\\"Simulate fetching data from different sources.\\\"\\\"\\\"\\n\",\n    \"        print(f\\\"üîÑ Starting to fetch from {source}...\\\")\\n\",\n    \"        await asyncio.sleep(delay)  # Simulate I/O delay\\n\",\n    \"        result = f\\\"Data from {source}\\\"\\n\",\n    \"        print(f\\\"‚úÖ Finished fetching from {source}\\\")\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    # Sequential execution (blocking)\\n\",\n    \"    print(\\\"üìä Sequential execution:\\\")\\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    result1 = await fetch_data(\\\"API-1\\\", 0.5)\\n\",\n    \"    result2 = await fetch_data(\\\"API-2\\\", 0.3)\\n\",\n    \"    result3 = await fetch_data(\\\"API-3\\\", 0.4)\\n\",\n    \"    \\n\",\n    \"    sequential_time = time.time() - start_time\\n\",\n    \"    print(f\\\"Sequential results: {[result1, result2, result3]}\\\")\\n\",\n    \"    print(f\\\"Sequential time: {sequential_time:.2f}s\\\")\\n\",\n    \"    \\n\",\n    \"    # Concurrent execution (non-blocking)\\n\",\n    \"    print(f\\\"\\\\nüöÄ Concurrent execution:\\\")\\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    # Create tasks for concurrent execution\\n\",\n    \"    task1 = asyncio.create_task(fetch_data(\\\"API-1\\\", 0.5))\\n\",\n    \"    task2 = asyncio.create_task(fetch_data(\\\"API-2\\\", 0.3))\\n\",\n    \"    task3 = asyncio.create_task(fetch_data(\\\"API-3\\\", 0.4))\\n\",\n    \"    \\n\",\n    \"    # Wait for all tasks to complete\\n\",\n    \"    concurrent_results = await asyncio.gather(task1, task2, task3)\\n\",\n    \"    \\n\",\n    \"    concurrent_time = time.time() - start_time\\n\",\n    \"    print(f\\\"Concurrent results: {concurrent_results}\\\")\\n\",\n    \"    print(f\\\"Concurrent time: {concurrent_time:.2f}s\\\")\\n\",\n    \"    \\n\",\n    \"    speedup = sequential_time / concurrent_time\\n\",\n    \"    print(f\\\"\\\\n‚ö° Speedup: {speedup:.1f}x faster with async!\\\")\\n\",\n    \"\\n\",\n    \"# Run the demo\\n\",\n    \"await simple_async_demo()\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüí° Key Takeaway: Async shines with I/O-bound operations!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 2: Streaming Patterns for Real-Time AI\\n\",\n    \"\\n\",\n    \"**First Principle**: Streaming enables real-time user feedback and better resource utilization in AI applications.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `async def` with `yield` to create async generators for streaming data.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class StreamingAIProvider:\\n\",\n    \"    \\\"\\\"\\\"AI provider that supports streaming responses.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, response_delay: float = 0.1):\\n\",\n    \"        self.response_delay = response_delay\\n\",\n    \"        self.call_count = 0\\n\",\n    \"    \\n\",\n    \"    async def stream_completion(\\n\",\n    \"        self, \\n\",\n    \"        model_id: str, \\n\",\n    \"        messages: List[Message]\\n\",\n    \"    ) -> AsyncGenerator[StreamingChunk, None]:\\n\",\n    \"        \\\"\\\"\\\"Stream chat completion chunks.\\\"\\\"\\\"\\n\",\n    \"        self.call_count += 1\\n\",\n    \"        \\n\",\n    \"        # Simulate streaming response\\n\",\n    \"        response_text = f\\\"This is a streaming response to: {messages[-1].content}\\\"\\n\",\n    \"        words = response_text.split()\\n\",\n    \"        \\n\",\n    \"        for i, word in enumerate(words):\\n\",\n    \"            await asyncio.sleep(self.response_delay)\\n\",\n    \"            \\n\",\n    \"            chunk = StreamingChunk(\\n\",\n    \"                content=word + (\\\" \\\" if i < len(words) - 1 else \\\"\\\"),\\n\",\n    \"                chunk_id=i,\\n\",\n    \"                is_final=i == len(words) - 1,\\n\",\n    \"                metadata={\\n\",\n    \"                    \\\"model\\\": model_id,\\n\",\n    \"                    \\\"total_chunks\\\": len(words),\\n\",\n    \"                    \\\"call_id\\\": self.call_count\\n\",\n    \"                }\\n\",\n    \"            )\\n\",\n    \"            \\n\",\n    \"            yield chunk\\n\",\n    \"    \\n\",\n    \"    async def stream_multiple_responses(\\n\",\n    \"        self,\\n\",\n    \"        queries: List[str]\\n\",\n    \"    ) -> AsyncGenerator[Dict[str, Any], None]:\\n\",\n    \"        \\\"\\\"\\\"Stream responses for multiple queries concurrently.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        async def process_query(query_id: int, query: str):\\n\",\n    \"            \\\"\\\"\\\"Process a single query and yield results.\\\"\\\"\\\"\\n\",\n    \"            messages = [Message(role=\\\"user\\\", content=query)]\\n\",\n    \"            \\n\",\n    \"            async for chunk in self.stream_completion(\\\"gpt-4\\\", messages):\\n\",\n    \"                yield {\\n\",\n    \"                    \\\"query_id\\\": query_id,\\n\",\n    \"                    \\\"query\\\": query,\\n\",\n    \"                    \\\"chunk\\\": chunk\\n\",\n    \"                }\\n\",\n    \"        \\n\",\n    \"        # Create tasks for all queries\\n\",\n    \"        tasks = [\\n\",\n    \"            process_query(i, query) \\n\",\n    \"            for i, query in enumerate(queries)\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        # Use async generators concurrently\\n\",\n    \"        async def merge_streams():\\n\",\n    \"            \\\"\\\"\\\"Merge multiple async generators.\\\"\\\"\\\"\\n\",\n    \"            active_tasks = [task.__aiter__() for task in tasks]\\n\",\n    \"            \\n\",\n    \"            while active_tasks:\\n\",\n    \"                # Try to get next item from each active stream\\n\",\n    \"                for i, stream in enumerate(active_tasks[:]):\\n\",\n    \"                    try:\\n\",\n    \"                        result = await stream.__anext__()\\n\",\n    \"                        yield result\\n\",\n    \"                    except StopAsyncIteration:\\n\",\n    \"                        active_tasks.remove(stream)\\n\",\n    \"                \\n\",\n    \"                # Small delay to prevent tight loop\\n\",\n    \"                if active_tasks:\\n\",\n    \"                    await asyncio.sleep(0.01)\\n\",\n    \"        \\n\",\n    \"        async for result in merge_streams():\\n\",\n    \"            yield result\\n\",\n    \"\\n\",\n    \"async def demonstrate_streaming():\\n\",\n    \"    \\\"\\\"\\\"Show streaming patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üåä Streaming Patterns\\\", style=\\\"bold blue\\\"))\\n\",\n    \"    \\n\",\n    \"    provider = StreamingAIProvider(response_delay=0.15)\\n\",\n    \"    \\n\",\n    \"    print(\\\"üì° Single streaming completion:\\\")\\n\",\n    \"    \\n\",\n    \"    messages = [Message(role=\\\"user\\\", content=\\\"Tell me about async programming\\\")]\\n\",\n    \"    \\n\",\n    \"    # Collect streaming chunks\\n\",\n    \"    chunks = []\\n\",\n    \"    full_response = \\\"\\\"\\n\",\n    \"    \\n\",\n    \"    async for chunk in provider.stream_completion(\\\"gpt-4\\\", messages):\\n\",\n    \"        chunks.append(chunk)\\n\",\n    \"        full_response += chunk.content\\n\",\n    \"        \\n\",\n    \"        # Show real-time progress\\n\",\n    \"        if chunk.is_final:\\n\",\n    \"            print(f\\\"  Chunk {chunk.chunk_id}: '{chunk.content.strip()}' [FINAL]\\\")\\n\",\n    \"        else:\\n\",\n    \"            print(f\\\"  Chunk {chunk.chunk_id}: '{chunk.content.strip()}'\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìä Streaming Summary:\\\")\\n\",\n    \"    print(f\\\"  Total chunks: {len(chunks)}\\\")\\n\",\n    \"    print(f\\\"  Full response: {full_response}\\\")\\n\",\n    \"    print(f\\\"  Final chunk metadata: {chunks[-1].metadata}\\\")\\n\",\n    \"    \\n\",\n    \"    # Multiple concurrent streams\\n\",\n    \"    print(f\\\"\\\\nüîÄ Multiple concurrent streams:\\\")\\n\",\n    \"    \\n\",\n    \"    queries = [\\n\",\n    \"        \\\"What is machine learning?\\\",\\n\",\n    \"        \\\"Explain neural networks\\\",\\n\",\n    \"        \\\"How does AI work?\\\"\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    query_responses = {i: \\\"\\\" for i in range(len(queries))}\\n\",\n    \"    \\n\",\n    \"    async for result in provider.stream_multiple_responses(queries):\\n\",\n    \"        query_id = result[\\\"query_id\\\"]\\n\",\n    \"        chunk = result[\\\"chunk\\\"]\\n\",\n    \"        query_responses[query_id] += chunk.content\\n\",\n    \"        \\n\",\n    \"        status = \\\"[FINAL]\\\" if chunk.is_final else \\\"\\\"\\n\",\n    \"        print(f\\\"  Query {query_id}: chunk {chunk.chunk_id} '{chunk.content.strip()}' {status}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìã Final responses:\\\")\\n\",\n    \"    for i, response in query_responses.items():\\n\",\n    \"        print(f\\\"  Query {i}: {response}\\\")\\n\",\n    \"\\n\",\n    \"# Real-time streaming with buffer management\\n\",\n    \"class StreamBuffer:\\n\",\n    \"    \\\"\\\"\\\"Buffer for managing streaming data.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_size: int = 100):\\n\",\n    \"        self.buffer = []\\n\",\n    \"        self.max_size = max_size\\n\",\n    \"        self.overflow_count = 0\\n\",\n    \"    \\n\",\n    \"    async def add_chunk(self, chunk: StreamingChunk) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Add chunk to buffer, return True if successful.\\\"\\\"\\\"\\n\",\n    \"        if len(self.buffer) >= self.max_size:\\n\",\n    \"            self.overflow_count += 1\\n\",\n    \"            return False\\n\",\n    \"        \\n\",\n    \"        self.buffer.append(chunk)\\n\",\n    \"        return True\\n\",\n    \"    \\n\",\n    \"    async def get_chunks(self, count: int = None) -> List[StreamingChunk]:\\n\",\n    \"        \\\"\\\"\\\"Get chunks from buffer.\\\"\\\"\\\"\\n\",\n    \"        if count is None:\\n\",\n    \"            chunks = self.buffer.copy()\\n\",\n    \"            self.buffer.clear()\\n\",\n    \"        else:\\n\",\n    \"            chunks = self.buffer[:count]\\n\",\n    \"            self.buffer = self.buffer[count:]\\n\",\n    \"        \\n\",\n    \"        return chunks\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, int]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"buffer_size\\\": len(self.buffer),\\n\",\n    \"            \\\"max_size\\\": self.max_size,\\n\",\n    \"            \\\"overflow_count\\\": self.overflow_count\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_buffered_streaming():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate streaming with buffer management.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüíæ Buffered streaming demonstration:\\\")\\n\",\n    \"    \\n\",\n    \"    provider = StreamingAIProvider(response_delay=0.05)\\n\",\n    \"    buffer = StreamBuffer(max_size=5)\\n\",\n    \"    \\n\",\n    \"    messages = [Message(role=\\\"user\\\", content=\\\"Generate a long response about AI benefits\\\")]\\n\",\n    \"    \\n\",\n    \"    # Producer: Add chunks to buffer\\n\",\n    \"    async def producer():\\n\",\n    \"        async for chunk in provider.stream_completion(\\\"gpt-4\\\", messages):\\n\",\n    \"            success = await buffer.add_chunk(chunk)\\n\",\n    \"            if not success:\\n\",\n    \"                print(f\\\"‚ö†Ô∏è  Buffer overflow! Chunk {chunk.chunk_id} dropped\\\")\\n\",\n    \"    \\n\",\n    \"    # Consumer: Process chunks from buffer\\n\",\n    \"    async def consumer():\\n\",\n    \"        processed_count = 0\\n\",\n    \"        while True:\\n\",\n    \"            chunks = await buffer.get_chunks(2)  # Process 2 at a time\\n\",\n    \"            if not chunks:\\n\",\n    \"                await asyncio.sleep(0.1)  # Wait for more data\\n\",\n    \"                continue\\n\",\n    \"            \\n\",\n    \"            for chunk in chunks:\\n\",\n    \"                processed_count += 1\\n\",\n    \"                print(f\\\"üîß Processed chunk {chunk.chunk_id}: '{chunk.content.strip()}'\\\")\\n\",\n    \"                \\n\",\n    \"                if chunk.is_final:\\n\",\n    \"                    print(f\\\"‚úÖ Processing complete! Total chunks: {processed_count}\\\")\\n\",\n    \"                    return\\n\",\n    \"            \\n\",\n    \"            await asyncio.sleep(0.1)  # Simulate processing time\\n\",\n    \"    \\n\",\n    \"    # Run producer and consumer concurrently\\n\",\n    \"    await asyncio.gather(producer(), consumer())\\n\",\n    \"    \\n\",\n    \"    stats = buffer.get_stats()\\n\",\n    \"    print(f\\\"üìä Buffer stats: {stats}\\\")\\n\",\n    \"\\n\",\n    \"# Run streaming demonstrations\\n\",\n    \"await demonstrate_streaming()\\n\",\n    \"await demonstrate_buffered_streaming()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Streaming Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Real-time user feedback\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Lower perceived latency\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Memory efficiency for large responses\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Better resource utilization\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 3: Rate Limiting and Resource Control\\n\",\n    \"\\n\",\n    \"**First Principle**: Rate limiting prevents resource exhaustion and ensures fair usage across multiple clients.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `asyncio.Semaphore` and `asyncio.Lock` for controlling concurrent access to resources.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class AsyncRateLimiter:\\n\",\n    \"    \\\"\\\"\\\"Async rate limiter for API calls.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_calls: int, time_window: float):\\n\",\n    \"        self.max_calls = max_calls\\n\",\n    \"        self.time_window = time_window\\n\",\n    \"        self.calls: List[float] = []\\n\",\n    \"        self._lock = asyncio.Lock()\\n\",\n    \"    \\n\",\n    \"    async def acquire(self) -> None:\\n\",\n    \"        \\\"\\\"\\\"Acquire permission to make a call.\\\"\\\"\\\"\\n\",\n    \"        async with self._lock:\\n\",\n    \"            now = time.time()\\n\",\n    \"            \\n\",\n    \"            # Remove old calls outside the window\\n\",\n    \"            self.calls = [call_time for call_time in self.calls \\n\",\n    \"                         if now - call_time < self.time_window]\\n\",\n    \"            \\n\",\n    \"            # If we're at the limit, wait\\n\",\n    \"            if len(self.calls) >= self.max_calls:\\n\",\n    \"                oldest_call = min(self.calls)\\n\",\n    \"                wait_time = self.time_window - (now - oldest_call)\\n\",\n    \"                if wait_time > 0:\\n\",\n    \"                    print(f\\\"üö¶ Rate limit reached, waiting {wait_time:.2f}s\\\")\\n\",\n    \"                    await asyncio.sleep(wait_time)\\n\",\n    \"                    # Recursive call to try again\\n\",\n    \"                    return await self.acquire()\\n\",\n    \"            \\n\",\n    \"            # Record this call\\n\",\n    \"            self.calls.append(now)\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get rate limiter statistics.\\\"\\\"\\\"\\n\",\n    \"        now = time.time()\\n\",\n    \"        recent_calls = [call for call in self.calls if now - call < self.time_window]\\n\",\n    \"        return {\\n\",\n    \"            \\\"current_calls\\\": len(recent_calls),\\n\",\n    \"            \\\"max_calls\\\": self.max_calls,\\n\",\n    \"            \\\"time_window\\\": self.time_window,\\n\",\n    \"            \\\"utilization\\\": len(recent_calls) / self.max_calls\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"class TokenBucketRateLimiter:\\n\",\n    \"    \\\"\\\"\\\"Token bucket rate limiter for smooth rate limiting.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, tokens_per_second: float, bucket_size: int):\\n\",\n    \"        self.tokens_per_second = tokens_per_second\\n\",\n    \"        self.bucket_size = bucket_size\\n\",\n    \"        self.tokens = bucket_size\\n\",\n    \"        self.last_refill = time.time()\\n\",\n    \"        self._lock = asyncio.Lock()\\n\",\n    \"    \\n\",\n    \"    async def acquire(self, tokens_needed: int = 1) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Try to acquire tokens from the bucket.\\\"\\\"\\\"\\n\",\n    \"        async with self._lock:\\n\",\n    \"            now = time.time()\\n\",\n    \"            \\n\",\n    \"            # Refill tokens based on time passed\\n\",\n    \"            time_passed = now - self.last_refill\\n\",\n    \"            tokens_to_add = time_passed * self.tokens_per_second\\n\",\n    \"            self.tokens = min(self.bucket_size, self.tokens + tokens_to_add)\\n\",\n    \"            self.last_refill = now\\n\",\n    \"            \\n\",\n    \"            # Check if we have enough tokens\\n\",\n    \"            if self.tokens >= tokens_needed:\\n\",\n    \"                self.tokens -= tokens_needed\\n\",\n    \"                return True\\n\",\n    \"            else:\\n\",\n    \"                # Calculate wait time for next token\\n\",\n    \"                tokens_deficit = tokens_needed - self.tokens\\n\",\n    \"                wait_time = tokens_deficit / self.tokens_per_second\\n\",\n    \"                print(f\\\"ü™£ Token bucket empty, waiting {wait_time:.2f}s\\\")\\n\",\n    \"                await asyncio.sleep(wait_time)\\n\",\n    \"                return await self.acquire(tokens_needed)\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"tokens\\\": self.tokens,\\n\",\n    \"            \\\"bucket_size\\\": self.bucket_size,\\n\",\n    \"            \\\"tokens_per_second\\\": self.tokens_per_second,\\n\",\n    \"            \\\"fill_percentage\\\": self.tokens / self.bucket_size * 100\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_rate_limiting():\\n\",\n    \"    \\\"\\\"\\\"Show rate limiting patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üö¶ Rate Limiting\\\", style=\\\"bold green\\\"))\\n\",\n    \"    \\n\",\n    \"    # Test sliding window rate limiter\\n\",\n    \"    print(\\\"üìä Sliding window rate limiter (5 calls per 2 seconds):\\\")\\n\",\n    \"    \\n\",\n    \"    rate_limiter = AsyncRateLimiter(max_calls=5, time_window=2.0)\\n\",\n    \"    \\n\",\n    \"    async def make_api_call(call_id: int) -> str:\\n\",\n    \"        \\\"\\\"\\\"Simulate API call with rate limiting.\\\"\\\"\\\"\\n\",\n    \"        await rate_limiter.acquire()\\n\",\n    \"        # Simulate API processing time\\n\",\n    \"        await asyncio.sleep(0.1)\\n\",\n    \"        return f\\\"API call {call_id} completed\\\"\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    # Create 10 rapid calls\\n\",\n    \"    tasks = [make_api_call(i) for i in range(10)]\\n\",\n    \"    \\n\",\n    \"    # Execute with progress tracking\\n\",\n    \"    results = []\\n\",\n    \"    for i, task in enumerate(asyncio.as_completed(tasks)):\\n\",\n    \"        result = await task\\n\",\n    \"        elapsed = time.time() - start_time\\n\",\n    \"        stats = rate_limiter.get_stats()\\n\",\n    \"        print(f\\\"  {elapsed:.2f}s: {result} (utilization: {stats['utilization']:.1%})\\\")\\n\",\n    \"        results.append(result)\\n\",\n    \"    \\n\",\n    \"    total_time = time.time() - start_time\\n\",\n    \"    print(f\\\"\\\\nüìà Rate limiting summary:\\\")\\n\",\n    \"    print(f\\\"  Total time: {total_time:.2f}s\\\")\\n\",\n    \"    print(f\\\"  Rate limiter stats: {rate_limiter.get_stats()}\\\")\\n\",\n    \"    \\n\",\n    \"    # Test token bucket rate limiter\\n\",\n    \"    print(f\\\"\\\\nü™£ Token bucket rate limiter (3 tokens/sec, bucket size 5):\\\")\\n\",\n    \"    \\n\",\n    \"    token_limiter = TokenBucketRateLimiter(tokens_per_second=3.0, bucket_size=5)\\n\",\n    \"    \\n\",\n    \"    async def make_weighted_call(call_id: int, tokens_needed: int = 1) -> str:\\n\",\n    \"        \\\"\\\"\\\"API call that may need multiple tokens.\\\"\\\"\\\"\\n\",\n    \"        await token_limiter.acquire(tokens_needed)\\n\",\n    \"        await asyncio.sleep(0.05)  # Simulate processing\\n\",\n    \"        return f\\\"Weighted call {call_id} (cost: {tokens_needed} tokens)\\\"\\n\",\n    \"    \\n\",\n    \"    # Different cost operations\\n\",\n    \"    operations = [\\n\",\n    \"        (1, 1),  # Light operation\\n\",\n    \"        (2, 2),  # Medium operation\\n\",\n    \"        (3, 1),  # Light operation\\n\",\n    \"        (4, 3),  # Heavy operation\\n\",\n    \"        (5, 1),  # Light operation\\n\",\n    \"        (6, 2),  # Medium operation\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    for call_id, tokens in operations:\\n\",\n    \"        result = await make_weighted_call(call_id, tokens)\\n\",\n    \"        elapsed = time.time() - start_time\\n\",\n    \"        stats = token_limiter.get_stats()\\n\",\n    \"        print(f\\\"  {elapsed:.2f}s: {result} (tokens left: {stats['tokens']:.1f})\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìä Final token bucket stats: {token_limiter.get_stats()}\\\")\\n\",\n    \"\\n\",\n    \"# Advanced rate limiting with priority queues\\n\",\n    \"class PriorityRateLimiter:\\n\",\n    \"    \\\"\\\"\\\"Rate limiter with priority queue support.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_calls: int, time_window: float):\\n\",\n    \"        self.base_limiter = AsyncRateLimiter(max_calls, time_window)\\n\",\n    \"        self.priority_queues = {\\n\",\n    \"            \\\"high\\\": asyncio.Queue(),\\n\",\n    \"            \\\"medium\\\": asyncio.Queue(),\\n\",\n    \"            \\\"low\\\": asyncio.Queue()\\n\",\n    \"        }\\n\",\n    \"        self._processing = False\\n\",\n    \"    \\n\",\n    \"    async def acquire(self, priority: str = \\\"medium\\\") -> None:\\n\",\n    \"        \\\"\\\"\\\"Acquire with priority.\\\"\\\"\\\"\\n\",\n    \"        if priority not in self.priority_queues:\\n\",\n    \"            priority = \\\"medium\\\"\\n\",\n    \"        \\n\",\n    \"        # Add request to appropriate queue\\n\",\n    \"        future = asyncio.Future()\\n\",\n    \"        await self.priority_queues[priority].put(future)\\n\",\n    \"        \\n\",\n    \"        # Start processing if not already running\\n\",\n    \"        if not self._processing:\\n\",\n    \"            asyncio.create_task(self._process_queues())\\n\",\n    \"        \\n\",\n    \"        # Wait for our turn\\n\",\n    \"        await future\\n\",\n    \"    \\n\",\n    \"    async def _process_queues(self):\\n\",\n    \"        \\\"\\\"\\\"Process requests in priority order.\\\"\\\"\\\"\\n\",\n    \"        self._processing = True\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            while True:\\n\",\n    \"                # Check queues in priority order\\n\",\n    \"                future = None\\n\",\n    \"                \\n\",\n    \"                for priority in [\\\"high\\\", \\\"medium\\\", \\\"low\\\"]:\\n\",\n    \"                    queue = self.priority_queues[priority]\\n\",\n    \"                    if not queue.empty():\\n\",\n    \"                        future = await queue.get()\\n\",\n    \"                        break\\n\",\n    \"                \\n\",\n    \"                if future is None:\\n\",\n    \"                    # No more requests\\n\",\n    \"                    break\\n\",\n    \"                \\n\",\n    \"                # Acquire from base rate limiter\\n\",\n    \"                await self.base_limiter.acquire()\\n\",\n    \"                \\n\",\n    \"                # Signal that request can proceed\\n\",\n    \"                if not future.cancelled():\\n\",\n    \"                    future.set_result(None)\\n\",\n    \"        \\n\",\n    \"        finally:\\n\",\n    \"            self._processing = False\\n\",\n    \"\\n\",\n    \"async def demonstrate_priority_rate_limiting():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate priority-based rate limiting.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüéØ Priority rate limiting:\\\")\\n\",\n    \"    \\n\",\n    \"    priority_limiter = PriorityRateLimiter(max_calls=3, time_window=1.0)\\n\",\n    \"    \\n\",\n    \"    async def priority_task(task_id: int, priority: str, delay: float = 0.1):\\n\",\n    \"        \\\"\\\"\\\"Task with specific priority.\\\"\\\"\\\"\\n\",\n    \"        print(f\\\"üîÑ Task {task_id} ({priority}) waiting...\\\")\\n\",\n    \"        await priority_limiter.acquire(priority)\\n\",\n    \"        print(f\\\"‚úÖ Task {task_id} ({priority}) executing\\\")\\n\",\n    \"        await asyncio.sleep(delay)\\n\",\n    \"        return f\\\"Task {task_id} ({priority}) completed\\\"\\n\",\n    \"    \\n\",\n    \"    # Submit tasks with different priorities\\n\",\n    \"    tasks = [\\n\",\n    \"        priority_task(1, \\\"low\\\"),\\n\",\n    \"        priority_task(2, \\\"high\\\"),\\n\",\n    \"        priority_task(3, \\\"medium\\\"),\\n\",\n    \"        priority_task(4, \\\"low\\\"),\\n\",\n    \"        priority_task(5, \\\"high\\\"),\\n\",\n    \"        priority_task(6, \\\"medium\\\"),\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    results = await asyncio.gather(*tasks)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìã Execution order shows priority handling:\\\")\\n\",\n    \"    for result in results:\\n\",\n    \"        print(f\\\"  {result}\\\")\\n\",\n    \"\\n\",\n    \"# Run rate limiting demonstrations\\n\",\n    \"await demonstrate_rate_limiting()\\n\",\n    \"await demonstrate_priority_rate_limiting()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Rate Limiting Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Prevents API quota exhaustion\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Ensures fair resource usage\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Protects against request bursts\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Enables priority-based processing\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 4: Connection Pooling and Resource Management\\n\",\n    \"\\n\",\n    \"**First Principle**: Connection pooling reuses expensive resources like database connections and HTTP sessions.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `asyncio.Queue` for resource pools and async context managers for automatic cleanup.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class AsyncConnectionPool:\\n\",\n    \"    \\\"\\\"\\\"Connection pool for managing async resources.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, create_connection: Callable, max_size: int = 10):\\n\",\n    \"        self.create_connection = create_connection\\n\",\n    \"        self.max_size = max_size\\n\",\n    \"        self.pool: asyncio.Queue = asyncio.Queue(maxsize=max_size)\\n\",\n    \"        self.created_count = 0\\n\",\n    \"        self.active_connections = 0\\n\",\n    \"        self._lock = asyncio.Lock()\\n\",\n    \"    \\n\",\n    \"    async def get_connection(self):\\n\",\n    \"        \\\"\\\"\\\"Get a connection from the pool.\\\"\\\"\\\"\\n\",\n    \"        try:\\n\",\n    \"            # Try to get existing connection with timeout\\n\",\n    \"            connection = await asyncio.wait_for(self.pool.get(), timeout=0.1)\\n\",\n    \"            self.active_connections += 1\\n\",\n    \"            return connection\\n\",\n    \"        except asyncio.TimeoutError:\\n\",\n    \"            # Create new connection if under limit\\n\",\n    \"            async with self._lock:\\n\",\n    \"                if self.created_count < self.max_size:\\n\",\n    \"                    connection = await self.create_connection()\\n\",\n    \"                    self.created_count += 1\\n\",\n    \"                    self.active_connections += 1\\n\",\n    \"                    return connection\\n\",\n    \"            \\n\",\n    \"            # Wait for available connection\\n\",\n    \"            connection = await self.pool.get()\\n\",\n    \"            self.active_connections += 1\\n\",\n    \"            return connection\\n\",\n    \"    \\n\",\n    \"    async def return_connection(self, connection):\\n\",\n    \"        \\\"\\\"\\\"Return connection to pool.\\\"\\\"\\\"\\n\",\n    \"        self.active_connections -= 1\\n\",\n    \"        await self.pool.put(connection)\\n\",\n    \"    \\n\",\n    \"    @asynccontextmanager\\n\",\n    \"    async def connection(self):\\n\",\n    \"        \\\"\\\"\\\"Context manager for automatic connection management.\\\"\\\"\\\"\\n\",\n    \"        conn = await self.get_connection()\\n\",\n    \"        try:\\n\",\n    \"            yield conn\\n\",\n    \"        finally:\\n\",\n    \"            await self.return_connection(conn)\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get pool statistics.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"pool_size\\\": self.pool.qsize(),\\n\",\n    \"            \\\"created_count\\\": self.created_count,\\n\",\n    \"            \\\"active_connections\\\": self.active_connections,\\n\",\n    \"            \\\"max_size\\\": self.max_size,\\n\",\n    \"            \\\"utilization\\\": self.active_connections / self.max_size\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"# Mock connection class\\n\",\n    \"class MockConnection:\\n\",\n    \"    \\\"\\\"\\\"Mock database/API connection.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, connection_id: str):\\n\",\n    \"        self.id = connection_id\\n\",\n    \"        self.created_at = time.time()\\n\",\n    \"        self.request_count = 0\\n\",\n    \"        self.is_healthy = True\\n\",\n    \"    \\n\",\n    \"    async def execute_query(self, query: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Execute a mock query.\\\"\\\"\\\"\\n\",\n    \"        if not self.is_healthy:\\n\",\n    \"            raise ConnectionError(f\\\"Connection {self.id} is unhealthy\\\")\\n\",\n    \"        \\n\",\n    \"        self.request_count += 1\\n\",\n    \"        await asyncio.sleep(0.1)  # Simulate query time\\n\",\n    \"        return f\\\"Result for '{query}' via connection {self.id}\\\"\\n\",\n    \"    \\n\",\n    \"    def get_info(self) -> Dict[str, Any]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"id\\\": self.id,\\n\",\n    \"            \\\"created_at\\\": self.created_at,\\n\",\n    \"            \\\"request_count\\\": self.request_count,\\n\",\n    \"            \\\"age_seconds\\\": time.time() - self.created_at\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_connection_pooling():\\n\",\n    \"    \\\"\\\"\\\"Show connection pooling patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üèä Connection Pooling\\\", style=\\\"bold magenta\\\"))\\n\",\n    \"    \\n\",\n    \"    async def create_mock_connection():\\n\",\n    \"        \\\"\\\"\\\"Create a mock database connection.\\\"\\\"\\\"\\n\",\n    \"        connection_id = f\\\"conn_{int(time.time() * 1000) % 10000}\\\"\\n\",\n    \"        await asyncio.sleep(0.1)  # Simulate connection setup\\n\",\n    \"        print(f\\\"üîß Created new connection: {connection_id}\\\")\\n\",\n    \"        return MockConnection(connection_id)\\n\",\n    \"    \\n\",\n    \"    # Create connection pool\\n\",\n    \"    pool = AsyncConnectionPool(create_mock_connection, max_size=3)\\n\",\n    \"    \\n\",\n    \"    async def database_operation(operation_id: int, query: str):\\n\",\n    \"        \\\"\\\"\\\"Simulate database operation using connection pool.\\\"\\\"\\\"\\n\",\n    \"        async with pool.connection() as conn:\\n\",\n    \"            result = await conn.execute_query(query)\\n\",\n    \"            return f\\\"Operation {operation_id}: {result}\\\"\\n\",\n    \"    \\n\",\n    \"    print(\\\"üìä Running concurrent database operations:\\\")\\n\",\n    \"    \\n\",\n    \"    # Run 8 concurrent operations with pool of 3 connections\\n\",\n    \"    queries = [\\n\",\n    \"        \\\"SELECT * FROM users\\\",\\n\",\n    \"        \\\"SELECT * FROM products\\\",\\n\",\n    \"        \\\"UPDATE inventory SET count=count-1\\\",\\n\",\n    \"        \\\"INSERT INTO logs VALUES (...)\\\",\\n\",\n    \"        \\\"SELECT AVG(price) FROM products\\\",\\n\",\n    \"        \\\"DELETE FROM cache WHERE expired=true\\\",\\n\",\n    \"        \\\"SELECT * FROM orders WHERE status='pending'\\\",\\n\",\n    \"        \\\"UPDATE users SET last_login=NOW()\\\"\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    tasks = [database_operation(i, query) for i, query in enumerate(queries)]\\n\",\n    \"    \\n\",\n    \"    for task in asyncio.as_completed(tasks):\\n\",\n    \"        result = await task\\n\",\n    \"        elapsed = time.time() - start_time\\n\",\n    \"        stats = pool.get_stats()\\n\",\n    \"        print(f\\\"  {elapsed:.2f}s: {result[:50]}...\\\")\\n\",\n    \"        print(f\\\"    Pool: {stats['active_connections']}/{stats['max_size']} active, {stats['pool_size']} available\\\")\\n\",\n    \"    \\n\",\n    \"    final_stats = pool.get_stats()\\n\",\n    \"    print(f\\\"\\\\nüìà Final Pool Stats:\\\")\\n\",\n    \"    print(f\\\"  Created connections: {final_stats['created_count']}\\\")\\n\",\n    \"    print(f\\\"  Active connections: {final_stats['active_connections']}\\\")\\n\",\n    \"    print(f\\\"  Pool utilization: {final_stats['utilization']:.1%}\\\")\\n\",\n    \"\\n\",\n    \"# Advanced connection pool with health checking\\n\",\n    \"class HealthAwareConnectionPool(AsyncConnectionPool):\\n\",\n    \"    \\\"\\\"\\\"Connection pool with health checking.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, create_connection: Callable, max_size: int = 10, health_check_interval: float = 30.0):\\n\",\n    \"        super().__init__(create_connection, max_size)\\n\",\n    \"        self.health_check_interval = health_check_interval\\n\",\n    \"        self.unhealthy_connections = set()\\n\",\n    \"        self._health_check_task = None\\n\",\n    \"    \\n\",\n    \"    async def start_health_checks(self):\\n\",\n    \"        \\\"\\\"\\\"Start background health checking.\\\"\\\"\\\"\\n\",\n    \"        if self._health_check_task is None:\\n\",\n    \"            self._health_check_task = asyncio.create_task(self._health_check_loop())\\n\",\n    \"    \\n\",\n    \"    async def stop_health_checks(self):\\n\",\n    \"        \\\"\\\"\\\"Stop background health checking.\\\"\\\"\\\"\\n\",\n    \"        if self._health_check_task:\\n\",\n    \"            self._health_check_task.cancel()\\n\",\n    \"            try:\\n\",\n    \"                await self._health_check_task\\n\",\n    \"            except asyncio.CancelledError:\\n\",\n    \"                pass\\n\",\n    \"            self._health_check_task = None\\n\",\n    \"    \\n\",\n    \"    async def _health_check_loop(self):\\n\",\n    \"        \\\"\\\"\\\"Background health checking loop.\\\"\\\"\\\"\\n\",\n    \"        while True:\\n\",\n    \"            try:\\n\",\n    \"                await asyncio.sleep(self.health_check_interval)\\n\",\n    \"                await self._check_pool_health()\\n\",\n    \"            except asyncio.CancelledError:\\n\",\n    \"                break\\n\",\n    \"            except Exception as e:\\n\",\n    \"                print(f\\\"‚ö†Ô∏è  Health check error: {e}\\\")\\n\",\n    \"    \\n\",\n    \"    async def _check_pool_health(self):\\n\",\n    \"        \\\"\\\"\\\"Check health of pooled connections.\\\"\\\"\\\"\\n\",\n    \"        # This is a simplified health check\\n\",\n    \"        # In practice, you'd check connections more thoroughly\\n\",\n    \"        print(f\\\"üè• Performing health check on {self.pool.qsize()} pooled connections\\\")\\n\",\n    \"        \\n\",\n    \"        # Remove old unhealthy connections from tracking\\n\",\n    \"        self.unhealthy_connections.clear()\\n\",\n    \"    \\n\",\n    \"    async def get_connection(self):\\n\",\n    \"        \\\"\\\"\\\"Get a healthy connection from the pool.\\\"\\\"\\\"\\n\",\n    \"        connection = await super().get_connection()\\n\",\n    \"        \\n\",\n    \"        # Check if connection is healthy\\n\",\n    \"        if hasattr(connection, 'is_healthy') and not connection.is_healthy:\\n\",\n    \"            print(f\\\"üö´ Connection {connection.id} is unhealthy, creating new one\\\")\\n\",\n    \"            # Create a replacement connection\\n\",\n    \"            new_connection = await self.create_connection()\\n\",\n    \"            return new_connection\\n\",\n    \"        \\n\",\n    \"        return connection\\n\",\n    \"\\n\",\n    \"async def demonstrate_health_aware_pooling():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate health-aware connection pooling.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüè• Health-aware connection pooling:\\\")\\n\",\n    \"    \\n\",\n    \"    async def create_connection_with_random_health():\\n\",\n    \"        \\\"\\\"\\\"Create connection that might become unhealthy.\\\"\\\"\\\"\\n\",\n    \"        import random\\n\",\n    \"        connection = await create_mock_connection()\\n\",\n    \"        # Simulate some connections becoming unhealthy\\n\",\n    \"        connection.is_healthy = random.random() > 0.3  # 30% chance of being unhealthy\\n\",\n    \"        return connection\\n\",\n    \"    \\n\",\n    \"    health_pool = HealthAwareConnectionPool(\\n\",\n    \"        create_connection_with_random_health, \\n\",\n    \"        max_size=2, \\n\",\n    \"        health_check_interval=2.0\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    # Start health checks\\n\",\n    \"    await health_pool.start_health_checks()\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Use the pool\\n\",\n    \"        for i in range(5):\\n\",\n    \"            async with health_pool.connection() as conn:\\n\",\n    \"                try:\\n\",\n    \"                    result = await conn.execute_query(f\\\"Query {i}\\\")\\n\",\n    \"                    print(f\\\"‚úÖ {result[:40]}...\\\")\\n\",\n    \"                except ConnectionError as e:\\n\",\n    \"                    print(f\\\"‚ùå {e}\\\")\\n\",\n    \"            \\n\",\n    \"            await asyncio.sleep(0.5)\\n\",\n    \"    \\n\",\n    \"    finally:\\n\",\n    \"        # Stop health checks\\n\",\n    \"        await health_pool.stop_health_checks()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"üìä Health-aware pool stats: {health_pool.get_stats()}\\\")\\n\",\n    \"\\n\",\n    \"# Resource manager with lifecycle\\n\",\n    \"@asynccontextmanager\\n\",\n    \"async def managed_ai_service(service_config: Dict[str, Any]):\\n\",\n    \"    \\\"\\\"\\\"Context manager for AI service with full lifecycle.\\\"\\\"\\\"\\n\",\n    \"    print(f\\\"üöÄ Starting AI service with config: {service_config}\\\")\\n\",\n    \"    \\n\",\n    \"    # Setup phase\\n\",\n    \"    service = {\\n\",\n    \"        \\\"id\\\": f\\\"service_{int(time.time())}\\\",\\n\",\n    \"        \\\"config\\\": service_config,\\n\",\n    \"        \\\"started_at\\\": time.time(),\\n\",\n    \"        \\\"request_count\\\": 0\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Simulate service initialization\\n\",\n    \"        await asyncio.sleep(0.1)\\n\",\n    \"        print(f\\\"‚úÖ AI service {service['id']} ready\\\")\\n\",\n    \"        \\n\",\n    \"        yield service\\n\",\n    \"        \\n\",\n    \"    finally:\\n\",\n    \"        # Cleanup phase\\n\",\n    \"        uptime = time.time() - service['started_at']\\n\",\n    \"        print(f\\\"üßπ Shutting down service {service['id']} (uptime: {uptime:.2f}s, requests: {service['request_count']})\\\")\\n\",\n    \"        await asyncio.sleep(0.05)  # Simulate cleanup\\n\",\n    \"\\n\",\n    \"async def demonstrate_resource_lifecycle():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate resource lifecycle management.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüîÑ Resource lifecycle management:\\\")\\n\",\n    \"    \\n\",\n    \"    config = {\\\"model\\\": \\\"gpt-4\\\", \\\"temperature\\\": 0.7, \\\"max_workers\\\": 5}\\n\",\n    \"    \\n\",\n    \"    async with managed_ai_service(config) as service:\\n\",\n    \"        print(f\\\"üìù Using service: {service['id']}\\\")\\n\",\n    \"        \\n\",\n    \"        # Simulate some work\\n\",\n    \"        for i in range(3):\\n\",\n    \"            service['request_count'] += 1\\n\",\n    \"            await asyncio.sleep(0.1)\\n\",\n    \"            print(f\\\"  Processed request {i+1}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"‚úÖ Service lifecycle completed cleanly\\\")\\n\",\n    \"\\n\",\n    \"# Run connection pooling demonstrations\\n\",\n    \"await demonstrate_connection_pooling()\\n\",\n    \"await demonstrate_health_aware_pooling()\\n\",\n    \"await demonstrate_resource_lifecycle()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Connection Pooling Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reuses expensive resources\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reduces connection overhead\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Controls resource usage\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Handles resource failures gracefully\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 5: Concurrent Processing and Task Management\\n\",\n    \"\\n\",\n    \"**First Principle**: Concurrent processing enables parallel execution of independent tasks, dramatically improving throughput.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `asyncio.gather()`, `asyncio.as_completed()`, and `asyncio.Semaphore` for managing concurrent operations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class ConcurrentInferenceEngine:\\n\",\n    \"    \\\"\\\"\\\"Engine for managing concurrent AI inference requests.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_concurrent: int = 5):\\n\",\n    \"        self.max_concurrent = max_concurrent\\n\",\n    \"        self.semaphore = asyncio.Semaphore(max_concurrent)\\n\",\n    \"        self.active_tasks: Dict[str, asyncio.Task] = {}\\n\",\n    \"        self.completed_tasks: List[Dict[str, Any]] = []\\n\",\n    \"        self.task_counter = 0\\n\",\n    \"    \\n\",\n    \"    async def submit_task(\\n\",\n    \"        self, \\n\",\n    \"        task_id: str, \\n\",\n    \"        coro: Callable,\\n\",\n    \"        *args, \\n\",\n    \"        **kwargs\\n\",\n    \"    ) -> str:\\n\",\n    \"        \\\"\\\"\\\"Submit a task for concurrent execution.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        async def wrapped_task():\\n\",\n    \"            async with self.semaphore:\\n\",\n    \"                start_time = time.time()\\n\",\n    \"                try:\\n\",\n    \"                    result = await coro(*args, **kwargs)\\n\",\n    \"                    end_time = time.time()\\n\",\n    \"                    \\n\",\n    \"                    self.completed_tasks.append({\\n\",\n    \"                        \\\"task_id\\\": task_id,\\n\",\n    \"                        \\\"result\\\": result,\\n\",\n    \"                        \\\"duration\\\": end_time - start_time,\\n\",\n    \"                        \\\"status\\\": \\\"success\\\",\\n\",\n    \"                        \\\"error\\\": None\\n\",\n    \"                    })\\n\",\n    \"                    \\n\",\n    \"                    return result\\n\",\n    \"                    \\n\",\n    \"                except Exception as e:\\n\",\n    \"                    end_time = time.time()\\n\",\n    \"                    \\n\",\n    \"                    self.completed_tasks.append({\\n\",\n    \"                        \\\"task_id\\\": task_id,\\n\",\n    \"                        \\\"result\\\": None,\\n\",\n    \"                        \\\"duration\\\": end_time - start_time,\\n\",\n    \"                        \\\"status\\\": \\\"error\\\",\\n\",\n    \"                        \\\"error\\\": str(e)\\n\",\n    \"                    })\\n\",\n    \"                    \\n\",\n    \"                    raise\\n\",\n    \"                finally:\\n\",\n    \"                    # Remove from active tasks\\n\",\n    \"                    if task_id in self.active_tasks:\\n\",\n    \"                        del self.active_tasks[task_id]\\n\",\n    \"        \\n\",\n    \"        # Create and store task\\n\",\n    \"        task = asyncio.create_task(wrapped_task())\\n\",\n    \"        self.active_tasks[task_id] = task\\n\",\n    \"        self.task_counter += 1\\n\",\n    \"        \\n\",\n    \"        return task_id\\n\",\n    \"    \\n\",\n    \"    async def wait_for_task(self, task_id: str) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Wait for specific task to complete.\\\"\\\"\\\"\\n\",\n    \"        if task_id in self.active_tasks:\\n\",\n    \"            return await self.active_tasks[task_id]\\n\",\n    \"        \\n\",\n    \"        # Check completed tasks\\n\",\n    \"        for completed in self.completed_tasks:\\n\",\n    \"            if completed[\\\"task_id\\\"] == task_id:\\n\",\n    \"                if completed[\\\"status\\\"] == \\\"error\\\":\\n\",\n    \"                    raise Exception(completed[\\\"error\\\"])\\n\",\n    \"                return completed[\\\"result\\\"]\\n\",\n    \"        \\n\",\n    \"        raise ValueError(f\\\"Task {task_id} not found\\\")\\n\",\n    \"    \\n\",\n    \"    async def wait_for_all(self) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Wait for all active tasks to complete.\\\"\\\"\\\"\\n\",\n    \"        if not self.active_tasks:\\n\",\n    \"            return []\\n\",\n    \"        \\n\",\n    \"        results = await asyncio.gather(*self.active_tasks.values(), return_exceptions=True)\\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    async def cancel_task(self, task_id: str) -> bool:\\n\",\n    \"        \\\"\\\"\\\"Cancel a specific task.\\\"\\\"\\\"\\n\",\n    \"        if task_id in self.active_tasks:\\n\",\n    \"            task = self.active_tasks[task_id]\\n\",\n    \"            task.cancel()\\n\",\n    \"            del self.active_tasks[task_id]\\n\",\n    \"            return True\\n\",\n    \"        return False\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get engine statistics.\\\"\\\"\\\"\\n\",\n    \"        completed_success = sum(1 for t in self.completed_tasks if t[\\\"status\\\"] == \\\"success\\\")\\n\",\n    \"        completed_error = sum(1 for t in self.completed_tasks if t[\\\"status\\\"] == \\\"error\\\")\\n\",\n    \"        \\n\",\n    \"        if self.completed_tasks:\\n\",\n    \"            avg_duration = sum(t[\\\"duration\\\"] for t in self.completed_tasks) / len(self.completed_tasks)\\n\",\n    \"        else:\\n\",\n    \"            avg_duration = 0.0\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            \\\"active_tasks\\\": len(self.active_tasks),\\n\",\n    \"            \\\"completed_tasks\\\": len(self.completed_tasks),\\n\",\n    \"            \\\"completed_success\\\": completed_success,\\n\",\n    \"            \\\"completed_error\\\": completed_error,\\n\",\n    \"            \\\"max_concurrent\\\": self.max_concurrent,\\n\",\n    \"            \\\"available_slots\\\": self.semaphore._value,\\n\",\n    \"            \\\"total_submitted\\\": self.task_counter,\\n\",\n    \"            \\\"avg_duration\\\": avg_duration\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_concurrent_processing():\\n\",\n    \"    \\\"\\\"\\\"Show concurrent processing patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"‚ö° Concurrent Processing\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create inference engine\\n\",\n    \"    engine = ConcurrentInferenceEngine(max_concurrent=3)\\n\",\n    \"    \\n\",\n    \"    async def mock_inference(text: str, delay: float = 0.5, fail_rate: float = 0.0) -> str:\\n\",\n    \"        \\\"\\\"\\\"Mock AI inference with variable delay and failure rate.\\\"\\\"\\\"\\n\",\n    \"        import random\\n\",\n    \"        \\n\",\n    \"        # Simulate random failure\\n\",\n    \"        if random.random() < fail_rate:\\n\",\n    \"            raise Exception(f\\\"Inference failed for: {text[:20]}...\\\")\\n\",\n    \"        \\n\",\n    \"        await asyncio.sleep(delay)\\n\",\n    \"        return f\\\"Processed: {text.upper()}\\\"\\n\",\n    \"    \\n\",\n    \"    print(\\\"ü§ñ Submitting concurrent inference tasks:\\\")\\n\",\n    \"    \\n\",\n    \"    # Submit multiple tasks with different characteristics\\n\",\n    \"    tasks_data = [\\n\",\n    \"        (\\\"task_1\\\", \\\"Analyze sentiment\\\", 0.3, 0.0),\\n\",\n    \"        (\\\"task_2\\\", \\\"Summarize document\\\", 0.8, 0.0),\\n\",\n    \"        (\\\"task_3\\\", \\\"Extract entities\\\", 0.5, 0.2),  # May fail\\n\",\n    \"        (\\\"task_4\\\", \\\"Classify text\\\", 0.4, 0.0),\\n\",\n    \"        (\\\"task_5\\\", \\\"Generate response\\\", 0.6, 0.1),  # May fail\\n\",\n    \"        (\\\"task_6\\\", \\\"Translate text\\\", 0.3, 0.0),\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    submitted_tasks = []\\n\",\n    \"    for task_id, text, delay, fail_rate in tasks_data:\\n\",\n    \"        await engine.submit_task(task_id, mock_inference, text, delay, fail_rate)\\n\",\n    \"        submitted_tasks.append(task_id)\\n\",\n    \"        print(f\\\"  ‚úÖ Submitted: {task_id}\\\")\\n\",\n    \"    \\n\",\n    \"    # Monitor progress\\n\",\n    \"    print(f\\\"\\\\nüìä Monitoring task execution:\\\")\\n\",\n    \"    \\n\",\n    \"    while engine.active_tasks:\\n\",\n    \"        stats = engine.get_stats()\\n\",\n    \"        print(f\\\"  Active: {stats['active_tasks']}, Completed: {stats['completed_tasks']}, Available slots: {stats['available_slots']}\\\")\\n\",\n    \"        await asyncio.sleep(0.2)\\n\",\n    \"    \\n\",\n    \"    # Show final results\\n\",\n    \"    print(f\\\"\\\\nüìã Task Results:\\\")\\n\",\n    \"    for completed in engine.completed_tasks:\\n\",\n    \"        status_icon = \\\"‚úÖ\\\" if completed[\\\"status\\\"] == \\\"success\\\" else \\\"‚ùå\\\"\\n\",\n    \"        result_preview = completed[\\\"result\\\"][:30] if completed[\\\"result\\\"] else completed[\\\"error\\\"][:30]\\n\",\n    \"        print(f\\\"  {status_icon} {completed['task_id']}: {completed['duration']:.2f}s - {result_preview}...\\\")\\n\",\n    \"    \\n\",\n    \"    final_stats = engine.get_stats()\\n\",\n    \"    print(f\\\"\\\\nüìà Final Statistics:\\\")\\n\",\n    \"    print(f\\\"  Total tasks: {final_stats['total_submitted']}\\\")\\n\",\n    \"    print(f\\\"  Successful: {final_stats['completed_success']}\\\")\\n\",\n    \"    print(f\\\"  Failed: {final_stats['completed_error']}\\\")\\n\",\n    \"    print(f\\\"  Average duration: {final_stats['avg_duration']:.2f}s\\\")\\n\",\n    \"\\n\",\n    \"# Advanced concurrent patterns\\n\",\n    \"class BatchProcessor:\\n\",\n    \"    \\\"\\\"\\\"Process items in batches with concurrency control.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, batch_size: int = 10, max_concurrent_batches: int = 3):\\n\",\n    \"        self.batch_size = batch_size\\n\",\n    \"        self.max_concurrent_batches = max_concurrent_batches\\n\",\n    \"        self.semaphore = asyncio.Semaphore(max_concurrent_batches)\\n\",\n    \"    \\n\",\n    \"    async def process_batch(\\n\",\n    \"        self,\\n\",\n    \"        items: List[Any],\\n\",\n    \"        processor: Callable,\\n\",\n    \"        **kwargs\\n\",\n    \"    ) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Process items in concurrent batches.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        async def process_single_batch(batch_items: List[Any], batch_id: int) -> List[Any]:\\n\",\n    \"            \\\"\\\"\\\"Process a single batch.\\\"\\\"\\\"\\n\",\n    \"            async with self.semaphore:\\n\",\n    \"                print(f\\\"üîÑ Processing batch {batch_id} ({len(batch_items)} items)\\\")\\n\",\n    \"                \\n\",\n    \"                # Process items in batch concurrently\\n\",\n    \"                tasks = [processor(item, **kwargs) for item in batch_items]\\n\",\n    \"                results = await asyncio.gather(*tasks, return_exceptions=True)\\n\",\n    \"                \\n\",\n    \"                print(f\\\"‚úÖ Completed batch {batch_id}\\\")\\n\",\n    \"                return results\\n\",\n    \"        \\n\",\n    \"        # Split items into batches\\n\",\n    \"        batches = [\\n\",\n    \"            items[i:i + self.batch_size] \\n\",\n    \"            for i in range(0, len(items), self.batch_size)\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        print(f\\\"üì¶ Processing {len(items)} items in {len(batches)} batches\\\")\\n\",\n    \"        \\n\",\n    \"        # Process batches concurrently\\n\",\n    \"        batch_tasks = [\\n\",\n    \"            process_single_batch(batch, i) \\n\",\n    \"            for i, batch in enumerate(batches)\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        batch_results = await asyncio.gather(*batch_tasks)\\n\",\n    \"        \\n\",\n    \"        # Flatten results\\n\",\n    \"        all_results = []\\n\",\n    \"        for batch_result in batch_results:\\n\",\n    \"            all_results.extend(batch_result)\\n\",\n    \"        \\n\",\n    \"        return all_results\\n\",\n    \"\\n\",\n    \"async def demonstrate_batch_processing():\\n\",\n    \"    \\\"\\\"\\\"Show batch processing patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüì¶ Batch Processing:\\\")\\n\",\n    \"    \\n\",\n    \"    async def process_text_item(text: str, operation: str = \\\"uppercase\\\") -> str:\\n\",\n    \"        \\\"\\\"\\\"Process a single text item.\\\"\\\"\\\"\\n\",\n    \"        await asyncio.sleep(0.1)  # Simulate processing\\n\",\n    \"        \\n\",\n    \"        if operation == \\\"uppercase\\\":\\n\",\n    \"            return text.upper()\\n\",\n    \"        elif operation == \\\"reverse\\\":\\n\",\n    \"            return text[::-1]\\n\",\n    \"        else:\\n\",\n    \"            return f\\\"PROCESSED: {text}\\\"\\n\",\n    \"    \\n\",\n    \"    # Create test data\\n\",\n    \"    texts = [f\\\"text item {i}\\\" for i in range(25)]\\n\",\n    \"    \\n\",\n    \"    processor = BatchProcessor(batch_size=5, max_concurrent_batches=2)\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    \\n\",\n    \"    results = await processor.process_batch(\\n\",\n    \"        texts, \\n\",\n    \"        process_text_item, \\n\",\n    \"        operation=\\\"uppercase\\\"\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    processing_time = time.time() - start_time\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìä Batch Processing Results:\\\")\\n\",\n    \"    print(f\\\"  Total items: {len(texts)}\\\")\\n\",\n    \"    print(f\\\"  Processing time: {processing_time:.2f}s\\\")\\n\",\n    \"    print(f\\\"  Items per second: {len(texts) / processing_time:.1f}\\\")\\n\",\n    \"    print(f\\\"  Sample results: {results[:3]}\\\")\\n\",\n    \"    \\n\",\n    \"    # Count errors\\n\",\n    \"    errors = [r for r in results if isinstance(r, Exception)]\\n\",\n    \"    print(f\\\"  Errors: {len(errors)}\\\")\\n\",\n    \"\\n\",\n    \"# Progress tracking for concurrent operations\\n\",\n    \"class ProgressTracker:\\n\",\n    \"    \\\"\\\"\\\"Track progress of concurrent operations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, total_tasks: int):\\n\",\n    \"        self.total_tasks = total_tasks\\n\",\n    \"        self.completed_tasks = 0\\n\",\n    \"        self.failed_tasks = 0\\n\",\n    \"        self.start_time = time.time()\\n\",\n    \"        self._lock = asyncio.Lock()\\n\",\n    \"    \\n\",\n    \"    async def task_completed(self, success: bool = True):\\n\",\n    \"        \\\"\\\"\\\"Mark a task as completed.\\\"\\\"\\\"\\n\",\n    \"        async with self._lock:\\n\",\n    \"            if success:\\n\",\n    \"                self.completed_tasks += 1\\n\",\n    \"            else:\\n\",\n    \"                self.failed_tasks += 1\\n\",\n    \"    \\n\",\n    \"    def get_progress(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get current progress.\\\"\\\"\\\"\\n\",\n    \"        total_finished = self.completed_tasks + self.failed_tasks\\n\",\n    \"        progress_pct = (total_finished / self.total_tasks) * 100 if self.total_tasks > 0 else 0\\n\",\n    \"        elapsed_time = time.time() - self.start_time\\n\",\n    \"        \\n\",\n    \"        if total_finished > 0:\\n\",\n    \"            estimated_total_time = elapsed_time * (self.total_tasks / total_finished)\\n\",\n    \"            eta = estimated_total_time - elapsed_time\\n\",\n    \"        else:\\n\",\n    \"            eta = 0\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            \\\"completed\\\": self.completed_tasks,\\n\",\n    \"            \\\"failed\\\": self.failed_tasks,\\n\",\n    \"            \\\"total\\\": self.total_tasks,\\n\",\n    \"            \\\"progress_pct\\\": progress_pct,\\n\",\n    \"            \\\"elapsed_time\\\": elapsed_time,\\n\",\n    \"            \\\"eta\\\": max(0, eta)\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_progress_tracking():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate progress tracking for concurrent tasks.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìà Progress Tracking:\\\")\\n\",\n    \"    \\n\",\n    \"    total_tasks = 20\\n\",\n    \"    tracker = ProgressTracker(total_tasks)\\n\",\n    \"    \\n\",\n    \"    async def tracked_task(task_id: int, delay: float, fail_rate: float = 0.1):\\n\",\n    \"        \\\"\\\"\\\"Task with progress tracking.\\\"\\\"\\\"\\n\",\n    \"        import random\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            await asyncio.sleep(delay)\\n\",\n    \"            \\n\",\n    \"            if random.random() < fail_rate:\\n\",\n    \"                raise Exception(f\\\"Task {task_id} failed\\\")\\n\",\n    \"            \\n\",\n    \"            await tracker.task_completed(success=True)\\n\",\n    \"            return f\\\"Task {task_id} completed\\\"\\n\",\n    \"            \\n\",\n    \"        except Exception as e:\\n\",\n    \"            await tracker.task_completed(success=False)\\n\",\n    \"            raise\\n\",\n    \"    \\n\",\n    \"    # Start all tasks\\n\",\n    \"    tasks = [\\n\",\n    \"        asyncio.create_task(tracked_task(i, random.uniform(0.1, 0.5), 0.15))\\n\",\n    \"        for i in range(total_tasks)\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    # Monitor progress\\n\",\n    \"    while True:\\n\",\n    \"        progress = tracker.get_progress()\\n\",\n    \"        finished = progress[\\\"completed\\\"] + progress[\\\"failed\\\"]\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\rüìä Progress: {finished}/{total_tasks} ({progress['progress_pct']:.1f}%) - \\\" +\\n\",\n    \"              f\\\"‚úÖ {progress['completed']} success, ‚ùå {progress['failed']} failed - \\\" +\\n\",\n    \"              f\\\"ETA: {progress['eta']:.1f}s\\\", end=\\\"\\\")\\n\",\n    \"        \\n\",\n    \"        if finished >= total_tasks:\\n\",\n    \"            break\\n\",\n    \"        \\n\",\n    \"        await asyncio.sleep(0.1)\\n\",\n    \"    \\n\",\n    \"    print()  # New line\\n\",\n    \"    \\n\",\n    \"    # Wait for all tasks to complete\\n\",\n    \"    results = await asyncio.gather(*tasks, return_exceptions=True)\\n\",\n    \"    \\n\",\n    \"    success_count = sum(1 for r in results if not isinstance(r, Exception))\\n\",\n    \"    error_count = sum(1 for r in results if isinstance(r, Exception))\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\n‚úÖ All tasks completed: {success_count} success, {error_count} errors\\\")\\n\",\n    \"\\n\",\n    \"# Run concurrent processing demonstrations\\n\",\n    \"await demonstrate_concurrent_processing()\\n\",\n    \"await demonstrate_batch_processing()\\n\",\n    \"await demonstrate_progress_tracking()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Concurrent Processing Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Dramatically improves throughput\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Better resource utilization\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Handles mixed workloads efficiently\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Provides failure isolation\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 6: Production-Ready Async AI System\\n\",\n    \"\\n\",\n    \"**First Principle**: Combine all async patterns to create a robust, scalable AI processing system.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use composition of async components to build complex, maintainable systems.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Complete async AI processing system\\n\",\n    \"from dataclasses import dataclass, field\\n\",\n    \"from typing import Protocol\\n\",\n    \"import json\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class AIRequest:\\n\",\n    \"    \\\"\\\"\\\"AI processing request.\\\"\\\"\\\"\\n\",\n    \"    request_id: str\\n\",\n    \"    text: str\\n\",\n    \"    operation: str  # \\\"classify\\\", \\\"summarize\\\", \\\"translate\\\", etc.\\n\",\n    \"    priority: str = \\\"medium\\\"  # \\\"high\\\", \\\"medium\\\", \\\"low\\\"\\n\",\n    \"    metadata: Dict[str, Any] = field(default_factory=dict)\\n\",\n    \"    created_at: float = field(default_factory=time.time)\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class AIResponse:\\n\",\n    \"    \\\"\\\"\\\"AI processing response.\\\"\\\"\\\"\\n\",\n    \"    request_id: str\\n\",\n    \"    result: Any\\n\",\n    \"    status: str  # \\\"success\\\", \\\"error\\\", \\\"timeout\\\"\\n\",\n    \"    processing_time: float\\n\",\n    \"    model_used: str\\n\",\n    \"    error_message: Optional[str] = None\\n\",\n    \"    completed_at: float = field(default_factory=time.time)\\n\",\n    \"\\n\",\n    \"class AsyncAISystem:\\n\",\n    \"    \\\"\\\"\\\"Production-ready async AI processing system.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(\\n\",\n    \"        self,\\n\",\n    \"        max_concurrent: int = 10,\\n\",\n    \"        rate_limit_per_second: float = 5.0,\\n\",\n    \"        connection_pool_size: int = 5,\\n\",\n    \"        request_timeout: float = 30.0\\n\",\n    \"    ):\\n\",\n    \"        self.max_concurrent = max_concurrent\\n\",\n    \"        self.request_timeout = request_timeout\\n\",\n    \"        \\n\",\n    \"        # Core components\\n\",\n    \"        self.inference_engine = ConcurrentInferenceEngine(max_concurrent)\\n\",\n    \"        self.rate_limiter = TokenBucketRateLimiter(rate_limit_per_second, bucket_size=10)\\n\",\n    \"        self.connection_pool = None  # Will be initialized with mock connections\\n\",\n    \"        \\n\",\n    \"        # Request tracking\\n\",\n    \"        self.active_requests: Dict[str, AIRequest] = {}\\n\",\n    \"        self.completed_requests: List[AIResponse] = []\\n\",\n    \"        \\n\",\n    \"        # Metrics\\n\",\n    \"        self.metrics = {\\n\",\n    \"            \\\"total_requests\\\": 0,\\n\",\n    \"            \\\"successful_requests\\\": 0,\\n\",\n    \"            \\\"failed_requests\\\": 0,\\n\",\n    \"            \\\"timeout_requests\\\": 0,\\n\",\n    \"            \\\"total_processing_time\\\": 0.0\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        # Background tasks\\n\",\n    \"        self._background_tasks = set()\\n\",\n    \"    \\n\",\n    \"    async def start(self):\\n\",\n    \"        \\\"\\\"\\\"Start the AI system.\\\"\\\"\\\"\\n\",\n    \"        print(\\\"üöÄ Starting Async AI System...\\\")\\n\",\n    \"        \\n\",\n    \"        # Initialize connection pool\\n\",\n    \"        async def create_ai_connection():\\n\",\n    \"            await asyncio.sleep(0.1)  # Simulate connection setup\\n\",\n    \"            return MockConnection(f\\\"ai_conn_{int(time.time() * 1000) % 1000}\\\")\\n\",\n    \"        \\n\",\n    \"        self.connection_pool = AsyncConnectionPool(\\n\",\n    \"            create_ai_connection, \\n\",\n    \"            max_size=5\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        # Start background monitoring\\n\",\n    \"        monitor_task = asyncio.create_task(self._monitor_system())\\n\",\n    \"        self._background_tasks.add(monitor_task)\\n\",\n    \"        \\n\",\n    \"        print(\\\"‚úÖ AI System started successfully\\\")\\n\",\n    \"    \\n\",\n    \"    async def stop(self):\\n\",\n    \"        \\\"\\\"\\\"Stop the AI system gracefully.\\\"\\\"\\\"\\n\",\n    \"        print(\\\"üõë Stopping Async AI System...\\\")\\n\",\n    \"        \\n\",\n    \"        # Cancel background tasks\\n\",\n    \"        for task in self._background_tasks:\\n\",\n    \"            task.cancel()\\n\",\n    \"        \\n\",\n    \"        # Wait for active requests to complete\\n\",\n    \"        if self.active_requests:\\n\",\n    \"            print(f\\\"‚è≥ Waiting for {len(self.active_requests)} active requests...\\\")\\n\",\n    \"            # In practice, you'd set a timeout here\\n\",\n    \"            await asyncio.sleep(1.0)\\n\",\n    \"        \\n\",\n    \"        print(\\\"‚úÖ AI System stopped\\\")\\n\",\n    \"    \\n\",\n    \"    async def process_request(self, request: AIRequest) -> AIResponse:\\n\",\n    \"        \\\"\\\"\\\"Process a single AI request.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Rate limiting\\n\",\n    \"        await self.rate_limiter.acquire(tokens_needed=1)\\n\",\n    \"        \\n\",\n    \"        # Track request\\n\",\n    \"        self.active_requests[request.request_id] = request\\n\",\n    \"        self.metrics[\\\"total_requests\\\"] += 1\\n\",\n    \"        \\n\",\n    \"        try:\\n\",\n    \"            # Process with timeout\\n\",\n    \"            response = await asyncio.wait_for(\\n\",\n    \"                self._execute_request(request),\\n\",\n    \"                timeout=self.request_timeout\\n\",\n    \"            )\\n\",\n    \"            \\n\",\n    \"            self.metrics[\\\"successful_requests\\\"] += 1\\n\",\n    \"            \\n\",\n    \"        except asyncio.TimeoutError:\\n\",\n    \"            response = AIResponse(\\n\",\n    \"                request_id=request.request_id,\\n\",\n    \"                result=None,\\n\",\n    \"                status=\\\"timeout\\\",\\n\",\n    \"                processing_time=self.request_timeout,\\n\",\n    \"                model_used=\\\"none\\\",\\n\",\n    \"                error_message=\\\"Request timeout\\\"\\n\",\n    \"            )\\n\",\n    \"            self.metrics[\\\"timeout_requests\\\"] += 1\\n\",\n    \"            \\n\",\n    \"        except Exception as e:\\n\",\n    \"            response = AIResponse(\\n\",\n    \"                request_id=request.request_id,\\n\",\n    \"                result=None,\\n\",\n    \"                status=\\\"error\\\",\\n\",\n    \"                processing_time=0.0,\\n\",\n    \"                model_used=\\\"none\\\",\\n\",\n    \"                error_message=str(e)\\n\",\n    \"            )\\n\",\n    \"            self.metrics[\\\"failed_requests\\\"] += 1\\n\",\n    \"        \\n\",\n    \"        finally:\\n\",\n    \"            # Clean up\\n\",\n    \"            if request.request_id in self.active_requests:\\n\",\n    \"                del self.active_requests[request.request_id]\\n\",\n    \"            \\n\",\n    \"            self.completed_requests.append(response)\\n\",\n    \"            self.metrics[\\\"total_processing_time\\\"] += response.processing_time\\n\",\n    \"        \\n\",\n    \"        return response\\n\",\n    \"    \\n\",\n    \"    async def _execute_request(self, request: AIRequest) -> AIResponse:\\n\",\n    \"        \\\"\\\"\\\"Execute the actual AI processing.\\\"\\\"\\\"\\n\",\n    \"        start_time = time.time()\\n\",\n    \"        \\n\",\n    \"        async with self.connection_pool.connection() as conn:\\n\",\n    \"            # Simulate different AI operations\\n\",\n    \"            if request.operation == \\\"classify\\\":\\n\",\n    \"                await asyncio.sleep(0.2)  # Simulate classification\\n\",\n    \"                result = {\\\"category\\\": \\\"positive\\\", \\\"confidence\\\": 0.85}\\n\",\n    \"                model = \\\"classifier-v1\\\"\\n\",\n    \"                \\n\",\n    \"            elif request.operation == \\\"summarize\\\":\\n\",\n    \"                await asyncio.sleep(0.5)  # Simulate summarization\\n\",\n    \"                result = {\\\"summary\\\": f\\\"Summary of: {request.text[:30]}...\\\"}\\n\",\n    \"                model = \\\"summarizer-v2\\\"\\n\",\n    \"                \\n\",\n    \"            elif request.operation == \\\"translate\\\":\\n\",\n    \"                await asyncio.sleep(0.3)  # Simulate translation\\n\",\n    \"                result = {\\\"translated_text\\\": f\\\"TRANSLATED: {request.text}\\\"}\\n\",\n    \"                model = \\\"translator-v1\\\"\\n\",\n    \"                \\n\",\n    \"            else:\\n\",\n    \"                raise ValueError(f\\\"Unknown operation: {request.operation}\\\")\\n\",\n    \"            \\n\",\n    \"            processing_time = time.time() - start_time\\n\",\n    \"            \\n\",\n    \"            return AIResponse(\\n\",\n    \"                request_id=request.request_id,\\n\",\n    \"                result=result,\\n\",\n    \"                status=\\\"success\\\",\\n\",\n    \"                processing_time=processing_time,\\n\",\n    \"                model_used=model\\n\",\n    \"            )\\n\",\n    \"    \\n\",\n    \"    async def process_batch(self, requests: List[AIRequest]) -> List[AIResponse]:\\n\",\n    \"        \\\"\\\"\\\"Process multiple requests concurrently.\\\"\\\"\\\"\\n\",\n    \"        print(f\\\"üì¶ Processing batch of {len(requests)} requests\\\")\\n\",\n    \"        \\n\",\n    \"        # Group by priority\\n\",\n    \"        priority_groups = {\\\"high\\\": [], \\\"medium\\\": [], \\\"low\\\": []}\\n\",\n    \"        for request in requests:\\n\",\n    \"            priority_groups[request.priority].append(request)\\n\",\n    \"        \\n\",\n    \"        # Process high priority first\\n\",\n    \"        all_responses = []\\n\",\n    \"        for priority in [\\\"high\\\", \\\"medium\\\", \\\"low\\\"]:\\n\",\n    \"            if priority_groups[priority]:\\n\",\n    \"                print(f\\\"üéØ Processing {len(priority_groups[priority])} {priority} priority requests\\\")\\n\",\n    \"                \\n\",\n    \"                tasks = [\\n\",\n    \"                    self.process_request(request) \\n\",\n    \"                    for request in priority_groups[priority]\\n\",\n    \"                ]\\n\",\n    \"                \\n\",\n    \"                responses = await asyncio.gather(*tasks, return_exceptions=True)\\n\",\n    \"                \\n\",\n    \"                # Handle any exceptions\\n\",\n    \"                for i, response in enumerate(responses):\\n\",\n    \"                    if isinstance(response, Exception):\\n\",\n    \"                        # Create error response\\n\",\n    \"                        request = priority_groups[priority][i]\\n\",\n    \"                        response = AIResponse(\\n\",\n    \"                            request_id=request.request_id,\\n\",\n    \"                            result=None,\\n\",\n    \"                            status=\\\"error\\\",\\n\",\n    \"                            processing_time=0.0,\\n\",\n    \"                            model_used=\\\"none\\\",\\n\",\n    \"                            error_message=str(response)\\n\",\n    \"                        )\\n\",\n    \"                \\n\",\n    \"                all_responses.extend(responses)\\n\",\n    \"        \\n\",\n    \"        return all_responses\\n\",\n    \"    \\n\",\n    \"    async def _monitor_system(self):\\n\",\n    \"        \\\"\\\"\\\"Background monitoring task.\\\"\\\"\\\"\\n\",\n    \"        while True:\\n\",\n    \"            try:\\n\",\n    \"                await asyncio.sleep(5.0)  # Monitor every 5 seconds\\n\",\n    \"                \\n\",\n    \"                stats = self.get_system_stats()\\n\",\n    \"                print(f\\\"üìä System Monitor: {stats['active_requests']} active, \\\"\\n\",\n    \"                      f\\\"{stats['total_requests']} total, \\\"\\n\",\n    \"                      f\\\"{stats['success_rate']:.1%} success rate\\\")\\n\",\n    \"                \\n\",\n    \"            except asyncio.CancelledError:\\n\",\n    \"                break\\n\",\n    \"            except Exception as e:\\n\",\n    \"                print(f\\\"‚ö†Ô∏è  Monitor error: {e}\\\")\\n\",\n    \"    \\n\",\n    \"    def get_system_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get comprehensive system statistics.\\\"\\\"\\\"\\n\",\n    \"        total_requests = self.metrics[\\\"total_requests\\\"]\\n\",\n    \"        success_rate = (self.metrics[\\\"successful_requests\\\"] / total_requests) if total_requests > 0 else 0\\n\",\n    \"        avg_processing_time = (self.metrics[\\\"total_processing_time\\\"] / self.metrics[\\\"successful_requests\\\"]) if self.metrics[\\\"successful_requests\\\"] > 0 else 0\\n\",\n    \"        \\n\",\n    \"        pool_stats = self.connection_pool.get_stats() if self.connection_pool else {}\\n\",\n    \"        rate_limiter_stats = self.rate_limiter.get_stats()\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            \\\"active_requests\\\": len(self.active_requests),\\n\",\n    \"            \\\"total_requests\\\": total_requests,\\n\",\n    \"            \\\"successful_requests\\\": self.metrics[\\\"successful_requests\\\"],\\n\",\n    \"            \\\"failed_requests\\\": self.metrics[\\\"failed_requests\\\"],\\n\",\n    \"            \\\"timeout_requests\\\": self.metrics[\\\"timeout_requests\\\"],\\n\",\n    \"            \\\"success_rate\\\": success_rate,\\n\",\n    \"            \\\"avg_processing_time\\\": avg_processing_time,\\n\",\n    \"            \\\"connection_pool\\\": pool_stats,\\n\",\n    \"            \\\"rate_limiter\\\": rate_limiter_stats\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"async def demonstrate_production_system():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate the complete production async AI system.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üè≠ Production Async AI System\\\", style=\\\"bold green\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create and start the system\\n\",\n    \"    ai_system = AsyncAISystem(\\n\",\n    \"        max_concurrent=5,\\n\",\n    \"        rate_limit_per_second=3.0,\\n\",\n    \"        connection_pool_size=3,\\n\",\n    \"        request_timeout=10.0\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    await ai_system.start()\\n\",\n    \"    \\n\",\n    \"    try:\\n\",\n    \"        # Create diverse test requests\\n\",\n    \"        requests = [\\n\",\n    \"            AIRequest(\\\"req_001\\\", \\\"Analyze this positive sentiment\\\", \\\"classify\\\", \\\"high\\\"),\\n\",\n    \"            AIRequest(\\\"req_002\\\", \\\"Long document that needs summarization...\\\", \\\"summarize\\\", \\\"medium\\\"),\\n\",\n    \"            AIRequest(\\\"req_003\\\", \\\"Hello world\\\", \\\"translate\\\", \\\"low\\\"),\\n\",\n    \"            AIRequest(\\\"req_004\\\", \\\"Another classification task\\\", \\\"classify\\\", \\\"medium\\\"),\\n\",\n    \"            AIRequest(\\\"req_005\\\", \\\"Important urgent request\\\", \\\"classify\\\", \\\"high\\\"),\\n\",\n    \"            AIRequest(\\\"req_006\\\", \\\"Text to translate\\\", \\\"translate\\\", \\\"low\\\"),\\n\",\n    \"            AIRequest(\\\"req_007\\\", \\\"Document for summary\\\", \\\"summarize\\\", \\\"medium\\\"),\\n\",\n    \"            AIRequest(\\\"req_008\\\", \\\"Final classification\\\", \\\"classify\\\", \\\"low\\\"),\\n\",\n    \"        ]\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüöÄ Processing {len(requests)} requests through production system:\\\")\\n\",\n    \"        \\n\",\n    \"        # Process requests in batch\\n\",\n    \"        start_time = time.time()\\n\",\n    \"        responses = await ai_system.process_batch(requests)\\n\",\n    \"        total_time = time.time() - start_time\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüìã Processing Results:\\\")\\n\",\n    \"        for response in responses:\\n\",\n    \"            status_icon = \\\"‚úÖ\\\" if response.status == \\\"success\\\" else \\\"‚ùå\\\" if response.status == \\\"error\\\" else \\\"‚è∞\\\"\\n\",\n    \"            result_preview = str(response.result)[:40] if response.result else response.error_message[:40]\\n\",\n    \"            print(f\\\"  {status_icon} {response.request_id}: {response.processing_time:.3f}s - {response.model_used} - {result_preview}...\\\")\\n\",\n    \"        \\n\",\n    \"        # Show comprehensive statistics\\n\",\n    \"        stats = ai_system.get_system_stats()\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüìä System Performance:\\\")\\n\",\n    \"        print(f\\\"  Total processing time: {total_time:.2f}s\\\")\\n\",\n    \"        print(f\\\"  Average per request: {total_time / len(requests):.3f}s\\\")\\n\",\n    \"        print(f\\\"  Success rate: {stats['success_rate']:.1%}\\\")\\n\",\n    \"        print(f\\\"  Average AI processing time: {stats['avg_processing_time']:.3f}s\\\")\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüîß Resource Utilization:\\\")\\n\",\n    \"        print(f\\\"  Connection pool: {stats['connection_pool']}\\\")\\n\",\n    \"        print(f\\\"  Rate limiter: {stats['rate_limiter']}\\\")\\n\",\n    \"        \\n\",\n    \"        # Test streaming capabilities\\n\",\n    \"        print(f\\\"\\\\nüåä Testing streaming response:\\\")\\n\",\n    \"        \\n\",\n    \"        provider = StreamingAIProvider(response_delay=0.05)\\n\",\n    \"        messages = [Message(role=\\\"user\\\", content=\\\"Stream a response about async programming\\\")]\\n\",\n    \"        \\n\",\n    \"        full_response = \\\"\\\"\\n\",\n    \"        chunk_count = 0\\n\",\n    \"        \\n\",\n    \"        async for chunk in provider.stream_completion(\\\"gpt-4\\\", messages):\\n\",\n    \"            full_response += chunk.content\\n\",\n    \"            chunk_count += 1\\n\",\n    \"            \\n\",\n    \"            if chunk.is_final:\\n\",\n    \"                print(f\\\"  üì° Streamed {chunk_count} chunks: '{full_response}'\\\")\\n\",\n    \"        \\n\",\n    \"        print(f\\\"\\\\nüéØ Production System Features Demonstrated:\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Concurrent request processing\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Priority-based scheduling\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Rate limiting and resource control\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Connection pooling\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Timeout handling\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Comprehensive monitoring\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Graceful error handling\\\")\\n\",\n    \"        print(f\\\"  ‚úÖ Streaming capabilities\\\")\\n\",\n    \"        \\n\",\n    \"    finally:\\n\",\n    \"        await ai_system.stop()\\n\",\n    \"\\n\",\n    \"# Run the production system demonstration\\n\",\n    \"await demonstrate_production_system()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüèÜ Async Patterns Mastery Complete!\\\")\\n\",\n    \"print(f\\\"\\\\nüí° Key Async Benefits for AI Systems:\\\")\\n\",\n    \"print(f\\\"  ‚ö° 10-100x performance improvement for I/O-bound operations\\\")\\n\",\n    \"print(f\\\"  üîÑ Real-time streaming and user feedback\\\")\\n\",\n    \"print(f\\\"  üìà Scales to thousands of concurrent requests\\\")\\n\",\n    \"print(f\\\"  üíæ Efficient resource utilization\\\")\\n\",\n    \"print(f\\\"  üõ°Ô∏è  Built-in error handling and timeout management\\\")\\n\",\n    \"print(f\\\"  üìä Comprehensive monitoring and metrics\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Summary: Async Patterns Mastery\\n\",\n    \"\\n\",\n    \"**üéØ Key Takeaways:**\\n\",\n    \"\\n\",\n    \"1. **Streaming** enables real-time user feedback and memory efficiency\\n\",\n    \"2. **Rate Limiting** prevents resource exhaustion and ensures fair usage\\n\",\n    \"3. **Connection Pooling** reuses expensive resources efficiently\\n\",\n    \"4. **Concurrent Processing** dramatically improves throughput\\n\",\n    \"5. **Batch Processing** optimizes resource usage for bulk operations\\n\",\n    \"6. **Async Context Managers** ensure proper resource cleanup\\n\",\n    \"7. **Progress Tracking** provides visibility into long-running operations\\n\",\n    \"\\n\",\n    \"**üöÄ Production Considerations:**\\n\",\n    \"- Always use timeouts to prevent hanging operations\\n\",\n    \"- Implement proper error handling and retry logic\\n\",\n    \"- Monitor resource usage and performance metrics\\n\",\n    \"- Use priority queues for important requests\\n\",\n    \"- Plan for graceful shutdown and cleanup\\n\",\n    \"\\n\",\n    \"**üí° Remember:** Async programming is essential for building scalable AI systems that can handle real-world load and provide responsive user experiences!\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}","structuredPatch":[]}}
{"parentUuid":"10310621-d474-403e-ae1d-d2cded1290b0","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01FR5iqZqH3Da5iQfP4asyuU","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me create the notebook for performance optimization (file 5):"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":139085,"cache_read_input_tokens":0,"output_tokens":3}},"requestId":"req_vrtx_011CRhSYczuuib9Tu6sh1g7f","type":"assistant","uuid":"78209563-7338-4e1a-a54f-9441f58f21a3","timestamp":"2025-08-01T18:11:47.372Z"}
{"parentUuid":"78209563-7338-4e1a-a54f-9441f58f21a3","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01FR5iqZqH3Da5iQfP4asyuU","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01NqgAZAe2ajWivvFZ31rVm2","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/05_performance_optimization_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Performance Optimization for AI Systems\\n\",\n    \"\\n\",\n    \"This notebook teaches performance optimization techniques for AI applications including profiling, memory management, vectorization, and parallel processing. You'll learn:\\n\",\n    \"\\n\",\n    \"1. **Memory-Efficient Data Structures** - Optimizing memory usage\\n\",\n    \"2. **Performance Profiling** - Identifying bottlenecks\\n\",\n    \"3. **Vectorized Operations** - NumPy and efficient computations\\n\",\n    \"4. **Parallel Processing** - CPU and I/O optimization\\n\",\n    \"5. **Lazy Loading** - Memory-efficient data processing\\n\",\n    \"6. **Batch Processing** - Optimizing throughput\\n\",\n    \"7. **Garbage Collection** - Memory management\\n\",\n    \"8. **Benchmarking** - Measuring and comparing performance\\n\",\n    \"\\n\",\n    \"## Prerequisites\\n\",\n    \"```bash\\n\",\n    \"pip install rich numpy\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 1: Performance Fundamentals and Setup\\n\",\n    \"\\n\",\n    \"**First Principle**: Performance optimization starts with measurement - profile first, optimize second.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `time.perf_counter()` for precise timing, `sys.getsizeof()` for memory measurement.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Essential imports for performance optimization\\n\",\n    \"import sys\\n\",\n    \"import time\\n\",\n    \"import gc\\n\",\n    \"import weakref\\n\",\n    \"import threading\\n\",\n    \"import multiprocessing as mp\\n\",\n    \"from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\\n\",\n    \"from typing import List, Dict, Any, Callable, Iterator, Optional, Tuple\\n\",\n    \"from dataclasses import dataclass\\n\",\n    \"from functools import lru_cache, partial\\n\",\n    \"import cProfile\\n\",\n    \"import pstats\\n\",\n    \"import tracemalloc\\n\",\n    \"from contextlib import contextmanager\\n\",\n    \"from collections import deque, defaultdict\\n\",\n    \"import numpy as np\\n\",\n    \"from rich.console import Console\\n\",\n    \"from rich.panel import Panel\\n\",\n    \"from rich.table import Table\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"‚ö° Performance Optimization Fundamentals\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"print(\\\"\\\"\\\"\\n\",\n    \"üîë Key Performance Principles:\\n\",\n    \"\\n\",\n    \"1. **Measure First**: Profile before optimizing\\n\",\n    \"2. **Bottleneck Focus**: Optimize the slowest parts\\n\",\n    \"3. **Memory Efficiency**: Reduce allocations and copies\\n\",\n    \"4. **Vectorization**: Use NumPy for numerical operations\\n\",\n    \"5. **Parallelization**: Leverage multiple cores\\n\",\n    \"6. **Caching**: Avoid redundant computations\\n\",\n    \"7. **Lazy Evaluation**: Compute only when needed\\n\",\n    \"\\n\",\n    \"üéØ Performance Areas for AI:\\n\",\n    \"‚Ä¢ Data loading and preprocessing\\n\",\n    \"‚Ä¢ Model inference and training\\n\",\n    \"‚Ä¢ Feature extraction and transformation\\n\",\n    \"‚Ä¢ Batch processing and streaming\\n\",\n    \"‚Ä¢ Memory management\\n\",\n    \"\\\"\\\"\\\")\\n\",\n    \"\\n\",\n    \"# Basic timing utilities\\n\",\n    \"@contextmanager\\n\",\n    \"def profile_time(operation_name: str = \\\"Operation\\\"):\\n\",\n    \"    \\\"\\\"\\\"Context manager for timing operations.\\\"\\\"\\\"\\n\",\n    \"    start_time = time.perf_counter()\\n\",\n    \"    yield\\n\",\n    \"    end_time = time.perf_counter()\\n\",\n    \"    duration = end_time - start_time\\n\",\n    \"    print(f\\\"‚è±Ô∏è  {operation_name}: {duration:.4f}s\\\")\\n\",\n    \"\\n\",\n    \"@contextmanager\\n\",\n    \"def profile_memory(operation_name: str = \\\"Operation\\\"):\\n\",\n    \"    \\\"\\\"\\\"Context manager for memory profiling.\\\"\\\"\\\"\\n\",\n    \"    tracemalloc.start()\\n\",\n    \"    start_memory = tracemalloc.get_traced_memory()[0]\\n\",\n    \"    \\n\",\n    \"    yield\\n\",\n    \"    \\n\",\n    \"    end_memory = tracemalloc.get_traced_memory()[0]\\n\",\n    \"    tracemalloc.stop()\\n\",\n    \"    \\n\",\n    \"    memory_used = (end_memory - start_memory) / (1024 * 1024)  # MB\\n\",\n    \"    print(f\\\"üß† {operation_name} memory: {memory_used:.2f} MB\\\")\\n\",\n    \"\\n\",\n    \"def compare_performance(functions: List[Tuple[str, Callable]], *args, **kwargs):\\n\",\n    \"    \\\"\\\"\\\"Compare performance of multiple functions.\\\"\\\"\\\"\\n\",\n    \"    results = []\\n\",\n    \"    \\n\",\n    \"    for name, func in functions:\\n\",\n    \"        # Warm up\\n\",\n    \"        try:\\n\",\n    \"            func(*args, **kwargs)\\n\",\n    \"        except:\\n\",\n    \"            pass\\n\",\n    \"        \\n\",\n    \"        # Measure\\n\",\n    \"        start_time = time.perf_counter()\\n\",\n    \"        try:\\n\",\n    \"            result = func(*args, **kwargs)\\n\",\n    \"            end_time = time.perf_counter()\\n\",\n    \"            duration = end_time - start_time\\n\",\n    \"            results.append((name, duration, \\\"success\\\", result))\\n\",\n    \"        except Exception as e:\\n\",\n    \"            end_time = time.perf_counter()\\n\",\n    \"            duration = end_time - start_time\\n\",\n    \"            results.append((name, duration, \\\"error\\\", str(e)))\\n\",\n    \"    \\n\",\n    \"    # Display results\\n\",\n    \"    print(f\\\"\\\\nüìä Performance Comparison:\\\")\\n\",\n    \"    fastest_time = min(r[1] for r in results if r[2] == \\\"success\\\")\\n\",\n    \"    \\n\",\n    \"    for name, duration, status, result in results:\\n\",\n    \"        if status == \\\"success\\\":\\n\",\n    \"            speedup = fastest_time / duration\\n\",\n    \"            print(f\\\"  {name}: {duration:.4f}s ({speedup:.1f}x)\\\")\\n\",\n    \"        else:\\n\",\n    \"            print(f\\\"  {name}: ERROR - {result}\\\")\\n\",\n    \"    \\n\",\n    \"    return results\\n\",\n    \"\\n\",\n    \"# Simple demonstration\\n\",\n    \"def demo_timing():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate basic timing.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Inefficient way\\n\",\n    \"    def slow_sum(n: int) -> int:\\n\",\n    \"        result = 0\\n\",\n    \"        for i in range(n):\\n\",\n    \"            result += i\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    # Efficient way\\n\",\n    \"    def fast_sum(n: int) -> int:\\n\",\n    \"        return n * (n - 1) // 2\\n\",\n    \"    \\n\",\n    \"    n = 1000000\\n\",\n    \"    \\n\",\n    \"    compare_performance([\\n\",\n    \"        (\\\"Slow loop\\\", slow_sum),\\n\",\n    \"        (\\\"Fast formula\\\", fast_sum)\\n\",\n    \"    ], n)\\n\",\n    \"\\n\",\n    \"demo_timing()\\n\",\n    \"print(\\\"\\\\n‚úÖ Performance toolkit ready!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 2: Memory-Efficient Data Structures\\n\",\n    \"\\n\",\n    \"**First Principle**: Memory efficiency is crucial for AI systems that process large datasets.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `deque`, `__slots__`, and custom memory-aware containers to reduce memory overhead.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class MemoryEfficientCache:\\n\",\n    \"    \\\"\\\"\\\"Memory-efficient LRU cache with size limits.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_size: int = 1000, max_memory_mb: float = 100.0):\\n\",\n    \"        self.max_size = max_size\\n\",\n    \"        self.max_memory_bytes = max_memory_mb * 1024 * 1024\\n\",\n    \"        self.cache = {}\\n\",\n    \"        self.access_order = deque()\\n\",\n    \"        self.current_memory = 0\\n\",\n    \"        self.hits = 0\\n\",\n    \"        self.misses = 0\\n\",\n    \"    \\n\",\n    \"    def _estimate_size(self, obj: Any) -> int:\\n\",\n    \"        \\\"\\\"\\\"Estimate object size in bytes.\\\"\\\"\\\"\\n\",\n    \"        return sys.getsizeof(obj)\\n\",\n    \"    \\n\",\n    \"    def _evict_if_needed(self):\\n\",\n    \"        \\\"\\\"\\\"Evict items if cache is too large.\\\"\\\"\\\"\\n\",\n    \"        while (len(self.cache) > self.max_size or \\n\",\n    \"               self.current_memory > self.max_memory_bytes):\\n\",\n    \"            if not self.access_order:\\n\",\n    \"                break\\n\",\n    \"            \\n\",\n    \"            key = self.access_order.popleft()\\n\",\n    \"            if key in self.cache:\\n\",\n    \"                value = self.cache.pop(key)\\n\",\n    \"                self.current_memory -= self._estimate_size(value)\\n\",\n    \"    \\n\",\n    \"    def get(self, key: str) -> Optional[Any]:\\n\",\n    \"        \\\"\\\"\\\"Get item from cache.\\\"\\\"\\\"\\n\",\n    \"        if key in self.cache:\\n\",\n    \"            # Move to end (most recent)\\n\",\n    \"            self.access_order.remove(key)\\n\",\n    \"            self.access_order.append(key)\\n\",\n    \"            self.hits += 1\\n\",\n    \"            return self.cache[key]\\n\",\n    \"        \\n\",\n    \"        self.misses += 1\\n\",\n    \"        return None\\n\",\n    \"    \\n\",\n    \"    def put(self, key: str, value: Any) -> None:\\n\",\n    \"        \\\"\\\"\\\"Put item in cache.\\\"\\\"\\\"\\n\",\n    \"        if key in self.cache:\\n\",\n    \"            # Update existing\\n\",\n    \"            old_value = self.cache[key]\\n\",\n    \"            self.current_memory -= self._estimate_size(old_value)\\n\",\n    \"            self.access_order.remove(key)\\n\",\n    \"        \\n\",\n    \"        size = self._estimate_size(value)\\n\",\n    \"        self.cache[key] = value\\n\",\n    \"        self.access_order.append(key)\\n\",\n    \"        self.current_memory += size\\n\",\n    \"        \\n\",\n    \"        self._evict_if_needed()\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get cache statistics.\\\"\\\"\\\"\\n\",\n    \"        total_requests = self.hits + self.misses\\n\",\n    \"        hit_ratio = self.hits / total_requests if total_requests > 0 else 0\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            \\\"size\\\": len(self.cache),\\n\",\n    \"            \\\"max_size\\\": self.max_size,\\n\",\n    \"            \\\"memory_mb\\\": self.current_memory / (1024 * 1024),\\n\",\n    \"            \\\"max_memory_mb\\\": self.max_memory_bytes / (1024 * 1024),\\n\",\n    \"            \\\"hit_ratio\\\": hit_ratio,\\n\",\n    \"            \\\"hits\\\": self.hits,\\n\",\n    \"            \\\"misses\\\": self.misses\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"class ObjectPool:\\n\",\n    \"    \\\"\\\"\\\"Object pool for expensive-to-create objects.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, factory: Callable, max_size: int = 10):\\n\",\n    \"        self.factory = factory\\n\",\n    \"        self.max_size = max_size\\n\",\n    \"        self.available = deque()\\n\",\n    \"        self.in_use = weakref.WeakSet()\\n\",\n    \"        self.total_created = 0\\n\",\n    \"        self.total_acquired = 0\\n\",\n    \"        self.total_released = 0\\n\",\n    \"    \\n\",\n    \"    def acquire(self):\\n\",\n    \"        \\\"\\\"\\\"Acquire object from pool.\\\"\\\"\\\"\\n\",\n    \"        self.total_acquired += 1\\n\",\n    \"        \\n\",\n    \"        if self.available:\\n\",\n    \"            obj = self.available.popleft()\\n\",\n    \"        else:\\n\",\n    \"            if self.total_created < self.max_size:\\n\",\n    \"                obj = self.factory()\\n\",\n    \"                self.total_created += 1\\n\",\n    \"            else:\\n\",\n    \"                # Create temporary object (not pooled)\\n\",\n    \"                obj = self.factory()\\n\",\n    \"        \\n\",\n    \"        self.in_use.add(obj)\\n\",\n    \"        return obj\\n\",\n    \"    \\n\",\n    \"    def release(self, obj):\\n\",\n    \"        \\\"\\\"\\\"Release object back to pool.\\\"\\\"\\\"\\n\",\n    \"        self.total_released += 1\\n\",\n    \"        \\n\",\n    \"        if obj in self.in_use:\\n\",\n    \"            self.in_use.remove(obj)\\n\",\n    \"            if len(self.available) < self.max_size:\\n\",\n    \"                # Reset object state if needed\\n\",\n    \"                if hasattr(obj, 'reset'):\\n\",\n    \"                    obj.reset()\\n\",\n    \"                self.available.append(obj)\\n\",\n    \"    \\n\",\n    \"    @contextmanager\\n\",\n    \"    def borrow(self):\\n\",\n    \"        \\\"\\\"\\\"Context manager for automatic object management.\\\"\\\"\\\"\\n\",\n    \"        obj = self.acquire()\\n\",\n    \"        try:\\n\",\n    \"            yield obj\\n\",\n    \"        finally:\\n\",\n    \"            self.release(obj)\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"available\\\": len(self.available),\\n\",\n    \"            \\\"in_use\\\": len(self.in_use),\\n\",\n    \"            \\\"total_created\\\": self.total_created,\\n\",\n    \"            \\\"total_acquired\\\": self.total_acquired,\\n\",\n    \"            \\\"total_released\\\": self.total_released,\\n\",\n    \"            \\\"max_size\\\": self.max_size,\\n\",\n    \"            \\\"pool_efficiency\\\": len(self.available) / self.max_size if self.max_size > 0 else 0\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"# Memory-efficient classes using __slots__\\n\",\n    \"class EfficientDataPoint:\\n\",\n    \"    \\\"\\\"\\\"Memory-efficient data point using __slots__.\\\"\\\"\\\"\\n\",\n    \"    __slots__ = ['x', 'y', 'label', 'confidence']\\n\",\n    \"    \\n\",\n    \"    def __init__(self, x: float, y: float, label: str, confidence: float):\\n\",\n    \"        self.x = x\\n\",\n    \"        self.y = y\\n\",\n    \"        self.label = label\\n\",\n    \"        self.confidence = confidence\\n\",\n    \"\\n\",\n    \"class RegularDataPoint:\\n\",\n    \"    \\\"\\\"\\\"Regular data point without __slots__.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, x: float, y: float, label: str, confidence: float):\\n\",\n    \"        self.x = x\\n\",\n    \"        self.y = y\\n\",\n    \"        self.label = label\\n\",\n    \"        self.confidence = confidence\\n\",\n    \"\\n\",\n    \"def demonstrate_memory_efficient_structures():\\n\",\n    \"    \\\"\\\"\\\"Show memory-efficient data structures.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üß† Memory-Efficient Structures\\\", style=\\\"bold blue\\\"))\\n\",\n    \"    \\n\",\n    \"    # Test memory-efficient cache\\n\",\n    \"    print(\\\"üíæ Testing memory-efficient cache:\\\")\\n\",\n    \"    \\n\",\n    \"    cache = MemoryEfficientCache(max_size=5, max_memory_mb=0.1)\\n\",\n    \"    \\n\",\n    \"    # Add items to cache\\n\",\n    \"    test_data = [f\\\"large_data_item_{i}\\\" * 100 for i in range(10)]\\n\",\n    \"    \\n\",\n    \"    for i, data in enumerate(test_data):\\n\",\n    \"        cache.put(f\\\"key_{i}\\\", data)\\n\",\n    \"        stats = cache.get_stats()\\n\",\n    \"        print(f\\\"  Added item {i}: cache size={stats['size']}, memory={stats['memory_mb']:.3f}MB\\\")\\n\",\n    \"    \\n\",\n    \"    # Test retrieval\\n\",\n    \"    print(f\\\"\\\\nüîç Cache retrieval test:\\\")\\n\",\n    \"    for i in [0, 3, 7, 9]:\\n\",\n    \"        result = cache.get(f\\\"key_{i}\\\")\\n\",\n    \"        status = \\\"HIT\\\" if result else \\\"MISS\\\"\\n\",\n    \"        print(f\\\"  key_{i}: {status}\\\")\\n\",\n    \"    \\n\",\n    \"    final_stats = cache.get_stats()\\n\",\n    \"    print(f\\\"\\\\nCache final stats: {final_stats}\\\")\\n\",\n    \"    \\n\",\n    \"    # Test object pool\\n\",\n    \"    print(f\\\"\\\\nüèä Testing object pool:\\\")\\n\",\n    \"    \\n\",\n    \"    class ExpensiveResource:\\n\",\n    \"        def __init__(self):\\n\",\n    \"            time.sleep(0.01)  # Simulate expensive creation\\n\",\n    \"            self.created_at = time.time()\\n\",\n    \"            self.usage_count = 0\\n\",\n    \"        \\n\",\n    \"        def reset(self):\\n\",\n    \"            self.usage_count = 0\\n\",\n    \"        \\n\",\n    \"        def use(self):\\n\",\n    \"            self.usage_count += 1\\n\",\n    \"            return f\\\"Used {self.usage_count} times\\\"\\n\",\n    \"    \\n\",\n    \"    pool = ObjectPool(ExpensiveResource, max_size=3)\\n\",\n    \"    \\n\",\n    \"    # Use objects from pool\\n\",\n    \"    with profile_time(\\\"Object pool operations\\\"):\\n\",\n    \"        for i in range(5):\\n\",\n    \"            with pool.borrow() as obj:\\n\",\n    \"                result = obj.use()\\n\",\n    \"                print(f\\\"  Operation {i}: {result}\\\")\\n\",\n    \"    \\n\",\n    \"    pool_stats = pool.get_stats()\\n\",\n    \"    print(f\\\"Pool stats: {pool_stats}\\\")\\n\",\n    \"    \\n\",\n    \"    # Compare __slots__ vs regular classes\\n\",\n    \"    print(f\\\"\\\\nüìè Memory comparison (__slots__ vs regular):\\\")\\n\",\n    \"    \\n\",\n    \"    # Create instances\\n\",\n    \"    efficient_point = EfficientDataPoint(1.0, 2.0, \\\"positive\\\", 0.95)\\n\",\n    \"    regular_point = RegularDataPoint(1.0, 2.0, \\\"positive\\\", 0.95)\\n\",\n    \"    \\n\",\n    \"    efficient_size = sys.getsizeof(efficient_point)\\n\",\n    \"    regular_size = sys.getsizeof(regular_point)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  EfficientDataPoint (__slots__): {efficient_size} bytes\\\")\\n\",\n    \"    print(f\\\"  RegularDataPoint: {regular_size} bytes\\\")\\n\",\n    \"    print(f\\\"  Memory savings: {((regular_size - efficient_size) / regular_size) * 100:.1f}%\\\")\\n\",\n    \"    \\n\",\n    \"    # Test with many objects\\n\",\n    \"    n_objects = 10000\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"Creating efficient objects\\\"):\\n\",\n    \"        efficient_objects = [\\n\",\n    \"            EfficientDataPoint(i, i*2, f\\\"label_{i%3}\\\", i/n_objects)\\n\",\n    \"            for i in range(n_objects)\\n\",\n    \"        ]\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"Creating regular objects\\\"):\\n\",\n    \"        regular_objects = [\\n\",\n    \"            RegularDataPoint(i, i*2, f\\\"label_{i%3}\\\", i/n_objects)\\n\",\n    \"            for i in range(n_objects)\\n\",\n    \"        ]\\n\",\n    \"    \\n\",\n    \"    # Clean up\\n\",\n    \"    del efficient_objects, regular_objects\\n\",\n    \"    gc.collect()\\n\",\n    \"\\n\",\n    \"demonstrate_memory_efficient_structures()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Memory Efficiency Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reduced memory usage with __slots__\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Smart caching with size and memory limits\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Object pooling for expensive resources\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Efficient data structures (deque, weakref)\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 3: Performance Profiling and Monitoring\\n\",\n    \"\\n\",\n    \"**First Principle**: Measure performance to identify bottlenecks before optimizing.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `cProfile`, `tracemalloc`, and custom monitoring for comprehensive profiling.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class PerformanceMonitor:\\n\",\n    \"    \\\"\\\"\\\"Monitor for tracking performance metrics.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.metrics = defaultdict(list)\\n\",\n    \"        self.counters = defaultdict(int)\\n\",\n    \"        self.start_times = {}\\n\",\n    \"    \\n\",\n    \"    def start_timer(self, operation: str):\\n\",\n    \"        \\\"\\\"\\\"Start timing an operation.\\\"\\\"\\\"\\n\",\n    \"        self.start_times[operation] = time.perf_counter()\\n\",\n    \"    \\n\",\n    \"    def stop_timer(self, operation: str):\\n\",\n    \"        \\\"\\\"\\\"Stop timing an operation and record the duration.\\\"\\\"\\\"\\n\",\n    \"        if operation in self.start_times:\\n\",\n    \"            duration = time.perf_counter() - self.start_times[operation]\\n\",\n    \"            self.record_time(operation, duration)\\n\",\n    \"            del self.start_times[operation]\\n\",\n    \"            return duration\\n\",\n    \"        return 0.0\\n\",\n    \"    \\n\",\n    \"    def record_time(self, operation: str, duration: float):\\n\",\n    \"        \\\"\\\"\\\"Record operation timing.\\\"\\\"\\\"\\n\",\n    \"        self.metrics[f\\\"{operation}_time\\\"].append(duration)\\n\",\n    \"        self.counters[f\\\"{operation}_count\\\"] += 1\\n\",\n    \"    \\n\",\n    \"    def record_memory(self, operation: str, memory_mb: float):\\n\",\n    \"        \\\"\\\"\\\"Record memory usage.\\\"\\\"\\\"\\n\",\n    \"        self.metrics[f\\\"{operation}_memory\\\"].append(memory_mb)\\n\",\n    \"    \\n\",\n    \"    def increment_counter(self, name: str, value: int = 1):\\n\",\n    \"        \\\"\\\"\\\"Increment a counter.\\\"\\\"\\\"\\n\",\n    \"        self.counters[name] += value\\n\",\n    \"    \\n\",\n    \"    @contextmanager\\n\",\n    \"    def time_operation(self, operation: str):\\n\",\n    \"        \\\"\\\"\\\"Context manager for timing operations.\\\"\\\"\\\"\\n\",\n    \"        self.start_timer(operation)\\n\",\n    \"        try:\\n\",\n    \"            yield\\n\",\n    \"        finally:\\n\",\n    \"            self.stop_timer(operation)\\n\",\n    \"    \\n\",\n    \"    def get_summary(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get performance summary.\\\"\\\"\\\"\\n\",\n    \"        summary = {}\\n\",\n    \"        \\n\",\n    \"        for metric_name, values in self.metrics.items():\\n\",\n    \"            if values:\\n\",\n    \"                summary[metric_name] = {\\n\",\n    \"                    \\\"avg\\\": sum(values) / len(values),\\n\",\n    \"                    \\\"min\\\": min(values),\\n\",\n    \"                    \\\"max\\\": max(values),\\n\",\n    \"                    \\\"count\\\": len(values),\\n\",\n    \"                    \\\"total\\\": sum(values)\\n\",\n    \"                }\\n\",\n    \"        \\n\",\n    \"        summary[\\\"counters\\\"] = dict(self.counters)\\n\",\n    \"        return summary\\n\",\n    \"\\n\",\n    \"# Example AI processing functions for profiling\\n\",\n    \"def simulate_data_loading(size: int) -> List[Dict[str, Any]]:\\n\",\n    \"    \\\"\\\"\\\"Simulate loading data.\\\"\\\"\\\"\\n\",\n    \"    return [{\\n\",\n    \"        \\\"id\\\": i,\\n\",\n    \"        \\\"features\\\": [j * 0.1 for j in range(10)],\\n\",\n    \"        \\\"label\\\": i % 3,\\n\",\n    \"        \\\"metadata\\\": {\\\"source\\\": f\\\"dataset_{i//100}\\\"}\\n\",\n    \"    } for i in range(size)]\\n\",\n    \"\\n\",\n    \"def simulate_feature_extraction(data_point: Dict[str, Any]) -> List[float]:\\n\",\n    \"    \\\"\\\"\\\"Simulate feature extraction.\\\"\\\"\\\"\\n\",\n    \"    features = data_point[\\\"features\\\"]\\n\",\n    \"    # Simulate some processing\\n\",\n    \"    processed = []\\n\",\n    \"    for f in features:\\n\",\n    \"        processed.append(f * f)  # Square\\n\",\n    \"        processed.append(f * 2)  # Double\\n\",\n    \"    return processed\\n\",\n    \"\\n\",\n    \"def simulate_model_inference(features: List[float]) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Simulate model inference.\\\"\\\"\\\"\\n\",\n    \"    # Simulate computation\\n\",\n    \"    score = sum(features) / len(features) if features else 0\\n\",\n    \"    prediction = \\\"positive\\\" if score > 0.5 else \\\"negative\\\"\\n\",\n    \"    confidence = min(abs(score), 1.0)\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"prediction\\\": prediction,\\n\",\n    \"        \\\"confidence\\\": confidence,\\n\",\n    \"        \\\"score\\\": score\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"@lru_cache(maxsize=1000)\\n\",\n    \"def cached_expensive_computation(data_hash: str) -> float:\\n\",\n    \"    \\\"\\\"\\\"Expensive computation with caching.\\\"\\\"\\\"\\n\",\n    \"    # Simulate expensive computation\\n\",\n    \"    result = 0.0\\n\",\n    \"    for i in range(1000):\\n\",\n    \"        result += hash(data_hash + str(i)) % 100 / 100.0\\n\",\n    \"    return result\\n\",\n    \"\\n\",\n    \"def demonstrate_profiling():\\n\",\n    \"    \\\"\\\"\\\"Show profiling techniques.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üìä Performance Profiling\\\", style=\\\"bold green\\\"))\\n\",\n    \"    \\n\",\n    \"    monitor = PerformanceMonitor()\\n\",\n    \"    \\n\",\n    \"    print(\\\"üîç Profiling AI processing pipeline:\\\")\\n\",\n    \"    \\n\",\n    \"    # Simulate processing pipeline\\n\",\n    \"    with monitor.time_operation(\\\"data_loading\\\"):\\n\",\n    \"        data = simulate_data_loading(1000)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Loaded {len(data)} data points\\\")\\n\",\n    \"    \\n\",\n    \"    # Process data with detailed monitoring\\n\",\n    \"    results = []\\n\",\n    \"    \\n\",\n    \"    for i, data_point in enumerate(data[:100]):  # Process subset\\n\",\n    \"        # Feature extraction\\n\",\n    \"        with monitor.time_operation(\\\"feature_extraction\\\"):\\n\",\n    \"            features = simulate_feature_extraction(data_point)\\n\",\n    \"        \\n\",\n    \"        # Model inference\\n\",\n    \"        with monitor.time_operation(\\\"model_inference\\\"):\\n\",\n    \"            prediction = simulate_model_inference(features)\\n\",\n    \"        \\n\",\n    \"        # Expensive computation (with caching)\\n\",\n    \"        data_hash = str(hash(str(data_point[\\\"features\\\"])))\\n\",\n    \"        with monitor.time_operation(\\\"expensive_computation\\\"):\\n\",\n    \"            expensive_result = cached_expensive_computation(data_hash)\\n\",\n    \"        \\n\",\n    \"        monitor.increment_counter(\\\"processed_items\\\")\\n\",\n    \"        \\n\",\n    \"        results.append({\\n\",\n    \"            \\\"prediction\\\": prediction,\\n\",\n    \"            \\\"expensive_result\\\": expensive_result\\n\",\n    \"        })\\n\",\n    \"    \\n\",\n    \"    # Show performance summary\\n\",\n    \"    summary = monitor.get_summary()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìà Performance Summary:\\\")\\n\",\n    \"    for metric, stats in summary.items():\\n\",\n    \"        if isinstance(stats, dict) and 'avg' in stats:\\n\",\n    \"            print(f\\\"  {metric}: avg={stats['avg']:.4f}s, min={stats['min']:.4f}s, max={stats['max']:.4f}s, count={stats['count']}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüî¢ Counters: {summary.get('counters', {})}\\\")\\n\",\n    \"    \\n\",\n    \"    # Cache performance\\n\",\n    \"    cache_info = cached_expensive_computation.cache_info()\\n\",\n    \"    print(f\\\"\\\\nüíæ Cache performance: {cache_info}\\\")\\n\",\n    \"    \\n\",\n    \"    # Profile memory usage of different operations\\n\",\n    \"    print(f\\\"\\\\nüß† Memory profiling different operations:\\\")\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"Large list creation\\\"):\\n\",\n    \"        large_list = [i for i in range(100000)]\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"Dictionary creation\\\"):\\n\",\n    \"        large_dict = {f\\\"key_{i}\\\": i for i in range(10000)}\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"NumPy array creation\\\"):\\n\",\n    \"        large_array = np.arange(100000)\\n\",\n    \"    \\n\",\n    \"    # Clean up\\n\",\n    \"    del large_list, large_dict, large_array\\n\",\n    \"    gc.collect()\\n\",\n    \"\\n\",\n    \"# Advanced profiling with cProfile\\n\",\n    \"def demonstrate_cprofile():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate cProfile usage.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\n‚ö° Advanced profiling with cProfile:\\\")\\n\",\n    \"    \\n\",\n    \"    def profile_target():\\n\",\n    \"        \\\"\\\"\\\"Function to profile.\\\"\\\"\\\"\\n\",\n    \"        data = simulate_data_loading(500)\\n\",\n    \"        \\n\",\n    \"        results = []\\n\",\n    \"        for item in data[:50]:\\n\",\n    \"            features = simulate_feature_extraction(item)\\n\",\n    \"            prediction = simulate_model_inference(features)\\n\",\n    \"            results.append(prediction)\\n\",\n    \"        \\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    # Profile with cProfile\\n\",\n    \"    profiler = cProfile.Profile()\\n\",\n    \"    profiler.enable()\\n\",\n    \"    \\n\",\n    \"    start_time = time.perf_counter()\\n\",\n    \"    results = profile_target()\\n\",\n    \"    duration = time.perf_counter() - start_time\\n\",\n    \"    \\n\",\n    \"    profiler.disable()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Processed {len(results)} items in {duration:.4f}s\\\")\\n\",\n    \"    print(f\\\"  Rate: {len(results) / duration:.1f} items/second\\\")\\n\",\n    \"    \\n\",\n    \"    # Analyze profiling results\\n\",\n    \"    stats = pstats.Stats(profiler)\\n\",\n    \"    stats.sort_stats('cumulative')\\n\",\n    \"    \\n\",\n    \"    # Get top functions by cumulative time\\n\",\n    \"    print(f\\\"\\\\nüìä Top functions by cumulative time:\\\")\\n\",\n    \"    stats.print_stats(10)  # Top 10 functions\\n\",\n    \"\\n\",\n    \"# Benchmark different approaches\\n\",\n    \"def demonstrate_benchmarking():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate benchmarking different approaches.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüèÅ Benchmarking different approaches:\\\")\\n\",\n    \"    \\n\",\n    \"    data = list(range(10000))\\n\",\n    \"    \\n\",\n    \"    # Different ways to process data\\n\",\n    \"    def approach_1_loop(data):\\n\",\n    \"        \\\"\\\"\\\"Traditional loop.\\\"\\\"\\\"\\n\",\n    \"        result = []\\n\",\n    \"        for item in data:\\n\",\n    \"            result.append(item * 2)\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    def approach_2_comprehension(data):\\n\",\n    \"        \\\"\\\"\\\"List comprehension.\\\"\\\"\\\"\\n\",\n    \"        return [item * 2 for item in data]\\n\",\n    \"    \\n\",\n    \"    def approach_3_map(data):\\n\",\n    \"        \\\"\\\"\\\"Map function.\\\"\\\"\\\"\\n\",\n    \"        return list(map(lambda x: x * 2, data))\\n\",\n    \"    \\n\",\n    \"    def approach_4_numpy(data):\\n\",\n    \"        \\\"\\\"\\\"NumPy vectorized.\\\"\\\"\\\"\\n\",\n    \"        arr = np.array(data)\\n\",\n    \"        return (arr * 2).tolist()\\n\",\n    \"    \\n\",\n    \"    # Benchmark all approaches\\n\",\n    \"    approaches = [\\n\",\n    \"        (\\\"Traditional loop\\\", approach_1_loop),\\n\",\n    \"        (\\\"List comprehension\\\", approach_2_comprehension),\\n\",\n    \"        (\\\"Map function\\\", approach_3_map),\\n\",\n    \"        (\\\"NumPy vectorized\\\", approach_4_numpy)\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    compare_performance(approaches, data)\\n\",\n    \"\\n\",\n    \"# Run profiling demonstrations\\n\",\n    \"demonstrate_profiling()\\n\",\n    \"demonstrate_cprofile()\\n\",\n    \"demonstrate_benchmarking()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Profiling Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Identifies performance bottlenecks\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Measures memory usage accurately\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Tracks cache effectiveness\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Compares different approaches objectively\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 4: Vectorized Operations with NumPy\\n\",\n    \"\\n\",\n    \"**First Principle**: Vectorized operations eliminate Python loops and leverage optimized C code for massive speedups.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use NumPy arrays and vectorized functions instead of Python loops for numerical computations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class VectorizedTextProcessor:\\n\",\n    \"    \\\"\\\"\\\"Text processor using vectorized operations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def batch_tokenize(texts: List[str]) -> List[List[str]]:\\n\",\n    \"        \\\"\\\"\\\"Vectorized tokenization.\\\"\\\"\\\"\\n\",\n    \"        # This is still Python-level, but demonstrates batch processing\\n\",\n    \"        return [text.lower().split() for text in texts]\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def batch_compute_lengths(texts: List[str]) -> np.ndarray:\\n\",\n    \"        \\\"\\\"\\\"Compute text lengths using NumPy.\\\"\\\"\\\"\\n\",\n    \"        return np.array([len(text) for text in texts])\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def batch_filter_by_length(texts: List[str], min_length: int) -> List[str]:\\n\",\n    \"        \\\"\\\"\\\"Filter texts by length using vectorized operations.\\\"\\\"\\\"\\n\",\n    \"        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\\n\",\n    \"        mask = lengths >= min_length\\n\",\n    \"        return [text for text, keep in zip(texts, mask) if keep]\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def batch_similarity_matrix(texts: List[str]) -> np.ndarray:\\n\",\n    \"        \\\"\\\"\\\"Compute similarity matrix using vectorized operations.\\\"\\\"\\\"\\n\",\n    \"        n = len(texts)\\n\",\n    \"        \\n\",\n    \"        # Get lengths as vectors\\n\",\n    \"        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\\n\",\n    \"        \\n\",\n    \"        # Compute pairwise length differences using broadcasting\\n\",\n    \"        length_diff = np.abs(lengths[:, np.newaxis] - lengths[np.newaxis, :])\\n\",\n    \"        \\n\",\n    \"        # Similarity based on length difference (simplified)\\n\",\n    \"        similarity = 1.0 / (1.0 + length_diff)\\n\",\n    \"        \\n\",\n    \"        # Set diagonal to 1.0 (self-similarity)\\n\",\n    \"        np.fill_diagonal(similarity, 1.0)\\n\",\n    \"        \\n\",\n    \"        return similarity\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def compute_text_features(texts: List[str]) -> np.ndarray:\\n\",\n    \"        \\\"\\\"\\\"Compute multiple text features efficiently.\\\"\\\"\\\"\\n\",\n    \"        n = len(texts)\\n\",\n    \"        features = np.zeros((n, 4))  # 4 features per text\\n\",\n    \"        \\n\",\n    \"        for i, text in enumerate(texts):\\n\",\n    \"            words = text.split()\\n\",\n    \"            features[i, 0] = len(text)  # Character count\\n\",\n    \"            features[i, 1] = len(words)  # Word count\\n\",\n    \"            features[i, 2] = len(set(words)) if words else 0  # Unique words\\n\",\n    \"            features[i, 3] = sum(len(word) for word in words) / len(words) if words else 0  # Avg word length\\n\",\n    \"        \\n\",\n    \"        return features\\n\",\n    \"\\n\",\n    \"def demonstrate_vectorization():\\n\",\n    \"    \\\"\\\"\\\"Show vectorized operations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"‚ö° Vectorized Operations\\\", style=\\\"bold magenta\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create test data\\n\",\n    \"    test_texts = [f\\\"Sample text number {i} for vectorized processing analysis\\\" for i in range(1000)]\\n\",\n    \"    \\n\",\n    \"    print(\\\"üìä Comparing scalar vs vectorized operations:\\\")\\n\",\n    \"    \\n\",\n    \"    # Scalar operations (Python loops)\\n\",\n    \"    with profile_time(\\\"Scalar length computation\\\"):\\n\",\n    \"        scalar_lengths = [len(text) for text in test_texts]\\n\",\n    \"    \\n\",\n    \"    # Vectorized operations\\n\",\n    \"    with profile_time(\\\"Vectorized length computation\\\"):\\n\",\n    \"        vector_lengths = VectorizedTextProcessor.batch_compute_lengths(test_texts)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results match: {np.array_equal(scalar_lengths, vector_lengths)}\\\")\\n\",\n    \"    \\n\",\n    \"    # Mathematical operations comparison\\n\",\n    \"    print(f\\\"\\\\nüî¢ Mathematical operations comparison:\\\")\\n\",\n    \"    \\n\",\n    \"    data = list(range(100000))\\n\",\n    \"    \\n\",\n    \"    # Scalar math\\n\",\n    \"    with profile_time(\\\"Scalar math (sum of squares)\\\"):\\n\",\n    \"        scalar_result = sum(x*x for x in data)\\n\",\n    \"    \\n\",\n    \"    # Vectorized math\\n\",\n    \"    with profile_time(\\\"Vectorized math (sum of squares)\\\"):\\n\",\n    \"        arr = np.array(data)\\n\",\n    \"        vector_result = np.sum(arr**2)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results match: {scalar_result == vector_result}\\\")\\n\",\n    \"    \\n\",\n    \"    # Complex vectorized operations\\n\",\n    \"    print(f\\\"\\\\nüßÆ Complex vectorized operations:\\\")\\n\",\n    \"    \\n\",\n    \"    # Text feature extraction\\n\",\n    \"    sample_texts = test_texts[:100]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Feature extraction\\\"):\\n\",\n    \"        features = VectorizedTextProcessor.compute_text_features(sample_texts)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Extracted features for {len(sample_texts)} texts\\\")\\n\",\n    \"    print(f\\\"  Feature matrix shape: {features.shape}\\\")\\n\",\n    \"    print(f\\\"  Average features: {np.mean(features, axis=0)}\\\")\\n\",\n    \"    \\n\",\n    \"    # Similarity matrix computation\\n\",\n    \"    small_sample = test_texts[:20]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Similarity matrix computation\\\"):\\n\",\n    \"        similarity_matrix = VectorizedTextProcessor.batch_similarity_matrix(small_sample)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Similarity matrix shape: {similarity_matrix.shape}\\\")\\n\",\n    \"    print(f\\\"  Average similarity: {np.mean(similarity_matrix[similarity_matrix != 1.0]):.3f}\\\")\\n\",\n    \"    \\n\",\n    \"    # Broadcasting examples\\n\",\n    \"    print(f\\\"\\\\nüì° Broadcasting examples:\\\")\\n\",\n    \"    \\n\",\n    \"    # Create sample matrices\\n\",\n    \"    matrix_a = np.random.rand(1000, 50)\\n\",\n    \"    matrix_b = np.random.rand(50)\\n\",\n    \"    \\n\",\n    \"    # Broadcasting operation\\n\",\n    \"    with profile_time(\\\"Broadcasting operation (1000x50 + 50)\\\"):\\n\",\n    \"        result = matrix_a + matrix_b  # Broadcasting automatically applied\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Result shape: {result.shape}\\\")\\n\",\n    \"    \\n\",\n    \"    # Element-wise operations\\n\",\n    \"    large_array = np.random.rand(100000)\\n\",\n    \"    \\n\",\n    \"    operations = [\\n\",\n    \"        (\\\"Square\\\", lambda x: x**2),\\n\",\n    \"        (\\\"Square root\\\", lambda x: np.sqrt(x)),\\n\",\n    \"        (\\\"Exponential\\\", lambda x: np.exp(x)),\\n\",\n    \"        (\\\"Logarithm\\\", lambda x: np.log(x + 1e-8)),  # Add small value to avoid log(0)\\n\",\n    \"        (\\\"Trigonometric\\\", lambda x: np.sin(x) + np.cos(x))\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\n‚ö° Element-wise operations on {len(large_array)} elements:\\\")\\n\",\n    \"    \\n\",\n    \"    for name, operation in operations:\\n\",\n    \"        with profile_time(f\\\"{name} operation\\\"):\\n\",\n    \"            result = operation(large_array)\\n\",\n    \"        print(f\\\"  {name}: min={np.min(result):.3f}, max={np.max(result):.3f}, mean={np.mean(result):.3f}\\\")\\n\",\n    \"\\n\",\n    \"# Advanced vectorization patterns\\n\",\n    \"def demonstrate_advanced_vectorization():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate advanced vectorization patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüöÄ Advanced vectorization patterns:\\\")\\n\",\n    \"    \\n\",\n    \"    # Conditional operations with where\\n\",\n    \"    data = np.random.randn(10000)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Conditional operations with np.where\\\"):\\n\",\n    \"        # Replace negative values with 0, keep positive\\n\",\n    \"        result = np.where(data > 0, data, 0)\\n\",\n    \"    \\n\",\n    \"    positive_count = np.sum(result > 0)\\n\",\n    \"    print(f\\\"  Positive values: {positive_count}/{len(data)}\\\")\\n\",\n    \"    \\n\",\n    \"    # Vectorized string operations\\n\",\n    \"    text_data = np.array([f\\\"text_{i}\\\" for i in range(1000)])\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Vectorized string operations\\\"):\\n\",\n    \"        # Vectorized string length\\n\",\n    \"        str_lengths = np.char.str_len(text_data)\\n\",\n    \"        # Vectorized string operations\\n\",\n    \"        upper_texts = np.char.upper(text_data)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  String operations completed on {len(text_data)} strings\\\")\\n\",\n    \"    \\n\",\n    \"    # Aggregation operations\\n\",\n    \"    matrix = np.random.rand(1000, 100)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Matrix aggregations\\\"):\\n\",\n    \"        row_sums = np.sum(matrix, axis=1)  # Sum each row\\n\",\n    \"        col_means = np.mean(matrix, axis=0)  # Mean of each column\\n\",\n    \"        global_std = np.std(matrix)  # Standard deviation of entire matrix\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Row sums: {len(row_sums)}, Col means: {len(col_means)}, Global std: {global_std:.3f}\\\")\\n\",\n    \"    \\n\",\n    \"    # Fancy indexing\\n\",\n    \"    large_data = np.random.rand(100000)\\n\",\n    \"    indices = np.random.randint(0, len(large_data), 10000)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Fancy indexing\\\"):\\n\",\n    \"        selected_data = large_data[indices]\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Selected {len(selected_data)} elements using fancy indexing\\\")\\n\",\n    \"    \\n\",\n    \"    # Boolean indexing\\n\",\n    \"    with profile_time(\\\"Boolean indexing\\\"):\\n\",\n    \"        filtered_data = large_data[large_data > 0.5]\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Filtered to {len(filtered_data)} elements using boolean indexing\\\")\\n\",\n    \"\\n\",\n    \"# Memory layout optimization\\n\",\n    \"def demonstrate_memory_layout():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate memory layout optimization.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüíæ Memory layout optimization:\\\")\\n\",\n    \"    \\n\",\n    \"    # C-contiguous vs Fortran-contiguous\\n\",\n    \"    size = (1000, 1000)\\n\",\n    \"    \\n\",\n    \"    c_array = np.random.rand(*size)  # C-contiguous (row-major)\\n\",\n    \"    f_array = np.asfortranarray(c_array)  # Fortran-contiguous (column-major)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  C-array is C-contiguous: {c_array.flags.c_contiguous}\\\")\\n\",\n    \"    print(f\\\"  F-array is Fortran-contiguous: {f_array.flags.f_contiguous}\\\")\\n\",\n    \"    \\n\",\n    \"    # Row-wise operations (faster on C-contiguous)\\n\",\n    \"    with profile_time(\\\"Row-wise sum on C-contiguous\\\"):\\n\",\n    \"        c_row_sums = np.sum(c_array, axis=1)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Row-wise sum on F-contiguous\\\"):\\n\",\n    \"        f_row_sums = np.sum(f_array, axis=1)\\n\",\n    \"    \\n\",\n    \"    # Column-wise operations (faster on Fortran-contiguous)\\n\",\n    \"    with profile_time(\\\"Column-wise sum on C-contiguous\\\"):\\n\",\n    \"        c_col_sums = np.sum(c_array, axis=0)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Column-wise sum on F-contiguous\\\"):\\n\",\n    \"        f_col_sums = np.sum(f_array, axis=0)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results are equal: {np.allclose(c_row_sums, f_row_sums) and np.allclose(c_col_sums, f_col_sums)}\\\")\\n\",\n    \"\\n\",\n    \"# Run vectorization demonstrations\\n\",\n    \"demonstrate_vectorization()\\n\",\n    \"demonstrate_advanced_vectorization()\\n\",\n    \"demonstrate_memory_layout()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Vectorization Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ 10-100x speedup for numerical operations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Memory-efficient operations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Automatic parallelization in many cases\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Leverages optimized BLAS/LAPACK libraries\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 5: Parallel Processing Patterns\\n\",\n    \"\\n\",\n    \"**First Principle**: Parallel processing leverages multiple cores for CPU-bound tasks and multiple threads for I/O-bound tasks.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `ThreadPoolExecutor` for I/O-bound tasks, `ProcessPoolExecutor` for CPU-bound tasks.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class ParallelProcessor:\\n\",\n    \"    \\\"\\\"\\\"Utilities for parallel processing.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def parallel_map_threads(\\n\",\n    \"        func: Callable, \\n\",\n    \"        items: List[Any], \\n\",\n    \"        max_workers: int = 4\\n\",\n    \"    ) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Parallel map using threads (I/O bound tasks).\\\"\\\"\\\"\\n\",\n    \"        with ThreadPoolExecutor(max_workers=max_workers) as executor:\\n\",\n    \"            results = list(executor.map(func, items))\\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def parallel_map_processes(\\n\",\n    \"        func: Callable, \\n\",\n    \"        items: List[Any], \\n\",\n    \"        max_workers: int = None\\n\",\n    \"    ) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Parallel map using processes (CPU bound tasks).\\\"\\\"\\\"\\n\",\n    \"        if max_workers is None:\\n\",\n    \"            max_workers = mp.cpu_count()\\n\",\n    \"        \\n\",\n    \"        with ProcessPoolExecutor(max_workers=max_workers) as executor:\\n\",\n    \"            results = list(executor.map(func, items))\\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def parallel_batch_process(\\n\",\n    \"        items: List[Any], \\n\",\n    \"        batch_func: Callable[[List[Any]], List[Any]], \\n\",\n    \"        batch_size: int = 100,\\n\",\n    \"        max_workers: int = 4,\\n\",\n    \"        use_processes: bool = False\\n\",\n    \"    ) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Process items in parallel batches.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Split into batches\\n\",\n    \"        batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\\n\",\n    \"        \\n\",\n    \"        # Process batches in parallel\\n\",\n    \"        if use_processes:\\n\",\n    \"            with ProcessPoolExecutor(max_workers=max_workers) as executor:\\n\",\n    \"                batch_results = list(executor.map(batch_func, batches))\\n\",\n    \"        else:\\n\",\n    \"            with ThreadPoolExecutor(max_workers=max_workers) as executor:\\n\",\n    \"                batch_results = list(executor.map(batch_func, batches))\\n\",\n    \"        \\n\",\n    \"        # Flatten results\\n\",\n    \"        results = []\\n\",\n    \"        for batch_result in batch_results:\\n\",\n    \"            results.extend(batch_result)\\n\",\n    \"        \\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def parallel_as_completed(\\n\",\n    \"        func: Callable,\\n\",\n    \"        items: List[Any],\\n\",\n    \"        max_workers: int = 4,\\n\",\n    \"        use_processes: bool = False\\n\",\n    \"    ) -> Iterator[Tuple[Any, Any]]:\\n\",\n    \"        \\\"\\\"\\\"Process items in parallel, yielding results as they complete.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        executor_class = ProcessPoolExecutor if use_processes else ThreadPoolExecutor\\n\",\n    \"        \\n\",\n    \"        with executor_class(max_workers=max_workers) as executor:\\n\",\n    \"            # Submit all tasks\\n\",\n    \"            future_to_item = {executor.submit(func, item): item for item in items}\\n\",\n    \"            \\n\",\n    \"            # Yield results as they complete\\n\",\n    \"            for future in as_completed(future_to_item):\\n\",\n    \"                item = future_to_item[future]\\n\",\n    \"                try:\\n\",\n    \"                    result = future.result()\\n\",\n    \"                    yield item, result\\n\",\n    \"                except Exception as exc:\\n\",\n    \"                    yield item, exc\\n\",\n    \"\\n\",\n    \"# Example functions for parallel processing\\n\",\n    \"def cpu_intensive_task(data: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"CPU-intensive task for parallel processing demo.\\\"\\\"\\\"\\n\",\n    \"    # Simulate CPU work with hashing\\n\",\n    \"    result = data\\n\",\n    \"    for _ in range(10000):  # Increased iterations for more CPU work\\n\",\n    \"        result = str(hash(result))[:10]\\n\",\n    \"    return f\\\"CPU_PROCESSED: {result}\\\"\\n\",\n    \"\\n\",\n    \"def io_intensive_task(data: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"I/O-intensive task for parallel processing demo.\\\"\\\"\\\"\\n\",\n    \"    # Simulate I/O wait\\n\",\n    \"    time.sleep(0.1)\\n\",\n    \"    return f\\\"IO_PROCESSED: {data.upper()}\\\"\\n\",\n    \"\\n\",\n    \"def mixed_workload_task(data: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"Mixed CPU and I/O task.\\\"\\\"\\\"\\n\",\n    \"    # CPU work\\n\",\n    \"    result = str(hash(data * 1000))\\n\",\n    \"    # I/O simulation\\n\",\n    \"    time.sleep(0.02)\\n\",\n    \"    return f\\\"MIXED: {result[:8]}\\\"\\n\",\n    \"\\n\",\n    \"def batch_text_processor(texts: List[str]) -> List[str]:\\n\",\n    \"    \\\"\\\"\\\"Process a batch of texts efficiently.\\\"\\\"\\\"\\n\",\n    \"    # Simulate batch processing efficiency\\n\",\n    \"    time.sleep(0.01)  # Setup overhead per batch\\n\",\n    \"    \\n\",\n    \"    results = []\\n\",\n    \"    for text in texts:\\n\",\n    \"        # Faster per-item processing in batch\\n\",\n    \"        processed = text.upper().replace(\\\" \\\", \\\"_\\\")\\n\",\n    \"        results.append(f\\\"BATCH_{processed}\\\")\\n\",\n    \"    \\n\",\n    \"    return results\\n\",\n    \"\\n\",\n    \"def demonstrate_parallel_processing():\\n\",\n    \"    \\\"\\\"\\\"Show parallel processing patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üöÄ Parallel Processing\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create test data\\n\",\n    \"    test_data = [f\\\"data_item_{i}\\\" for i in range(20)]\\n\",\n    \"    \\n\",\n    \"    print(\\\"‚öôÔ∏è Comparing sequential vs parallel processing:\\\")\\n\",\n    \"    \\n\",\n    \"    # CPU-bound task comparison\\n\",\n    \"    print(f\\\"\\\\nüßÆ CPU-bound tasks:\\\")\\n\",\n    \"    \\n\",\n    \"    # Sequential processing\\n\",\n    \"    with profile_time(\\\"Sequential CPU processing\\\"):\\n\",\n    \"        sequential_cpu_results = [cpu_intensive_task(item) for item in test_data[:8]]  # Smaller subset\\n\",\n    \"    \\n\",\n    \"    # Parallel processing with processes\\n\",\n    \"    with profile_time(\\\"Parallel CPU processing (processes)\\\"):\\n\",\n    \"        parallel_cpu_results = ParallelProcessor.parallel_map_processes(\\n\",\n    \"            cpu_intensive_task, test_data[:8], max_workers=2\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results match: {sequential_cpu_results == parallel_cpu_results}\\\")\\n\",\n    \"    print(f\\\"  Processed {len(parallel_cpu_results)} items\\\")\\n\",\n    \"    \\n\",\n    \"    # I/O-bound task comparison\\n\",\n    \"    print(f\\\"\\\\nüíæ I/O-bound tasks:\\\")\\n\",\n    \"    \\n\",\n    \"    # Sequential I/O\\n\",\n    \"    with profile_time(\\\"Sequential I/O processing\\\"):\\n\",\n    \"        sequential_io_results = [io_intensive_task(item) for item in test_data[:10]]\\n\",\n    \"    \\n\",\n    \"    # Parallel I/O with threads\\n\",\n    \"    with profile_time(\\\"Parallel I/O processing (threads)\\\"):\\n\",\n    \"        parallel_io_results = ParallelProcessor.parallel_map_threads(\\n\",\n    \"            io_intensive_task, test_data[:10], max_workers=5\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results match: {set(sequential_io_results) == set(parallel_io_results)}\\\")\\n\",\n    \"    print(f\\\"  Processed {len(parallel_io_results)} items\\\")\\n\",\n    \"    \\n\",\n    \"    # Batch processing comparison\\n\",\n    \"    print(f\\\"\\\\nüì¶ Batch processing:\\\")\\n\",\n    \"    \\n\",\n    \"    large_dataset = [f\\\"text_{i}\\\" for i in range(100)]\\n\",\n    \"    \\n\",\n    \"    # Individual processing\\n\",\n    \"    with profile_time(\\\"Individual processing\\\"):\\n\",\n    \"        individual_results = [item.upper().replace(\\\" \\\", \\\"_\\\") for item in large_dataset]\\n\",\n    \"    \\n\",\n    \"    # Batch processing\\n\",\n    \"    with profile_time(\\\"Parallel batch processing\\\"):\\n\",\n    \"        batch_results = ParallelProcessor.parallel_batch_process(\\n\",\n    \"            large_dataset, batch_text_processor, batch_size=20, max_workers=3\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Individual results: {len(individual_results)}\\\")\\n\",\n    \"    print(f\\\"  Batch results: {len(batch_results)}\\\")\\n\",\n    \"    \\n\",\n    \"    # Progressive results with as_completed\\n\",\n    \"    print(f\\\"\\\\n‚è±Ô∏è Progressive results (as_completed pattern):\\\")\\n\",\n    \"    \\n\",\n    \"    mixed_data = [f\\\"mixed_{i}\\\" for i in range(8)]\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    completed_count = 0\\n\",\n    \"    \\n\",\n    \"    for item, result in ParallelProcessor.parallel_as_completed(\\n\",\n    \"        mixed_workload_task, mixed_data, max_workers=3\\n\",\n    \"    ):\\n\",\n    \"        completed_count += 1\\n\",\n    \"        elapsed = time.time() - start_time\\n\",\n    \"        \\n\",\n    \"        if isinstance(result, Exception):\\n\",\n    \"            print(f\\\"  {elapsed:.2f}s: {item} FAILED - {result}\\\")\\n\",\n    \"        else:\\n\",\n    \"            print(f\\\"  {elapsed:.2f}s: {item} -> {result[:20]}...\\\")\\n\",\n    \"    \\n\",\n    \"    total_time = time.time() - start_time\\n\",\n    \"    print(f\\\"  Completed {completed_count} tasks in {total_time:.2f}s\\\")\\n\",\n    \"\\n\",\n    \"# Advanced parallel patterns\\n\",\n    \"def demonstrate_advanced_parallel_patterns():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate advanced parallel processing patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüéØ Advanced parallel patterns:\\\")\\n\",\n    \"    \\n\",\n    \"    # Map-Reduce pattern\\n\",\n    \"    def map_phase(data_chunk: List[int]) -> Dict[str, int]:\\n\",\n    \"        \\\"\\\"\\\"Map phase: process chunk and return key-value pairs.\\\"\\\"\\\"\\n\",\n    \"        result = {\\\"even\\\": 0, \\\"odd\\\": 0, \\\"sum\\\": 0}\\n\",\n    \"        for num in data_chunk:\\n\",\n    \"            if num % 2 == 0:\\n\",\n    \"                result[\\\"even\\\"] += 1\\n\",\n    \"            else:\\n\",\n    \"                result[\\\"odd\\\"] += 1\\n\",\n    \"            result[\\\"sum\\\"] += num\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    def reduce_phase(results: List[Dict[str, int]]) -> Dict[str, int]:\\n\",\n    \"        \\\"\\\"\\\"Reduce phase: combine results from all workers.\\\"\\\"\\\"\\n\",\n    \"        final_result = {\\\"even\\\": 0, \\\"odd\\\": 0, \\\"sum\\\": 0}\\n\",\n    \"        for result in results:\\n\",\n    \"            for key in final_result:\\n\",\n    \"                final_result[key] += result[key]\\n\",\n    \"        return final_result\\n\",\n    \"    \\n\",\n    \"    # Generate test data\\n\",\n    \"    large_numbers = list(range(10000))\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Map-Reduce pattern\\\"):\\n\",\n    \"        # Split data into chunks\\n\",\n    \"        chunk_size = 1000\\n\",\n    \"        chunks = [large_numbers[i:i + chunk_size] for i in range(0, len(large_numbers), chunk_size)]\\n\",\n    \"        \\n\",\n    \"        # Map phase (parallel)\\n\",\n    \"        map_results = ParallelProcessor.parallel_map_processes(map_phase, chunks, max_workers=3)\\n\",\n    \"        \\n\",\n    \"        # Reduce phase (sequential)\\n\",\n    \"        final_result = reduce_phase(map_results)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Map-Reduce result: {final_result}\\\")\\n\",\n    \"    \\n\",\n    \"    # Pipeline pattern\\n\",\n    \"    print(f\\\"\\\\nüîÑ Pipeline processing pattern:\\\")\\n\",\n    \"    \\n\",\n    \"    def stage1_preprocess(data: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Stage 1: Preprocessing.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.01)\\n\",\n    \"        return f\\\"PREPROCESSED_{data}\\\"\\n\",\n    \"    \\n\",\n    \"    def stage2_analyze(data: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Stage 2: Analysis.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.02)\\n\",\n    \"        return f\\\"ANALYZED_{data}\\\"\\n\",\n    \"    \\n\",\n    \"    def stage3_postprocess(data: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Stage 3: Post-processing.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.01)\\n\",\n    \"        return f\\\"FINAL_{data}\\\"\\n\",\n    \"    \\n\",\n    \"    pipeline_data = [f\\\"item_{i}\\\" for i in range(20)]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Sequential pipeline\\\"):\\n\",\n    \"        # Sequential pipeline\\n\",\n    \"        stage1_results = [stage1_preprocess(item) for item in pipeline_data]\\n\",\n    \"        stage2_results = [stage2_analyze(item) for item in stage1_results]\\n\",\n    \"        stage3_results = [stage3_postprocess(item) for item in stage2_results]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Parallel pipeline\\\"):\\n\",\n    \"        # Parallel pipeline\\n\",\n    \"        stage1_parallel = ParallelProcessor.parallel_map_threads(stage1_preprocess, pipeline_data, max_workers=3)\\n\",\n    \"        stage2_parallel = ParallelProcessor.parallel_map_threads(stage2_analyze, stage1_parallel, max_workers=3)\\n\",\n    \"        stage3_parallel = ParallelProcessor.parallel_map_threads(stage3_postprocess, stage2_parallel, max_workers=3)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Sequential pipeline: {len(stage3_results)} items\\\")\\n\",\n    \"    print(f\\\"  Parallel pipeline: {len(stage3_parallel)} items\\\")\\n\",\n    \"    \\n\",\n    \"    # Worker pool with different task types\\n\",\n    \"    print(f\\\"\\\\nüë• Mixed workload distribution:\\\")\\n\",\n    \"    \\n\",\n    \"    # Create mixed tasks\\n\",\n    \"    cpu_tasks = [(\\\"cpu\\\", f\\\"cpu_task_{i}\\\") for i in range(4)]\\n\",\n    \"    io_tasks = [(\\\"io\\\", f\\\"io_task_{i}\\\") for i in range(8)]\\n\",\n    \"    all_tasks = cpu_tasks + io_tasks\\n\",\n    \"    \\n\",\n    \"    def process_mixed_task(task_info: Tuple[str, str]) -> str:\\n\",\n    \"        task_type, data = task_info\\n\",\n    \"        if task_type == \\\"cpu\\\":\\n\",\n    \"            return cpu_intensive_task(data)\\n\",\n    \"        else:\\n\",\n    \"            return io_intensive_task(data)\\n\",\n    \"    \\n\",\n    \"    # Process with appropriate parallelization\\n\",\n    \"    with profile_time(\\\"Mixed workload processing\\\"):\\n\",\n    \"        # Separate CPU and I/O tasks\\n\",\n    \"        cpu_results = ParallelProcessor.parallel_map_processes(\\n\",\n    \"            process_mixed_task, cpu_tasks, max_workers=2\\n\",\n    \"        )\\n\",\n    \"        io_results = ParallelProcessor.parallel_map_threads(\\n\",\n    \"            process_mixed_task, io_tasks, max_workers=4\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  CPU tasks completed: {len(cpu_results)}\\\")\\n\",\n    \"    print(f\\\"  I/O tasks completed: {len(io_results)}\\\")\\n\",\n    \"\\n\",\n    \"# System information\\n\",\n    \"def show_system_info():\\n\",\n    \"    \\\"\\\"\\\"Show system information for parallel processing.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüíª System Information:\\\")\\n\",\n    \"    print(f\\\"  CPU cores: {mp.cpu_count()}\\\")\\n\",\n    \"    print(f\\\"  Active threads: {threading.active_count()}\\\")\\n\",\n    \"    print(f\\\"  Current process ID: {mp.current_process().pid}\\\")\\n\",\n    \"\\n\",\n    \"# Run parallel processing demonstrations\\n\",\n    \"show_system_info()\\n\",\n    \"demonstrate_parallel_processing()\\n\",\n    \"demonstrate_advanced_parallel_patterns()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Parallel Processing Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Utilizes multiple CPU cores\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Overlaps I/O operations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Scales with available hardware\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Improves throughput for batch operations\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 6: Production Performance System\\n\",\n    \"\\n\",\n    \"**First Principle**: Combine all performance techniques to create a comprehensive, optimized AI processing system.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use composition of optimized components to build high-performance systems.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Complete high-performance AI processing system\\n\",\n    \"from dataclasses import dataclass, field\\n\",\n    \"from typing import Union\\n\",\n    \"import pickle\\n\",\n    \"import hashlib\\n\",\n    \"import os\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class PerformanceConfig:\\n\",\n    \"    \\\"\\\"\\\"Configuration for performance optimization.\\\"\\\"\\\"\\n\",\n    \"    enable_caching: bool = True\\n\",\n    \"    cache_size: int = 1000\\n\",\n    \"    cache_ttl: float = 3600.0  # 1 hour\\n\",\n    \"    enable_parallel: bool = True\\n\",\n    \"    max_workers: int = 4\\n\",\n    \"    batch_size: int = 100\\n\",\n    \"    enable_vectorization: bool = True\\n\",\n    \"    memory_limit_mb: float = 500.0\\n\",\n    \"    enable_profiling: bool = False\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class ProcessingResult:\\n\",\n    \"    \\\"\\\"\\\"Result from AI processing.\\\"\\\"\\\"\\n\",\n    \"    data: Any\\n\",\n    \"    processing_time: float\\n\",\n    \"    from_cache: bool = False\\n\",\n    \"    worker_id: Optional[str] = None\\n\",\n    \"    memory_used_mb: float = 0.0\\n\",\n    \"\\n\",\n    \"class HighPerformanceAIProcessor:\\n\",\n    \"    \\\"\\\"\\\"High-performance AI processor using all optimization techniques.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, config: PerformanceConfig):\\n\",\n    \"        self.config = config\\n\",\n    \"        \\n\",\n    \"        # Performance components\\n\",\n    \"        self.cache = MemoryEfficientCache(\\n\",\n    \"            max_size=config.cache_size,\\n\",\n    \"            max_memory_mb=config.memory_limit_mb * 0.5  # Use half memory limit for cache\\n\",\n    \"        ) if config.enable_caching else None\\n\",\n    \"        \\n\",\n    \"        self.monitor = PerformanceMonitor() if config.enable_profiling else None\\n\",\n    \"        \\n\",\n    \"        # Object pool for expensive resources\\n\",\n    \"        self.resource_pool = ObjectPool(\\n\",\n    \"            factory=lambda: {\\\"id\\\": f\\\"resource_{time.time()}\\\", \\\"data\\\": np.zeros(1000)},\\n\",\n    \"            max_size=config.max_workers\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        # Processing statistics\\n\",\n    \"        self.stats = {\\n\",\n    \"            \\\"total_processed\\\": 0,\\n\",\n    \"            \\\"cache_hits\\\": 0,\\n\",\n    \"            \\\"cache_misses\\\": 0,\\n\",\n    \"            \\\"total_time\\\": 0.0,\\n\",\n    \"            \\\"parallel_batches\\\": 0,\\n\",\n    \"            \\\"vectorized_operations\\\": 0\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    def _generate_cache_key(self, data: Any, operation: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Generate cache key for data and operation.\\\"\\\"\\\"\\n\",\n    \"        data_str = str(data) if len(str(data)) < 1000 else str(hash(str(data)))\\n\",\n    \"        return hashlib.md5(f\\\"{operation}_{data_str}\\\".encode()).hexdigest()\\n\",\n    \"    \\n\",\n    \"    def _check_cache(self, cache_key: str) -> Optional[ProcessingResult]:\\n\",\n    \"        \\\"\\\"\\\"Check if result is cached.\\\"\\\"\\\"\\n\",\n    \"        if not self.cache:\\n\",\n    \"            return None\\n\",\n    \"        \\n\",\n    \"        cached = self.cache.get(cache_key)\\n\",\n    \"        if cached:\\n\",\n    \"            self.stats[\\\"cache_hits\\\"] += 1\\n\",\n    \"            return ProcessingResult(\\n\",\n    \"                data=cached,\\n\",\n    \"                processing_time=0.0,\\n\",\n    \"                from_cache=True\\n\",\n    \"            )\\n\",\n    \"        \\n\",\n    \"        self.stats[\\\"cache_misses\\\"] += 1\\n\",\n    \"        return None\\n\",\n    \"    \\n\",\n    \"    def _store_cache(self, cache_key: str, result: Any):\\n\",\n    \"        \\\"\\\"\\\"Store result in cache.\\\"\\\"\\\"\\n\",\n    \"        if self.cache:\\n\",\n    \"            self.cache.put(cache_key, result)\\n\",\n    \"    \\n\",\n    \"    def process_single(self, data: Any, operation: str = \\\"default\\\") -> ProcessingResult:\\n\",\n    \"        \\\"\\\"\\\"Process a single item with full optimization.\\\"\\\"\\\"\\n\",\n    \"        start_time = time.perf_counter()\\n\",\n    \"        \\n\",\n    \"        # Check cache\\n\",\n    \"        cache_key = self._generate_cache_key(data, operation)\\n\",\n    \"        cached_result = self._check_cache(cache_key)\\n\",\n    \"        if cached_result:\\n\",\n    \"            return cached_result\\n\",\n    \"        \\n\",\n    \"        # Process with resource pooling\\n\",\n    \"        with self.resource_pool.borrow() as resource:\\n\",\n    \"            # Simulate AI processing based on operation\\n\",\n    \"            if operation == \\\"classify\\\":\\n\",\n    \"                result = self._classify(data, resource)\\n\",\n    \"            elif operation == \\\"vectorize\\\":\\n\",\n    \"                result = self._vectorize(data, resource)\\n\",\n    \"            elif operation == \\\"analyze\\\":\\n\",\n    \"                result = self._analyze(data, resource)\\n\",\n    \"            else:\\n\",\n    \"                result = self._default_process(data, resource)\\n\",\n    \"        \\n\",\n    \"        processing_time = time.perf_counter() - start_time\\n\",\n    \"        \\n\",\n    \"        # Store in cache\\n\",\n    \"        self._store_cache(cache_key, result)\\n\",\n    \"        \\n\",\n    \"        # Update statistics\\n\",\n    \"        self.stats[\\\"total_processed\\\"] += 1\\n\",\n    \"        self.stats[\\\"total_time\\\"] += processing_time\\n\",\n    \"        \\n\",\n    \"        if self.monitor:\\n\",\n    \"            self.monitor.record_time(operation, processing_time)\\n\",\n    \"        \\n\",\n    \"        return ProcessingResult(\\n\",\n    \"            data=result,\\n\",\n    \"            processing_time=processing_time,\\n\",\n    \"            from_cache=False,\\n\",\n    \"            worker_id=resource[\\\"id\\\"]\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    def process_batch(self, data_list: List[Any], operation: str = \\\"default\\\") -> List[ProcessingResult]:\\n\",\n    \"        \\\"\\\"\\\"Process batch with parallel and vectorized optimization.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        if not self.config.enable_parallel or len(data_list) < self.config.batch_size:\\n\",\n    \"            # Sequential processing for small batches\\n\",\n    \"            return [self.process_single(data, operation) for data in data_list]\\n\",\n    \"        \\n\",\n    \"        # Parallel batch processing\\n\",\n    \"        def process_batch_chunk(chunk: List[Any]) -> List[ProcessingResult]:\\n\",\n    \"            return [self.process_single(data, operation) for data in chunk]\\n\",\n    \"        \\n\",\n    \"        start_time = time.perf_counter()\\n\",\n    \"        \\n\",\n    \"        results = ParallelProcessor.parallel_batch_process(\\n\",\n    \"            data_list,\\n\",\n    \"            process_batch_chunk,\\n\",\n    \"            batch_size=self.config.batch_size,\\n\",\n    \"            max_workers=self.config.max_workers,\\n\",\n    \"            use_processes=False  # Use threads for I/O-bound AI operations\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        batch_time = time.perf_counter() - start_time\\n\",\n    \"        self.stats[\\\"parallel_batches\\\"] += 1\\n\",\n    \"        \\n\",\n    \"        if self.monitor:\\n\",\n    \"            self.monitor.record_time(f\\\"batch_{operation}\\\", batch_time)\\n\",\n    \"        \\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    def process_vectorized(self, data_array: np.ndarray, operation: str = \\\"vectorized\\\") -> ProcessingResult:\\n\",\n    \"        \\\"\\\"\\\"Process data using vectorized operations.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        if not self.config.enable_vectorization:\\n\",\n    \"            # Fall back to element-wise processing\\n\",\n    \"            results = [self.process_single(item, operation) for item in data_array]\\n\",\n    \"            return ProcessingResult(\\n\",\n    \"                data=[r.data for r in results],\\n\",\n    \"                processing_time=sum(r.processing_time for r in results),\\n\",\n    \"                from_cache=False\\n\",\n    \"            )\\n\",\n    \"        \\n\",\n    \"        start_time = time.perf_counter()\\n\",\n    \"        \\n\",\n    \"        # Vectorized processing\\n\",\n    \"        if operation == \\\"normalize\\\":\\n\",\n    \"            # Normalize data using vectorized operations\\n\",\n    \"            mean = np.mean(data_array, axis=0)\\n\",\n    \"            std = np.std(data_array, axis=0)\\n\",\n    \"            result = (data_array - mean) / (std + 1e-8)\\n\",\n    \"            \\n\",\n    \"        elif operation == \\\"similarity\\\":\\n\",\n    \"            # Compute pairwise similarities\\n\",\n    \"            result = np.dot(data_array, data_array.T)\\n\",\n    \"            \\n\",\n    \"        elif operation == \\\"transform\\\":\\n\",\n    \"            # Apply mathematical transformations\\n\",\n    \"            result = np.log(1 + np.abs(data_array))\\n\",\n    \"            \\n\",\n    \"        else:\\n\",\n    \"            # Default vectorized operation\\n\",\n    \"            result = data_array ** 2 + np.sin(data_array)\\n\",\n    \"        \\n\",\n    \"        processing_time = time.perf_counter() - start_time\\n\",\n    \"        \\n\",\n    \"        self.stats[\\\"vectorized_operations\\\"] += 1\\n\",\n    \"        self.stats[\\\"total_processed\\\"] += len(data_array)\\n\",\n    \"        self.stats[\\\"total_time\\\"] += processing_time\\n\",\n    \"        \\n\",\n    \"        if self.monitor:\\n\",\n    \"            self.monitor.record_time(f\\\"vectorized_{operation}\\\", processing_time)\\n\",\n    \"        \\n\",\n    \"        return ProcessingResult(\\n\",\n    \"            data=result,\\n\",\n    \"            processing_time=processing_time,\\n\",\n    \"            from_cache=False\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    def _classify(self, data: Any, resource: Dict) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Simulate classification.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.01)  # Simulate processing\\n\",\n    \"        confidence = hash(str(data)) % 100 / 100.0\\n\",\n    \"        return {\\n\",\n    \"            \\\"label\\\": \\\"positive\\\" if confidence > 0.5 else \\\"negative\\\",\\n\",\n    \"            \\\"confidence\\\": confidence,\\n\",\n    \"            \\\"resource_id\\\": resource[\\\"id\\\"]\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    def _vectorize(self, data: Any, resource: Dict) -> List[float]:\\n\",\n    \"        \\\"\\\"\\\"Simulate vectorization.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.005)\\n\",\n    \"        # Create feature vector\\n\",\n    \"        features = []\\n\",\n    \"        data_str = str(data)\\n\",\n    \"        for i in range(10):\\n\",\n    \"            features.append(hash(data_str + str(i)) % 1000 / 1000.0)\\n\",\n    \"        return features\\n\",\n    \"    \\n\",\n    \"    def _analyze(self, data: Any, resource: Dict) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Simulate analysis.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.02)\\n\",\n    \"        data_str = str(data)\\n\",\n    \"        return {\\n\",\n    \"            \\\"length\\\": len(data_str),\\n\",\n    \"            \\\"complexity\\\": len(set(data_str)) / len(data_str) if data_str else 0,\\n\",\n    \"            \\\"hash_value\\\": hash(data_str),\\n\",\n    \"            \\\"resource_id\\\": resource[\\\"id\\\"]\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    def _default_process(self, data: Any, resource: Dict) -> str:\\n\",\n    \"        \\\"\\\"\\\"Default processing.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.001)\\n\",\n    \"        return f\\\"PROCESSED_{data}_{resource['id']}\\\"\\n\",\n    \"    \\n\",\n    \"    def get_performance_report(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get comprehensive performance report.\\\"\\\"\\\"\\n\",\n    \"        report = {\\n\",\n    \"            \\\"processing_stats\\\": self.stats.copy(),\\n\",\n    \"            \\\"cache_stats\\\": self.cache.get_stats() if self.cache else None,\\n\",\n    \"            \\\"resource_pool_stats\\\": self.resource_pool.get_stats(),\\n\",\n    \"            \\\"performance_summary\\\": {}\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        # Calculate performance metrics\\n\",\n    \"        if self.stats[\\\"total_processed\\\"] > 0:\\n\",\n    \"            avg_time = self.stats[\\\"total_time\\\"] / self.stats[\\\"total_processed\\\"]\\n\",\n    \"            throughput = self.stats[\\\"total_processed\\\"] / self.stats[\\\"total_time\\\"] if self.stats[\\\"total_time\\\"] > 0 else 0\\n\",\n    \"            \\n\",\n    \"            report[\\\"performance_summary\\\"] = {\\n\",\n    \"                \\\"avg_processing_time\\\": avg_time,\\n\",\n    \"                \\\"throughput_items_per_sec\\\": throughput,\\n\",\n    \"                \\\"cache_hit_rate\\\": self.stats[\\\"cache_hits\\\"] / (self.stats[\\\"cache_hits\\\"] + self.stats[\\\"cache_misses\\\"]) if (self.stats[\\\"cache_hits\\\"] + self.stats[\\\"cache_misses\\\"]) > 0 else 0\\n\",\n    \"            }\\n\",\n    \"        \\n\",\n    \"        if self.monitor:\\n\",\n    \"            report[\\\"detailed_monitoring\\\"] = self.monitor.get_summary()\\n\",\n    \"        \\n\",\n    \"        return report\\n\",\n    \"    \\n\",\n    \"    def optimize_configuration(self) -> PerformanceConfig:\\n\",\n    \"        \\\"\\\"\\\"Suggest optimized configuration based on usage patterns.\\\"\\\"\\\"\\n\",\n    \"        current_config = self.config\\n\",\n    \"        \\n\",\n    \"        # Simple optimization heuristics\\n\",\n    \"        if self.cache and self.cache.get_stats()[\\\"hit_ratio\\\"] < 0.3:\\n\",\n    \"            # Low cache hit rate, increase cache size\\n\",\n    \"            current_config.cache_size = min(current_config.cache_size * 2, 5000)\\n\",\n    \"        \\n\",\n    \"        if self.stats[\\\"parallel_batches\\\"] > 10:\\n\",\n    \"            # Frequent parallel processing, optimize batch size\\n\",\n    \"            current_config.batch_size = max(current_config.batch_size // 2, 50)\\n\",\n    \"        \\n\",\n    \"        return current_config\\n\",\n    \"\\n\",\n    \"def demonstrate_production_system():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate the complete production performance system.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üè≠ Production Performance System\\\", style=\\\"bold green\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create optimized configuration\\n\",\n    \"    config = PerformanceConfig(\\n\",\n    \"        enable_caching=True,\\n\",\n    \"        cache_size=500,\\n\",\n    \"        enable_parallel=True,\\n\",\n    \"        max_workers=3,\\n\",\n    \"        batch_size=50,\\n\",\n    \"        enable_vectorization=True,\\n\",\n    \"        memory_limit_mb=200.0,\\n\",\n    \"        enable_profiling=True\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    processor = HighPerformanceAIProcessor(config)\\n\",\n    \"    \\n\",\n    \"    print(\\\"üöÄ Testing high-performance AI processing:\\\")\\n\",\n    \"    \\n\",\n    \"    # Test 1: Single item processing with caching\\n\",\n    \"    print(f\\\"\\\\nüìù Single item processing (with caching):\\\")\\n\",\n    \"    \\n\",\n    \"    test_items = [\\\"test_item_1\\\", \\\"test_item_2\\\", \\\"test_item_1\\\"]  # Duplicate for cache test\\n\",\n    \"    \\n\",\n    \"    for item in test_items:\\n\",\n    \"        result = processor.process_single(item, \\\"classify\\\")\\n\",\n    \"        cache_status = \\\"CACHE HIT\\\" if result.from_cache else \\\"COMPUTED\\\"\\n\",\n    \"        print(f\\\"  {item}: {result.data['label']} (confidence: {result.data['confidence']:.3f}) - {cache_status} - {result.processing_time:.4f}s\\\")\\n\",\n    \"    \\n\",\n    \"    # Test 2: Batch processing\\n\",\n    \"    print(f\\\"\\\\nüì¶ Batch processing:\\\")\\n\",\n    \"    \\n\",\n    \"    batch_data = [f\\\"batch_item_{i}\\\" for i in range(20)]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Batch processing\\\"):\\n\",\n    \"        batch_results = processor.process_batch(batch_data, \\\"analyze\\\")\\n\",\n    \"    \\n\",\n    \"    successful_results = [r for r in batch_results if r.data]\\n\",\n    \"    avg_time = sum(r.processing_time for r in successful_results) / len(successful_results)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Processed {len(successful_results)} items\\\")\\n\",\n    \"    print(f\\\"  Average processing time: {avg_time:.4f}s\\\")\\n\",\n    \"    print(f\\\"  Sample result: {successful_results[0].data}\\\")\\n\",\n    \"    \\n\",\n    \"    # Test 3: Vectorized processing\\n\",\n    \"    print(f\\\"\\\\n‚ö° Vectorized processing:\\\")\\n\",\n    \"    \\n\",\n    \"    # Create test matrices\\n\",\n    \"    test_matrix = np.random.rand(100, 10)\\n\",\n    \"    \\n\",\n    \"    operations = [\\\"normalize\\\", \\\"similarity\\\", \\\"transform\\\"]\\n\",\n    \"    \\n\",\n    \"    for operation in operations:\\n\",\n    \"        result = processor.process_vectorized(test_matrix, operation)\\n\",\n    \"        print(f\\\"  {operation}: shape={result.data.shape}, time={result.processing_time:.4f}s\\\")\\n\",\n    \"    \\n\",\n    \"    # Test 4: Mixed workload\\n\",\n    \"    print(f\\\"\\\\nüîÄ Mixed workload processing:\\\")\\n\",\n    \"    \\n\",\n    \"    mixed_data = [f\\\"mixed_{i}\\\" for i in range(30)]\\n\",\n    \"    operations = [\\\"classify\\\", \\\"vectorize\\\", \\\"analyze\\\"]\\n\",\n    \"    \\n\",\n    \"    start_time = time.perf_counter()\\n\",\n    \"    \\n\",\n    \"    for i, data in enumerate(mixed_data):\\n\",\n    \"        operation = operations[i % len(operations)]\\n\",\n    \"        result = processor.process_single(data, operation)\\n\",\n    \"    \\n\",\n    \"    mixed_time = time.perf_counter() - start_time\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Mixed workload: {len(mixed_data)} items in {mixed_time:.4f}s\\\")\\n\",\n    \"    \\n\",\n    \"    # Performance report\\n\",\n    \"    report = processor.get_performance_report()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìä Performance Report:\\\")\\n\",\n    \"    print(f\\\"  Total processed: {report['processing_stats']['total_processed']}\\\")\\n\",\n    \"    print(f\\\"  Cache hit rate: {report['performance_summary']['cache_hit_rate']:.1%}\\\")\\n\",\n    \"    print(f\\\"  Average processing time: {report['performance_summary']['avg_processing_time']:.4f}s\\\")\\n\",\n    \"    print(f\\\"  Throughput: {report['performance_summary']['throughput_items_per_sec']:.1f} items/sec\\\")\\n\",\n    \"    \\n\",\n    \"    if report['cache_stats']:\\n\",\n    \"        print(f\\\"  Cache efficiency: {report['cache_stats']['memory_mb']:.1f}MB used\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Resource pool: {report['resource_pool_stats']['pool_efficiency']:.1%} efficiency\\\")\\n\",\n    \"    \\n\",\n    \"    # Configuration optimization\\n\",\n    \"    print(f\\\"\\\\nüîß Configuration optimization:\\\")\\n\",\n    \"    optimized_config = processor.optimize_configuration()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Original batch size: {config.batch_size}\\\")\\n\",\n    \"    print(f\\\"  Optimized batch size: {optimized_config.batch_size}\\\")\\n\",\n    \"    print(f\\\"  Original cache size: {config.cache_size}\\\")\\n\",\n    \"    print(f\\\"  Optimized cache size: {optimized_config.cache_size}\\\")\\n\",\n    \"    \\n\",\n    \"    # Memory cleanup demonstration\\n\",\n    \"    print(f\\\"\\\\nüßπ Memory cleanup:\\\")\\n\",\n    \"    \\n\",\n    \"    objects_before = len(gc.get_objects())\\n\",\n    \"    \\n\",\n    \"    # Clear caches and force garbage collection\\n\",\n    \"    if processor.cache:\\n\",\n    \"        processor.cache.cache.clear()\\n\",\n    \"    \\n\",\n    \"    del batch_results, successful_results, mixed_data\\n\",\n    \"    gc.collect()\\n\",\n    \"    \\n\",\n    \"    objects_after = len(gc.get_objects())\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Objects before cleanup: {objects_before}\\\")\\n\",\n    \"    print(f\\\"  Objects after cleanup: {objects_after}\\\")\\n\",\n    \"    print(f\\\"  Objects freed: {objects_before - objects_after}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüèÜ Production System Features:\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Intelligent caching with memory limits\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Parallel batch processing\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Vectorized mathematical operations\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Resource pooling and management\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Comprehensive performance monitoring\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Automatic configuration optimization\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Memory-efficient data structures\\\")\\n\",\n    \"\\n\",\n    \"# Run the complete production system demonstration\\n\",\n    \"demonstrate_production_system()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüéØ Performance Optimization Mastery Complete!\\\")\\n\",\n    \"print(f\\\"\\\\nüí° Key Performance Optimization Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚ö° 10-100x speedup with proper optimization\\\")\\n\",\n    \"print(f\\\"  üíæ Efficient memory usage and management\\\")\\n\",\n    \"print(f\\\"  üìä Data-driven performance monitoring\\\")\\n\",\n    \"print(f\\\"  üîß Adaptive configuration optimization\\\")\\n\",\n    \"print(f\\\"  üöÄ Production-ready scalable architecture\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Summary: Performance Optimization Mastery\\n\",\n    \"\\n\",\n    \"**üéØ Key Takeaways:**\\n\",\n    \"\\n\",\n    \"1. **Memory Efficiency** reduces allocation overhead and improves cache locality\\n\",\n    \"2. **Performance Profiling** identifies bottlenecks and guides optimization efforts\\n\",\n    \"3. **Vectorization** provides massive speedups for numerical operations\\n\",\n    \"4. **Parallel Processing** utilizes multiple cores for CPU-bound and I/O-bound tasks\\n\",\n    \"5. **Smart Caching** eliminates redundant computations\\n\",\n    \"6. **Resource Pooling** reuses expensive objects efficiently\\n\",\n    \"7. **Monitoring** provides insights for continuous optimization\\n\",\n    \"\\n\",\n    \"**üöÄ Production Considerations:**\\n\",\n    \"- Always profile before optimizing\\n\",\n    \"- Use appropriate data structures for your use case\\n\",\n    \"- Leverage NumPy for numerical computations\\n\",\n    \"- Choose threads vs processes based on workload type\\n\",\n    \"- Monitor memory usage and implement limits\\n\",\n    \"- Cache strategically with appropriate TTL\\n\",\n    \"\\n\",\n    \"**üí° Remember:** Performance optimization is about making the right trade-offs between speed, memory, and code complexity. Focus on the bottlenecks that matter most for your specific AI application!\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":139085,"cache_read_input_tokens":0,"output_tokens":3}},"requestId":"req_vrtx_011CRhSYczuuib9Tu6sh1g7f","type":"assistant","uuid":"fbd9399c-81a6-4745-b36e-24d4ce4f6718","timestamp":"2025-08-01T18:17:56.192Z"}
{"parentUuid":"fbd9399c-81a6-4745-b36e-24d4ce4f6718","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01NqgAZAe2ajWivvFZ31rVm2","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/05_performance_optimization_learning.ipynb"}]},"uuid":"5a30ebeb-98ed-4a5c-ad5b-51c5e118b88f","timestamp":"2025-08-01T18:17:56.220Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/05_performance_optimization_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Performance Optimization for AI Systems\\n\",\n    \"\\n\",\n    \"This notebook teaches performance optimization techniques for AI applications including profiling, memory management, vectorization, and parallel processing. You'll learn:\\n\",\n    \"\\n\",\n    \"1. **Memory-Efficient Data Structures** - Optimizing memory usage\\n\",\n    \"2. **Performance Profiling** - Identifying bottlenecks\\n\",\n    \"3. **Vectorized Operations** - NumPy and efficient computations\\n\",\n    \"4. **Parallel Processing** - CPU and I/O optimization\\n\",\n    \"5. **Lazy Loading** - Memory-efficient data processing\\n\",\n    \"6. **Batch Processing** - Optimizing throughput\\n\",\n    \"7. **Garbage Collection** - Memory management\\n\",\n    \"8. **Benchmarking** - Measuring and comparing performance\\n\",\n    \"\\n\",\n    \"## Prerequisites\\n\",\n    \"```bash\\n\",\n    \"pip install rich numpy\\n\",\n    \"```\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 1: Performance Fundamentals and Setup\\n\",\n    \"\\n\",\n    \"**First Principle**: Performance optimization starts with measurement - profile first, optimize second.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `time.perf_counter()` for precise timing, `sys.getsizeof()` for memory measurement.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Essential imports for performance optimization\\n\",\n    \"import sys\\n\",\n    \"import time\\n\",\n    \"import gc\\n\",\n    \"import weakref\\n\",\n    \"import threading\\n\",\n    \"import multiprocessing as mp\\n\",\n    \"from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\\n\",\n    \"from typing import List, Dict, Any, Callable, Iterator, Optional, Tuple\\n\",\n    \"from dataclasses import dataclass\\n\",\n    \"from functools import lru_cache, partial\\n\",\n    \"import cProfile\\n\",\n    \"import pstats\\n\",\n    \"import tracemalloc\\n\",\n    \"from contextlib import contextmanager\\n\",\n    \"from collections import deque, defaultdict\\n\",\n    \"import numpy as np\\n\",\n    \"from rich.console import Console\\n\",\n    \"from rich.panel import Panel\\n\",\n    \"from rich.table import Table\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"\\n\",\n    \"console.print(Panel(\\\"‚ö° Performance Optimization Fundamentals\\\", style=\\\"bold blue\\\"))\\n\",\n    \"\\n\",\n    \"print(\\\"\\\"\\\"\\n\",\n    \"üîë Key Performance Principles:\\n\",\n    \"\\n\",\n    \"1. **Measure First**: Profile before optimizing\\n\",\n    \"2. **Bottleneck Focus**: Optimize the slowest parts\\n\",\n    \"3. **Memory Efficiency**: Reduce allocations and copies\\n\",\n    \"4. **Vectorization**: Use NumPy for numerical operations\\n\",\n    \"5. **Parallelization**: Leverage multiple cores\\n\",\n    \"6. **Caching**: Avoid redundant computations\\n\",\n    \"7. **Lazy Evaluation**: Compute only when needed\\n\",\n    \"\\n\",\n    \"üéØ Performance Areas for AI:\\n\",\n    \"‚Ä¢ Data loading and preprocessing\\n\",\n    \"‚Ä¢ Model inference and training\\n\",\n    \"‚Ä¢ Feature extraction and transformation\\n\",\n    \"‚Ä¢ Batch processing and streaming\\n\",\n    \"‚Ä¢ Memory management\\n\",\n    \"\\\"\\\"\\\")\\n\",\n    \"\\n\",\n    \"# Basic timing utilities\\n\",\n    \"@contextmanager\\n\",\n    \"def profile_time(operation_name: str = \\\"Operation\\\"):\\n\",\n    \"    \\\"\\\"\\\"Context manager for timing operations.\\\"\\\"\\\"\\n\",\n    \"    start_time = time.perf_counter()\\n\",\n    \"    yield\\n\",\n    \"    end_time = time.perf_counter()\\n\",\n    \"    duration = end_time - start_time\\n\",\n    \"    print(f\\\"‚è±Ô∏è  {operation_name}: {duration:.4f}s\\\")\\n\",\n    \"\\n\",\n    \"@contextmanager\\n\",\n    \"def profile_memory(operation_name: str = \\\"Operation\\\"):\\n\",\n    \"    \\\"\\\"\\\"Context manager for memory profiling.\\\"\\\"\\\"\\n\",\n    \"    tracemalloc.start()\\n\",\n    \"    start_memory = tracemalloc.get_traced_memory()[0]\\n\",\n    \"    \\n\",\n    \"    yield\\n\",\n    \"    \\n\",\n    \"    end_memory = tracemalloc.get_traced_memory()[0]\\n\",\n    \"    tracemalloc.stop()\\n\",\n    \"    \\n\",\n    \"    memory_used = (end_memory - start_memory) / (1024 * 1024)  # MB\\n\",\n    \"    print(f\\\"üß† {operation_name} memory: {memory_used:.2f} MB\\\")\\n\",\n    \"\\n\",\n    \"def compare_performance(functions: List[Tuple[str, Callable]], *args, **kwargs):\\n\",\n    \"    \\\"\\\"\\\"Compare performance of multiple functions.\\\"\\\"\\\"\\n\",\n    \"    results = []\\n\",\n    \"    \\n\",\n    \"    for name, func in functions:\\n\",\n    \"        # Warm up\\n\",\n    \"        try:\\n\",\n    \"            func(*args, **kwargs)\\n\",\n    \"        except:\\n\",\n    \"            pass\\n\",\n    \"        \\n\",\n    \"        # Measure\\n\",\n    \"        start_time = time.perf_counter()\\n\",\n    \"        try:\\n\",\n    \"            result = func(*args, **kwargs)\\n\",\n    \"            end_time = time.perf_counter()\\n\",\n    \"            duration = end_time - start_time\\n\",\n    \"            results.append((name, duration, \\\"success\\\", result))\\n\",\n    \"        except Exception as e:\\n\",\n    \"            end_time = time.perf_counter()\\n\",\n    \"            duration = end_time - start_time\\n\",\n    \"            results.append((name, duration, \\\"error\\\", str(e)))\\n\",\n    \"    \\n\",\n    \"    # Display results\\n\",\n    \"    print(f\\\"\\\\nüìä Performance Comparison:\\\")\\n\",\n    \"    fastest_time = min(r[1] for r in results if r[2] == \\\"success\\\")\\n\",\n    \"    \\n\",\n    \"    for name, duration, status, result in results:\\n\",\n    \"        if status == \\\"success\\\":\\n\",\n    \"            speedup = fastest_time / duration\\n\",\n    \"            print(f\\\"  {name}: {duration:.4f}s ({speedup:.1f}x)\\\")\\n\",\n    \"        else:\\n\",\n    \"            print(f\\\"  {name}: ERROR - {result}\\\")\\n\",\n    \"    \\n\",\n    \"    return results\\n\",\n    \"\\n\",\n    \"# Simple demonstration\\n\",\n    \"def demo_timing():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate basic timing.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Inefficient way\\n\",\n    \"    def slow_sum(n: int) -> int:\\n\",\n    \"        result = 0\\n\",\n    \"        for i in range(n):\\n\",\n    \"            result += i\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    # Efficient way\\n\",\n    \"    def fast_sum(n: int) -> int:\\n\",\n    \"        return n * (n - 1) // 2\\n\",\n    \"    \\n\",\n    \"    n = 1000000\\n\",\n    \"    \\n\",\n    \"    compare_performance([\\n\",\n    \"        (\\\"Slow loop\\\", slow_sum),\\n\",\n    \"        (\\\"Fast formula\\\", fast_sum)\\n\",\n    \"    ], n)\\n\",\n    \"\\n\",\n    \"demo_timing()\\n\",\n    \"print(\\\"\\\\n‚úÖ Performance toolkit ready!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 2: Memory-Efficient Data Structures\\n\",\n    \"\\n\",\n    \"**First Principle**: Memory efficiency is crucial for AI systems that process large datasets.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `deque`, `__slots__`, and custom memory-aware containers to reduce memory overhead.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class MemoryEfficientCache:\\n\",\n    \"    \\\"\\\"\\\"Memory-efficient LRU cache with size limits.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_size: int = 1000, max_memory_mb: float = 100.0):\\n\",\n    \"        self.max_size = max_size\\n\",\n    \"        self.max_memory_bytes = max_memory_mb * 1024 * 1024\\n\",\n    \"        self.cache = {}\\n\",\n    \"        self.access_order = deque()\\n\",\n    \"        self.current_memory = 0\\n\",\n    \"        self.hits = 0\\n\",\n    \"        self.misses = 0\\n\",\n    \"    \\n\",\n    \"    def _estimate_size(self, obj: Any) -> int:\\n\",\n    \"        \\\"\\\"\\\"Estimate object size in bytes.\\\"\\\"\\\"\\n\",\n    \"        return sys.getsizeof(obj)\\n\",\n    \"    \\n\",\n    \"    def _evict_if_needed(self):\\n\",\n    \"        \\\"\\\"\\\"Evict items if cache is too large.\\\"\\\"\\\"\\n\",\n    \"        while (len(self.cache) > self.max_size or \\n\",\n    \"               self.current_memory > self.max_memory_bytes):\\n\",\n    \"            if not self.access_order:\\n\",\n    \"                break\\n\",\n    \"            \\n\",\n    \"            key = self.access_order.popleft()\\n\",\n    \"            if key in self.cache:\\n\",\n    \"                value = self.cache.pop(key)\\n\",\n    \"                self.current_memory -= self._estimate_size(value)\\n\",\n    \"    \\n\",\n    \"    def get(self, key: str) -> Optional[Any]:\\n\",\n    \"        \\\"\\\"\\\"Get item from cache.\\\"\\\"\\\"\\n\",\n    \"        if key in self.cache:\\n\",\n    \"            # Move to end (most recent)\\n\",\n    \"            self.access_order.remove(key)\\n\",\n    \"            self.access_order.append(key)\\n\",\n    \"            self.hits += 1\\n\",\n    \"            return self.cache[key]\\n\",\n    \"        \\n\",\n    \"        self.misses += 1\\n\",\n    \"        return None\\n\",\n    \"    \\n\",\n    \"    def put(self, key: str, value: Any) -> None:\\n\",\n    \"        \\\"\\\"\\\"Put item in cache.\\\"\\\"\\\"\\n\",\n    \"        if key in self.cache:\\n\",\n    \"            # Update existing\\n\",\n    \"            old_value = self.cache[key]\\n\",\n    \"            self.current_memory -= self._estimate_size(old_value)\\n\",\n    \"            self.access_order.remove(key)\\n\",\n    \"        \\n\",\n    \"        size = self._estimate_size(value)\\n\",\n    \"        self.cache[key] = value\\n\",\n    \"        self.access_order.append(key)\\n\",\n    \"        self.current_memory += size\\n\",\n    \"        \\n\",\n    \"        self._evict_if_needed()\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get cache statistics.\\\"\\\"\\\"\\n\",\n    \"        total_requests = self.hits + self.misses\\n\",\n    \"        hit_ratio = self.hits / total_requests if total_requests > 0 else 0\\n\",\n    \"        \\n\",\n    \"        return {\\n\",\n    \"            \\\"size\\\": len(self.cache),\\n\",\n    \"            \\\"max_size\\\": self.max_size,\\n\",\n    \"            \\\"memory_mb\\\": self.current_memory / (1024 * 1024),\\n\",\n    \"            \\\"max_memory_mb\\\": self.max_memory_bytes / (1024 * 1024),\\n\",\n    \"            \\\"hit_ratio\\\": hit_ratio,\\n\",\n    \"            \\\"hits\\\": self.hits,\\n\",\n    \"            \\\"misses\\\": self.misses\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"class ObjectPool:\\n\",\n    \"    \\\"\\\"\\\"Object pool for expensive-to-create objects.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, factory: Callable, max_size: int = 10):\\n\",\n    \"        self.factory = factory\\n\",\n    \"        self.max_size = max_size\\n\",\n    \"        self.available = deque()\\n\",\n    \"        self.in_use = weakref.WeakSet()\\n\",\n    \"        self.total_created = 0\\n\",\n    \"        self.total_acquired = 0\\n\",\n    \"        self.total_released = 0\\n\",\n    \"    \\n\",\n    \"    def acquire(self):\\n\",\n    \"        \\\"\\\"\\\"Acquire object from pool.\\\"\\\"\\\"\\n\",\n    \"        self.total_acquired += 1\\n\",\n    \"        \\n\",\n    \"        if self.available:\\n\",\n    \"            obj = self.available.popleft()\\n\",\n    \"        else:\\n\",\n    \"            if self.total_created < self.max_size:\\n\",\n    \"                obj = self.factory()\\n\",\n    \"                self.total_created += 1\\n\",\n    \"            else:\\n\",\n    \"                # Create temporary object (not pooled)\\n\",\n    \"                obj = self.factory()\\n\",\n    \"        \\n\",\n    \"        self.in_use.add(obj)\\n\",\n    \"        return obj\\n\",\n    \"    \\n\",\n    \"    def release(self, obj):\\n\",\n    \"        \\\"\\\"\\\"Release object back to pool.\\\"\\\"\\\"\\n\",\n    \"        self.total_released += 1\\n\",\n    \"        \\n\",\n    \"        if obj in self.in_use:\\n\",\n    \"            self.in_use.remove(obj)\\n\",\n    \"            if len(self.available) < self.max_size:\\n\",\n    \"                # Reset object state if needed\\n\",\n    \"                if hasattr(obj, 'reset'):\\n\",\n    \"                    obj.reset()\\n\",\n    \"                self.available.append(obj)\\n\",\n    \"    \\n\",\n    \"    @contextmanager\\n\",\n    \"    def borrow(self):\\n\",\n    \"        \\\"\\\"\\\"Context manager for automatic object management.\\\"\\\"\\\"\\n\",\n    \"        obj = self.acquire()\\n\",\n    \"        try:\\n\",\n    \"            yield obj\\n\",\n    \"        finally:\\n\",\n    \"            self.release(obj)\\n\",\n    \"    \\n\",\n    \"    def get_stats(self) -> Dict[str, Any]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"available\\\": len(self.available),\\n\",\n    \"            \\\"in_use\\\": len(self.in_use),\\n\",\n    \"            \\\"total_created\\\": self.total_created,\\n\",\n    \"            \\\"total_acquired\\\": self.total_acquired,\\n\",\n    \"            \\\"total_released\\\": self.total_released,\\n\",\n    \"            \\\"max_size\\\": self.max_size,\\n\",\n    \"            \\\"pool_efficiency\\\": len(self.available) / self.max_size if self.max_size > 0 else 0\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"# Memory-efficient classes using __slots__\\n\",\n    \"class EfficientDataPoint:\\n\",\n    \"    \\\"\\\"\\\"Memory-efficient data point using __slots__.\\\"\\\"\\\"\\n\",\n    \"    __slots__ = ['x', 'y', 'label', 'confidence']\\n\",\n    \"    \\n\",\n    \"    def __init__(self, x: float, y: float, label: str, confidence: float):\\n\",\n    \"        self.x = x\\n\",\n    \"        self.y = y\\n\",\n    \"        self.label = label\\n\",\n    \"        self.confidence = confidence\\n\",\n    \"\\n\",\n    \"class RegularDataPoint:\\n\",\n    \"    \\\"\\\"\\\"Regular data point without __slots__.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, x: float, y: float, label: str, confidence: float):\\n\",\n    \"        self.x = x\\n\",\n    \"        self.y = y\\n\",\n    \"        self.label = label\\n\",\n    \"        self.confidence = confidence\\n\",\n    \"\\n\",\n    \"def demonstrate_memory_efficient_structures():\\n\",\n    \"    \\\"\\\"\\\"Show memory-efficient data structures.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üß† Memory-Efficient Structures\\\", style=\\\"bold blue\\\"))\\n\",\n    \"    \\n\",\n    \"    # Test memory-efficient cache\\n\",\n    \"    print(\\\"üíæ Testing memory-efficient cache:\\\")\\n\",\n    \"    \\n\",\n    \"    cache = MemoryEfficientCache(max_size=5, max_memory_mb=0.1)\\n\",\n    \"    \\n\",\n    \"    # Add items to cache\\n\",\n    \"    test_data = [f\\\"large_data_item_{i}\\\" * 100 for i in range(10)]\\n\",\n    \"    \\n\",\n    \"    for i, data in enumerate(test_data):\\n\",\n    \"        cache.put(f\\\"key_{i}\\\", data)\\n\",\n    \"        stats = cache.get_stats()\\n\",\n    \"        print(f\\\"  Added item {i}: cache size={stats['size']}, memory={stats['memory_mb']:.3f}MB\\\")\\n\",\n    \"    \\n\",\n    \"    # Test retrieval\\n\",\n    \"    print(f\\\"\\\\nüîç Cache retrieval test:\\\")\\n\",\n    \"    for i in [0, 3, 7, 9]:\\n\",\n    \"        result = cache.get(f\\\"key_{i}\\\")\\n\",\n    \"        status = \\\"HIT\\\" if result else \\\"MISS\\\"\\n\",\n    \"        print(f\\\"  key_{i}: {status}\\\")\\n\",\n    \"    \\n\",\n    \"    final_stats = cache.get_stats()\\n\",\n    \"    print(f\\\"\\\\nCache final stats: {final_stats}\\\")\\n\",\n    \"    \\n\",\n    \"    # Test object pool\\n\",\n    \"    print(f\\\"\\\\nüèä Testing object pool:\\\")\\n\",\n    \"    \\n\",\n    \"    class ExpensiveResource:\\n\",\n    \"        def __init__(self):\\n\",\n    \"            time.sleep(0.01)  # Simulate expensive creation\\n\",\n    \"            self.created_at = time.time()\\n\",\n    \"            self.usage_count = 0\\n\",\n    \"        \\n\",\n    \"        def reset(self):\\n\",\n    \"            self.usage_count = 0\\n\",\n    \"        \\n\",\n    \"        def use(self):\\n\",\n    \"            self.usage_count += 1\\n\",\n    \"            return f\\\"Used {self.usage_count} times\\\"\\n\",\n    \"    \\n\",\n    \"    pool = ObjectPool(ExpensiveResource, max_size=3)\\n\",\n    \"    \\n\",\n    \"    # Use objects from pool\\n\",\n    \"    with profile_time(\\\"Object pool operations\\\"):\\n\",\n    \"        for i in range(5):\\n\",\n    \"            with pool.borrow() as obj:\\n\",\n    \"                result = obj.use()\\n\",\n    \"                print(f\\\"  Operation {i}: {result}\\\")\\n\",\n    \"    \\n\",\n    \"    pool_stats = pool.get_stats()\\n\",\n    \"    print(f\\\"Pool stats: {pool_stats}\\\")\\n\",\n    \"    \\n\",\n    \"    # Compare __slots__ vs regular classes\\n\",\n    \"    print(f\\\"\\\\nüìè Memory comparison (__slots__ vs regular):\\\")\\n\",\n    \"    \\n\",\n    \"    # Create instances\\n\",\n    \"    efficient_point = EfficientDataPoint(1.0, 2.0, \\\"positive\\\", 0.95)\\n\",\n    \"    regular_point = RegularDataPoint(1.0, 2.0, \\\"positive\\\", 0.95)\\n\",\n    \"    \\n\",\n    \"    efficient_size = sys.getsizeof(efficient_point)\\n\",\n    \"    regular_size = sys.getsizeof(regular_point)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  EfficientDataPoint (__slots__): {efficient_size} bytes\\\")\\n\",\n    \"    print(f\\\"  RegularDataPoint: {regular_size} bytes\\\")\\n\",\n    \"    print(f\\\"  Memory savings: {((regular_size - efficient_size) / regular_size) * 100:.1f}%\\\")\\n\",\n    \"    \\n\",\n    \"    # Test with many objects\\n\",\n    \"    n_objects = 10000\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"Creating efficient objects\\\"):\\n\",\n    \"        efficient_objects = [\\n\",\n    \"            EfficientDataPoint(i, i*2, f\\\"label_{i%3}\\\", i/n_objects)\\n\",\n    \"            for i in range(n_objects)\\n\",\n    \"        ]\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"Creating regular objects\\\"):\\n\",\n    \"        regular_objects = [\\n\",\n    \"            RegularDataPoint(i, i*2, f\\\"label_{i%3}\\\", i/n_objects)\\n\",\n    \"            for i in range(n_objects)\\n\",\n    \"        ]\\n\",\n    \"    \\n\",\n    \"    # Clean up\\n\",\n    \"    del efficient_objects, regular_objects\\n\",\n    \"    gc.collect()\\n\",\n    \"\\n\",\n    \"demonstrate_memory_efficient_structures()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Memory Efficiency Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Reduced memory usage with __slots__\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Smart caching with size and memory limits\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Object pooling for expensive resources\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Efficient data structures (deque, weakref)\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 3: Performance Profiling and Monitoring\\n\",\n    \"\\n\",\n    \"**First Principle**: Measure performance to identify bottlenecks before optimizing.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `cProfile`, `tracemalloc`, and custom monitoring for comprehensive profiling.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class PerformanceMonitor:\\n\",\n    \"    \\\"\\\"\\\"Monitor for tracking performance metrics.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.metrics = defaultdict(list)\\n\",\n    \"        self.counters = defaultdict(int)\\n\",\n    \"        self.start_times = {}\\n\",\n    \"    \\n\",\n    \"    def start_timer(self, operation: str):\\n\",\n    \"        \\\"\\\"\\\"Start timing an operation.\\\"\\\"\\\"\\n\",\n    \"        self.start_times[operation] = time.perf_counter()\\n\",\n    \"    \\n\",\n    \"    def stop_timer(self, operation: str):\\n\",\n    \"        \\\"\\\"\\\"Stop timing an operation and record the duration.\\\"\\\"\\\"\\n\",\n    \"        if operation in self.start_times:\\n\",\n    \"            duration = time.perf_counter() - self.start_times[operation]\\n\",\n    \"            self.record_time(operation, duration)\\n\",\n    \"            del self.start_times[operation]\\n\",\n    \"            return duration\\n\",\n    \"        return 0.0\\n\",\n    \"    \\n\",\n    \"    def record_time(self, operation: str, duration: float):\\n\",\n    \"        \\\"\\\"\\\"Record operation timing.\\\"\\\"\\\"\\n\",\n    \"        self.metrics[f\\\"{operation}_time\\\"].append(duration)\\n\",\n    \"        self.counters[f\\\"{operation}_count\\\"] += 1\\n\",\n    \"    \\n\",\n    \"    def record_memory(self, operation: str, memory_mb: float):\\n\",\n    \"        \\\"\\\"\\\"Record memory usage.\\\"\\\"\\\"\\n\",\n    \"        self.metrics[f\\\"{operation}_memory\\\"].append(memory_mb)\\n\",\n    \"    \\n\",\n    \"    def increment_counter(self, name: str, value: int = 1):\\n\",\n    \"        \\\"\\\"\\\"Increment a counter.\\\"\\\"\\\"\\n\",\n    \"        self.counters[name] += value\\n\",\n    \"    \\n\",\n    \"    @contextmanager\\n\",\n    \"    def time_operation(self, operation: str):\\n\",\n    \"        \\\"\\\"\\\"Context manager for timing operations.\\\"\\\"\\\"\\n\",\n    \"        self.start_timer(operation)\\n\",\n    \"        try:\\n\",\n    \"            yield\\n\",\n    \"        finally:\\n\",\n    \"            self.stop_timer(operation)\\n\",\n    \"    \\n\",\n    \"    def get_summary(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get performance summary.\\\"\\\"\\\"\\n\",\n    \"        summary = {}\\n\",\n    \"        \\n\",\n    \"        for metric_name, values in self.metrics.items():\\n\",\n    \"            if values:\\n\",\n    \"                summary[metric_name] = {\\n\",\n    \"                    \\\"avg\\\": sum(values) / len(values),\\n\",\n    \"                    \\\"min\\\": min(values),\\n\",\n    \"                    \\\"max\\\": max(values),\\n\",\n    \"                    \\\"count\\\": len(values),\\n\",\n    \"                    \\\"total\\\": sum(values)\\n\",\n    \"                }\\n\",\n    \"        \\n\",\n    \"        summary[\\\"counters\\\"] = dict(self.counters)\\n\",\n    \"        return summary\\n\",\n    \"\\n\",\n    \"# Example AI processing functions for profiling\\n\",\n    \"def simulate_data_loading(size: int) -> List[Dict[str, Any]]:\\n\",\n    \"    \\\"\\\"\\\"Simulate loading data.\\\"\\\"\\\"\\n\",\n    \"    return [{\\n\",\n    \"        \\\"id\\\": i,\\n\",\n    \"        \\\"features\\\": [j * 0.1 for j in range(10)],\\n\",\n    \"        \\\"label\\\": i % 3,\\n\",\n    \"        \\\"metadata\\\": {\\\"source\\\": f\\\"dataset_{i//100}\\\"}\\n\",\n    \"    } for i in range(size)]\\n\",\n    \"\\n\",\n    \"def simulate_feature_extraction(data_point: Dict[str, Any]) -> List[float]:\\n\",\n    \"    \\\"\\\"\\\"Simulate feature extraction.\\\"\\\"\\\"\\n\",\n    \"    features = data_point[\\\"features\\\"]\\n\",\n    \"    # Simulate some processing\\n\",\n    \"    processed = []\\n\",\n    \"    for f in features:\\n\",\n    \"        processed.append(f * f)  # Square\\n\",\n    \"        processed.append(f * 2)  # Double\\n\",\n    \"    return processed\\n\",\n    \"\\n\",\n    \"def simulate_model_inference(features: List[float]) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Simulate model inference.\\\"\\\"\\\"\\n\",\n    \"    # Simulate computation\\n\",\n    \"    score = sum(features) / len(features) if features else 0\\n\",\n    \"    prediction = \\\"positive\\\" if score > 0.5 else \\\"negative\\\"\\n\",\n    \"    confidence = min(abs(score), 1.0)\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"prediction\\\": prediction,\\n\",\n    \"        \\\"confidence\\\": confidence,\\n\",\n    \"        \\\"score\\\": score\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"@lru_cache(maxsize=1000)\\n\",\n    \"def cached_expensive_computation(data_hash: str) -> float:\\n\",\n    \"    \\\"\\\"\\\"Expensive computation with caching.\\\"\\\"\\\"\\n\",\n    \"    # Simulate expensive computation\\n\",\n    \"    result = 0.0\\n\",\n    \"    for i in range(1000):\\n\",\n    \"        result += hash(data_hash + str(i)) % 100 / 100.0\\n\",\n    \"    return result\\n\",\n    \"\\n\",\n    \"def demonstrate_profiling():\\n\",\n    \"    \\\"\\\"\\\"Show profiling techniques.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üìä Performance Profiling\\\", style=\\\"bold green\\\"))\\n\",\n    \"    \\n\",\n    \"    monitor = PerformanceMonitor()\\n\",\n    \"    \\n\",\n    \"    print(\\\"üîç Profiling AI processing pipeline:\\\")\\n\",\n    \"    \\n\",\n    \"    # Simulate processing pipeline\\n\",\n    \"    with monitor.time_operation(\\\"data_loading\\\"):\\n\",\n    \"        data = simulate_data_loading(1000)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Loaded {len(data)} data points\\\")\\n\",\n    \"    \\n\",\n    \"    # Process data with detailed monitoring\\n\",\n    \"    results = []\\n\",\n    \"    \\n\",\n    \"    for i, data_point in enumerate(data[:100]):  # Process subset\\n\",\n    \"        # Feature extraction\\n\",\n    \"        with monitor.time_operation(\\\"feature_extraction\\\"):\\n\",\n    \"            features = simulate_feature_extraction(data_point)\\n\",\n    \"        \\n\",\n    \"        # Model inference\\n\",\n    \"        with monitor.time_operation(\\\"model_inference\\\"):\\n\",\n    \"            prediction = simulate_model_inference(features)\\n\",\n    \"        \\n\",\n    \"        # Expensive computation (with caching)\\n\",\n    \"        data_hash = str(hash(str(data_point[\\\"features\\\"])))\\n\",\n    \"        with monitor.time_operation(\\\"expensive_computation\\\"):\\n\",\n    \"            expensive_result = cached_expensive_computation(data_hash)\\n\",\n    \"        \\n\",\n    \"        monitor.increment_counter(\\\"processed_items\\\")\\n\",\n    \"        \\n\",\n    \"        results.append({\\n\",\n    \"            \\\"prediction\\\": prediction,\\n\",\n    \"            \\\"expensive_result\\\": expensive_result\\n\",\n    \"        })\\n\",\n    \"    \\n\",\n    \"    # Show performance summary\\n\",\n    \"    summary = monitor.get_summary()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìà Performance Summary:\\\")\\n\",\n    \"    for metric, stats in summary.items():\\n\",\n    \"        if isinstance(stats, dict) and 'avg' in stats:\\n\",\n    \"            print(f\\\"  {metric}: avg={stats['avg']:.4f}s, min={stats['min']:.4f}s, max={stats['max']:.4f}s, count={stats['count']}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüî¢ Counters: {summary.get('counters', {})}\\\")\\n\",\n    \"    \\n\",\n    \"    # Cache performance\\n\",\n    \"    cache_info = cached_expensive_computation.cache_info()\\n\",\n    \"    print(f\\\"\\\\nüíæ Cache performance: {cache_info}\\\")\\n\",\n    \"    \\n\",\n    \"    # Profile memory usage of different operations\\n\",\n    \"    print(f\\\"\\\\nüß† Memory profiling different operations:\\\")\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"Large list creation\\\"):\\n\",\n    \"        large_list = [i for i in range(100000)]\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"Dictionary creation\\\"):\\n\",\n    \"        large_dict = {f\\\"key_{i}\\\": i for i in range(10000)}\\n\",\n    \"    \\n\",\n    \"    with profile_memory(\\\"NumPy array creation\\\"):\\n\",\n    \"        large_array = np.arange(100000)\\n\",\n    \"    \\n\",\n    \"    # Clean up\\n\",\n    \"    del large_list, large_dict, large_array\\n\",\n    \"    gc.collect()\\n\",\n    \"\\n\",\n    \"# Advanced profiling with cProfile\\n\",\n    \"def demonstrate_cprofile():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate cProfile usage.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\n‚ö° Advanced profiling with cProfile:\\\")\\n\",\n    \"    \\n\",\n    \"    def profile_target():\\n\",\n    \"        \\\"\\\"\\\"Function to profile.\\\"\\\"\\\"\\n\",\n    \"        data = simulate_data_loading(500)\\n\",\n    \"        \\n\",\n    \"        results = []\\n\",\n    \"        for item in data[:50]:\\n\",\n    \"            features = simulate_feature_extraction(item)\\n\",\n    \"            prediction = simulate_model_inference(features)\\n\",\n    \"            results.append(prediction)\\n\",\n    \"        \\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    # Profile with cProfile\\n\",\n    \"    profiler = cProfile.Profile()\\n\",\n    \"    profiler.enable()\\n\",\n    \"    \\n\",\n    \"    start_time = time.perf_counter()\\n\",\n    \"    results = profile_target()\\n\",\n    \"    duration = time.perf_counter() - start_time\\n\",\n    \"    \\n\",\n    \"    profiler.disable()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Processed {len(results)} items in {duration:.4f}s\\\")\\n\",\n    \"    print(f\\\"  Rate: {len(results) / duration:.1f} items/second\\\")\\n\",\n    \"    \\n\",\n    \"    # Analyze profiling results\\n\",\n    \"    stats = pstats.Stats(profiler)\\n\",\n    \"    stats.sort_stats('cumulative')\\n\",\n    \"    \\n\",\n    \"    # Get top functions by cumulative time\\n\",\n    \"    print(f\\\"\\\\nüìä Top functions by cumulative time:\\\")\\n\",\n    \"    stats.print_stats(10)  # Top 10 functions\\n\",\n    \"\\n\",\n    \"# Benchmark different approaches\\n\",\n    \"def demonstrate_benchmarking():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate benchmarking different approaches.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüèÅ Benchmarking different approaches:\\\")\\n\",\n    \"    \\n\",\n    \"    data = list(range(10000))\\n\",\n    \"    \\n\",\n    \"    # Different ways to process data\\n\",\n    \"    def approach_1_loop(data):\\n\",\n    \"        \\\"\\\"\\\"Traditional loop.\\\"\\\"\\\"\\n\",\n    \"        result = []\\n\",\n    \"        for item in data:\\n\",\n    \"            result.append(item * 2)\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    def approach_2_comprehension(data):\\n\",\n    \"        \\\"\\\"\\\"List comprehension.\\\"\\\"\\\"\\n\",\n    \"        return [item * 2 for item in data]\\n\",\n    \"    \\n\",\n    \"    def approach_3_map(data):\\n\",\n    \"        \\\"\\\"\\\"Map function.\\\"\\\"\\\"\\n\",\n    \"        return list(map(lambda x: x * 2, data))\\n\",\n    \"    \\n\",\n    \"    def approach_4_numpy(data):\\n\",\n    \"        \\\"\\\"\\\"NumPy vectorized.\\\"\\\"\\\"\\n\",\n    \"        arr = np.array(data)\\n\",\n    \"        return (arr * 2).tolist()\\n\",\n    \"    \\n\",\n    \"    # Benchmark all approaches\\n\",\n    \"    approaches = [\\n\",\n    \"        (\\\"Traditional loop\\\", approach_1_loop),\\n\",\n    \"        (\\\"List comprehension\\\", approach_2_comprehension),\\n\",\n    \"        (\\\"Map function\\\", approach_3_map),\\n\",\n    \"        (\\\"NumPy vectorized\\\", approach_4_numpy)\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    compare_performance(approaches, data)\\n\",\n    \"\\n\",\n    \"# Run profiling demonstrations\\n\",\n    \"demonstrate_profiling()\\n\",\n    \"demonstrate_cprofile()\\n\",\n    \"demonstrate_benchmarking()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Profiling Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Identifies performance bottlenecks\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Measures memory usage accurately\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Tracks cache effectiveness\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Compares different approaches objectively\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 4: Vectorized Operations with NumPy\\n\",\n    \"\\n\",\n    \"**First Principle**: Vectorized operations eliminate Python loops and leverage optimized C code for massive speedups.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use NumPy arrays and vectorized functions instead of Python loops for numerical computations.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class VectorizedTextProcessor:\\n\",\n    \"    \\\"\\\"\\\"Text processor using vectorized operations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def batch_tokenize(texts: List[str]) -> List[List[str]]:\\n\",\n    \"        \\\"\\\"\\\"Vectorized tokenization.\\\"\\\"\\\"\\n\",\n    \"        # This is still Python-level, but demonstrates batch processing\\n\",\n    \"        return [text.lower().split() for text in texts]\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def batch_compute_lengths(texts: List[str]) -> np.ndarray:\\n\",\n    \"        \\\"\\\"\\\"Compute text lengths using NumPy.\\\"\\\"\\\"\\n\",\n    \"        return np.array([len(text) for text in texts])\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def batch_filter_by_length(texts: List[str], min_length: int) -> List[str]:\\n\",\n    \"        \\\"\\\"\\\"Filter texts by length using vectorized operations.\\\"\\\"\\\"\\n\",\n    \"        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\\n\",\n    \"        mask = lengths >= min_length\\n\",\n    \"        return [text for text, keep in zip(texts, mask) if keep]\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def batch_similarity_matrix(texts: List[str]) -> np.ndarray:\\n\",\n    \"        \\\"\\\"\\\"Compute similarity matrix using vectorized operations.\\\"\\\"\\\"\\n\",\n    \"        n = len(texts)\\n\",\n    \"        \\n\",\n    \"        # Get lengths as vectors\\n\",\n    \"        lengths = VectorizedTextProcessor.batch_compute_lengths(texts)\\n\",\n    \"        \\n\",\n    \"        # Compute pairwise length differences using broadcasting\\n\",\n    \"        length_diff = np.abs(lengths[:, np.newaxis] - lengths[np.newaxis, :])\\n\",\n    \"        \\n\",\n    \"        # Similarity based on length difference (simplified)\\n\",\n    \"        similarity = 1.0 / (1.0 + length_diff)\\n\",\n    \"        \\n\",\n    \"        # Set diagonal to 1.0 (self-similarity)\\n\",\n    \"        np.fill_diagonal(similarity, 1.0)\\n\",\n    \"        \\n\",\n    \"        return similarity\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def compute_text_features(texts: List[str]) -> np.ndarray:\\n\",\n    \"        \\\"\\\"\\\"Compute multiple text features efficiently.\\\"\\\"\\\"\\n\",\n    \"        n = len(texts)\\n\",\n    \"        features = np.zeros((n, 4))  # 4 features per text\\n\",\n    \"        \\n\",\n    \"        for i, text in enumerate(texts):\\n\",\n    \"            words = text.split()\\n\",\n    \"            features[i, 0] = len(text)  # Character count\\n\",\n    \"            features[i, 1] = len(words)  # Word count\\n\",\n    \"            features[i, 2] = len(set(words)) if words else 0  # Unique words\\n\",\n    \"            features[i, 3] = sum(len(word) for word in words) / len(words) if words else 0  # Avg word length\\n\",\n    \"        \\n\",\n    \"        return features\\n\",\n    \"\\n\",\n    \"def demonstrate_vectorization():\\n\",\n    \"    \\\"\\\"\\\"Show vectorized operations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"‚ö° Vectorized Operations\\\", style=\\\"bold magenta\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create test data\\n\",\n    \"    test_texts = [f\\\"Sample text number {i} for vectorized processing analysis\\\" for i in range(1000)]\\n\",\n    \"    \\n\",\n    \"    print(\\\"üìä Comparing scalar vs vectorized operations:\\\")\\n\",\n    \"    \\n\",\n    \"    # Scalar operations (Python loops)\\n\",\n    \"    with profile_time(\\\"Scalar length computation\\\"):\\n\",\n    \"        scalar_lengths = [len(text) for text in test_texts]\\n\",\n    \"    \\n\",\n    \"    # Vectorized operations\\n\",\n    \"    with profile_time(\\\"Vectorized length computation\\\"):\\n\",\n    \"        vector_lengths = VectorizedTextProcessor.batch_compute_lengths(test_texts)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results match: {np.array_equal(scalar_lengths, vector_lengths)}\\\")\\n\",\n    \"    \\n\",\n    \"    # Mathematical operations comparison\\n\",\n    \"    print(f\\\"\\\\nüî¢ Mathematical operations comparison:\\\")\\n\",\n    \"    \\n\",\n    \"    data = list(range(100000))\\n\",\n    \"    \\n\",\n    \"    # Scalar math\\n\",\n    \"    with profile_time(\\\"Scalar math (sum of squares)\\\"):\\n\",\n    \"        scalar_result = sum(x*x for x in data)\\n\",\n    \"    \\n\",\n    \"    # Vectorized math\\n\",\n    \"    with profile_time(\\\"Vectorized math (sum of squares)\\\"):\\n\",\n    \"        arr = np.array(data)\\n\",\n    \"        vector_result = np.sum(arr**2)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results match: {scalar_result == vector_result}\\\")\\n\",\n    \"    \\n\",\n    \"    # Complex vectorized operations\\n\",\n    \"    print(f\\\"\\\\nüßÆ Complex vectorized operations:\\\")\\n\",\n    \"    \\n\",\n    \"    # Text feature extraction\\n\",\n    \"    sample_texts = test_texts[:100]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Feature extraction\\\"):\\n\",\n    \"        features = VectorizedTextProcessor.compute_text_features(sample_texts)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Extracted features for {len(sample_texts)} texts\\\")\\n\",\n    \"    print(f\\\"  Feature matrix shape: {features.shape}\\\")\\n\",\n    \"    print(f\\\"  Average features: {np.mean(features, axis=0)}\\\")\\n\",\n    \"    \\n\",\n    \"    # Similarity matrix computation\\n\",\n    \"    small_sample = test_texts[:20]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Similarity matrix computation\\\"):\\n\",\n    \"        similarity_matrix = VectorizedTextProcessor.batch_similarity_matrix(small_sample)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Similarity matrix shape: {similarity_matrix.shape}\\\")\\n\",\n    \"    print(f\\\"  Average similarity: {np.mean(similarity_matrix[similarity_matrix != 1.0]):.3f}\\\")\\n\",\n    \"    \\n\",\n    \"    # Broadcasting examples\\n\",\n    \"    print(f\\\"\\\\nüì° Broadcasting examples:\\\")\\n\",\n    \"    \\n\",\n    \"    # Create sample matrices\\n\",\n    \"    matrix_a = np.random.rand(1000, 50)\\n\",\n    \"    matrix_b = np.random.rand(50)\\n\",\n    \"    \\n\",\n    \"    # Broadcasting operation\\n\",\n    \"    with profile_time(\\\"Broadcasting operation (1000x50 + 50)\\\"):\\n\",\n    \"        result = matrix_a + matrix_b  # Broadcasting automatically applied\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Result shape: {result.shape}\\\")\\n\",\n    \"    \\n\",\n    \"    # Element-wise operations\\n\",\n    \"    large_array = np.random.rand(100000)\\n\",\n    \"    \\n\",\n    \"    operations = [\\n\",\n    \"        (\\\"Square\\\", lambda x: x**2),\\n\",\n    \"        (\\\"Square root\\\", lambda x: np.sqrt(x)),\\n\",\n    \"        (\\\"Exponential\\\", lambda x: np.exp(x)),\\n\",\n    \"        (\\\"Logarithm\\\", lambda x: np.log(x + 1e-8)),  # Add small value to avoid log(0)\\n\",\n    \"        (\\\"Trigonometric\\\", lambda x: np.sin(x) + np.cos(x))\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\n‚ö° Element-wise operations on {len(large_array)} elements:\\\")\\n\",\n    \"    \\n\",\n    \"    for name, operation in operations:\\n\",\n    \"        with profile_time(f\\\"{name} operation\\\"):\\n\",\n    \"            result = operation(large_array)\\n\",\n    \"        print(f\\\"  {name}: min={np.min(result):.3f}, max={np.max(result):.3f}, mean={np.mean(result):.3f}\\\")\\n\",\n    \"\\n\",\n    \"# Advanced vectorization patterns\\n\",\n    \"def demonstrate_advanced_vectorization():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate advanced vectorization patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüöÄ Advanced vectorization patterns:\\\")\\n\",\n    \"    \\n\",\n    \"    # Conditional operations with where\\n\",\n    \"    data = np.random.randn(10000)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Conditional operations with np.where\\\"):\\n\",\n    \"        # Replace negative values with 0, keep positive\\n\",\n    \"        result = np.where(data > 0, data, 0)\\n\",\n    \"    \\n\",\n    \"    positive_count = np.sum(result > 0)\\n\",\n    \"    print(f\\\"  Positive values: {positive_count}/{len(data)}\\\")\\n\",\n    \"    \\n\",\n    \"    # Vectorized string operations\\n\",\n    \"    text_data = np.array([f\\\"text_{i}\\\" for i in range(1000)])\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Vectorized string operations\\\"):\\n\",\n    \"        # Vectorized string length\\n\",\n    \"        str_lengths = np.char.str_len(text_data)\\n\",\n    \"        # Vectorized string operations\\n\",\n    \"        upper_texts = np.char.upper(text_data)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  String operations completed on {len(text_data)} strings\\\")\\n\",\n    \"    \\n\",\n    \"    # Aggregation operations\\n\",\n    \"    matrix = np.random.rand(1000, 100)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Matrix aggregations\\\"):\\n\",\n    \"        row_sums = np.sum(matrix, axis=1)  # Sum each row\\n\",\n    \"        col_means = np.mean(matrix, axis=0)  # Mean of each column\\n\",\n    \"        global_std = np.std(matrix)  # Standard deviation of entire matrix\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Row sums: {len(row_sums)}, Col means: {len(col_means)}, Global std: {global_std:.3f}\\\")\\n\",\n    \"    \\n\",\n    \"    # Fancy indexing\\n\",\n    \"    large_data = np.random.rand(100000)\\n\",\n    \"    indices = np.random.randint(0, len(large_data), 10000)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Fancy indexing\\\"):\\n\",\n    \"        selected_data = large_data[indices]\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Selected {len(selected_data)} elements using fancy indexing\\\")\\n\",\n    \"    \\n\",\n    \"    # Boolean indexing\\n\",\n    \"    with profile_time(\\\"Boolean indexing\\\"):\\n\",\n    \"        filtered_data = large_data[large_data > 0.5]\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Filtered to {len(filtered_data)} elements using boolean indexing\\\")\\n\",\n    \"\\n\",\n    \"# Memory layout optimization\\n\",\n    \"def demonstrate_memory_layout():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate memory layout optimization.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüíæ Memory layout optimization:\\\")\\n\",\n    \"    \\n\",\n    \"    # C-contiguous vs Fortran-contiguous\\n\",\n    \"    size = (1000, 1000)\\n\",\n    \"    \\n\",\n    \"    c_array = np.random.rand(*size)  # C-contiguous (row-major)\\n\",\n    \"    f_array = np.asfortranarray(c_array)  # Fortran-contiguous (column-major)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  C-array is C-contiguous: {c_array.flags.c_contiguous}\\\")\\n\",\n    \"    print(f\\\"  F-array is Fortran-contiguous: {f_array.flags.f_contiguous}\\\")\\n\",\n    \"    \\n\",\n    \"    # Row-wise operations (faster on C-contiguous)\\n\",\n    \"    with profile_time(\\\"Row-wise sum on C-contiguous\\\"):\\n\",\n    \"        c_row_sums = np.sum(c_array, axis=1)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Row-wise sum on F-contiguous\\\"):\\n\",\n    \"        f_row_sums = np.sum(f_array, axis=1)\\n\",\n    \"    \\n\",\n    \"    # Column-wise operations (faster on Fortran-contiguous)\\n\",\n    \"    with profile_time(\\\"Column-wise sum on C-contiguous\\\"):\\n\",\n    \"        c_col_sums = np.sum(c_array, axis=0)\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Column-wise sum on F-contiguous\\\"):\\n\",\n    \"        f_col_sums = np.sum(f_array, axis=0)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results are equal: {np.allclose(c_row_sums, f_row_sums) and np.allclose(c_col_sums, f_col_sums)}\\\")\\n\",\n    \"\\n\",\n    \"# Run vectorization demonstrations\\n\",\n    \"demonstrate_vectorization()\\n\",\n    \"demonstrate_advanced_vectorization()\\n\",\n    \"demonstrate_memory_layout()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Vectorization Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ 10-100x speedup for numerical operations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Memory-efficient operations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Automatic parallelization in many cases\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Leverages optimized BLAS/LAPACK libraries\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 5: Parallel Processing Patterns\\n\",\n    \"\\n\",\n    \"**First Principle**: Parallel processing leverages multiple cores for CPU-bound tasks and multiple threads for I/O-bound tasks.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use `ThreadPoolExecutor` for I/O-bound tasks, `ProcessPoolExecutor` for CPU-bound tasks.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class ParallelProcessor:\\n\",\n    \"    \\\"\\\"\\\"Utilities for parallel processing.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def parallel_map_threads(\\n\",\n    \"        func: Callable, \\n\",\n    \"        items: List[Any], \\n\",\n    \"        max_workers: int = 4\\n\",\n    \"    ) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Parallel map using threads (I/O bound tasks).\\\"\\\"\\\"\\n\",\n    \"        with ThreadPoolExecutor(max_workers=max_workers) as executor:\\n\",\n    \"            results = list(executor.map(func, items))\\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def parallel_map_processes(\\n\",\n    \"        func: Callable, \\n\",\n    \"        items: List[Any], \\n\",\n    \"        max_workers: int = None\\n\",\n    \"    ) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Parallel map using processes (CPU bound tasks).\\\"\\\"\\\"\\n\",\n    \"        if max_workers is None:\\n\",\n    \"            max_workers = mp.cpu_count()\\n\",\n    \"        \\n\",\n    \"        with ProcessPoolExecutor(max_workers=max_workers) as executor:\\n\",\n    \"            results = list(executor.map(func, items))\\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def parallel_batch_process(\\n\",\n    \"        items: List[Any], \\n\",\n    \"        batch_func: Callable[[List[Any]], List[Any]], \\n\",\n    \"        batch_size: int = 100,\\n\",\n    \"        max_workers: int = 4,\\n\",\n    \"        use_processes: bool = False\\n\",\n    \"    ) -> List[Any]:\\n\",\n    \"        \\\"\\\"\\\"Process items in parallel batches.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Split into batches\\n\",\n    \"        batches = [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\\n\",\n    \"        \\n\",\n    \"        # Process batches in parallel\\n\",\n    \"        if use_processes:\\n\",\n    \"            with ProcessPoolExecutor(max_workers=max_workers) as executor:\\n\",\n    \"                batch_results = list(executor.map(batch_func, batches))\\n\",\n    \"        else:\\n\",\n    \"            with ThreadPoolExecutor(max_workers=max_workers) as executor:\\n\",\n    \"                batch_results = list(executor.map(batch_func, batches))\\n\",\n    \"        \\n\",\n    \"        # Flatten results\\n\",\n    \"        results = []\\n\",\n    \"        for batch_result in batch_results:\\n\",\n    \"            results.extend(batch_result)\\n\",\n    \"        \\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def parallel_as_completed(\\n\",\n    \"        func: Callable,\\n\",\n    \"        items: List[Any],\\n\",\n    \"        max_workers: int = 4,\\n\",\n    \"        use_processes: bool = False\\n\",\n    \"    ) -> Iterator[Tuple[Any, Any]]:\\n\",\n    \"        \\\"\\\"\\\"Process items in parallel, yielding results as they complete.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        executor_class = ProcessPoolExecutor if use_processes else ThreadPoolExecutor\\n\",\n    \"        \\n\",\n    \"        with executor_class(max_workers=max_workers) as executor:\\n\",\n    \"            # Submit all tasks\\n\",\n    \"            future_to_item = {executor.submit(func, item): item for item in items}\\n\",\n    \"            \\n\",\n    \"            # Yield results as they complete\\n\",\n    \"            for future in as_completed(future_to_item):\\n\",\n    \"                item = future_to_item[future]\\n\",\n    \"                try:\\n\",\n    \"                    result = future.result()\\n\",\n    \"                    yield item, result\\n\",\n    \"                except Exception as exc:\\n\",\n    \"                    yield item, exc\\n\",\n    \"\\n\",\n    \"# Example functions for parallel processing\\n\",\n    \"def cpu_intensive_task(data: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"CPU-intensive task for parallel processing demo.\\\"\\\"\\\"\\n\",\n    \"    # Simulate CPU work with hashing\\n\",\n    \"    result = data\\n\",\n    \"    for _ in range(10000):  # Increased iterations for more CPU work\\n\",\n    \"        result = str(hash(result))[:10]\\n\",\n    \"    return f\\\"CPU_PROCESSED: {result}\\\"\\n\",\n    \"\\n\",\n    \"def io_intensive_task(data: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"I/O-intensive task for parallel processing demo.\\\"\\\"\\\"\\n\",\n    \"    # Simulate I/O wait\\n\",\n    \"    time.sleep(0.1)\\n\",\n    \"    return f\\\"IO_PROCESSED: {data.upper()}\\\"\\n\",\n    \"\\n\",\n    \"def mixed_workload_task(data: str) -> str:\\n\",\n    \"    \\\"\\\"\\\"Mixed CPU and I/O task.\\\"\\\"\\\"\\n\",\n    \"    # CPU work\\n\",\n    \"    result = str(hash(data * 1000))\\n\",\n    \"    # I/O simulation\\n\",\n    \"    time.sleep(0.02)\\n\",\n    \"    return f\\\"MIXED: {result[:8]}\\\"\\n\",\n    \"\\n\",\n    \"def batch_text_processor(texts: List[str]) -> List[str]:\\n\",\n    \"    \\\"\\\"\\\"Process a batch of texts efficiently.\\\"\\\"\\\"\\n\",\n    \"    # Simulate batch processing efficiency\\n\",\n    \"    time.sleep(0.01)  # Setup overhead per batch\\n\",\n    \"    \\n\",\n    \"    results = []\\n\",\n    \"    for text in texts:\\n\",\n    \"        # Faster per-item processing in batch\\n\",\n    \"        processed = text.upper().replace(\\\" \\\", \\\"_\\\")\\n\",\n    \"        results.append(f\\\"BATCH_{processed}\\\")\\n\",\n    \"    \\n\",\n    \"    return results\\n\",\n    \"\\n\",\n    \"def demonstrate_parallel_processing():\\n\",\n    \"    \\\"\\\"\\\"Show parallel processing patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üöÄ Parallel Processing\\\", style=\\\"bold cyan\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create test data\\n\",\n    \"    test_data = [f\\\"data_item_{i}\\\" for i in range(20)]\\n\",\n    \"    \\n\",\n    \"    print(\\\"‚öôÔ∏è Comparing sequential vs parallel processing:\\\")\\n\",\n    \"    \\n\",\n    \"    # CPU-bound task comparison\\n\",\n    \"    print(f\\\"\\\\nüßÆ CPU-bound tasks:\\\")\\n\",\n    \"    \\n\",\n    \"    # Sequential processing\\n\",\n    \"    with profile_time(\\\"Sequential CPU processing\\\"):\\n\",\n    \"        sequential_cpu_results = [cpu_intensive_task(item) for item in test_data[:8]]  # Smaller subset\\n\",\n    \"    \\n\",\n    \"    # Parallel processing with processes\\n\",\n    \"    with profile_time(\\\"Parallel CPU processing (processes)\\\"):\\n\",\n    \"        parallel_cpu_results = ParallelProcessor.parallel_map_processes(\\n\",\n    \"            cpu_intensive_task, test_data[:8], max_workers=2\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results match: {sequential_cpu_results == parallel_cpu_results}\\\")\\n\",\n    \"    print(f\\\"  Processed {len(parallel_cpu_results)} items\\\")\\n\",\n    \"    \\n\",\n    \"    # I/O-bound task comparison\\n\",\n    \"    print(f\\\"\\\\nüíæ I/O-bound tasks:\\\")\\n\",\n    \"    \\n\",\n    \"    # Sequential I/O\\n\",\n    \"    with profile_time(\\\"Sequential I/O processing\\\"):\\n\",\n    \"        sequential_io_results = [io_intensive_task(item) for item in test_data[:10]]\\n\",\n    \"    \\n\",\n    \"    # Parallel I/O with threads\\n\",\n    \"    with profile_time(\\\"Parallel I/O processing (threads)\\\"):\\n\",\n    \"        parallel_io_results = ParallelProcessor.parallel_map_threads(\\n\",\n    \"            io_intensive_task, test_data[:10], max_workers=5\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Results match: {set(sequential_io_results) == set(parallel_io_results)}\\\")\\n\",\n    \"    print(f\\\"  Processed {len(parallel_io_results)} items\\\")\\n\",\n    \"    \\n\",\n    \"    # Batch processing comparison\\n\",\n    \"    print(f\\\"\\\\nüì¶ Batch processing:\\\")\\n\",\n    \"    \\n\",\n    \"    large_dataset = [f\\\"text_{i}\\\" for i in range(100)]\\n\",\n    \"    \\n\",\n    \"    # Individual processing\\n\",\n    \"    with profile_time(\\\"Individual processing\\\"):\\n\",\n    \"        individual_results = [item.upper().replace(\\\" \\\", \\\"_\\\") for item in large_dataset]\\n\",\n    \"    \\n\",\n    \"    # Batch processing\\n\",\n    \"    with profile_time(\\\"Parallel batch processing\\\"):\\n\",\n    \"        batch_results = ParallelProcessor.parallel_batch_process(\\n\",\n    \"            large_dataset, batch_text_processor, batch_size=20, max_workers=3\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Individual results: {len(individual_results)}\\\")\\n\",\n    \"    print(f\\\"  Batch results: {len(batch_results)}\\\")\\n\",\n    \"    \\n\",\n    \"    # Progressive results with as_completed\\n\",\n    \"    print(f\\\"\\\\n‚è±Ô∏è Progressive results (as_completed pattern):\\\")\\n\",\n    \"    \\n\",\n    \"    mixed_data = [f\\\"mixed_{i}\\\" for i in range(8)]\\n\",\n    \"    \\n\",\n    \"    start_time = time.time()\\n\",\n    \"    completed_count = 0\\n\",\n    \"    \\n\",\n    \"    for item, result in ParallelProcessor.parallel_as_completed(\\n\",\n    \"        mixed_workload_task, mixed_data, max_workers=3\\n\",\n    \"    ):\\n\",\n    \"        completed_count += 1\\n\",\n    \"        elapsed = time.time() - start_time\\n\",\n    \"        \\n\",\n    \"        if isinstance(result, Exception):\\n\",\n    \"            print(f\\\"  {elapsed:.2f}s: {item} FAILED - {result}\\\")\\n\",\n    \"        else:\\n\",\n    \"            print(f\\\"  {elapsed:.2f}s: {item} -> {result[:20]}...\\\")\\n\",\n    \"    \\n\",\n    \"    total_time = time.time() - start_time\\n\",\n    \"    print(f\\\"  Completed {completed_count} tasks in {total_time:.2f}s\\\")\\n\",\n    \"\\n\",\n    \"# Advanced parallel patterns\\n\",\n    \"def demonstrate_advanced_parallel_patterns():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate advanced parallel processing patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüéØ Advanced parallel patterns:\\\")\\n\",\n    \"    \\n\",\n    \"    # Map-Reduce pattern\\n\",\n    \"    def map_phase(data_chunk: List[int]) -> Dict[str, int]:\\n\",\n    \"        \\\"\\\"\\\"Map phase: process chunk and return key-value pairs.\\\"\\\"\\\"\\n\",\n    \"        result = {\\\"even\\\": 0, \\\"odd\\\": 0, \\\"sum\\\": 0}\\n\",\n    \"        for num in data_chunk:\\n\",\n    \"            if num % 2 == 0:\\n\",\n    \"                result[\\\"even\\\"] += 1\\n\",\n    \"            else:\\n\",\n    \"                result[\\\"odd\\\"] += 1\\n\",\n    \"            result[\\\"sum\\\"] += num\\n\",\n    \"        return result\\n\",\n    \"    \\n\",\n    \"    def reduce_phase(results: List[Dict[str, int]]) -> Dict[str, int]:\\n\",\n    \"        \\\"\\\"\\\"Reduce phase: combine results from all workers.\\\"\\\"\\\"\\n\",\n    \"        final_result = {\\\"even\\\": 0, \\\"odd\\\": 0, \\\"sum\\\": 0}\\n\",\n    \"        for result in results:\\n\",\n    \"            for key in final_result:\\n\",\n    \"                final_result[key] += result[key]\\n\",\n    \"        return final_result\\n\",\n    \"    \\n\",\n    \"    # Generate test data\\n\",\n    \"    large_numbers = list(range(10000))\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Map-Reduce pattern\\\"):\\n\",\n    \"        # Split data into chunks\\n\",\n    \"        chunk_size = 1000\\n\",\n    \"        chunks = [large_numbers[i:i + chunk_size] for i in range(0, len(large_numbers), chunk_size)]\\n\",\n    \"        \\n\",\n    \"        # Map phase (parallel)\\n\",\n    \"        map_results = ParallelProcessor.parallel_map_processes(map_phase, chunks, max_workers=3)\\n\",\n    \"        \\n\",\n    \"        # Reduce phase (sequential)\\n\",\n    \"        final_result = reduce_phase(map_results)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Map-Reduce result: {final_result}\\\")\\n\",\n    \"    \\n\",\n    \"    # Pipeline pattern\\n\",\n    \"    print(f\\\"\\\\nüîÑ Pipeline processing pattern:\\\")\\n\",\n    \"    \\n\",\n    \"    def stage1_preprocess(data: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Stage 1: Preprocessing.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.01)\\n\",\n    \"        return f\\\"PREPROCESSED_{data}\\\"\\n\",\n    \"    \\n\",\n    \"    def stage2_analyze(data: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Stage 2: Analysis.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.02)\\n\",\n    \"        return f\\\"ANALYZED_{data}\\\"\\n\",\n    \"    \\n\",\n    \"    def stage3_postprocess(data: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Stage 3: Post-processing.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.01)\\n\",\n    \"        return f\\\"FINAL_{data}\\\"\\n\",\n    \"    \\n\",\n    \"    pipeline_data = [f\\\"item_{i}\\\" for i in range(20)]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Sequential pipeline\\\"):\\n\",\n    \"        # Sequential pipeline\\n\",\n    \"        stage1_results = [stage1_preprocess(item) for item in pipeline_data]\\n\",\n    \"        stage2_results = [stage2_analyze(item) for item in stage1_results]\\n\",\n    \"        stage3_results = [stage3_postprocess(item) for item in stage2_results]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Parallel pipeline\\\"):\\n\",\n    \"        # Parallel pipeline\\n\",\n    \"        stage1_parallel = ParallelProcessor.parallel_map_threads(stage1_preprocess, pipeline_data, max_workers=3)\\n\",\n    \"        stage2_parallel = ParallelProcessor.parallel_map_threads(stage2_analyze, stage1_parallel, max_workers=3)\\n\",\n    \"        stage3_parallel = ParallelProcessor.parallel_map_threads(stage3_postprocess, stage2_parallel, max_workers=3)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Sequential pipeline: {len(stage3_results)} items\\\")\\n\",\n    \"    print(f\\\"  Parallel pipeline: {len(stage3_parallel)} items\\\")\\n\",\n    \"    \\n\",\n    \"    # Worker pool with different task types\\n\",\n    \"    print(f\\\"\\\\nüë• Mixed workload distribution:\\\")\\n\",\n    \"    \\n\",\n    \"    # Create mixed tasks\\n\",\n    \"    cpu_tasks = [(\\\"cpu\\\", f\\\"cpu_task_{i}\\\") for i in range(4)]\\n\",\n    \"    io_tasks = [(\\\"io\\\", f\\\"io_task_{i}\\\") for i in range(8)]\\n\",\n    \"    all_tasks = cpu_tasks + io_tasks\\n\",\n    \"    \\n\",\n    \"    def process_mixed_task(task_info: Tuple[str, str]) -> str:\\n\",\n    \"        task_type, data = task_info\\n\",\n    \"        if task_type == \\\"cpu\\\":\\n\",\n    \"            return cpu_intensive_task(data)\\n\",\n    \"        else:\\n\",\n    \"            return io_intensive_task(data)\\n\",\n    \"    \\n\",\n    \"    # Process with appropriate parallelization\\n\",\n    \"    with profile_time(\\\"Mixed workload processing\\\"):\\n\",\n    \"        # Separate CPU and I/O tasks\\n\",\n    \"        cpu_results = ParallelProcessor.parallel_map_processes(\\n\",\n    \"            process_mixed_task, cpu_tasks, max_workers=2\\n\",\n    \"        )\\n\",\n    \"        io_results = ParallelProcessor.parallel_map_threads(\\n\",\n    \"            process_mixed_task, io_tasks, max_workers=4\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  CPU tasks completed: {len(cpu_results)}\\\")\\n\",\n    \"    print(f\\\"  I/O tasks completed: {len(io_results)}\\\")\\n\",\n    \"\\n\",\n    \"# System information\\n\",\n    \"def show_system_info():\\n\",\n    \"    \\\"\\\"\\\"Show system information for parallel processing.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüíª System Information:\\\")\\n\",\n    \"    print(f\\\"  CPU cores: {mp.cpu_count()}\\\")\\n\",\n    \"    print(f\\\"  Active threads: {threading.active_count()}\\\")\\n\",\n    \"    print(f\\\"  Current process ID: {mp.current_process().pid}\\\")\\n\",\n    \"\\n\",\n    \"# Run parallel processing demonstrations\\n\",\n    \"show_system_info()\\n\",\n    \"demonstrate_parallel_processing()\\n\",\n    \"demonstrate_advanced_parallel_patterns()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüí° Parallel Processing Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Utilizes multiple CPU cores\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Overlaps I/O operations\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Scales with available hardware\\\")\\n\",\n    \"print(f\\\"  ‚úÖ Improves throughput for batch operations\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Cell 6: Production Performance System\\n\",\n    \"\\n\",\n    \"**First Principle**: Combine all performance techniques to create a comprehensive, optimized AI processing system.\\n\",\n    \"\\n\",\n    \"**Python Idiom**: Use composition of optimized components to build high-performance systems.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Complete high-performance AI processing system\\n\",\n    \"from dataclasses import dataclass, field\\n\",\n    \"from typing import Union\\n\",\n    \"import pickle\\n\",\n    \"import hashlib\\n\",\n    \"import os\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class PerformanceConfig:\\n\",\n    \"    \\\"\\\"\\\"Configuration for performance optimization.\\\"\\\"\\\"\\n\",\n    \"    enable_caching: bool = True\\n\",\n    \"    cache_size: int = 1000\\n\",\n    \"    cache_ttl: float = 3600.0  # 1 hour\\n\",\n    \"    enable_parallel: bool = True\\n\",\n    \"    max_workers: int = 4\\n\",\n    \"    batch_size: int = 100\\n\",\n    \"    enable_vectorization: bool = True\\n\",\n    \"    memory_limit_mb: float = 500.0\\n\",\n    \"    enable_profiling: bool = False\\n\",\n    \"\\n\",\n    \"@dataclass\\n\",\n    \"class ProcessingResult:\\n\",\n    \"    \\\"\\\"\\\"Result from AI processing.\\\"\\\"\\\"\\n\",\n    \"    data: Any\\n\",\n    \"    processing_time: float\\n\",\n    \"    from_cache: bool = False\\n\",\n    \"    worker_id: Optional[str] = None\\n\",\n    \"    memory_used_mb: float = 0.0\\n\",\n    \"\\n\",\n    \"class HighPerformanceAIProcessor:\\n\",\n    \"    \\\"\\\"\\\"High-performance AI processor using all optimization techniques.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, config: PerformanceConfig):\\n\",\n    \"        self.config = config\\n\",\n    \"        \\n\",\n    \"        # Performance components\\n\",\n    \"        self.cache = MemoryEfficientCache(\\n\",\n    \"            max_size=config.cache_size,\\n\",\n    \"            max_memory_mb=config.memory_limit_mb * 0.5  # Use half memory limit for cache\\n\",\n    \"        ) if config.enable_caching else None\\n\",\n    \"        \\n\",\n    \"        self.monitor = PerformanceMonitor() if config.enable_profiling else None\\n\",\n    \"        \\n\",\n    \"        # Object pool for expensive resources\\n\",\n    \"        self.resource_pool = ObjectPool(\\n\",\n    \"            factory=lambda: {\\\"id\\\": f\\\"resource_{time.time()}\\\", \\\"data\\\": np.zeros(1000)},\\n\",\n    \"            max_size=config.max_workers\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        # Processing statistics\\n\",\n    \"        self.stats = {\\n\",\n    \"            \\\"total_processed\\\": 0,\\n\",\n    \"            \\\"cache_hits\\\": 0,\\n\",\n    \"            \\\"cache_misses\\\": 0,\\n\",\n    \"            \\\"total_time\\\": 0.0,\\n\",\n    \"            \\\"parallel_batches\\\": 0,\\n\",\n    \"            \\\"vectorized_operations\\\": 0\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    def _generate_cache_key(self, data: Any, operation: str) -> str:\\n\",\n    \"        \\\"\\\"\\\"Generate cache key for data and operation.\\\"\\\"\\\"\\n\",\n    \"        data_str = str(data) if len(str(data)) < 1000 else str(hash(str(data)))\\n\",\n    \"        return hashlib.md5(f\\\"{operation}_{data_str}\\\".encode()).hexdigest()\\n\",\n    \"    \\n\",\n    \"    def _check_cache(self, cache_key: str) -> Optional[ProcessingResult]:\\n\",\n    \"        \\\"\\\"\\\"Check if result is cached.\\\"\\\"\\\"\\n\",\n    \"        if not self.cache:\\n\",\n    \"            return None\\n\",\n    \"        \\n\",\n    \"        cached = self.cache.get(cache_key)\\n\",\n    \"        if cached:\\n\",\n    \"            self.stats[\\\"cache_hits\\\"] += 1\\n\",\n    \"            return ProcessingResult(\\n\",\n    \"                data=cached,\\n\",\n    \"                processing_time=0.0,\\n\",\n    \"                from_cache=True\\n\",\n    \"            )\\n\",\n    \"        \\n\",\n    \"        self.stats[\\\"cache_misses\\\"] += 1\\n\",\n    \"        return None\\n\",\n    \"    \\n\",\n    \"    def _store_cache(self, cache_key: str, result: Any):\\n\",\n    \"        \\\"\\\"\\\"Store result in cache.\\\"\\\"\\\"\\n\",\n    \"        if self.cache:\\n\",\n    \"            self.cache.put(cache_key, result)\\n\",\n    \"    \\n\",\n    \"    def process_single(self, data: Any, operation: str = \\\"default\\\") -> ProcessingResult:\\n\",\n    \"        \\\"\\\"\\\"Process a single item with full optimization.\\\"\\\"\\\"\\n\",\n    \"        start_time = time.perf_counter()\\n\",\n    \"        \\n\",\n    \"        # Check cache\\n\",\n    \"        cache_key = self._generate_cache_key(data, operation)\\n\",\n    \"        cached_result = self._check_cache(cache_key)\\n\",\n    \"        if cached_result:\\n\",\n    \"            return cached_result\\n\",\n    \"        \\n\",\n    \"        # Process with resource pooling\\n\",\n    \"        with self.resource_pool.borrow() as resource:\\n\",\n    \"            # Simulate AI processing based on operation\\n\",\n    \"            if operation == \\\"classify\\\":\\n\",\n    \"                result = self._classify(data, resource)\\n\",\n    \"            elif operation == \\\"vectorize\\\":\\n\",\n    \"                result = self._vectorize(data, resource)\\n\",\n    \"            elif operation == \\\"analyze\\\":\\n\",\n    \"                result = self._analyze(data, resource)\\n\",\n    \"            else:\\n\",\n    \"                result = self._default_process(data, resource)\\n\",\n    \"        \\n\",\n    \"        processing_time = time.perf_counter() - start_time\\n\",\n    \"        \\n\",\n    \"        # Store in cache\\n\",\n    \"        self._store_cache(cache_key, result)\\n\",\n    \"        \\n\",\n    \"        # Update statistics\\n\",\n    \"        self.stats[\\\"total_processed\\\"] += 1\\n\",\n    \"        self.stats[\\\"total_time\\\"] += processing_time\\n\",\n    \"        \\n\",\n    \"        if self.monitor:\\n\",\n    \"            self.monitor.record_time(operation, processing_time)\\n\",\n    \"        \\n\",\n    \"        return ProcessingResult(\\n\",\n    \"            data=result,\\n\",\n    \"            processing_time=processing_time,\\n\",\n    \"            from_cache=False,\\n\",\n    \"            worker_id=resource[\\\"id\\\"]\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    def process_batch(self, data_list: List[Any], operation: str = \\\"default\\\") -> List[ProcessingResult]:\\n\",\n    \"        \\\"\\\"\\\"Process batch with parallel and vectorized optimization.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        if not self.config.enable_parallel or len(data_list) < self.config.batch_size:\\n\",\n    \"            # Sequential processing for small batches\\n\",\n    \"            return [self.process_single(data, operation) for data in data_list]\\n\",\n    \"        \\n\",\n    \"        # Parallel batch processing\\n\",\n    \"        def process_batch_chunk(chunk: List[Any]) -> List[ProcessingResult]:\\n\",\n    \"            return [self.process_single(data, operation) for data in chunk]\\n\",\n    \"        \\n\",\n    \"        start_time = time.perf_counter()\\n\",\n    \"        \\n\",\n    \"        results = ParallelProcessor.parallel_batch_process(\\n\",\n    \"            data_list,\\n\",\n    \"            process_batch_chunk,\\n\",\n    \"            batch_size=self.config.batch_size,\\n\",\n    \"            max_workers=self.config.max_workers,\\n\",\n    \"            use_processes=False  # Use threads for I/O-bound AI operations\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        batch_time = time.perf_counter() - start_time\\n\",\n    \"        self.stats[\\\"parallel_batches\\\"] += 1\\n\",\n    \"        \\n\",\n    \"        if self.monitor:\\n\",\n    \"            self.monitor.record_time(f\\\"batch_{operation}\\\", batch_time)\\n\",\n    \"        \\n\",\n    \"        return results\\n\",\n    \"    \\n\",\n    \"    def process_vectorized(self, data_array: np.ndarray, operation: str = \\\"vectorized\\\") -> ProcessingResult:\\n\",\n    \"        \\\"\\\"\\\"Process data using vectorized operations.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        if not self.config.enable_vectorization:\\n\",\n    \"            # Fall back to element-wise processing\\n\",\n    \"            results = [self.process_single(item, operation) for item in data_array]\\n\",\n    \"            return ProcessingResult(\\n\",\n    \"                data=[r.data for r in results],\\n\",\n    \"                processing_time=sum(r.processing_time for r in results),\\n\",\n    \"                from_cache=False\\n\",\n    \"            )\\n\",\n    \"        \\n\",\n    \"        start_time = time.perf_counter()\\n\",\n    \"        \\n\",\n    \"        # Vectorized processing\\n\",\n    \"        if operation == \\\"normalize\\\":\\n\",\n    \"            # Normalize data using vectorized operations\\n\",\n    \"            mean = np.mean(data_array, axis=0)\\n\",\n    \"            std = np.std(data_array, axis=0)\\n\",\n    \"            result = (data_array - mean) / (std + 1e-8)\\n\",\n    \"            \\n\",\n    \"        elif operation == \\\"similarity\\\":\\n\",\n    \"            # Compute pairwise similarities\\n\",\n    \"            result = np.dot(data_array, data_array.T)\\n\",\n    \"            \\n\",\n    \"        elif operation == \\\"transform\\\":\\n\",\n    \"            # Apply mathematical transformations\\n\",\n    \"            result = np.log(1 + np.abs(data_array))\\n\",\n    \"            \\n\",\n    \"        else:\\n\",\n    \"            # Default vectorized operation\\n\",\n    \"            result = data_array ** 2 + np.sin(data_array)\\n\",\n    \"        \\n\",\n    \"        processing_time = time.perf_counter() - start_time\\n\",\n    \"        \\n\",\n    \"        self.stats[\\\"vectorized_operations\\\"] += 1\\n\",\n    \"        self.stats[\\\"total_processed\\\"] += len(data_array)\\n\",\n    \"        self.stats[\\\"total_time\\\"] += processing_time\\n\",\n    \"        \\n\",\n    \"        if self.monitor:\\n\",\n    \"            self.monitor.record_time(f\\\"vectorized_{operation}\\\", processing_time)\\n\",\n    \"        \\n\",\n    \"        return ProcessingResult(\\n\",\n    \"            data=result,\\n\",\n    \"            processing_time=processing_time,\\n\",\n    \"            from_cache=False\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    def _classify(self, data: Any, resource: Dict) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Simulate classification.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.01)  # Simulate processing\\n\",\n    \"        confidence = hash(str(data)) % 100 / 100.0\\n\",\n    \"        return {\\n\",\n    \"            \\\"label\\\": \\\"positive\\\" if confidence > 0.5 else \\\"negative\\\",\\n\",\n    \"            \\\"confidence\\\": confidence,\\n\",\n    \"            \\\"resource_id\\\": resource[\\\"id\\\"]\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    def _vectorize(self, data: Any, resource: Dict) -> List[float]:\\n\",\n    \"        \\\"\\\"\\\"Simulate vectorization.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.005)\\n\",\n    \"        # Create feature vector\\n\",\n    \"        features = []\\n\",\n    \"        data_str = str(data)\\n\",\n    \"        for i in range(10):\\n\",\n    \"            features.append(hash(data_str + str(i)) % 1000 / 1000.0)\\n\",\n    \"        return features\\n\",\n    \"    \\n\",\n    \"    def _analyze(self, data: Any, resource: Dict) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Simulate analysis.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.02)\\n\",\n    \"        data_str = str(data)\\n\",\n    \"        return {\\n\",\n    \"            \\\"length\\\": len(data_str),\\n\",\n    \"            \\\"complexity\\\": len(set(data_str)) / len(data_str) if data_str else 0,\\n\",\n    \"            \\\"hash_value\\\": hash(data_str),\\n\",\n    \"            \\\"resource_id\\\": resource[\\\"id\\\"]\\n\",\n    \"        }\\n\",\n    \"    \\n\",\n    \"    def _default_process(self, data: Any, resource: Dict) -> str:\\n\",\n    \"        \\\"\\\"\\\"Default processing.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.001)\\n\",\n    \"        return f\\\"PROCESSED_{data}_{resource['id']}\\\"\\n\",\n    \"    \\n\",\n    \"    def get_performance_report(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Get comprehensive performance report.\\\"\\\"\\\"\\n\",\n    \"        report = {\\n\",\n    \"            \\\"processing_stats\\\": self.stats.copy(),\\n\",\n    \"            \\\"cache_stats\\\": self.cache.get_stats() if self.cache else None,\\n\",\n    \"            \\\"resource_pool_stats\\\": self.resource_pool.get_stats(),\\n\",\n    \"            \\\"performance_summary\\\": {}\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        # Calculate performance metrics\\n\",\n    \"        if self.stats[\\\"total_processed\\\"] > 0:\\n\",\n    \"            avg_time = self.stats[\\\"total_time\\\"] / self.stats[\\\"total_processed\\\"]\\n\",\n    \"            throughput = self.stats[\\\"total_processed\\\"] / self.stats[\\\"total_time\\\"] if self.stats[\\\"total_time\\\"] > 0 else 0\\n\",\n    \"            \\n\",\n    \"            report[\\\"performance_summary\\\"] = {\\n\",\n    \"                \\\"avg_processing_time\\\": avg_time,\\n\",\n    \"                \\\"throughput_items_per_sec\\\": throughput,\\n\",\n    \"                \\\"cache_hit_rate\\\": self.stats[\\\"cache_hits\\\"] / (self.stats[\\\"cache_hits\\\"] + self.stats[\\\"cache_misses\\\"]) if (self.stats[\\\"cache_hits\\\"] + self.stats[\\\"cache_misses\\\"]) > 0 else 0\\n\",\n    \"            }\\n\",\n    \"        \\n\",\n    \"        if self.monitor:\\n\",\n    \"            report[\\\"detailed_monitoring\\\"] = self.monitor.get_summary()\\n\",\n    \"        \\n\",\n    \"        return report\\n\",\n    \"    \\n\",\n    \"    def optimize_configuration(self) -> PerformanceConfig:\\n\",\n    \"        \\\"\\\"\\\"Suggest optimized configuration based on usage patterns.\\\"\\\"\\\"\\n\",\n    \"        current_config = self.config\\n\",\n    \"        \\n\",\n    \"        # Simple optimization heuristics\\n\",\n    \"        if self.cache and self.cache.get_stats()[\\\"hit_ratio\\\"] < 0.3:\\n\",\n    \"            # Low cache hit rate, increase cache size\\n\",\n    \"            current_config.cache_size = min(current_config.cache_size * 2, 5000)\\n\",\n    \"        \\n\",\n    \"        if self.stats[\\\"parallel_batches\\\"] > 10:\\n\",\n    \"            # Frequent parallel processing, optimize batch size\\n\",\n    \"            current_config.batch_size = max(current_config.batch_size // 2, 50)\\n\",\n    \"        \\n\",\n    \"        return current_config\\n\",\n    \"\\n\",\n    \"def demonstrate_production_system():\\n\",\n    \"    \\\"\\\"\\\"Demonstrate the complete production performance system.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    console.print(Panel(\\\"üè≠ Production Performance System\\\", style=\\\"bold green\\\"))\\n\",\n    \"    \\n\",\n    \"    # Create optimized configuration\\n\",\n    \"    config = PerformanceConfig(\\n\",\n    \"        enable_caching=True,\\n\",\n    \"        cache_size=500,\\n\",\n    \"        enable_parallel=True,\\n\",\n    \"        max_workers=3,\\n\",\n    \"        batch_size=50,\\n\",\n    \"        enable_vectorization=True,\\n\",\n    \"        memory_limit_mb=200.0,\\n\",\n    \"        enable_profiling=True\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    processor = HighPerformanceAIProcessor(config)\\n\",\n    \"    \\n\",\n    \"    print(\\\"üöÄ Testing high-performance AI processing:\\\")\\n\",\n    \"    \\n\",\n    \"    # Test 1: Single item processing with caching\\n\",\n    \"    print(f\\\"\\\\nüìù Single item processing (with caching):\\\")\\n\",\n    \"    \\n\",\n    \"    test_items = [\\\"test_item_1\\\", \\\"test_item_2\\\", \\\"test_item_1\\\"]  # Duplicate for cache test\\n\",\n    \"    \\n\",\n    \"    for item in test_items:\\n\",\n    \"        result = processor.process_single(item, \\\"classify\\\")\\n\",\n    \"        cache_status = \\\"CACHE HIT\\\" if result.from_cache else \\\"COMPUTED\\\"\\n\",\n    \"        print(f\\\"  {item}: {result.data['label']} (confidence: {result.data['confidence']:.3f}) - {cache_status} - {result.processing_time:.4f}s\\\")\\n\",\n    \"    \\n\",\n    \"    # Test 2: Batch processing\\n\",\n    \"    print(f\\\"\\\\nüì¶ Batch processing:\\\")\\n\",\n    \"    \\n\",\n    \"    batch_data = [f\\\"batch_item_{i}\\\" for i in range(20)]\\n\",\n    \"    \\n\",\n    \"    with profile_time(\\\"Batch processing\\\"):\\n\",\n    \"        batch_results = processor.process_batch(batch_data, \\\"analyze\\\")\\n\",\n    \"    \\n\",\n    \"    successful_results = [r for r in batch_results if r.data]\\n\",\n    \"    avg_time = sum(r.processing_time for r in successful_results) / len(successful_results)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Processed {len(successful_results)} items\\\")\\n\",\n    \"    print(f\\\"  Average processing time: {avg_time:.4f}s\\\")\\n\",\n    \"    print(f\\\"  Sample result: {successful_results[0].data}\\\")\\n\",\n    \"    \\n\",\n    \"    # Test 3: Vectorized processing\\n\",\n    \"    print(f\\\"\\\\n‚ö° Vectorized processing:\\\")\\n\",\n    \"    \\n\",\n    \"    # Create test matrices\\n\",\n    \"    test_matrix = np.random.rand(100, 10)\\n\",\n    \"    \\n\",\n    \"    operations = [\\\"normalize\\\", \\\"similarity\\\", \\\"transform\\\"]\\n\",\n    \"    \\n\",\n    \"    for operation in operations:\\n\",\n    \"        result = processor.process_vectorized(test_matrix, operation)\\n\",\n    \"        print(f\\\"  {operation}: shape={result.data.shape}, time={result.processing_time:.4f}s\\\")\\n\",\n    \"    \\n\",\n    \"    # Test 4: Mixed workload\\n\",\n    \"    print(f\\\"\\\\nüîÄ Mixed workload processing:\\\")\\n\",\n    \"    \\n\",\n    \"    mixed_data = [f\\\"mixed_{i}\\\" for i in range(30)]\\n\",\n    \"    operations = [\\\"classify\\\", \\\"vectorize\\\", \\\"analyze\\\"]\\n\",\n    \"    \\n\",\n    \"    start_time = time.perf_counter()\\n\",\n    \"    \\n\",\n    \"    for i, data in enumerate(mixed_data):\\n\",\n    \"        operation = operations[i % len(operations)]\\n\",\n    \"        result = processor.process_single(data, operation)\\n\",\n    \"    \\n\",\n    \"    mixed_time = time.perf_counter() - start_time\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Mixed workload: {len(mixed_data)} items in {mixed_time:.4f}s\\\")\\n\",\n    \"    \\n\",\n    \"    # Performance report\\n\",\n    \"    report = processor.get_performance_report()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüìä Performance Report:\\\")\\n\",\n    \"    print(f\\\"  Total processed: {report['processing_stats']['total_processed']}\\\")\\n\",\n    \"    print(f\\\"  Cache hit rate: {report['performance_summary']['cache_hit_rate']:.1%}\\\")\\n\",\n    \"    print(f\\\"  Average processing time: {report['performance_summary']['avg_processing_time']:.4f}s\\\")\\n\",\n    \"    print(f\\\"  Throughput: {report['performance_summary']['throughput_items_per_sec']:.1f} items/sec\\\")\\n\",\n    \"    \\n\",\n    \"    if report['cache_stats']:\\n\",\n    \"        print(f\\\"  Cache efficiency: {report['cache_stats']['memory_mb']:.1f}MB used\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Resource pool: {report['resource_pool_stats']['pool_efficiency']:.1%} efficiency\\\")\\n\",\n    \"    \\n\",\n    \"    # Configuration optimization\\n\",\n    \"    print(f\\\"\\\\nüîß Configuration optimization:\\\")\\n\",\n    \"    optimized_config = processor.optimize_configuration()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Original batch size: {config.batch_size}\\\")\\n\",\n    \"    print(f\\\"  Optimized batch size: {optimized_config.batch_size}\\\")\\n\",\n    \"    print(f\\\"  Original cache size: {config.cache_size}\\\")\\n\",\n    \"    print(f\\\"  Optimized cache size: {optimized_config.cache_size}\\\")\\n\",\n    \"    \\n\",\n    \"    # Memory cleanup demonstration\\n\",\n    \"    print(f\\\"\\\\nüßπ Memory cleanup:\\\")\\n\",\n    \"    \\n\",\n    \"    objects_before = len(gc.get_objects())\\n\",\n    \"    \\n\",\n    \"    # Clear caches and force garbage collection\\n\",\n    \"    if processor.cache:\\n\",\n    \"        processor.cache.cache.clear()\\n\",\n    \"    \\n\",\n    \"    del batch_results, successful_results, mixed_data\\n\",\n    \"    gc.collect()\\n\",\n    \"    \\n\",\n    \"    objects_after = len(gc.get_objects())\\n\",\n    \"    \\n\",\n    \"    print(f\\\"  Objects before cleanup: {objects_before}\\\")\\n\",\n    \"    print(f\\\"  Objects after cleanup: {objects_after}\\\")\\n\",\n    \"    print(f\\\"  Objects freed: {objects_before - objects_after}\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüèÜ Production System Features:\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Intelligent caching with memory limits\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Parallel batch processing\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Vectorized mathematical operations\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Resource pooling and management\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Comprehensive performance monitoring\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Automatic configuration optimization\\\")\\n\",\n    \"    print(f\\\"  ‚úÖ Memory-efficient data structures\\\")\\n\",\n    \"\\n\",\n    \"# Run the complete production system demonstration\\n\",\n    \"demonstrate_production_system()\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüéØ Performance Optimization Mastery Complete!\\\")\\n\",\n    \"print(f\\\"\\\\nüí° Key Performance Optimization Benefits:\\\")\\n\",\n    \"print(f\\\"  ‚ö° 10-100x speedup with proper optimization\\\")\\n\",\n    \"print(f\\\"  üíæ Efficient memory usage and management\\\")\\n\",\n    \"print(f\\\"  üìä Data-driven performance monitoring\\\")\\n\",\n    \"print(f\\\"  üîß Adaptive configuration optimization\\\")\\n\",\n    \"print(f\\\"  üöÄ Production-ready scalable architecture\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Summary: Performance Optimization Mastery\\n\",\n    \"\\n\",\n    \"**üéØ Key Takeaways:**\\n\",\n    \"\\n\",\n    \"1. **Memory Efficiency** reduces allocation overhead and improves cache locality\\n\",\n    \"2. **Performance Profiling** identifies bottlenecks and guides optimization efforts\\n\",\n    \"3. **Vectorization** provides massive speedups for numerical operations\\n\",\n    \"4. **Parallel Processing** utilizes multiple cores for CPU-bound and I/O-bound tasks\\n\",\n    \"5. **Smart Caching** eliminates redundant computations\\n\",\n    \"6. **Resource Pooling** reuses expensive objects efficiently\\n\",\n    \"7. **Monitoring** provides insights for continuous optimization\\n\",\n    \"\\n\",\n    \"**üöÄ Production Considerations:**\\n\",\n    \"- Always profile before optimizing\\n\",\n    \"- Use appropriate data structures for your use case\\n\",\n    \"- Leverage NumPy for numerical computations\\n\",\n    \"- Choose threads vs processes based on workload type\\n\",\n    \"- Monitor memory usage and implement limits\\n\",\n    \"- Cache strategically with appropriate TTL\\n\",\n    \"\\n\",\n    \"**üí° Remember:** Performance optimization is about making the right trade-offs between speed, memory, and code complexity. Focus on the bottlenecks that matter most for your specific AI application!\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.11.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}","structuredPatch":[]}}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":"This session is being continued from a previous conversation that ran out of context. The conversation is summarized below:\nAnalysis:\nLet me chronologically analyze this conversation to capture all the user's requests, my actions, and technical details:\n\n1. **Initial Request**: The user asked me to read a specific Python file (`03-python-idioms/examples/01_type_system_advanced.py`) and create a Jupyter notebook for learning, with explanations of first principles and Python idioms to make them \"well-rounded\".\n\n2. **My Response**: I read the file (which contained advanced Python typing examples for AI systems) and created a comprehensive Jupyter notebook (`01_type_system_advanced_learning.ipynb`) with 10 progressive cells covering:\n   - Type variables and generics\n   - Dataclasses \n   - Protocols\n   - TypedDict\n   - Function overloading\n   - Advanced decorators\n   - Real-world integration\n\n3. **User's Follow-up Request**: The user asked me to:\n   - Move the notebook to a notebooks directory\n   - Create similar notebooks for files 2, 3, 4, 5 \n   - Start with files in order: 3, 2, 5, 4\n\n4. **My Implementation**: I:\n   - Created the notebooks directory\n   - Moved the existing notebook there\n   - Read the other Python files (02_async_patterns.py, 05_performance_optimization.py, 03_functional_programming.py, 04_metaprogramming.py)\n   - Created comprehensive learning notebooks for files 3, 2, and 5 in the requested order\n   - Each notebook followed the same educational structure with first principles, Python idioms, and progressive learning cells\n\n5. **Technical Patterns**: Each notebook contained:\n   - 10+ progressive cells building complexity\n   - First principle explanations\n   - Python idiom demonstrations\n   - Production-ready examples\n   - Interactive learning with rich console output\n\n6. **File Status**: I successfully created:\n   - `/notebooks/01_type_system_advanced_learning.ipynb` (moved)\n   - `/notebooks/03_functional_programming_learning.ipynb` (created)\n   - `/notebooks/02_async_patterns_learning.ipynb` (created) \n   - `/notebooks/05_performance_optimization_learning.ipynb` (created)\n\n7. **Remaining Work**: I still need to create the notebook for file 4 (metaprogramming) to complete the user's request.\n\nThe conversation shows a clear educational focus on Python patterns for AI systems, with comprehensive notebook creation following a consistent pedagogical structure.\n\nSummary:\n1. Primary Request and Intent:\n   The user requested me to read a specific Python file (`@03-python-idioms/examples/01_type_system_advanced.py`) and create a Jupyter notebook for learning from these examples, with explanations of first principles and Python idioms to help them become \"well-rounded.\" After the initial notebook, they requested me to move it to a notebooks directory and create similar learning notebooks for other Python files (2, 3, 4, 5) in a specific order: 3, 2, 5, 4.\n\n2. Key Technical Concepts:\n   - Advanced Python Type System (Type variables, Generics, Protocols, TypedDict)\n   - Functional Programming (Immutability, Pure functions, Composition, Monadic error handling)\n   - Async Patterns (Streaming, Rate limiting, Connection pooling, Concurrent processing)\n   - Performance Optimization (Memory efficiency, Vectorization, Parallel processing, Profiling)\n   - Dataclasses and frozen dataclasses\n   - Context managers and async context managers\n   - Decorators and function overloading\n   - Rich console library for formatted output\n   - AI system architecture patterns\n\n3. Files and Code Sections:\n   - `/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/01_type_system_advanced.py`\n     - Source file containing advanced typing examples for AI systems\n     - Contains examples of TypeVar, Generic, Protocol, TypedDict, overload decorators\n     - Key snippet: ModelRegistryMeta metaclass for automatic model registration\n   \n   - `/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/01_type_system_advanced_learning.ipynb`\n     - Comprehensive learning notebook with 10 progressive cells\n     - Covers type variables, dataclasses, protocols, and real-world integration\n     - Key snippet: Complete functional AI pipeline combining all patterns\n   \n   - `/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/03_functional_programming.py`\n     - Source file with functional programming patterns for AI\n     - Contains immutable data structures, pure functions, monadic error handling\n     - Key snippet: Result monad for safe computation chains\n   \n   - `/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/03_functional_programming_learning.ipynb`\n     - Educational notebook covering functional programming concepts\n     - 11 cells progressing from basics to production system\n     - Key snippet: Complete functional AI text processing system with immutable config\n   \n   - `/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/02_async_patterns.py`\n     - Source file with async patterns for high-performance AI systems\n     - Contains streaming, rate limiting, connection pooling examples\n     - Key snippet: ConcurrentInferenceEngine for managing parallel AI requests\n   \n   - `/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/02_async_patterns_learning.ipynb`\n     - Comprehensive async programming tutorial\n     - 6 cells covering streaming, rate limiting, resource management\n     - Key snippet: Production-ready AsyncAISystem with full async patterns\n   \n   - `/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/examples/05_performance_optimization.py`\n     - Source file with performance optimization techniques\n     - Contains memory-efficient structures, vectorization, parallel processing\n     - Key snippet: MemoryEfficientCache with size and memory limits\n   \n   - `/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/05_performance_optimization_learning.ipynb`\n     - Performance optimization tutorial\n     - 6 cells covering profiling, vectorization, parallel processing\n     - Key snippet: HighPerformanceAIProcessor combining all optimization techniques\n\n4. Errors and fixes:\n   - File not found error when trying to read `03_context_managers.py` and `04_error_handling.py` - these files don't exist\n   - Fixed by checking directory contents and finding the correct files: `03_functional_programming.py` and `04_metaprogramming.py`\n   - No user feedback on errors received\n\n5. Problem Solving:\n   - Successfully created a consistent educational structure across all notebooks\n   - Solved the challenge of making complex Python concepts accessible through progressive learning cells\n   - Implemented rich console output for interactive learning experience\n   - Created production-ready examples that combine multiple concepts\n\n6. All user messages:\n   - \"@03-python-idioms/examples/01_type_system_advanced.py can you create a juptyer notebook for me to learn form these examples. explain each cell what are the first priciples, python idioms also so i come out well rounded Called the Read tool with the following input...\"\n   - \"ok great, can you move notebooks directory this one and create similiar for other 2,3,4,5 also start with 3, 2,5,4 python files\"\n\n7. Pending Tasks:\n   - Create learning notebook for file 4 (04_metaprogramming.py) to complete the user's request\n\n8. Current Work:\n   I just completed creating the performance optimization learning notebook (`05_performance_optimization_learning.ipynb`). This was the third notebook in the requested sequence (3, 2, 5, 4). The notebook contains 6 comprehensive cells covering memory-efficient data structures, performance profiling, vectorized operations, parallel processing patterns, and a complete production-ready high-performance AI processing system. The final cell demonstrates a `HighPerformanceAIProcessor` class that combines all optimization techniques including intelligent caching, parallel batch processing, vectorized operations, and comprehensive performance monitoring.\n\n9. Optional Next Step:\n   Create the learning notebook for file 4 (04_metaprogramming.py) to complete the user's request sequence. Based on the user's explicit request: \"create similiar for other 2,3,4,5 also start with 3, 2,5,4 python files\" - I have completed notebooks for 3, 2, and 5, so the final step is to create the notebook for file 4 (metaprogramming)..\nPlease continue the conversation from where we left it off without asking the user any further questions. Continue with the last task that you were asked to work on."},"isCompactSummary":true,"uuid":"aa5c88df-042a-49e4-b866-bc17a9c25e36","timestamp":"2025-08-01T18:18:40.329Z"}
{"parentUuid":"aa5c88df-042a-49e4-b866-bc17a9c25e36","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_0199yfXy6oTnpwC2qDYXCE2g","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now I'll create the final learning notebook for metaprogramming patterns. This will complete the requested sequence of notebooks (3, 2, 5, 4)."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":30505,"cache_read_input_tokens":15686,"output_tokens":1}},"requestId":"req_vrtx_011CRhT5NxvheVJdmpDzBavw","type":"assistant","uuid":"5dc6b5f4-98be-4ec9-a1bb-a455a70bc66c","timestamp":"2025-08-01T18:18:46.749Z"}
{"parentUuid":"5dc6b5f4-98be-4ec9-a1bb-a455a70bc66c","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_0199yfXy6oTnpwC2qDYXCE2g","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01LaPQXY1PdDL7iy3yoAp7gk","name":"Write","input":{"file_path":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/04_metaprogramming_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# üè≠ Metaprogramming for AI Systems - Learning Notebook\\n\",\n    \"\\n\",\n    \"**First Principles**: Metaprogramming is code that manipulates code. It allows us to create flexible, self-adapting systems that can modify their behavior at runtime.\\n\",\n    \"\\n\",\n    \"**Python Idioms**: Descriptors, metaclasses, decorators, dynamic class generation, and introspection enable powerful metaprogramming patterns.\\n\",\n    \"\\n\",\n    \"**AI Applications**: Dynamic model registration, automatic instrumentation, configuration management, and adaptive behavior.\\n\",\n    \"\\n\",\n    \"---\\n\",\n    \"\\n\",\n    \"## Learning Path\\n\",\n    \"1. **Descriptors** - Custom attribute access patterns\\n\",\n    \"2. **Metaclasses** - Class creation and automatic registration\\n\",\n    \"3. **Dynamic Classes** - Runtime class generation\\n\",\n    \"4. **Advanced Decorators** - Behavior modification patterns\\n\",\n    \"5. **Property Factories** - Type-safe dynamic properties\\n\",\n    \"6. **Context Managers** - Dynamic configuration management\\n\",\n    \"7. **Reflection** - Runtime introspection capabilities\\n\",\n    \"8. **Complete AI Framework** - Production-ready metaprogramming system\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 1: Descriptors - Custom Attribute Access\\n\",\n    \"from typing import Any, Type, Callable, Dict, Optional\\n\",\n    \"from functools import update_wrapper\\n\",\n    \"import time\\n\",\n    \"from rich.console import Console\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"\\n\",\n    \"class ValidatedProperty:\\n\",\n    \"    \\\"\\\"\\\"Descriptor that validates property values.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, validator: Callable[[Any], bool], error_msg: str):\\n\",\n    \"        self.validator = validator\\n\",\n    \"        self.error_msg = error_msg\\n\",\n    \"        self.name = None\\n\",\n    \"    \\n\",\n    \"    def __set_name__(self, owner: Type, name: str):\\n\",\n    \"        \\\"\\\"\\\"Called when descriptor is assigned to a class attribute.\\\"\\\"\\\"\\n\",\n    \"        self.name = name\\n\",\n    \"        self.private_name = f'_{name}'\\n\",\n    \"    \\n\",\n    \"    def __get__(self, obj: Any, objtype: Type = None) -> Any:\\n\",\n    \"        if obj is None:\\n\",\n    \"            return self\\n\",\n    \"        return getattr(obj, self.private_name, None)\\n\",\n    \"    \\n\",\n    \"    def __set__(self, obj: Any, value: Any) -> None:\\n\",\n    \"        if not self.validator(value):\\n\",\n    \"            raise ValueError(f\\\"{self.error_msg}: {value}\\\")\\n\",\n    \"        setattr(obj, self.private_name, value)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class CachedProperty:\\n\",\n    \"    \\\"\\\"\\\"Descriptor that caches expensive computations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, func: Callable):\\n\",\n    \"        self.func = func\\n\",\n    \"        self.name = func.__name__\\n\",\n    \"        update_wrapper(self, func)\\n\",\n    \"    \\n\",\n    \"    def __get__(self, obj: Any, objtype: Type = None) -> Any:\\n\",\n    \"        if obj is None:\\n\",\n    \"            return self\\n\",\n    \"        \\n\",\n    \"        cache_name = f'_cached_{self.name}'\\n\",\n    \"        if hasattr(obj, cache_name):\\n\",\n    \"            console.print(f\\\"[dim green]üíæ Cache hit for {self.name}[/dim green]\\\")\\n\",\n    \"            return getattr(obj, cache_name)\\n\",\n    \"        \\n\",\n    \"        console.print(f\\\"[dim yellow]üîÑ Computing {self.name}[/dim yellow]\\\")\\n\",\n    \"        value = self.func(obj)\\n\",\n    \"        setattr(obj, cache_name, value)\\n\",\n    \"        return value\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Example: AI Model with Validated Properties\\n\",\n    \"class AIModel:\\n\",\n    \"    \\\"\\\"\\\"AI model with validated properties using descriptors.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    temperature = ValidatedProperty(\\n\",\n    \"        lambda x: 0.0 <= x <= 2.0,\\n\",\n    \"        \\\"Temperature must be between 0.0 and 2.0\\\"\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    max_tokens = ValidatedProperty(\\n\",\n    \"        lambda x: isinstance(x, int) and x > 0,\\n\",\n    \"        \\\"Max tokens must be a positive integer\\\"\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    def __init__(self, temperature: float = 0.7, max_tokens: int = 1000):\\n\",\n    \"        self.temperature = temperature\\n\",\n    \"        self.max_tokens = max_tokens\\n\",\n    \"    \\n\",\n    \"    @CachedProperty\\n\",\n    \"    def model_signature(self) -> str:\\n\",\n    \"        \\\"\\\"\\\"Expensive computation that gets cached.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.1)  # Simulate expensive computation\\n\",\n    \"        return f\\\"Model_{hash(str(self.temperature) + str(self.max_tokens))}\\\"\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üîç Testing Descriptors[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Test validation\\n\",\n    \"model = AIModel()\\n\",\n    \"console.print(f\\\"Default temperature: {model.temperature}\\\")\\n\",\n    \"\\n\",\n    \"model.temperature = 1.5\\n\",\n    \"console.print(f\\\"‚úÖ Set temperature to {model.temperature}\\\")\\n\",\n    \"\\n\",\n    \"try:\\n\",\n    \"    model.temperature = 3.0  # Should fail\\nexcept ValueError as e:\\n\",\n    \"    console.print(f\\\"‚ùå Validation caught: {e}\\\")\\n\",\n    \"\\n\",\n    \"# Test cached property\\n\",\n    \"sig1 = model.model_signature  # Should compute\\n\",\n    \"sig2 = model.model_signature  # Should use cache\\n\",\n    \"console.print(f\\\"Signatures match: {sig1 == sig2}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Descriptors control attribute access\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] __get__, __set__, __set_name__ methods\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 2: Metaclasses - Automatic Registration and Instrumentation\\n\",\n    \"\\n\",\n    \"class ModelRegistryMeta(type):\\n\",\n    \"    \\\"\\\"\\\"Metaclass that automatically registers AI models.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    _registry: Dict[str, Type] = {}\\n\",\n    \"    \\n\",\n    \"    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\\n\",\n    \"        # Add automatic ID generation\\n\",\n    \"        if 'model_id' not in namespace and name != 'BaseModel':\\n\",\n    \"            namespace['model_id'] = name.lower().replace('model', '')\\n\",\n    \"        \\n\",\n    \"        cls = super().__new__(mcs, name, bases, namespace)\\n\",\n    \"        \\n\",\n    \"        # Register the model\\n\",\n    \"        if name != 'BaseModel' and hasattr(cls, 'model_id'):\\n\",\n    \"            mcs._registry[cls.model_id] = cls\\n\",\n    \"            console.print(f\\\"[dim]üîß Registered model: {cls.model_id} -> {name}[/dim]\\\")\\n\",\n    \"        \\n\",\n    \"        return cls\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def get_model(mcs, model_id: str) -> Optional[Type]:\\n\",\n    \"        return mcs._registry.get(model_id)\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def list_models(mcs) -> list:\\n\",\n    \"        return list(mcs._registry.keys())\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class InstrumentedMeta(type):\\n\",\n    \"    \\\"\\\"\\\"Metaclass that adds automatic instrumentation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\\n\",\n    \"        # Instrument methods that start with 'predict'\\n\",\n    \"        for attr_name, attr_value in namespace.items():\\n\",\n    \"            if (attr_name.startswith('predict') and \\n\",\n    \"                callable(attr_value) and \\n\",\n    \"                not attr_name.startswith('_')):\\n\",\n    \"                \\n\",\n    \"                namespace[attr_name] = mcs._instrument_method(attr_value, attr_name)\\n\",\n    \"        \\n\",\n    \"        return super().__new__(mcs, name, bases, namespace)\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def _instrument_method(method: Callable, method_name: str) -> Callable:\\n\",\n    \"        from functools import wraps\\n\",\n    \"        \\n\",\n    \"        @wraps(method)\\n\",\n    \"        def wrapper(self, *args, **kwargs):\\n\",\n    \"            start_time = time.time()\\n\",\n    \"            try:\\n\",\n    \"                result = method(self, *args, **kwargs)\\n\",\n    \"                duration = time.time() - start_time\\n\",\n    \"                \\n\",\n    \"                # Store metrics\\n\",\n    \"                if not hasattr(self, '_metrics'):\\n\",\n    \"                    self._metrics = {}\\n\",\n    \"                if method_name not in self._metrics:\\n\",\n    \"                    self._metrics[method_name] = []\\n\",\n    \"                \\n\",\n    \"                self._metrics[method_name].append({\\n\",\n    \"                    'duration': duration,\\n\",\n    \"                    'timestamp': start_time,\\n\",\n    \"                    'success': True\\n\",\n    \"                })\\n\",\n    \"                \\n\",\n    \"                console.print(f\\\"[dim]üìä {self.__class__.__name__}.{method_name} took {duration:.4f}s[/dim]\\\")\\n\",\n    \"                return result\\n\",\n    \"                \\n\",\n    \"            except Exception as e:\\n\",\n    \"                duration = time.time() - start_time\\n\",\n    \"                console.print(f\\\"[dim red]‚ùå {self.__class__.__name__}.{method_name} failed after {duration:.4f}s[/dim red]\\\")\\n\",\n    \"                raise\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Base model with both metaclasses\\n\",\n    \"class BaseModel(metaclass=ModelRegistryMeta):\\n\",\n    \"    \\\"\\\"\\\"Base class for AI models.\\\"\\\"\\\"\\n\",\n    \"    pass\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class GPTModel(BaseModel, metaclass=InstrumentedMeta):\\n\",\n    \"    \\\"\\\"\\\"GPT model with automatic instrumentation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def predict_text(self, text: str) -> str:\\n\",\n    \"        time.sleep(0.05)  # Simulate prediction\\n\",\n    \"        return f\\\"GPT prediction for: {text}\\\"\\n\",\n    \"    \\n\",\n    \"    def predict_classification(self, text: str) -> Dict[str, float]:\\n\",\n    \"        time.sleep(0.03)\\n\",\n    \"        return {\\\"positive\\\": 0.8, \\\"negative\\\": 0.1, \\\"neutral\\\": 0.1}\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class ClaudeModel(BaseModel, metaclass=InstrumentedMeta):\\n\",\n    \"    \\\"\\\"\\\"Claude model with automatic instrumentation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def predict_text(self, text: str) -> str:\\n\",\n    \"        time.sleep(0.04)\\n\",\n    \"        return f\\\"Claude prediction for: {text}\\\"\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üè≠ Testing Metaclasses[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Show automatic registration\\n\",\n    \"registered_models = ModelRegistryMeta.list_models()\\n\",\n    \"console.print(f\\\"Registered models: {registered_models}\\\")\\n\",\n    \"\\n\",\n    \"# Create models from registry\\n\",\n    \"gpt_class = ModelRegistryMeta.get_model('gpt')\\n\",\n    \"claude_class = ModelRegistryMeta.get_model('claude')\\n\",\n    \"\\n\",\n    \"if gpt_class:\\n\",\n    \"    gpt = gpt_class()\\n\",\n    \"    result = gpt.predict_text(\\\"Hello world\\\")\\n\",\n    \"    console.print(f\\\"GPT: {result}\\\")\\n\",\n    \"\\n\",\n    \"if claude_class:\\n\",\n    \"    claude = claude_class()\\n\",\n    \"    result = claude.predict_text(\\\"Hello world\\\")\\n\",\n    \"    console.print(f\\\"Claude: {result}\\\")\\n\",\n    \"\\n\",\n    \"# Show automatic instrumentation\\n\",\n    \"if hasattr(gpt, '_metrics'):\\n\",\n    \"    for method, metrics in gpt._metrics.items():\\n\",\n    \"        avg_duration = sum(m['duration'] for m in metrics) / len(metrics)\\n\",\n    \"        console.print(f\\\"Metrics - {method}: {len(metrics)} calls, avg {avg_duration:.4f}s\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Metaclasses control class creation\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] __new__ method in metaclass\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 3: Dynamic Class Generation\\n\",\n    \"\\n\",\n    \"def create_model_class(\\n\",\n    \"    name: str, \\n\",\n    \"    attributes: Dict[str, Any], \\n\",\n    \"    methods: Dict[str, Callable] = None,\\n\",\n    \"    base_classes: tuple = ()\\n\",\n    \") -> Type:\\n\",\n    \"    \\\"\\\"\\\"Dynamically create a model class.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    if not base_classes:\\n\",\n    \"        base_classes = (object,)\\n\",\n    \"    \\n\",\n    \"    namespace = attributes.copy()\\n\",\n    \"    if methods:\\n\",\n    \"        namespace.update(methods)\\n\",\n    \"    \\n\",\n    \"    # Add default __init__ if not provided\\n\",\n    \"    if '__init__' not in namespace:\\n\",\n    \"        def __init__(self, **kwargs):\\n\",\n    \"            for key, value in kwargs.items():\\n\",\n    \"                setattr(self, key, value)\\n\",\n    \"        namespace['__init__'] = __init__\\n\",\n    \"    \\n\",\n    \"    return type(name, base_classes, namespace)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class ModelFactory:\\n\",\n    \"    \\\"\\\"\\\"Factory for creating model classes dynamically.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def create_simple_model(name: str, prediction_delay: float = 0.1) -> Type:\\n\",\n    \"        \\\"\\\"\\\"Create a simple model class.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        def predict(self, text: str) -> str:\\n\",\n    \"            time.sleep(prediction_delay)\\n\",\n    \"            return f\\\"{name} processed: {text}\\\"\\n\",\n    \"        \\n\",\n    \"        def get_info(self) -> Dict[str, Any]:\\n\",\n    \"            return {\\n\",\n    \"                \\\"model_name\\\": name,\\n\",\n    \"                \\\"prediction_delay\\\": prediction_delay,\\n\",\n    \"                \\\"class_name\\\": self.__class__.__name__\\n\",\n    \"            }\\n\",\n    \"        \\n\",\n    \"        return create_model_class(\\n\",\n    \"            name=f\\\"{name}Model\\\",\\n\",\n    \"            attributes={\\n\",\n    \"                \\\"model_name\\\": name,\\n\",\n    \"                \\\"prediction_delay\\\": prediction_delay\\n\",\n    \"            },\\n\",\n    \"            methods={\\n\",\n    \"                \\\"predict\\\": predict,\\n\",\n    \"                \\\"get_info\\\": get_info\\n\",\n    \"            },\\n\",\n    \"            base_classes=(BaseModel,)\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def create_ensemble_model(models: list) -> Type:\\n\",\n    \"        \\\"\\\"\\\"Create an ensemble model from multiple model classes.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        def predict(self, text: str) -> list:\\n\",\n    \"            results = []\\n\",\n    \"            for model_class in models:\\n\",\n    \"                model = model_class()\\n\",\n    \"                if hasattr(model, 'predict'):\\n\",\n    \"                    results.append(model.predict(text))\\n\",\n    \"                elif hasattr(model, 'predict_text'):\\n\",\n    \"                    results.append(model.predict_text(text))\\n\",\n    \"            return results\\n\",\n    \"        \\n\",\n    \"        def get_model_info(self) -> list:\\n\",\n    \"            return [model.__name__ for model in models]\\n\",\n    \"        \\n\",\n    \"        return create_model_class(\\n\",\n    \"            name=\\\"EnsembleModel\\\",\\n\",\n    \"            attributes={\\\"component_models\\\": models},\\n\",\n    \"            methods={\\n\",\n    \"                \\\"predict\\\": predict,\\n\",\n    \"                \\\"get_model_info\\\": get_model_info\\n\",\n    \"            }\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]‚ö° Testing Dynamic Class Creation[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Create models using factory\\n\",\n    \"FastModel = ModelFactory.create_simple_model(\\\"FastAI\\\", prediction_delay=0.02)\\n\",\n    \"SlowModel = ModelFactory.create_simple_model(\\\"DeepThink\\\", prediction_delay=0.08)\\n\",\n    \"\\n\",\n    \"fast_model = FastModel()\\n\",\n    \"slow_model = SlowModel()\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Fast model: {fast_model.get_info()}\\\")\\n\",\n    \"console.print(f\\\"Slow model: {slow_model.get_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Test predictions\\n\",\n    \"start_time = time.time()\\n\",\n    \"fast_result = fast_model.predict(\\\"test input\\\")\\n\",\n    \"fast_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"start_time = time.time()\\n\",\n    \"slow_result = slow_model.predict(\\\"test input\\\")\\n\",\n    \"slow_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Performance - Fast: {fast_time:.4f}s, Slow: {slow_time:.4f}s\\\")\\n\",\n    \"\\n\",\n    \"# Create ensemble model\\n\",\n    \"EnsembleModel = ModelFactory.create_ensemble_model([FastModel, SlowModel])\\n\",\n    \"ensemble = EnsembleModel()\\n\",\n    \"\\n\",\n    \"ensemble_results = ensemble.predict(\\\"ensemble test\\\")\\n\",\n    \"console.print(f\\\"Ensemble results: {ensemble_results}\\\")\\n\",\n    \"console.print(f\\\"Component models: {ensemble.get_model_info()}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Classes are objects that can be created at runtime\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] type() function for dynamic class creation\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 4: Advanced Decorators with State\\n\",\n    \"from functools import wraps\\n\",\n    \"\\n\",\n    \"class memoize_with_ttl:\\n\",\n    \"    \\\"\\\"\\\"Decorator that memoizes with time-to-live.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, ttl_seconds: float = 60.0):\\n\",\n    \"        self.ttl_seconds = ttl_seconds\\n\",\n    \"        self.cache = {}\\n\",\n    \"    \\n\",\n    \"    def __call__(self, func: Callable) -> Callable:\\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            # Create cache key\\n\",\n    \"            key = str(args) + str(sorted(kwargs.items()))\\n\",\n    \"            current_time = time.time()\\n\",\n    \"            \\n\",\n    \"            # Check if cached and not expired\\n\",\n    \"            if key in self.cache:\\n\",\n    \"                result, timestamp = self.cache[key]\\n\",\n    \"                if current_time - timestamp < self.ttl_seconds:\\n\",\n    \"                    console.print(f\\\"[dim green]üíæ Cache hit for {func.__name__}[/dim green]\\\")\\n\",\n    \"                    return result\\n\",\n    \"            \\n\",\n    \"            # Compute new result\\n\",\n    \"            result = func(*args, **kwargs)\\n\",\n    \"            self.cache[key] = (result, current_time)\\n\",\n    \"            console.print(f\\\"[dim yellow]üîÑ Cache miss for {func.__name__}[/dim yellow]\\\")\\n\",\n    \"            return result\\n\",\n    \"        \\n\",\n    \"        # Add cache management methods\\n\",\n    \"        wrapper.clear_cache = lambda: self.cache.clear()\\n\",\n    \"        wrapper.cache_info = lambda: {\\n\",\n    \"            'size': len(self.cache),\\n\",\n    \"            'ttl_seconds': self.ttl_seconds\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class retry_with_backoff:\\n\",\n    \"    \\\"\\\"\\\"Decorator that retries with exponential backoff.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\\n\",\n    \"        self.max_retries = max_retries\\n\",\n    \"        self.base_delay = base_delay\\n\",\n    \"    \\n\",\n    \"    def __call__(self, func: Callable) -> Callable:\\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            last_exception = None\\n\",\n    \"            \\n\",\n    \"            for attempt in range(self.max_retries + 1):\\n\",\n    \"                try:\\n\",\n    \"                    return func(*args, **kwargs)\\n\",\n    \"                except Exception as e:\\n\",\n    \"                    last_exception = e\\n\",\n    \"                    \\n\",\n    \"                    if attempt < self.max_retries:\\n\",\n    \"                        delay = self.base_delay * (2 ** attempt)\\n\",\n    \"                        console.print(f\\\"[yellow]üîÑ Retry {attempt + 1}/{self.max_retries} in {delay:.1f}s[/yellow]\\\")\\n\",\n    \"                        time.sleep(delay)\\n\",\n    \"                    else:\\n\",\n    \"                        console.print(f\\\"[red]‚ùå All retries exhausted for {func.__name__}[/red]\\\")\\n\",\n    \"            \\n\",\n    \"            raise last_exception\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üé® Testing Advanced Decorators[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"@memoize_with_ttl(ttl_seconds=2.0)\\n\",\n    \"@retry_with_backoff(max_retries=2, base_delay=0.5)\\n\",\n    \"def expensive_ai_call(query: str, fail_rate: float = 0.0) -> str:\\n\",\n    \"    \\\"\\\"\\\"Simulate expensive AI call that might fail.\\\"\\\"\\\"\\n\",\n    \"    import random\\n\",\n    \"    \\n\",\n    \"    if random.random() < fail_rate:\\n\",\n    \"        raise ConnectionError(\\\"Simulated API failure\\\")\\n\",\n    \"    \\n\",\n    \"    time.sleep(0.1)  # Simulate processing time\\n\",\n    \"    return f\\\"AI response to: {query}\\\"\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Test decorated function\\n\",\n    \"result1 = expensive_ai_call(\\\"What is AI?\\\")\\n\",\n    \"console.print(f\\\"Result 1: {result1}\\\")\\n\",\n    \"\\n\",\n    \"result2 = expensive_ai_call(\\\"What is AI?\\\")  # Should use cache\\n\",\n    \"console.print(f\\\"Result 2: {result2}\\\")\\n\",\n    \"\\n\",\n    \"# Test with failures\\n\",\n    \"try:\\n\",\n    \"    result3 = expensive_ai_call(\\\"Risky query\\\", fail_rate=0.7)\\n\",\n    \"    console.print(f\\\"Success: {result3}\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    console.print(f\\\"Failed: {e}\\\")\\n\",\n    \"\\n\",\n    \"cache_info = expensive_ai_call.cache_info()\\n\",\n    \"console.print(f\\\"Cache info: {cache_info}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Decorators modify function behavior\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] __call__ method for stateful decorators\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 5: Property Factory - Type-Safe Dynamic Properties\\n\",\n    \"\\n\",\n    \"def typed_property(property_type: Type, default: Any = None, validator: Callable = None):\\n\",\n    \"    \\\"\\\"\\\"Factory for creating typed properties with optional validation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def property_decorator(func: Callable) -> property:\\n\",\n    \"        name = func.__name__\\n\",\n    \"        private_name = f'_{name}'\\n\",\n    \"        \\n\",\n    \"        def getter(self):\\n\",\n    \"            return getattr(self, private_name, default)\\n\",\n    \"        \\n\",\n    \"        def setter(self, value):\\n\",\n    \"            # Type checking\\n\",\n    \"            if not isinstance(value, property_type):\\n\",\n    \"                try:\\n\",\n    \"                    value = property_type(value)\\n\",\n    \"                except (ValueError, TypeError):\\n\",\n    \"                    raise TypeError(f\\\"{name} must be of type {property_type.__name__}\\\")\\n\",\n    \"            \\n\",\n    \"            # Optional validation\\n\",\n    \"            if validator and not validator(value):\\n\",\n    \"                raise ValueError(f\\\"Invalid value for {name}: {value}\\\")\\n\",\n    \"            \\n\",\n    \"            setattr(self, private_name, value)\\n\",\n    \"        \\n\",\n    \"        return property(getter, setter, doc=func.__doc__)\\n\",\n    \"    \\n\",\n    \"    return property_decorator\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class ConfigurableModel:\\n\",\n    \"    \\\"\\\"\\\"Model with typed properties using property factory.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    @typed_property(float, default=0.7, validator=lambda x: 0.0 <= x <= 2.0)\\n\",\n    \"    def temperature(self):\\n\",\n    \"        \\\"\\\"\\\"Model temperature parameter.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"    \\n\",\n    \"    @typed_property(int, default=1000, validator=lambda x: x > 0)\\n\",\n    \"    def max_tokens(self):\\n\",\n    \"        \\\"\\\"\\\"Maximum tokens to generate.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"    \\n\",\n    \"    @typed_property(str, default=\\\"gpt-4\\\")\\n\",\n    \"    def model_name(self):\\n\",\n    \"        \\\"\\\"\\\"Name of the model.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üè∑Ô∏è Testing Typed Properties[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"model = ConfigurableModel()\\n\",\n    \"\\n\",\n    \"# Test default values\\n\",\n    \"console.print(f\\\"Default temperature: {model.temperature}\\\")\\n\",\n    \"console.print(f\\\"Default max_tokens: {model.max_tokens}\\\")\\n\",\n    \"console.print(f\\\"Default model_name: {model.model_name}\\\")\\n\",\n    \"\\n\",\n    \"# Test type conversion\\n\",\n    \"model.temperature = \\\"1.2\\\"  # String should convert to float\\n\",\n    \"model.max_tokens = \\\"500\\\"   # String should convert to int\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Converted temperature: {model.temperature} (type: {type(model.temperature)})\\\")\\n\",\n    \"console.print(f\\\"Converted max_tokens: {model.max_tokens} (type: {type(model.max_tokens)})\\\")\\n\",\n    \"\\n\",\n    \"# Test validation\\n\",\n    \"try:\\n\",\n    \"    model.temperature = 3.0  # Should fail validation\\nexcept ValueError as e:\\n\",\n    \"    console.print(f\\\"‚ùå Validation error: {e}\\\")\\n\",\n    \"\\n\",\n    \"try:\\n\",\n    \"    model.max_tokens = -100  # Should fail validation\\nexcept ValueError as e:\\n\",\n    \"    console.print(f\\\"‚ùå Validation error: {e}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Properties provide controlled attribute access\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] Property factory pattern for dynamic properties\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 6: Dynamic Configuration Context Manager\\n\",\n    \"\\n\",\n    \"class DynamicConfig:\\n\",\n    \"    \\\"\\\"\\\"Context manager that temporarily modifies class attributes.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, target_class: Type, **config_changes):\\n\",\n    \"        self.target_class = target_class\\n\",\n    \"        self.config_changes = config_changes\\n\",\n    \"        self.original_values = {}\\n\",\n    \"    \\n\",\n    \"    def __enter__(self):\\n\",\n    \"        # Save original values\\n\",\n    \"        for attr_name, new_value in self.config_changes.items():\\n\",\n    \"            if hasattr(self.target_class, attr_name):\\n\",\n    \"                self.original_values[attr_name] = getattr(self.target_class, attr_name)\\n\",\n    \"            setattr(self.target_class, attr_name, new_value)\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def __exit__(self, exc_type, exc_val, exc_tb):\\n\",\n    \"        # Restore original values\\n\",\n    \"        for attr_name, original_value in self.original_values.items():\\n\",\n    \"            setattr(self.target_class, attr_name, original_value)\\n\",\n    \"        \\n\",\n    \"        # Remove attributes that didn't exist originally\\n\",\n    \"        for attr_name in self.config_changes:\\n\",\n    \"            if attr_name not in self.original_values:\\n\",\n    \"                delattr(self.target_class, attr_name)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Example service class\\n\",\n    \"class AIService:\\n\",\n    \"    timeout = 30.0\\n\",\n    \"    retry_count = 3\\n\",\n    \"    debug_mode = False\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def get_config(cls) -> Dict[str, Any]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"timeout\\\": cls.timeout,\\n\",\n    \"            \\\"retry_count\\\": cls.retry_count,\\n\",\n    \"            \\\"debug_mode\\\": cls.debug_mode\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]‚öôÔ∏è Testing Dynamic Configuration[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Original configuration: {AIService.get_config()}\\\")\\n\",\n    \"\\n\",\n    \"# Temporarily modify configuration\\n\",\n    \"with DynamicConfig(AIService, timeout=60.0, debug_mode=True, new_feature=True):\\n\",\n    \"    console.print(f\\\"Inside context: {AIService.get_config()}\\\")\\n\",\n    \"    console.print(f\\\"New feature: {getattr(AIService, 'new_feature', 'Not found')}\\\")\\n\",\n    \"\\n\",\n    \"console.print(f\\\"After context: {AIService.get_config()}\\\")\\n\",\n    \"console.print(f\\\"New feature: {getattr(AIService, 'new_feature', 'Not found')}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Context managers provide resource management\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] __enter__ and __exit__ methods\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 7: Reflection and Introspection\\n\",\n    \"import inspect\\n\",\n    \"from typing import get_type_hints, get_origin, get_args\\n\",\n    \"\\n\",\n    \"def analyze_function_signature(func: Callable) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Metaprogramming function to analyze function signatures.\\\"\\\"\\\"\\n\",\n    \"    sig = inspect.signature(func)\\n\",\n    \"    type_hints = get_type_hints(func)\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"name\\\": func.__name__,\\n\",\n    \"        \\\"parameters\\\": {\\n\",\n    \"            name: {\\n\",\n    \"                \\\"annotation\\\": type_hints.get(name, param.annotation),\\n\",\n    \"                \\\"default\\\": param.default if param.default != param.empty else None,\\n\",\n    \"                \\\"kind\\\": param.kind.name\\n\",\n    \"            }\\n\",\n    \"            for name, param in sig.parameters.items()\\n\",\n    \"        },\\n\",\n    \"        \\\"return_type\\\": type_hints.get('return', sig.return_annotation),\\n\",\n    \"        \\\"docstring\\\": inspect.getdoc(func)\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"def analyze_class_structure(cls: Type) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Analyze class structure using reflection.\\\"\\\"\\\"\\n\",\n    \"    instance = cls() if not inspect.isabstract(cls) else None\\n\",\n    \"    \\n\",\n    \"    methods = [name for name, method in inspect.getmembers(cls, predicate=inspect.ismethod)\\n\",\n    \"               if not name.startswith('_')]\\n\",\n    \"    \\n\",\n    \"    functions = [name for name, func in inspect.getmembers(cls, predicate=inspect.isfunction)\\n\",\n    \"                 if not name.startswith('_')]\\n\",\n    \"    \\n\",\n    \"    properties = [name for name in dir(cls) \\n\",\n    \"                  if isinstance(getattr(cls, name, None), property)]\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"class_name\\\": cls.__name__,\\n\",\n    \"        \\\"methods\\\": methods,\\n\",\n    \"        \\\"functions\\\": functions,\\n\",\n    \"        \\\"properties\\\": properties,\\n\",\n    \"        \\\"mro\\\": [c.__name__ for c in cls.__mro__],\\n\",\n    \"        \\\"docstring\\\": inspect.getdoc(cls)\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üîç Testing Reflection & Introspection[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Analyze function signatures\\n\",\n    \"def sample_ai_function(text: str, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Sample AI function for analysis.\\\"\\\"\\\"\\n\",\n    \"    return {\\\"processed\\\": text, \\\"config\\\": {\\\"temperature\\\": temperature, \\\"max_tokens\\\": max_tokens}}\\n\",\n    \"\\n\",\n    \"analysis = analyze_function_signature(sample_ai_function)\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Function: {analysis['name']}\\\")\\n\",\n    \"console.print(f\\\"Return type: {analysis['return_type']}\\\")\\n\",\n    \"console.print(\\\"Parameters:\\\")\\n\",\n    \"for param_name, param_info in analysis['parameters'].items():\\n\",\n    \"    console.print(f\\\"  {param_name}: {param_info['annotation']} (default: {param_info['default']})\\\")\\n\",\n    \"\\n\",\n    \"# Analyze class structure\\n\",\n    \"class_analysis = analyze_class_structure(GPTModel)\\n\",\n    \"\\n\",\n    \"console.print(f\\\"\\\\nClass: {class_analysis['class_name']}\\\")\\n\",\n    \"console.print(f\\\"Methods: {class_analysis['methods']}\\\")\\n\",\n    \"console.print(f\\\"Functions: {class_analysis['functions']}\\\")\\n\",\n    \"console.print(f\\\"Properties: {class_analysis['properties']}\\\")\\n\",\n    \"console.print(f\\\"MRO: {class_analysis['mro']}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Reflection enables runtime introspection\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] inspect module for code analysis\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 8: Complete Metaprogramming AI Framework\\n\",\n    \"\\n\",\n    \"class AdaptiveAIFramework:\\n\",\n    \"    \\\"\\\"\\\"Complete AI framework using all metaprogramming patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.models = {}\\n\",\n    \"        self.configurations = {}\\n\",\n    \"        self.performance_metrics = {}\\n\",\n    \"    \\n\",\n    \"    def register_model_class(self, model_class: Type, config: Dict[str, Any] = None):\\n\",\n    \"        \\\"\\\"\\\"Register a model class with optional configuration.\\\"\\\"\\\"\\n\",\n    \"        model_id = getattr(model_class, 'model_id', model_class.__name__.lower())\\n\",\n    \"        self.models[model_id] = {\\n\",\n    \"            'class': model_class,\\n\",\n    \"            'config': config or {},\\n\",\n    \"            'analysis': analyze_class_structure(model_class)\\n\",\n    \"        }\\n\",\n    \"        console.print(f\\\"[dim]‚úÖ Registered {model_id} with framework[/dim]\\\")\\n\",\n    \"    \\n\",\n    \"    def create_model(self, model_id: str, **kwargs) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Create model instance with configuration.\\\"\\\"\\\"\\n\",\n    \"        if model_id not in self.models:\\n\",\n    \"            raise ValueError(f\\\"Model {model_id} not registered\\\")\\n\",\n    \"        \\n\",\n    \"        model_info = self.models[model_id]\\n\",\n    \"        config = {**model_info['config'], **kwargs}\\n\",\n    \"        \\n\",\n    \"        return model_info['class'](**config)\\n\",\n    \"    \\n\",\n    \"    def create_dynamic_ensemble(self, model_ids: list, ensemble_strategy: str = \\\"average\\\") -> Type:\\n\",\n    \"        \\\"\\\"\\\"Create dynamic ensemble from registered models.\\\"\\\"\\\"\\n\",\n    \"        model_classes = [self.models[mid]['class'] for mid in model_ids if mid in self.models]\\n\",\n    \"        \\n\",\n    \"        def ensemble_predict(self, text: str) -> Dict[str, Any]:\\n\",\n    \"            predictions = []\\n\",\n    \"            for model_class in model_classes:\\n\",\n    \"                model = model_class()\\n\",\n    \"                if hasattr(model, 'predict_text'):\\n\",\n    \"                    pred = model.predict_text(text)\\n\",\n    \"                    predictions.append(pred)\\n\",\n    \"                elif hasattr(model, 'predict'):\\n\",\n    \"                    pred = model.predict(text)\\n\",\n    \"                    predictions.append(pred)\\n\",\n    \"            \\n\",\n    \"            return {\\n\",\n    \"                \\\"ensemble_result\\\": predictions,\\n\",\n    \"                \\\"strategy\\\": ensemble_strategy,\\n\",\n    \"                \\\"model_count\\\": len(predictions)\\n\",\n    \"            }\\n\",\n    \"        \\n\",\n    \"        ensemble_class = create_model_class(\\n\",\n    \"            name=f\\\"DynamicEnsemble_{ensemble_strategy}\\\",\\n\",\n    \"            attributes={\\n\",\n    \"                \\\"component_models\\\": model_classes,\\n\",\n    \"                \\\"strategy\\\": ensemble_strategy\\n\",\n    \"            },\\n\",\n    \"            methods={\\\"predict_text\\\": ensemble_predict}\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        return ensemble_class\\n\",\n    \"    \\n\",\n    \"    def analyze_framework(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Comprehensive framework analysis.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"registered_models\\\": list(self.models.keys()),\\n\",\n    \"            \\\"model_details\\\": {\\n\",\n    \"                mid: {\\n\",\n    \"                    \\\"class_name\\\": info['class'].__name__,\\n\",\n    \"                    \\\"methods\\\": info['analysis']['functions'],\\n\",\n    \"                    \\\"config_keys\\\": list(info['config'].keys())\\n\",\n    \"                }\\n\",\n    \"                for mid, info in self.models.items()\\n\",\n    \"            },\\n\",\n    \"            \\\"framework_capabilities\\\": [\\n\",\n    \"                \\\"Dynamic model registration\\\",\\n\",\n    \"                \\\"Automatic instrumentation\\\",\\n\",\n    \"                \\\"Runtime ensemble creation\\\",\\n\",\n    \"                \\\"Configuration management\\\",\\n\",\n    \"                \\\"Performance monitoring\\\"\\n\",\n    \"            ]\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üöÄ Testing Complete Metaprogramming AI Framework[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Create framework instance\\n\",\n    \"framework = AdaptiveAIFramework()\\n\",\n    \"\\n\",\n    \"# Register models with configurations\\n\",\n    \"framework.register_model_class(GPTModel, {\\\"temperature\\\": 0.8})\\n\",\n    \"framework.register_model_class(ClaudeModel, {\\\"temperature\\\": 0.6})\\n\",\n    \"\\n\",\n    \"# Create individual models\\n\",\n    \"gpt_model = framework.create_model(\\\"gptmodel\\\", max_tokens=500)\\n\",\n    \"claude_model = framework.create_model(\\\"claudemodel\\\")\\n\",\n    \"\\n\",\n    \"# Test predictions\\n\",\n    \"test_text = \\\"Explain quantum computing\\\"\\n\",\n    \"gpt_result = gpt_model.predict_text(test_text)\\n\",\n    \"claude_result = claude_model.predict_text(test_text)\\n\",\n    \"\\n\",\n    \"console.print(f\\\"GPT: {gpt_result}\\\")\\n\",\n    \"console.print(f\\\"Claude: {claude_result}\\\")\\n\",\n    \"\\n\",\n    \"# Create dynamic ensemble\\n\",\n    \"EnsembleClass = framework.create_dynamic_ensemble([\\\"gptmodel\\\", \\\"claudemodel\\\"], \\\"consensus\\\")\\n\",\n    \"ensemble = EnsembleClass()\\n\",\n    \"\\n\",\n    \"ensemble_result = ensemble.predict_text(test_text)\\n\",\n    \"console.print(f\\\"Ensemble: {ensemble_result}\\\")\\n\",\n    \"\\n\",\n    \"# Framework analysis\\n\",\n    \"analysis = framework.analyze_framework()\\n\",\n    \"console.print(f\\\"\\\\nFramework Analysis:\\\")\\n\",\n    \"console.print(f\\\"Registered models: {analysis['registered_models']}\\\")\\n\",\n    \"console.print(f\\\"Capabilities: {analysis['framework_capabilities']}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]üéØ Metaprogramming Mastery Achieved![/bold green]\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Key Patterns:[/bold yellow] Descriptors, Metaclasses, Dynamic Classes, Decorators, Reflection\\\")\\n\",\n    \"console.print(\\\"[bold cyan]Production Benefits:[/bold cyan] Flexible, self-adapting, maintainable AI systems\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.5\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":30505,"cache_read_input_tokens":15686,"output_tokens":1}},"requestId":"req_vrtx_011CRhT5NxvheVJdmpDzBavw","type":"assistant","uuid":"c75e198c-9548-48c7-9599-4266cbedc341","timestamp":"2025-08-01T18:21:37.564Z"}
{"parentUuid":"c75e198c-9548-48c7-9599-4266cbedc341","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01LaPQXY1PdDL7iy3yoAp7gk","type":"tool_result","content":"File created successfully at: /home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/04_metaprogramming_learning.ipynb"}]},"uuid":"0e6c0a3f-ca13-42b2-b62a-949e0509d956","timestamp":"2025-08-01T18:21:37.592Z","toolUseResult":{"type":"create","filePath":"/home/skamenan/git/code/ai-patterns-hands-on/03-python-idioms/notebooks/04_metaprogramming_learning.ipynb","content":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# üè≠ Metaprogramming for AI Systems - Learning Notebook\\n\",\n    \"\\n\",\n    \"**First Principles**: Metaprogramming is code that manipulates code. It allows us to create flexible, self-adapting systems that can modify their behavior at runtime.\\n\",\n    \"\\n\",\n    \"**Python Idioms**: Descriptors, metaclasses, decorators, dynamic class generation, and introspection enable powerful metaprogramming patterns.\\n\",\n    \"\\n\",\n    \"**AI Applications**: Dynamic model registration, automatic instrumentation, configuration management, and adaptive behavior.\\n\",\n    \"\\n\",\n    \"---\\n\",\n    \"\\n\",\n    \"## Learning Path\\n\",\n    \"1. **Descriptors** - Custom attribute access patterns\\n\",\n    \"2. **Metaclasses** - Class creation and automatic registration\\n\",\n    \"3. **Dynamic Classes** - Runtime class generation\\n\",\n    \"4. **Advanced Decorators** - Behavior modification patterns\\n\",\n    \"5. **Property Factories** - Type-safe dynamic properties\\n\",\n    \"6. **Context Managers** - Dynamic configuration management\\n\",\n    \"7. **Reflection** - Runtime introspection capabilities\\n\",\n    \"8. **Complete AI Framework** - Production-ready metaprogramming system\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 1: Descriptors - Custom Attribute Access\\n\",\n    \"from typing import Any, Type, Callable, Dict, Optional\\n\",\n    \"from functools import update_wrapper\\n\",\n    \"import time\\n\",\n    \"from rich.console import Console\\n\",\n    \"\\n\",\n    \"console = Console()\\n\",\n    \"\\n\",\n    \"class ValidatedProperty:\\n\",\n    \"    \\\"\\\"\\\"Descriptor that validates property values.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, validator: Callable[[Any], bool], error_msg: str):\\n\",\n    \"        self.validator = validator\\n\",\n    \"        self.error_msg = error_msg\\n\",\n    \"        self.name = None\\n\",\n    \"    \\n\",\n    \"    def __set_name__(self, owner: Type, name: str):\\n\",\n    \"        \\\"\\\"\\\"Called when descriptor is assigned to a class attribute.\\\"\\\"\\\"\\n\",\n    \"        self.name = name\\n\",\n    \"        self.private_name = f'_{name}'\\n\",\n    \"    \\n\",\n    \"    def __get__(self, obj: Any, objtype: Type = None) -> Any:\\n\",\n    \"        if obj is None:\\n\",\n    \"            return self\\n\",\n    \"        return getattr(obj, self.private_name, None)\\n\",\n    \"    \\n\",\n    \"    def __set__(self, obj: Any, value: Any) -> None:\\n\",\n    \"        if not self.validator(value):\\n\",\n    \"            raise ValueError(f\\\"{self.error_msg}: {value}\\\")\\n\",\n    \"        setattr(obj, self.private_name, value)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class CachedProperty:\\n\",\n    \"    \\\"\\\"\\\"Descriptor that caches expensive computations.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, func: Callable):\\n\",\n    \"        self.func = func\\n\",\n    \"        self.name = func.__name__\\n\",\n    \"        update_wrapper(self, func)\\n\",\n    \"    \\n\",\n    \"    def __get__(self, obj: Any, objtype: Type = None) -> Any:\\n\",\n    \"        if obj is None:\\n\",\n    \"            return self\\n\",\n    \"        \\n\",\n    \"        cache_name = f'_cached_{self.name}'\\n\",\n    \"        if hasattr(obj, cache_name):\\n\",\n    \"            console.print(f\\\"[dim green]üíæ Cache hit for {self.name}[/dim green]\\\")\\n\",\n    \"            return getattr(obj, cache_name)\\n\",\n    \"        \\n\",\n    \"        console.print(f\\\"[dim yellow]üîÑ Computing {self.name}[/dim yellow]\\\")\\n\",\n    \"        value = self.func(obj)\\n\",\n    \"        setattr(obj, cache_name, value)\\n\",\n    \"        return value\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Example: AI Model with Validated Properties\\n\",\n    \"class AIModel:\\n\",\n    \"    \\\"\\\"\\\"AI model with validated properties using descriptors.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    temperature = ValidatedProperty(\\n\",\n    \"        lambda x: 0.0 <= x <= 2.0,\\n\",\n    \"        \\\"Temperature must be between 0.0 and 2.0\\\"\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    max_tokens = ValidatedProperty(\\n\",\n    \"        lambda x: isinstance(x, int) and x > 0,\\n\",\n    \"        \\\"Max tokens must be a positive integer\\\"\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    def __init__(self, temperature: float = 0.7, max_tokens: int = 1000):\\n\",\n    \"        self.temperature = temperature\\n\",\n    \"        self.max_tokens = max_tokens\\n\",\n    \"    \\n\",\n    \"    @CachedProperty\\n\",\n    \"    def model_signature(self) -> str:\\n\",\n    \"        \\\"\\\"\\\"Expensive computation that gets cached.\\\"\\\"\\\"\\n\",\n    \"        time.sleep(0.1)  # Simulate expensive computation\\n\",\n    \"        return f\\\"Model_{hash(str(self.temperature) + str(self.max_tokens))}\\\"\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üîç Testing Descriptors[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Test validation\\n\",\n    \"model = AIModel()\\n\",\n    \"console.print(f\\\"Default temperature: {model.temperature}\\\")\\n\",\n    \"\\n\",\n    \"model.temperature = 1.5\\n\",\n    \"console.print(f\\\"‚úÖ Set temperature to {model.temperature}\\\")\\n\",\n    \"\\n\",\n    \"try:\\n\",\n    \"    model.temperature = 3.0  # Should fail\\nexcept ValueError as e:\\n\",\n    \"    console.print(f\\\"‚ùå Validation caught: {e}\\\")\\n\",\n    \"\\n\",\n    \"# Test cached property\\n\",\n    \"sig1 = model.model_signature  # Should compute\\n\",\n    \"sig2 = model.model_signature  # Should use cache\\n\",\n    \"console.print(f\\\"Signatures match: {sig1 == sig2}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Descriptors control attribute access\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] __get__, __set__, __set_name__ methods\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 2: Metaclasses - Automatic Registration and Instrumentation\\n\",\n    \"\\n\",\n    \"class ModelRegistryMeta(type):\\n\",\n    \"    \\\"\\\"\\\"Metaclass that automatically registers AI models.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    _registry: Dict[str, Type] = {}\\n\",\n    \"    \\n\",\n    \"    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\\n\",\n    \"        # Add automatic ID generation\\n\",\n    \"        if 'model_id' not in namespace and name != 'BaseModel':\\n\",\n    \"            namespace['model_id'] = name.lower().replace('model', '')\\n\",\n    \"        \\n\",\n    \"        cls = super().__new__(mcs, name, bases, namespace)\\n\",\n    \"        \\n\",\n    \"        # Register the model\\n\",\n    \"        if name != 'BaseModel' and hasattr(cls, 'model_id'):\\n\",\n    \"            mcs._registry[cls.model_id] = cls\\n\",\n    \"            console.print(f\\\"[dim]üîß Registered model: {cls.model_id} -> {name}[/dim]\\\")\\n\",\n    \"        \\n\",\n    \"        return cls\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def get_model(mcs, model_id: str) -> Optional[Type]:\\n\",\n    \"        return mcs._registry.get(model_id)\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def list_models(mcs) -> list:\\n\",\n    \"        return list(mcs._registry.keys())\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class InstrumentedMeta(type):\\n\",\n    \"    \\\"\\\"\\\"Metaclass that adds automatic instrumentation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __new__(mcs, name: str, bases: tuple, namespace: dict, **kwargs):\\n\",\n    \"        # Instrument methods that start with 'predict'\\n\",\n    \"        for attr_name, attr_value in namespace.items():\\n\",\n    \"            if (attr_name.startswith('predict') and \\n\",\n    \"                callable(attr_value) and \\n\",\n    \"                not attr_name.startswith('_')):\\n\",\n    \"                \\n\",\n    \"                namespace[attr_name] = mcs._instrument_method(attr_value, attr_name)\\n\",\n    \"        \\n\",\n    \"        return super().__new__(mcs, name, bases, namespace)\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def _instrument_method(method: Callable, method_name: str) -> Callable:\\n\",\n    \"        from functools import wraps\\n\",\n    \"        \\n\",\n    \"        @wraps(method)\\n\",\n    \"        def wrapper(self, *args, **kwargs):\\n\",\n    \"            start_time = time.time()\\n\",\n    \"            try:\\n\",\n    \"                result = method(self, *args, **kwargs)\\n\",\n    \"                duration = time.time() - start_time\\n\",\n    \"                \\n\",\n    \"                # Store metrics\\n\",\n    \"                if not hasattr(self, '_metrics'):\\n\",\n    \"                    self._metrics = {}\\n\",\n    \"                if method_name not in self._metrics:\\n\",\n    \"                    self._metrics[method_name] = []\\n\",\n    \"                \\n\",\n    \"                self._metrics[method_name].append({\\n\",\n    \"                    'duration': duration,\\n\",\n    \"                    'timestamp': start_time,\\n\",\n    \"                    'success': True\\n\",\n    \"                })\\n\",\n    \"                \\n\",\n    \"                console.print(f\\\"[dim]üìä {self.__class__.__name__}.{method_name} took {duration:.4f}s[/dim]\\\")\\n\",\n    \"                return result\\n\",\n    \"                \\n\",\n    \"            except Exception as e:\\n\",\n    \"                duration = time.time() - start_time\\n\",\n    \"                console.print(f\\\"[dim red]‚ùå {self.__class__.__name__}.{method_name} failed after {duration:.4f}s[/dim red]\\\")\\n\",\n    \"                raise\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Base model with both metaclasses\\n\",\n    \"class BaseModel(metaclass=ModelRegistryMeta):\\n\",\n    \"    \\\"\\\"\\\"Base class for AI models.\\\"\\\"\\\"\\n\",\n    \"    pass\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class GPTModel(BaseModel, metaclass=InstrumentedMeta):\\n\",\n    \"    \\\"\\\"\\\"GPT model with automatic instrumentation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def predict_text(self, text: str) -> str:\\n\",\n    \"        time.sleep(0.05)  # Simulate prediction\\n\",\n    \"        return f\\\"GPT prediction for: {text}\\\"\\n\",\n    \"    \\n\",\n    \"    def predict_classification(self, text: str) -> Dict[str, float]:\\n\",\n    \"        time.sleep(0.03)\\n\",\n    \"        return {\\\"positive\\\": 0.8, \\\"negative\\\": 0.1, \\\"neutral\\\": 0.1}\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class ClaudeModel(BaseModel, metaclass=InstrumentedMeta):\\n\",\n    \"    \\\"\\\"\\\"Claude model with automatic instrumentation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def predict_text(self, text: str) -> str:\\n\",\n    \"        time.sleep(0.04)\\n\",\n    \"        return f\\\"Claude prediction for: {text}\\\"\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üè≠ Testing Metaclasses[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Show automatic registration\\n\",\n    \"registered_models = ModelRegistryMeta.list_models()\\n\",\n    \"console.print(f\\\"Registered models: {registered_models}\\\")\\n\",\n    \"\\n\",\n    \"# Create models from registry\\n\",\n    \"gpt_class = ModelRegistryMeta.get_model('gpt')\\n\",\n    \"claude_class = ModelRegistryMeta.get_model('claude')\\n\",\n    \"\\n\",\n    \"if gpt_class:\\n\",\n    \"    gpt = gpt_class()\\n\",\n    \"    result = gpt.predict_text(\\\"Hello world\\\")\\n\",\n    \"    console.print(f\\\"GPT: {result}\\\")\\n\",\n    \"\\n\",\n    \"if claude_class:\\n\",\n    \"    claude = claude_class()\\n\",\n    \"    result = claude.predict_text(\\\"Hello world\\\")\\n\",\n    \"    console.print(f\\\"Claude: {result}\\\")\\n\",\n    \"\\n\",\n    \"# Show automatic instrumentation\\n\",\n    \"if hasattr(gpt, '_metrics'):\\n\",\n    \"    for method, metrics in gpt._metrics.items():\\n\",\n    \"        avg_duration = sum(m['duration'] for m in metrics) / len(metrics)\\n\",\n    \"        console.print(f\\\"Metrics - {method}: {len(metrics)} calls, avg {avg_duration:.4f}s\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Metaclasses control class creation\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] __new__ method in metaclass\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 3: Dynamic Class Generation\\n\",\n    \"\\n\",\n    \"def create_model_class(\\n\",\n    \"    name: str, \\n\",\n    \"    attributes: Dict[str, Any], \\n\",\n    \"    methods: Dict[str, Callable] = None,\\n\",\n    \"    base_classes: tuple = ()\\n\",\n    \") -> Type:\\n\",\n    \"    \\\"\\\"\\\"Dynamically create a model class.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    if not base_classes:\\n\",\n    \"        base_classes = (object,)\\n\",\n    \"    \\n\",\n    \"    namespace = attributes.copy()\\n\",\n    \"    if methods:\\n\",\n    \"        namespace.update(methods)\\n\",\n    \"    \\n\",\n    \"    # Add default __init__ if not provided\\n\",\n    \"    if '__init__' not in namespace:\\n\",\n    \"        def __init__(self, **kwargs):\\n\",\n    \"            for key, value in kwargs.items():\\n\",\n    \"                setattr(self, key, value)\\n\",\n    \"        namespace['__init__'] = __init__\\n\",\n    \"    \\n\",\n    \"    return type(name, base_classes, namespace)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class ModelFactory:\\n\",\n    \"    \\\"\\\"\\\"Factory for creating model classes dynamically.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def create_simple_model(name: str, prediction_delay: float = 0.1) -> Type:\\n\",\n    \"        \\\"\\\"\\\"Create a simple model class.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        def predict(self, text: str) -> str:\\n\",\n    \"            time.sleep(prediction_delay)\\n\",\n    \"            return f\\\"{name} processed: {text}\\\"\\n\",\n    \"        \\n\",\n    \"        def get_info(self) -> Dict[str, Any]:\\n\",\n    \"            return {\\n\",\n    \"                \\\"model_name\\\": name,\\n\",\n    \"                \\\"prediction_delay\\\": prediction_delay,\\n\",\n    \"                \\\"class_name\\\": self.__class__.__name__\\n\",\n    \"            }\\n\",\n    \"        \\n\",\n    \"        return create_model_class(\\n\",\n    \"            name=f\\\"{name}Model\\\",\\n\",\n    \"            attributes={\\n\",\n    \"                \\\"model_name\\\": name,\\n\",\n    \"                \\\"prediction_delay\\\": prediction_delay\\n\",\n    \"            },\\n\",\n    \"            methods={\\n\",\n    \"                \\\"predict\\\": predict,\\n\",\n    \"                \\\"get_info\\\": get_info\\n\",\n    \"            },\\n\",\n    \"            base_classes=(BaseModel,)\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    @staticmethod\\n\",\n    \"    def create_ensemble_model(models: list) -> Type:\\n\",\n    \"        \\\"\\\"\\\"Create an ensemble model from multiple model classes.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        def predict(self, text: str) -> list:\\n\",\n    \"            results = []\\n\",\n    \"            for model_class in models:\\n\",\n    \"                model = model_class()\\n\",\n    \"                if hasattr(model, 'predict'):\\n\",\n    \"                    results.append(model.predict(text))\\n\",\n    \"                elif hasattr(model, 'predict_text'):\\n\",\n    \"                    results.append(model.predict_text(text))\\n\",\n    \"            return results\\n\",\n    \"        \\n\",\n    \"        def get_model_info(self) -> list:\\n\",\n    \"            return [model.__name__ for model in models]\\n\",\n    \"        \\n\",\n    \"        return create_model_class(\\n\",\n    \"            name=\\\"EnsembleModel\\\",\\n\",\n    \"            attributes={\\\"component_models\\\": models},\\n\",\n    \"            methods={\\n\",\n    \"                \\\"predict\\\": predict,\\n\",\n    \"                \\\"get_model_info\\\": get_model_info\\n\",\n    \"            }\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]‚ö° Testing Dynamic Class Creation[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Create models using factory\\n\",\n    \"FastModel = ModelFactory.create_simple_model(\\\"FastAI\\\", prediction_delay=0.02)\\n\",\n    \"SlowModel = ModelFactory.create_simple_model(\\\"DeepThink\\\", prediction_delay=0.08)\\n\",\n    \"\\n\",\n    \"fast_model = FastModel()\\n\",\n    \"slow_model = SlowModel()\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Fast model: {fast_model.get_info()}\\\")\\n\",\n    \"console.print(f\\\"Slow model: {slow_model.get_info()}\\\")\\n\",\n    \"\\n\",\n    \"# Test predictions\\n\",\n    \"start_time = time.time()\\n\",\n    \"fast_result = fast_model.predict(\\\"test input\\\")\\n\",\n    \"fast_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"start_time = time.time()\\n\",\n    \"slow_result = slow_model.predict(\\\"test input\\\")\\n\",\n    \"slow_time = time.time() - start_time\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Performance - Fast: {fast_time:.4f}s, Slow: {slow_time:.4f}s\\\")\\n\",\n    \"\\n\",\n    \"# Create ensemble model\\n\",\n    \"EnsembleModel = ModelFactory.create_ensemble_model([FastModel, SlowModel])\\n\",\n    \"ensemble = EnsembleModel()\\n\",\n    \"\\n\",\n    \"ensemble_results = ensemble.predict(\\\"ensemble test\\\")\\n\",\n    \"console.print(f\\\"Ensemble results: {ensemble_results}\\\")\\n\",\n    \"console.print(f\\\"Component models: {ensemble.get_model_info()}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Classes are objects that can be created at runtime\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] type() function for dynamic class creation\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 4: Advanced Decorators with State\\n\",\n    \"from functools import wraps\\n\",\n    \"\\n\",\n    \"class memoize_with_ttl:\\n\",\n    \"    \\\"\\\"\\\"Decorator that memoizes with time-to-live.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, ttl_seconds: float = 60.0):\\n\",\n    \"        self.ttl_seconds = ttl_seconds\\n\",\n    \"        self.cache = {}\\n\",\n    \"    \\n\",\n    \"    def __call__(self, func: Callable) -> Callable:\\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            # Create cache key\\n\",\n    \"            key = str(args) + str(sorted(kwargs.items()))\\n\",\n    \"            current_time = time.time()\\n\",\n    \"            \\n\",\n    \"            # Check if cached and not expired\\n\",\n    \"            if key in self.cache:\\n\",\n    \"                result, timestamp = self.cache[key]\\n\",\n    \"                if current_time - timestamp < self.ttl_seconds:\\n\",\n    \"                    console.print(f\\\"[dim green]üíæ Cache hit for {func.__name__}[/dim green]\\\")\\n\",\n    \"                    return result\\n\",\n    \"            \\n\",\n    \"            # Compute new result\\n\",\n    \"            result = func(*args, **kwargs)\\n\",\n    \"            self.cache[key] = (result, current_time)\\n\",\n    \"            console.print(f\\\"[dim yellow]üîÑ Cache miss for {func.__name__}[/dim yellow]\\\")\\n\",\n    \"            return result\\n\",\n    \"        \\n\",\n    \"        # Add cache management methods\\n\",\n    \"        wrapper.clear_cache = lambda: self.cache.clear()\\n\",\n    \"        wrapper.cache_info = lambda: {\\n\",\n    \"            'size': len(self.cache),\\n\",\n    \"            'ttl_seconds': self.ttl_seconds\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class retry_with_backoff:\\n\",\n    \"    \\\"\\\"\\\"Decorator that retries with exponential backoff.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\\n\",\n    \"        self.max_retries = max_retries\\n\",\n    \"        self.base_delay = base_delay\\n\",\n    \"    \\n\",\n    \"    def __call__(self, func: Callable) -> Callable:\\n\",\n    \"        @wraps(func)\\n\",\n    \"        def wrapper(*args, **kwargs):\\n\",\n    \"            last_exception = None\\n\",\n    \"            \\n\",\n    \"            for attempt in range(self.max_retries + 1):\\n\",\n    \"                try:\\n\",\n    \"                    return func(*args, **kwargs)\\n\",\n    \"                except Exception as e:\\n\",\n    \"                    last_exception = e\\n\",\n    \"                    \\n\",\n    \"                    if attempt < self.max_retries:\\n\",\n    \"                        delay = self.base_delay * (2 ** attempt)\\n\",\n    \"                        console.print(f\\\"[yellow]üîÑ Retry {attempt + 1}/{self.max_retries} in {delay:.1f}s[/yellow]\\\")\\n\",\n    \"                        time.sleep(delay)\\n\",\n    \"                    else:\\n\",\n    \"                        console.print(f\\\"[red]‚ùå All retries exhausted for {func.__name__}[/red]\\\")\\n\",\n    \"            \\n\",\n    \"            raise last_exception\\n\",\n    \"        \\n\",\n    \"        return wrapper\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üé® Testing Advanced Decorators[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"@memoize_with_ttl(ttl_seconds=2.0)\\n\",\n    \"@retry_with_backoff(max_retries=2, base_delay=0.5)\\n\",\n    \"def expensive_ai_call(query: str, fail_rate: float = 0.0) -> str:\\n\",\n    \"    \\\"\\\"\\\"Simulate expensive AI call that might fail.\\\"\\\"\\\"\\n\",\n    \"    import random\\n\",\n    \"    \\n\",\n    \"    if random.random() < fail_rate:\\n\",\n    \"        raise ConnectionError(\\\"Simulated API failure\\\")\\n\",\n    \"    \\n\",\n    \"    time.sleep(0.1)  # Simulate processing time\\n\",\n    \"    return f\\\"AI response to: {query}\\\"\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Test decorated function\\n\",\n    \"result1 = expensive_ai_call(\\\"What is AI?\\\")\\n\",\n    \"console.print(f\\\"Result 1: {result1}\\\")\\n\",\n    \"\\n\",\n    \"result2 = expensive_ai_call(\\\"What is AI?\\\")  # Should use cache\\n\",\n    \"console.print(f\\\"Result 2: {result2}\\\")\\n\",\n    \"\\n\",\n    \"# Test with failures\\n\",\n    \"try:\\n\",\n    \"    result3 = expensive_ai_call(\\\"Risky query\\\", fail_rate=0.7)\\n\",\n    \"    console.print(f\\\"Success: {result3}\\\")\\n\",\n    \"except Exception as e:\\n\",\n    \"    console.print(f\\\"Failed: {e}\\\")\\n\",\n    \"\\n\",\n    \"cache_info = expensive_ai_call.cache_info()\\n\",\n    \"console.print(f\\\"Cache info: {cache_info}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Decorators modify function behavior\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] __call__ method for stateful decorators\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 5: Property Factory - Type-Safe Dynamic Properties\\n\",\n    \"\\n\",\n    \"def typed_property(property_type: Type, default: Any = None, validator: Callable = None):\\n\",\n    \"    \\\"\\\"\\\"Factory for creating typed properties with optional validation.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def property_decorator(func: Callable) -> property:\\n\",\n    \"        name = func.__name__\\n\",\n    \"        private_name = f'_{name}'\\n\",\n    \"        \\n\",\n    \"        def getter(self):\\n\",\n    \"            return getattr(self, private_name, default)\\n\",\n    \"        \\n\",\n    \"        def setter(self, value):\\n\",\n    \"            # Type checking\\n\",\n    \"            if not isinstance(value, property_type):\\n\",\n    \"                try:\\n\",\n    \"                    value = property_type(value)\\n\",\n    \"                except (ValueError, TypeError):\\n\",\n    \"                    raise TypeError(f\\\"{name} must be of type {property_type.__name__}\\\")\\n\",\n    \"            \\n\",\n    \"            # Optional validation\\n\",\n    \"            if validator and not validator(value):\\n\",\n    \"                raise ValueError(f\\\"Invalid value for {name}: {value}\\\")\\n\",\n    \"            \\n\",\n    \"            setattr(self, private_name, value)\\n\",\n    \"        \\n\",\n    \"        return property(getter, setter, doc=func.__doc__)\\n\",\n    \"    \\n\",\n    \"    return property_decorator\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class ConfigurableModel:\\n\",\n    \"    \\\"\\\"\\\"Model with typed properties using property factory.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    @typed_property(float, default=0.7, validator=lambda x: 0.0 <= x <= 2.0)\\n\",\n    \"    def temperature(self):\\n\",\n    \"        \\\"\\\"\\\"Model temperature parameter.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"    \\n\",\n    \"    @typed_property(int, default=1000, validator=lambda x: x > 0)\\n\",\n    \"    def max_tokens(self):\\n\",\n    \"        \\\"\\\"\\\"Maximum tokens to generate.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"    \\n\",\n    \"    @typed_property(str, default=\\\"gpt-4\\\")\\n\",\n    \"    def model_name(self):\\n\",\n    \"        \\\"\\\"\\\"Name of the model.\\\"\\\"\\\"\\n\",\n    \"        pass\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üè∑Ô∏è Testing Typed Properties[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"model = ConfigurableModel()\\n\",\n    \"\\n\",\n    \"# Test default values\\n\",\n    \"console.print(f\\\"Default temperature: {model.temperature}\\\")\\n\",\n    \"console.print(f\\\"Default max_tokens: {model.max_tokens}\\\")\\n\",\n    \"console.print(f\\\"Default model_name: {model.model_name}\\\")\\n\",\n    \"\\n\",\n    \"# Test type conversion\\n\",\n    \"model.temperature = \\\"1.2\\\"  # String should convert to float\\n\",\n    \"model.max_tokens = \\\"500\\\"   # String should convert to int\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Converted temperature: {model.temperature} (type: {type(model.temperature)})\\\")\\n\",\n    \"console.print(f\\\"Converted max_tokens: {model.max_tokens} (type: {type(model.max_tokens)})\\\")\\n\",\n    \"\\n\",\n    \"# Test validation\\n\",\n    \"try:\\n\",\n    \"    model.temperature = 3.0  # Should fail validation\\nexcept ValueError as e:\\n\",\n    \"    console.print(f\\\"‚ùå Validation error: {e}\\\")\\n\",\n    \"\\n\",\n    \"try:\\n\",\n    \"    model.max_tokens = -100  # Should fail validation\\nexcept ValueError as e:\\n\",\n    \"    console.print(f\\\"‚ùå Validation error: {e}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Properties provide controlled attribute access\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] Property factory pattern for dynamic properties\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 6: Dynamic Configuration Context Manager\\n\",\n    \"\\n\",\n    \"class DynamicConfig:\\n\",\n    \"    \\\"\\\"\\\"Context manager that temporarily modifies class attributes.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, target_class: Type, **config_changes):\\n\",\n    \"        self.target_class = target_class\\n\",\n    \"        self.config_changes = config_changes\\n\",\n    \"        self.original_values = {}\\n\",\n    \"    \\n\",\n    \"    def __enter__(self):\\n\",\n    \"        # Save original values\\n\",\n    \"        for attr_name, new_value in self.config_changes.items():\\n\",\n    \"            if hasattr(self.target_class, attr_name):\\n\",\n    \"                self.original_values[attr_name] = getattr(self.target_class, attr_name)\\n\",\n    \"            setattr(self.target_class, attr_name, new_value)\\n\",\n    \"        return self\\n\",\n    \"    \\n\",\n    \"    def __exit__(self, exc_type, exc_val, exc_tb):\\n\",\n    \"        # Restore original values\\n\",\n    \"        for attr_name, original_value in self.original_values.items():\\n\",\n    \"            setattr(self.target_class, attr_name, original_value)\\n\",\n    \"        \\n\",\n    \"        # Remove attributes that didn't exist originally\\n\",\n    \"        for attr_name in self.config_changes:\\n\",\n    \"            if attr_name not in self.original_values:\\n\",\n    \"                delattr(self.target_class, attr_name)\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# Example service class\\n\",\n    \"class AIService:\\n\",\n    \"    timeout = 30.0\\n\",\n    \"    retry_count = 3\\n\",\n    \"    debug_mode = False\\n\",\n    \"    \\n\",\n    \"    @classmethod\\n\",\n    \"    def get_config(cls) -> Dict[str, Any]:\\n\",\n    \"        return {\\n\",\n    \"            \\\"timeout\\\": cls.timeout,\\n\",\n    \"            \\\"retry_count\\\": cls.retry_count,\\n\",\n    \"            \\\"debug_mode\\\": cls.debug_mode\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]‚öôÔ∏è Testing Dynamic Configuration[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Original configuration: {AIService.get_config()}\\\")\\n\",\n    \"\\n\",\n    \"# Temporarily modify configuration\\n\",\n    \"with DynamicConfig(AIService, timeout=60.0, debug_mode=True, new_feature=True):\\n\",\n    \"    console.print(f\\\"Inside context: {AIService.get_config()}\\\")\\n\",\n    \"    console.print(f\\\"New feature: {getattr(AIService, 'new_feature', 'Not found')}\\\")\\n\",\n    \"\\n\",\n    \"console.print(f\\\"After context: {AIService.get_config()}\\\")\\n\",\n    \"console.print(f\\\"New feature: {getattr(AIService, 'new_feature', 'Not found')}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Context managers provide resource management\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] __enter__ and __exit__ methods\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 7: Reflection and Introspection\\n\",\n    \"import inspect\\n\",\n    \"from typing import get_type_hints, get_origin, get_args\\n\",\n    \"\\n\",\n    \"def analyze_function_signature(func: Callable) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Metaprogramming function to analyze function signatures.\\\"\\\"\\\"\\n\",\n    \"    sig = inspect.signature(func)\\n\",\n    \"    type_hints = get_type_hints(func)\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"name\\\": func.__name__,\\n\",\n    \"        \\\"parameters\\\": {\\n\",\n    \"            name: {\\n\",\n    \"                \\\"annotation\\\": type_hints.get(name, param.annotation),\\n\",\n    \"                \\\"default\\\": param.default if param.default != param.empty else None,\\n\",\n    \"                \\\"kind\\\": param.kind.name\\n\",\n    \"            }\\n\",\n    \"            for name, param in sig.parameters.items()\\n\",\n    \"        },\\n\",\n    \"        \\\"return_type\\\": type_hints.get('return', sig.return_annotation),\\n\",\n    \"        \\\"docstring\\\": inspect.getdoc(func)\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"def analyze_class_structure(cls: Type) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Analyze class structure using reflection.\\\"\\\"\\\"\\n\",\n    \"    instance = cls() if not inspect.isabstract(cls) else None\\n\",\n    \"    \\n\",\n    \"    methods = [name for name, method in inspect.getmembers(cls, predicate=inspect.ismethod)\\n\",\n    \"               if not name.startswith('_')]\\n\",\n    \"    \\n\",\n    \"    functions = [name for name, func in inspect.getmembers(cls, predicate=inspect.isfunction)\\n\",\n    \"                 if not name.startswith('_')]\\n\",\n    \"    \\n\",\n    \"    properties = [name for name in dir(cls) \\n\",\n    \"                  if isinstance(getattr(cls, name, None), property)]\\n\",\n    \"    \\n\",\n    \"    return {\\n\",\n    \"        \\\"class_name\\\": cls.__name__,\\n\",\n    \"        \\\"methods\\\": methods,\\n\",\n    \"        \\\"functions\\\": functions,\\n\",\n    \"        \\\"properties\\\": properties,\\n\",\n    \"        \\\"mro\\\": [c.__name__ for c in cls.__mro__],\\n\",\n    \"        \\\"docstring\\\": inspect.getdoc(cls)\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üîç Testing Reflection & Introspection[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Analyze function signatures\\n\",\n    \"def sample_ai_function(text: str, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:\\n\",\n    \"    \\\"\\\"\\\"Sample AI function for analysis.\\\"\\\"\\\"\\n\",\n    \"    return {\\\"processed\\\": text, \\\"config\\\": {\\\"temperature\\\": temperature, \\\"max_tokens\\\": max_tokens}}\\n\",\n    \"\\n\",\n    \"analysis = analyze_function_signature(sample_ai_function)\\n\",\n    \"\\n\",\n    \"console.print(f\\\"Function: {analysis['name']}\\\")\\n\",\n    \"console.print(f\\\"Return type: {analysis['return_type']}\\\")\\n\",\n    \"console.print(\\\"Parameters:\\\")\\n\",\n    \"for param_name, param_info in analysis['parameters'].items():\\n\",\n    \"    console.print(f\\\"  {param_name}: {param_info['annotation']} (default: {param_info['default']})\\\")\\n\",\n    \"\\n\",\n    \"# Analyze class structure\\n\",\n    \"class_analysis = analyze_class_structure(GPTModel)\\n\",\n    \"\\n\",\n    \"console.print(f\\\"\\\\nClass: {class_analysis['class_name']}\\\")\\n\",\n    \"console.print(f\\\"Methods: {class_analysis['methods']}\\\")\\n\",\n    \"console.print(f\\\"Functions: {class_analysis['functions']}\\\")\\n\",\n    \"console.print(f\\\"Properties: {class_analysis['properties']}\\\")\\n\",\n    \"console.print(f\\\"MRO: {class_analysis['mro']}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]First Principle:[/bold green] Reflection enables runtime introspection\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Python Idiom:[/bold yellow] inspect module for code analysis\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Cell 8: Complete Metaprogramming AI Framework\\n\",\n    \"\\n\",\n    \"class AdaptiveAIFramework:\\n\",\n    \"    \\\"\\\"\\\"Complete AI framework using all metaprogramming patterns.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self):\\n\",\n    \"        self.models = {}\\n\",\n    \"        self.configurations = {}\\n\",\n    \"        self.performance_metrics = {}\\n\",\n    \"    \\n\",\n    \"    def register_model_class(self, model_class: Type, config: Dict[str, Any] = None):\\n\",\n    \"        \\\"\\\"\\\"Register a model class with optional configuration.\\\"\\\"\\\"\\n\",\n    \"        model_id = getattr(model_class, 'model_id', model_class.__name__.lower())\\n\",\n    \"        self.models[model_id] = {\\n\",\n    \"            'class': model_class,\\n\",\n    \"            'config': config or {},\\n\",\n    \"            'analysis': analyze_class_structure(model_class)\\n\",\n    \"        }\\n\",\n    \"        console.print(f\\\"[dim]‚úÖ Registered {model_id} with framework[/dim]\\\")\\n\",\n    \"    \\n\",\n    \"    def create_model(self, model_id: str, **kwargs) -> Any:\\n\",\n    \"        \\\"\\\"\\\"Create model instance with configuration.\\\"\\\"\\\"\\n\",\n    \"        if model_id not in self.models:\\n\",\n    \"            raise ValueError(f\\\"Model {model_id} not registered\\\")\\n\",\n    \"        \\n\",\n    \"        model_info = self.models[model_id]\\n\",\n    \"        config = {**model_info['config'], **kwargs}\\n\",\n    \"        \\n\",\n    \"        return model_info['class'](**config)\\n\",\n    \"    \\n\",\n    \"    def create_dynamic_ensemble(self, model_ids: list, ensemble_strategy: str = \\\"average\\\") -> Type:\\n\",\n    \"        \\\"\\\"\\\"Create dynamic ensemble from registered models.\\\"\\\"\\\"\\n\",\n    \"        model_classes = [self.models[mid]['class'] for mid in model_ids if mid in self.models]\\n\",\n    \"        \\n\",\n    \"        def ensemble_predict(self, text: str) -> Dict[str, Any]:\\n\",\n    \"            predictions = []\\n\",\n    \"            for model_class in model_classes:\\n\",\n    \"                model = model_class()\\n\",\n    \"                if hasattr(model, 'predict_text'):\\n\",\n    \"                    pred = model.predict_text(text)\\n\",\n    \"                    predictions.append(pred)\\n\",\n    \"                elif hasattr(model, 'predict'):\\n\",\n    \"                    pred = model.predict(text)\\n\",\n    \"                    predictions.append(pred)\\n\",\n    \"            \\n\",\n    \"            return {\\n\",\n    \"                \\\"ensemble_result\\\": predictions,\\n\",\n    \"                \\\"strategy\\\": ensemble_strategy,\\n\",\n    \"                \\\"model_count\\\": len(predictions)\\n\",\n    \"            }\\n\",\n    \"        \\n\",\n    \"        ensemble_class = create_model_class(\\n\",\n    \"            name=f\\\"DynamicEnsemble_{ensemble_strategy}\\\",\\n\",\n    \"            attributes={\\n\",\n    \"                \\\"component_models\\\": model_classes,\\n\",\n    \"                \\\"strategy\\\": ensemble_strategy\\n\",\n    \"            },\\n\",\n    \"            methods={\\\"predict_text\\\": ensemble_predict}\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        return ensemble_class\\n\",\n    \"    \\n\",\n    \"    def analyze_framework(self) -> Dict[str, Any]:\\n\",\n    \"        \\\"\\\"\\\"Comprehensive framework analysis.\\\"\\\"\\\"\\n\",\n    \"        return {\\n\",\n    \"            \\\"registered_models\\\": list(self.models.keys()),\\n\",\n    \"            \\\"model_details\\\": {\\n\",\n    \"                mid: {\\n\",\n    \"                    \\\"class_name\\\": info['class'].__name__,\\n\",\n    \"                    \\\"methods\\\": info['analysis']['functions'],\\n\",\n    \"                    \\\"config_keys\\\": list(info['config'].keys())\\n\",\n    \"                }\\n\",\n    \"                for mid, info in self.models.items()\\n\",\n    \"            },\\n\",\n    \"            \\\"framework_capabilities\\\": [\\n\",\n    \"                \\\"Dynamic model registration\\\",\\n\",\n    \"                \\\"Automatic instrumentation\\\",\\n\",\n    \"                \\\"Runtime ensemble creation\\\",\\n\",\n    \"                \\\"Configuration management\\\",\\n\",\n    \"                \\\"Performance monitoring\\\"\\n\",\n    \"            ]\\n\",\n    \"        }\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"console.print(\\\"[bold blue]üöÄ Testing Complete Metaprogramming AI Framework[/bold blue]\\\")\\n\",\n    \"\\n\",\n    \"# Create framework instance\\n\",\n    \"framework = AdaptiveAIFramework()\\n\",\n    \"\\n\",\n    \"# Register models with configurations\\n\",\n    \"framework.register_model_class(GPTModel, {\\\"temperature\\\": 0.8})\\n\",\n    \"framework.register_model_class(ClaudeModel, {\\\"temperature\\\": 0.6})\\n\",\n    \"\\n\",\n    \"# Create individual models\\n\",\n    \"gpt_model = framework.create_model(\\\"gptmodel\\\", max_tokens=500)\\n\",\n    \"claude_model = framework.create_model(\\\"claudemodel\\\")\\n\",\n    \"\\n\",\n    \"# Test predictions\\n\",\n    \"test_text = \\\"Explain quantum computing\\\"\\n\",\n    \"gpt_result = gpt_model.predict_text(test_text)\\n\",\n    \"claude_result = claude_model.predict_text(test_text)\\n\",\n    \"\\n\",\n    \"console.print(f\\\"GPT: {gpt_result}\\\")\\n\",\n    \"console.print(f\\\"Claude: {claude_result}\\\")\\n\",\n    \"\\n\",\n    \"# Create dynamic ensemble\\n\",\n    \"EnsembleClass = framework.create_dynamic_ensemble([\\\"gptmodel\\\", \\\"claudemodel\\\"], \\\"consensus\\\")\\n\",\n    \"ensemble = EnsembleClass()\\n\",\n    \"\\n\",\n    \"ensemble_result = ensemble.predict_text(test_text)\\n\",\n    \"console.print(f\\\"Ensemble: {ensemble_result}\\\")\\n\",\n    \"\\n\",\n    \"# Framework analysis\\n\",\n    \"analysis = framework.analyze_framework()\\n\",\n    \"console.print(f\\\"\\\\nFramework Analysis:\\\")\\n\",\n    \"console.print(f\\\"Registered models: {analysis['registered_models']}\\\")\\n\",\n    \"console.print(f\\\"Capabilities: {analysis['framework_capabilities']}\\\")\\n\",\n    \"\\n\",\n    \"console.print(\\\"\\\\n[bold green]üéØ Metaprogramming Mastery Achieved![/bold green]\\\")\\n\",\n    \"console.print(\\\"[bold yellow]Key Patterns:[/bold yellow] Descriptors, Metaclasses, Dynamic Classes, Decorators, Reflection\\\")\\n\",\n    \"console.print(\\\"[bold cyan]Production Benefits:[/bold cyan] Flexible, self-adapting, maintainable AI systems\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.5\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}","structuredPatch":[]}}
{"parentUuid":"0e6c0a3f-ca13-42b2-b62a-949e0509d956","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/ai-patterns-hands-on","sessionId":"7341357d-7df3-4d52-acd0-2d3b8e57d414","version":"1.0.65","gitBranch":"","message":{"id":"msg_vrtx_01YQDQv2QW5jUarRFDpY8oJ7","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Perfect! I've completed the final notebook in the requested sequence. The metaprogramming learning notebook covers all the advanced Python patterns for creating flexible, self-adapting AI systems:\n\n**Notebooks Created (in requested order 3, 2, 5, 4):**\n1. ‚úÖ `03_functional_programming_learning.ipynb` \n2. ‚úÖ `02_async_patterns_learning.ipynb`\n3. ‚úÖ `05_performance_optimization_learning.ipynb`\n4. ‚úÖ `04_metaprogramming_learning.ipynb` (just completed)\n\n**The Metaprogramming notebook contains 8 progressive cells:**\n\n1. **Descriptors** - Custom attribute access with validation and caching\n2. **Metaclasses** - Automatic model registration and instrumentation\n3. **Dynamic Classes** - Runtime class generation and model factories\n4. **Advanced Decorators** - Stateful decorators with TTL caching and retry logic\n5. **Property Factories** - Type-safe dynamic properties with validation\n6. **Context Managers** - Dynamic configuration management\n7. **Reflection** - Runtime introspection and code analysis\n8. **Complete Framework** - Production-ready metaprogramming AI system\n\nEach cell demonstrates first principles, Python idioms, and practical AI applications with rich console output for interactive learning."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":13018,"cache_read_input_tokens":46191,"output_tokens":1}},"requestId":"req_vrtx_011CRhTJSenoddskuqzwWqiR","type":"assistant","uuid":"403dcfda-b369-470a-931d-fe05d414c626","timestamp":"2025-08-01T18:21:46.173Z"}
