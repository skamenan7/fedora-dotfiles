{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/2729-global-vector-store-config-v2","sessionId":"6be16b69-7c02-459e-9e44-8c75d319582d","version":"1.0.60","gitBranch":"2729-global-vector-store-config-v2","type":"user","message":{"role":"user","content":"@IMPLEMENTATION_PLAN.md check the plan use firecrawler to review the comment sin 2729 issue comments on github"},"uuid":"74fd75e3-f660-4b13-93be-eba07b9ba6fe","timestamp":"2025-07-25T14:58:52.140Z"}
{"parentUuid":"74fd75e3-f660-4b13-93be-eba07b9ba6fe","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/2729-global-vector-store-config-v2","sessionId":"6be16b69-7c02-459e-9e44-8c75d319582d","version":"1.0.60","gitBranch":"2729-global-vector-store-config-v2","message":{"id":"msg_01XA6MZAxyxKJtN2cWdPKqEq","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Looking at the implementation plan, I need to use Firecrawl to review the comments in GitHub issue #2729 to understand any additional context or feedback that might affect the implementation."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":24256,"cache_read_input_tokens":0,"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CRTwBDr822T2SMz86enBh","type":"assistant","uuid":"8348deda-239f-49a9-9deb-ee90e9e1601b","timestamp":"2025-07-25T14:59:06.331Z"}
{"parentUuid":"8348deda-239f-49a9-9deb-ee90e9e1601b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/2729-global-vector-store-config-v2","sessionId":"6be16b69-7c02-459e-9e44-8c75d319582d","version":"1.0.60","gitBranch":"2729-global-vector-store-config-v2","message":{"id":"msg_01XA6MZAxyxKJtN2cWdPKqEq","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01F4fxgGopXwmfyeEB5FW7Kf","name":"mcp__firecrawl__firecrawl_scrape","input":{"url":"https://github.com/meta-llama/llama-stack/issues/2729","formats":["markdown"],"onlyMainContent":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":24256,"cache_read_input_tokens":0,"output_tokens":163,"service_tier":"standard"}},"requestId":"req_011CRTwBDr822T2SMz86enBh","type":"assistant","uuid":"caa4d4ab-d2e4-4bfb-98af-b9271fbf1d62","timestamp":"2025-07-25T14:59:07.561Z"}
{"parentUuid":"caa4d4ab-d2e4-4bfb-98af-b9271fbf1d62","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/2729-global-vector-store-config-v2","sessionId":"6be16b69-7c02-459e-9e44-8c75d319582d","version":"1.0.60","gitBranch":"2729-global-vector-store-config-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01F4fxgGopXwmfyeEB5FW7Kf","type":"tool_result","content":[{"type":"text","text":"[Skip to content](https://github.com/meta-llama/llama-stack/issues/2729#start-of-content)\n\n[meta-llama](https://github.com/meta-llama)/ **[llama-stack](https://github.com/meta-llama/llama-stack)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack) You must be signed in to change notification settings\n- [Fork\\\\\n1.1k](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack)\n- [Star\\\\\n7.9k](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack)\n\n\n# Make embedding model configurable for vector-io providers\\#2729\n\n[New issue](https://github.com/login?return_to=https://github.com/meta-llama/llama-stack/issues/2729)\n\nCopy link\n\n[New issue](https://github.com/login?return_to=https://github.com/meta-llama/llama-stack/issues/2729)\n\nCopy link\n\nOpen\n\nOpen\n\n[Make embedding model configurable for vector-io providers](https://github.com/meta-llama/llama-stack/issues/2729#top)#2729\n\nCopy link\n\nAssignees\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)](https://github.com/skamenan7)\n\nLabels\n\n[enhancementNew feature or request](https://github.com/meta-llama/llama-stack/issues?q=state%3Aopen%20label%3A%22enhancement%22) New feature or request\n\n[![@derekhiggins](https://avatars.githubusercontent.com/u/883848?v=4&size=80)](https://github.com/derekhiggins)\n\n## Description\n\n[![@derekhiggins](https://avatars.githubusercontent.com/u/883848?v=4&size=48)](https://github.com/derekhiggins)\n\n[derekhiggins](https://github.com/derekhiggins)\n\nopened [on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issue-3223215728)\n\nContributor\n\nIssue body actions\n\n### 🚀 Describe the new functionality needed\n\nSpawned off from [#2703](https://github.com/meta-llama/llama-stack/pull/2703)\n\nCurrently, vector-io providers do not have a way to configure the embedding model at the provider level.\n\nWhen creating vector stores, users must manually specify the embedding model or a default is used(first embedding model that appears in run.yaml\n\nThe requested functionality is to add embedding model configuration to vector-io provider configurations, allowing users to:\n\n1. Specify a default embedding model in the vector-io provider config\n2. Override the default embedding model at vector store creation time if needed\n\n### 💡 Why is this needed? What if we don't build it?\n\ne.g.\n\n```\nvector_io:\n  - provider_id: faiss\n    provider_type: inline::faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"  # NEW: configurable embedding model\n      embedding_dimension: 384              # NEW: configurable dimension\n```\n\n> What if we don't build it?\n\nIf not specified on vector store creation the embedding model used will be the first model available in run.yaml, which can varies on restart based on environment variables. This can cause confusion to the user and surprising results.\n\n### Other thoughts\n\nBackward compatibility should be maintained - if no embedding model is specified at provider level, fall back to current behaviour\n\n## Activity\n\n[![](https://avatars.githubusercontent.com/u/883848?s=64&v=4)derekhiggins](https://github.com/derekhiggins)\n\nadded\n\n[enhancementNew feature or request](https://github.com/meta-llama/llama-stack/issues?q=state%3Aopen%20label%3A%22enhancement%22) New feature or request\n\n[on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-18588196448)\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=80)](https://github.com/skamenan7)\n\n### skamenan7 commented on Jul 11, 2025\n\n[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=48)](https://github.com/skamenan7)\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3062724201)\n\nContributor\n\n[@leseb](https://github.com/leseb) can I work on this next\n\n[![](https://avatars.githubusercontent.com/u/912735?s=64&v=4)leseb](https://github.com/leseb)\n\nassigned\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-18589600648)\n\n[![](https://avatars.githubusercontent.com/u/912735?s=64&v=4)leseb](https://github.com/leseb)\n\nmentioned this [on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-2751913471)\n\n- [fix: Move sentence-transformers to the top #2703](https://github.com/meta-llama/llama-stack/pull/2703)\n\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=80)](https://github.com/skamenan7)\n\n### skamenan7 commented on Jul 11, 2025\n\n[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=48)](https://github.com/skamenan7)\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3063726570)\n\nContributor\n\n* * *\n\n## Quick question about embedding dimensions\n\nHey team! Working on the configurable embedding models feature and wanted to run something by you.\n\nRight now I'm making users specify both the model AND the dimension in their config:\n\n```\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"\n      embedding_dimension: 384\n```\n\nBut honestly, this feels redundant. Since each model has a fixed dimension (all-MiniLM-L6-v2 is always 384), why make users look this up and type it manually?\n\nI'm thinking we should just auto-lookup the dimension from the model registry:\n\n```\n# Much cleaner - system figures out it's 384 dimensions\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"\n```\n\nThe dimension field would only be needed for custom models that aren't in our registry.\n\nDoes this make sense? Or is there a reason users should explicitly specify dimensions even for known models?\n\nAlso - are there any embedding models that have multiple valid dimensions? I can't think of any but wanted to double check before assuming one model = one dimension.\n\nIf it makes it optional, I can see the following scenarios.\n\n```\n## RECOMMENDED: Auto-lookup dimension\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"  # System finds 384 dimensions automatically\n\n## REQUIRED: Custom model needs explicit dimension\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"my-custom-model\"\n      embedding_dimension: 768  # Required - not in registry\n\n## OPTIONAL: Explicit validation\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"\n      embedding_dimension: 384  # Validates against registry (384)\n```\n\n* * *\n\n[![derekhiggins](https://avatars.githubusercontent.com/u/883848?v=4&size=80)](https://github.com/derekhiggins)\n\n### derekhiggins commented on Jul 14, 2025\n\n[![@derekhiggins](https://avatars.githubusercontent.com/u/883848?v=4&size=48)](https://github.com/derekhiggins)\n\n[derekhiggins](https://github.com/derekhiggins)\n\n[on Jul 14, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3068658790)\n\nContributorAuthor\n\nHmm, I wonder why we have `embedding_dimension` in the [API](https://llama-stack.readthedocs.io/en/latest/references/api_reference/index.html#/paths/v1-openai-v1-vector_stores/post)\n\n`POST /v1/openai/v1/vector_stores`\n\nperhaps its redundant there ?\n\n[![mattf](https://avatars.githubusercontent.com/u/112653?v=4&size=80)](https://github.com/mattf)\n\n### mattf commented on Jul 14, 2025\n\n[![@mattf](https://avatars.githubusercontent.com/u/112653?v=4&size=48)](https://github.com/mattf)\n\n[mattf](https://github.com/mattf)\n\n[on Jul 14, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3070436989)\n\nCollaborator\n\n> Also - are there any embedding models that have multiple valid dimensions? I can't think of any but wanted to double check before assuming one model = one dimension.\n\n[https://huggingface.co/blog/matryoshka](https://huggingface.co/blog/matryoshka)\n\nit makes sense to have the embedding\\_dimension as optional. if not provided then it's looked up from the model registration, else hopefully the model supports the specified dimension.\n\n👍React with 👍1derekhiggins\n\n[![](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)skamenan7](https://github.com/skamenan7)\n\nadded a commit that references this issue [on Jul 15, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-18646557946)\n\n[Add configurable embedding models for vector IO providers](https://github.com/skamenan7/llama-stack/commit/474b50b422bf8beadc88ef37dda0c670b1b3244a)\n\n...\n\nThis issue will close once commit 474b50b is merged into the 'main' branch. [474b50b](https://github.com/skamenan7/llama-stack/commit/474b50b422bf8beadc88ef37dda0c670b1b3244a)\n\n[![](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)skamenan7](https://github.com/skamenan7)\n\nmentioned this [on Jul 15, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-2770612690)\n\n- [feat: add configurable embedding models for vector IO providers #2766](https://github.com/meta-llama/llama-stack/pull/2766)\n\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=80)](https://github.com/skamenan7)\n\n### skamenan7 commented on Jul 17, 2025\n\n[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=48)](https://github.com/skamenan7)\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 17, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3084474881) · edited by [skamenan7](https://github.com/skamenan7)\n\nEdits\n\nContributor\n\nThank you [@mattf](https://github.com/mattf) . That link was helpful.\n\n[![](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)skamenan7](https://github.com/skamenan7)\n\nadded a commit that references this issue [on Jul 17, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-18683242460)\n\n[feat(vector-io): configurable embedding models for all providers (v2)…](https://github.com/skamenan7/llama-stack/commit/d69155da2d8e723eff0114e4da6d95e1ba595061)\n\n...\n\n[d69155d](https://github.com/skamenan7/llama-stack/commit/d69155da2d8e723eff0114e4da6d95e1ba595061)\n\n[![](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)skamenan7](https://github.com/skamenan7)\n\nmentioned this [on Jul 17, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-2781186076)\n\n- [feat(vector-io): configurable embedding models for all providers #2802](https://github.com/meta-llama/llama-stack/pull/2802)\n\n\n[![ashwinb](https://avatars.githubusercontent.com/u/19390?u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4&size=80)](https://github.com/ashwinb)\n\n### ashwinb commented on Jul 17, 2025\n\n[![@ashwinb](https://avatars.githubusercontent.com/u/19390?u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4&size=48)](https://github.com/ashwinb)\n\n[ashwinb](https://github.com/ashwinb)\n\n[on Jul 17, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3085381113)\n\nContributor\n\nI am not convinced about the utility of this feature. Can someone ( [@mattf](https://github.com/mattf), [@derekhiggins](https://github.com/derekhiggins), [@skamenan7](https://github.com/skamenan7)) crisply summarize what would go wrong if this is not done? I have looked at the linked issues and PRs but I still don't get it.\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=80)](https://github.com/skamenan7)\n\n### skamenan7 commented on Jul 18, 2025\n\n[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=48)](https://github.com/skamenan7)\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 18, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3089516741)\n\nContributor\n\nHi [@ashwinb](https://github.com/ashwinb) , per my understanding,\n\nIf a user omits embedding\\_model when creating a vector store, the stack grabs “the first model in run.yaml”—that can change between deployments, so identical API calls embed with different models/dimensions.\n\nTeams must repeat the same embedding\\_model/embedding\\_dimension on every request; one missed field re-introduces the above nondeterminism.\n\nAdding a provider-level default makes the behavior deterministic, removes boiler-plate, and aligns vector\\_io with other APIs that already support provider defaults.\n\n👍React with 👍1ashwinb\n\n[![ashwinb](https://avatars.githubusercontent.com/u/19390?u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4&size=80)](https://github.com/ashwinb)\n\n### ashwinb commented on Jul 18, 2025\n\n[![@ashwinb](https://avatars.githubusercontent.com/u/19390?u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4&size=48)](https://github.com/ashwinb)\n\n[ashwinb](https://github.com/ashwinb)\n\n[on Jul 18, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3090494578)\n\nContributor\n\n> If a user omits embedding\\_model when creating a vector store, the stack grabs “the first model in run.yaml”\n\nThanks [@skamenan7](https://github.com/skamenan7) \\-\\- could you show me the place in the code where this happens?\n\n### 13 remaining items\n\nLoad more\n\nLoading\n\n[Sign up for free](https://github.com/signup?return_to=https://github.com/meta-llama/llama-stack/issues/2729)**to join this conversation on GitHub.** Already have an account? [Sign in to comment](https://github.com/login?return_to=https://github.com/meta-llama/llama-stack/issues/2729)\n\n## Metadata\n\n## Metadata\n\n### Assignees\n\n- [![@skamenan7](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)\\\\\nskamenan7](https://github.com/skamenan7)\n\n### Labels\n\n[enhancementNew feature or request](https://github.com/meta-llama/llama-stack/issues?q=state%3Aopen%20label%3A%22enhancement%22) New feature or request\n\n### Type\n\nNo type\n\n### Projects\n\nNo projects\n\n### Milestone\n\nNo milestone\n\n### Relationships\n\nNone yet\n\n### Development\n\nCode with agent mode\n\nSelect code repository\n\n- [feat: add configurable embedding models for vector IO providersmeta-llama/llama-stack](https://github.com/meta-llama/llama-stack/pull/2766)\n- [feat(vector-io): configurable embedding models for all providersmeta-llama/llama-stack](https://github.com/meta-llama/llama-stack/pull/2802)\n\n### Participants\n\n[![@ashwinb](https://avatars.githubusercontent.com/u/19390?s=64&u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4)](https://github.com/ashwinb)[![@mattf](https://avatars.githubusercontent.com/u/112653?s=64&v=4)](https://github.com/mattf)[![@derekhiggins](https://avatars.githubusercontent.com/u/883848?s=64&v=4)](https://github.com/derekhiggins)[![@franciscojavierarceo](https://avatars.githubusercontent.com/u/4163062?s=64&u=0b41d2cc98b23ad8954ab8004f40d48731c795e1&v=4)](https://github.com/franciscojavierarceo)[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)](https://github.com/skamenan7)\n\n## Issue actions\n\nMake embedding model configurable for vector-io providers · Issue #2729 · meta-llama/llama-stack"}]}]},"uuid":"14ac53fd-12a3-4d78-9065-22a7a174625b","timestamp":"2025-07-25T14:59:19.954Z","toolUseResult":[{"type":"text","text":"[Skip to content](https://github.com/meta-llama/llama-stack/issues/2729#start-of-content)\n\n[meta-llama](https://github.com/meta-llama)/ **[llama-stack](https://github.com/meta-llama/llama-stack)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack) You must be signed in to change notification settings\n- [Fork\\\\\n1.1k](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack)\n- [Star\\\\\n7.9k](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack)\n\n\n# Make embedding model configurable for vector-io providers\\#2729\n\n[New issue](https://github.com/login?return_to=https://github.com/meta-llama/llama-stack/issues/2729)\n\nCopy link\n\n[New issue](https://github.com/login?return_to=https://github.com/meta-llama/llama-stack/issues/2729)\n\nCopy link\n\nOpen\n\nOpen\n\n[Make embedding model configurable for vector-io providers](https://github.com/meta-llama/llama-stack/issues/2729#top)#2729\n\nCopy link\n\nAssignees\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)](https://github.com/skamenan7)\n\nLabels\n\n[enhancementNew feature or request](https://github.com/meta-llama/llama-stack/issues?q=state%3Aopen%20label%3A%22enhancement%22) New feature or request\n\n[![@derekhiggins](https://avatars.githubusercontent.com/u/883848?v=4&size=80)](https://github.com/derekhiggins)\n\n## Description\n\n[![@derekhiggins](https://avatars.githubusercontent.com/u/883848?v=4&size=48)](https://github.com/derekhiggins)\n\n[derekhiggins](https://github.com/derekhiggins)\n\nopened [on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issue-3223215728)\n\nContributor\n\nIssue body actions\n\n### 🚀 Describe the new functionality needed\n\nSpawned off from [#2703](https://github.com/meta-llama/llama-stack/pull/2703)\n\nCurrently, vector-io providers do not have a way to configure the embedding model at the provider level.\n\nWhen creating vector stores, users must manually specify the embedding model or a default is used(first embedding model that appears in run.yaml\n\nThe requested functionality is to add embedding model configuration to vector-io provider configurations, allowing users to:\n\n1. Specify a default embedding model in the vector-io provider config\n2. Override the default embedding model at vector store creation time if needed\n\n### 💡 Why is this needed? What if we don't build it?\n\ne.g.\n\n```\nvector_io:\n  - provider_id: faiss\n    provider_type: inline::faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"  # NEW: configurable embedding model\n      embedding_dimension: 384              # NEW: configurable dimension\n```\n\n> What if we don't build it?\n\nIf not specified on vector store creation the embedding model used will be the first model available in run.yaml, which can varies on restart based on environment variables. This can cause confusion to the user and surprising results.\n\n### Other thoughts\n\nBackward compatibility should be maintained - if no embedding model is specified at provider level, fall back to current behaviour\n\n## Activity\n\n[![](https://avatars.githubusercontent.com/u/883848?s=64&v=4)derekhiggins](https://github.com/derekhiggins)\n\nadded\n\n[enhancementNew feature or request](https://github.com/meta-llama/llama-stack/issues?q=state%3Aopen%20label%3A%22enhancement%22) New feature or request\n\n[on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-18588196448)\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=80)](https://github.com/skamenan7)\n\n### skamenan7 commented on Jul 11, 2025\n\n[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=48)](https://github.com/skamenan7)\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3062724201)\n\nContributor\n\n[@leseb](https://github.com/leseb) can I work on this next\n\n[![](https://avatars.githubusercontent.com/u/912735?s=64&v=4)leseb](https://github.com/leseb)\n\nassigned\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-18589600648)\n\n[![](https://avatars.githubusercontent.com/u/912735?s=64&v=4)leseb](https://github.com/leseb)\n\nmentioned this [on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-2751913471)\n\n- [fix: Move sentence-transformers to the top #2703](https://github.com/meta-llama/llama-stack/pull/2703)\n\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=80)](https://github.com/skamenan7)\n\n### skamenan7 commented on Jul 11, 2025\n\n[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=48)](https://github.com/skamenan7)\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 11, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3063726570)\n\nContributor\n\n* * *\n\n## Quick question about embedding dimensions\n\nHey team! Working on the configurable embedding models feature and wanted to run something by you.\n\nRight now I'm making users specify both the model AND the dimension in their config:\n\n```\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"\n      embedding_dimension: 384\n```\n\nBut honestly, this feels redundant. Since each model has a fixed dimension (all-MiniLM-L6-v2 is always 384), why make users look this up and type it manually?\n\nI'm thinking we should just auto-lookup the dimension from the model registry:\n\n```\n# Much cleaner - system figures out it's 384 dimensions\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"\n```\n\nThe dimension field would only be needed for custom models that aren't in our registry.\n\nDoes this make sense? Or is there a reason users should explicitly specify dimensions even for known models?\n\nAlso - are there any embedding models that have multiple valid dimensions? I can't think of any but wanted to double check before assuming one model = one dimension.\n\nIf it makes it optional, I can see the following scenarios.\n\n```\n## RECOMMENDED: Auto-lookup dimension\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"  # System finds 384 dimensions automatically\n\n## REQUIRED: Custom model needs explicit dimension\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"my-custom-model\"\n      embedding_dimension: 768  # Required - not in registry\n\n## OPTIONAL: Explicit validation\nvector_io:\n  - provider_id: faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"\n      embedding_dimension: 384  # Validates against registry (384)\n```\n\n* * *\n\n[![derekhiggins](https://avatars.githubusercontent.com/u/883848?v=4&size=80)](https://github.com/derekhiggins)\n\n### derekhiggins commented on Jul 14, 2025\n\n[![@derekhiggins](https://avatars.githubusercontent.com/u/883848?v=4&size=48)](https://github.com/derekhiggins)\n\n[derekhiggins](https://github.com/derekhiggins)\n\n[on Jul 14, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3068658790)\n\nContributorAuthor\n\nHmm, I wonder why we have `embedding_dimension` in the [API](https://llama-stack.readthedocs.io/en/latest/references/api_reference/index.html#/paths/v1-openai-v1-vector_stores/post)\n\n`POST /v1/openai/v1/vector_stores`\n\nperhaps its redundant there ?\n\n[![mattf](https://avatars.githubusercontent.com/u/112653?v=4&size=80)](https://github.com/mattf)\n\n### mattf commented on Jul 14, 2025\n\n[![@mattf](https://avatars.githubusercontent.com/u/112653?v=4&size=48)](https://github.com/mattf)\n\n[mattf](https://github.com/mattf)\n\n[on Jul 14, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3070436989)\n\nCollaborator\n\n> Also - are there any embedding models that have multiple valid dimensions? I can't think of any but wanted to double check before assuming one model = one dimension.\n\n[https://huggingface.co/blog/matryoshka](https://huggingface.co/blog/matryoshka)\n\nit makes sense to have the embedding\\_dimension as optional. if not provided then it's looked up from the model registration, else hopefully the model supports the specified dimension.\n\n👍React with 👍1derekhiggins\n\n[![](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)skamenan7](https://github.com/skamenan7)\n\nadded a commit that references this issue [on Jul 15, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-18646557946)\n\n[Add configurable embedding models for vector IO providers](https://github.com/skamenan7/llama-stack/commit/474b50b422bf8beadc88ef37dda0c670b1b3244a)\n\n...\n\nThis issue will close once commit 474b50b is merged into the 'main' branch. [474b50b](https://github.com/skamenan7/llama-stack/commit/474b50b422bf8beadc88ef37dda0c670b1b3244a)\n\n[![](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)skamenan7](https://github.com/skamenan7)\n\nmentioned this [on Jul 15, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-2770612690)\n\n- [feat: add configurable embedding models for vector IO providers #2766](https://github.com/meta-llama/llama-stack/pull/2766)\n\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=80)](https://github.com/skamenan7)\n\n### skamenan7 commented on Jul 17, 2025\n\n[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=48)](https://github.com/skamenan7)\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 17, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3084474881) · edited by [skamenan7](https://github.com/skamenan7)\n\nEdits\n\nContributor\n\nThank you [@mattf](https://github.com/mattf) . That link was helpful.\n\n[![](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)skamenan7](https://github.com/skamenan7)\n\nadded a commit that references this issue [on Jul 17, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-18683242460)\n\n[feat(vector-io): configurable embedding models for all providers (v2)…](https://github.com/skamenan7/llama-stack/commit/d69155da2d8e723eff0114e4da6d95e1ba595061)\n\n...\n\n[d69155d](https://github.com/skamenan7/llama-stack/commit/d69155da2d8e723eff0114e4da6d95e1ba595061)\n\n[![](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)skamenan7](https://github.com/skamenan7)\n\nmentioned this [on Jul 17, 2025](https://github.com/meta-llama/llama-stack/issues/2729#event-2781186076)\n\n- [feat(vector-io): configurable embedding models for all providers #2802](https://github.com/meta-llama/llama-stack/pull/2802)\n\n\n[![ashwinb](https://avatars.githubusercontent.com/u/19390?u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4&size=80)](https://github.com/ashwinb)\n\n### ashwinb commented on Jul 17, 2025\n\n[![@ashwinb](https://avatars.githubusercontent.com/u/19390?u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4&size=48)](https://github.com/ashwinb)\n\n[ashwinb](https://github.com/ashwinb)\n\n[on Jul 17, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3085381113)\n\nContributor\n\nI am not convinced about the utility of this feature. Can someone ( [@mattf](https://github.com/mattf), [@derekhiggins](https://github.com/derekhiggins), [@skamenan7](https://github.com/skamenan7)) crisply summarize what would go wrong if this is not done? I have looked at the linked issues and PRs but I still don't get it.\n\n[![skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=80)](https://github.com/skamenan7)\n\n### skamenan7 commented on Jul 18, 2025\n\n[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4&size=48)](https://github.com/skamenan7)\n\n[skamenan7](https://github.com/skamenan7)\n\n[on Jul 18, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3089516741)\n\nContributor\n\nHi [@ashwinb](https://github.com/ashwinb) , per my understanding,\n\nIf a user omits embedding\\_model when creating a vector store, the stack grabs “the first model in run.yaml”—that can change between deployments, so identical API calls embed with different models/dimensions.\n\nTeams must repeat the same embedding\\_model/embedding\\_dimension on every request; one missed field re-introduces the above nondeterminism.\n\nAdding a provider-level default makes the behavior deterministic, removes boiler-plate, and aligns vector\\_io with other APIs that already support provider defaults.\n\n👍React with 👍1ashwinb\n\n[![ashwinb](https://avatars.githubusercontent.com/u/19390?u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4&size=80)](https://github.com/ashwinb)\n\n### ashwinb commented on Jul 18, 2025\n\n[![@ashwinb](https://avatars.githubusercontent.com/u/19390?u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4&size=48)](https://github.com/ashwinb)\n\n[ashwinb](https://github.com/ashwinb)\n\n[on Jul 18, 2025](https://github.com/meta-llama/llama-stack/issues/2729#issuecomment-3090494578)\n\nContributor\n\n> If a user omits embedding\\_model when creating a vector store, the stack grabs “the first model in run.yaml”\n\nThanks [@skamenan7](https://github.com/skamenan7) \\-\\- could you show me the place in the code where this happens?\n\n### 13 remaining items\n\nLoad more\n\nLoading\n\n[Sign up for free](https://github.com/signup?return_to=https://github.com/meta-llama/llama-stack/issues/2729)**to join this conversation on GitHub.** Already have an account? [Sign in to comment](https://github.com/login?return_to=https://github.com/meta-llama/llama-stack/issues/2729)\n\n## Metadata\n\n## Metadata\n\n### Assignees\n\n- [![@skamenan7](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)\\\\\nskamenan7](https://github.com/skamenan7)\n\n### Labels\n\n[enhancementNew feature or request](https://github.com/meta-llama/llama-stack/issues?q=state%3Aopen%20label%3A%22enhancement%22) New feature or request\n\n### Type\n\nNo type\n\n### Projects\n\nNo projects\n\n### Milestone\n\nNo milestone\n\n### Relationships\n\nNone yet\n\n### Development\n\nCode with agent mode\n\nSelect code repository\n\n- [feat: add configurable embedding models for vector IO providersmeta-llama/llama-stack](https://github.com/meta-llama/llama-stack/pull/2766)\n- [feat(vector-io): configurable embedding models for all providersmeta-llama/llama-stack](https://github.com/meta-llama/llama-stack/pull/2802)\n\n### Participants\n\n[![@ashwinb](https://avatars.githubusercontent.com/u/19390?s=64&u=c34771c6a16bc6fa9a2efe997f6918d7f4a15d4b&v=4)](https://github.com/ashwinb)[![@mattf](https://avatars.githubusercontent.com/u/112653?s=64&v=4)](https://github.com/mattf)[![@derekhiggins](https://avatars.githubusercontent.com/u/883848?s=64&v=4)](https://github.com/derekhiggins)[![@franciscojavierarceo](https://avatars.githubusercontent.com/u/4163062?s=64&u=0b41d2cc98b23ad8954ab8004f40d48731c795e1&v=4)](https://github.com/franciscojavierarceo)[![@skamenan7](https://avatars.githubusercontent.com/u/30124837?s=64&u=4ab2e8e848c8e98713c0f79e72b5991b064b0ea2&v=4)](https://github.com/skamenan7)\n\n## Issue actions\n\nMake embedding model configurable for vector-io providers · Issue #2729 · meta-llama/llama-stack"}]}
{"parentUuid":"14ac53fd-12a3-4d78-9065-22a7a174625b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/2729-global-vector-store-config-v2","sessionId":"6be16b69-7c02-459e-9e44-8c75d319582d","version":"1.0.60","gitBranch":"2729-global-vector-store-config-v2","message":{"id":"msg_01CTAB7dBnEhBTiUAN23yVsk","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Based on the GitHub issue comments, I can see several key insights that should update the implementation plan:\n\n1. **Different Approach**: The original issue suggests provider-level configuration, but @ashwinb's feedback questions the utility and asks for clarification on the problem\n\n2. **Current Status**: There are already PRs #2766 and #2802 attempting to implement provider-level defaults\n\n3. **Key Technical Insight**: @mattf mentions Matryoshka embeddings where models can have multiple valid dimensions, making `embedding_dimension` optional but important\n\n4. **Implementation Direction**: The comments suggest making embedding models configurable at the **provider level** rather than **stack level** as proposed in the current plan\n\nThe current implementation plan focuses on **global stack-level configuration**, but the GitHub issue discussion suggests **provider-level configuration** is the preferred approach. This is a significant difference that needs to be addressed.\n\n**Key changes needed in the implementation plan:**\n\n1. **Configuration Location**: Move from global `vector_store_config` to provider-level `embedding_model` config\n2. **Matryoshka Support**: Handle models with multiple valid dimensions \n3. **Provider-Specific Defaults**: Each vector-io provider can have its own embedding model default\n4. **API Precedence**: Request override → provider default → error (not global default)\n\nThe plan should be updated to align with the provider-level approach discussed in the GitHub issue."}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":5677,"cache_read_input_tokens":24256,"output_tokens":317,"service_tier":"standard"}},"requestId":"req_011CRTwDGbd7fSK3yWuHLshc","type":"assistant","uuid":"42fe58b4-1d7b-4d74-b488-30841204288a","timestamp":"2025-07-25T14:59:33.058Z"}
{"parentUuid":"42fe58b4-1d7b-4d74-b488-30841204288a","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/2729-global-vector-store-config-v2","sessionId":"6be16b69-7c02-459e-9e44-8c75d319582d","version":"1.0.60","gitBranch":"2729-global-vector-store-config-v2","type":"user","message":{"role":"user","content":"do i need  new file:   test_vector_config_run.yaml"},"uuid":"cd41d081-6041-4cd1-a18c-52b9776a92b1","timestamp":"2025-07-25T21:07:33.904Z"}
{"parentUuid":"cd41d081-6041-4cd1-a18c-52b9776a92b1","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/2729-global-vector-store-config-v2","sessionId":"6be16b69-7c02-459e-9e44-8c75d319582d","version":"1.0.60","gitBranch":"2729-global-vector-store-config-v2","message":{"id":"msg_017Jh6p7SrB7WkrUV1yaJ2en","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Looking at the implementation plan and the GitHub issue discussion, you don't need a new `test_vector_config_run.yaml` file. Here's why:\n\n1. **Provider-Level vs Stack-Level**: The GitHub issue discussion indicates the preference is for **provider-level configuration** (within each vector-io provider config), not the **stack-level global configuration** described in your implementation plan.\n\n2. **Existing Test Infrastructure**: The codebase likely already has test YAML files for vector-io providers that can be updated with the new embedding model configuration.\n\n3. **Integration with Existing Templates**: Rather than creating new files, you should update existing `run.yaml` templates in `llama_stack/templates/*/run.yaml` to include the provider-level embedding model configuration.\n\n**What you should do instead:**\n\n1. **Update existing test files** in `tests/` that already test vector-io functionality\n2. **Modify existing `run.yaml` templates** to include provider-level embedding model config\n3. **Create unit tests** for the new configuration model, but use the existing test infrastructure\n\n**Example of provider-level config** (based on GitHub issue):\n```yaml\nvector_io:\n  - provider_id: faiss\n    provider_type: inline::faiss\n    config:\n      embedding_model: \"all-MiniLM-L6-v2\"  # Provider-level default\n      embedding_dimension: 384              # Optional\n      kvstore:\n        type: sqlite\n        db_path: ~/.llama/distributions/starter/faiss_store.db\n```\n\nThe implementation plan needs updating to align with the provider-level approach discussed in the GitHub issue rather than the global stack-level approach currently documented."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":29988,"cache_read_input_tokens":0,"output_tokens":390,"service_tier":"standard"}},"requestId":"req_011CRURHuqc4q4M71evkWWfW","type":"assistant","uuid":"8a0566c9-a380-4342-9e29-21d4f3ad6394","timestamp":"2025-07-25T21:07:44.915Z"}
