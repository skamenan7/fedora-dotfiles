{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/pr/telemetry-1","sessionId":"1f63084b-3202-4338-a686-021d7c6bd211","version":"1.0.61","gitBranch":"pr/telemetry-1","type":"user","message":{"role":"user","content":"Caveat: The messages below were generated by the user while running local commands. DO NOT respond to these messages or otherwise consider them in your response unless the user explicitly asks you to."},"isMeta":true,"uuid":"92de5926-3ad6-451d-b1d2-9dbfb4a68a3c","timestamp":"2025-07-28T20:40:45.351Z"}
{"parentUuid":"92de5926-3ad6-451d-b1d2-9dbfb4a68a3c","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/pr/telemetry-1","sessionId":"1f63084b-3202-4338-a686-021d7c6bd211","version":"1.0.61","gitBranch":"pr/telemetry-1","type":"user","message":{"role":"user","content":"<command-name>/clear</command-name>\n          <command-message>clear</command-message>\n          <command-args></command-args>"},"uuid":"d0fa15f3-98de-41ff-8b89-5e5cd61e021a","timestamp":"2025-07-29T15:03:11.568Z"}
{"parentUuid":"d0fa15f3-98de-41ff-8b89-5e5cd61e021a","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/pr/telemetry-1","sessionId":"1f63084b-3202-4338-a686-021d7c6bd211","version":"1.0.61","gitBranch":"pr/telemetry-1","type":"user","message":{"role":"user","content":"<local-command-stdout></local-command-stdout>"},"uuid":"2e559ea8-e05d-46cb-95ba-c4055e7d6814","timestamp":"2025-07-29T15:03:11.571Z"}
{"parentUuid":"2e559ea8-e05d-46cb-95ba-c4055e7d6814","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/pr/telemetry-1","sessionId":"1f63084b-3202-4338-a686-021d7c6bd211","version":"1.0.61","gitBranch":"pr/telemetry-1","type":"user","message":{"role":"user","content":"explain me this change - https://github.com/meta-llama/llama-stack/pull/2637/files"},"uuid":"bb7e128a-9d97-4281-9800-b998fc0573d6","timestamp":"2025-07-29T15:03:14.247Z"}
{"parentUuid":"bb7e128a-9d97-4281-9800-b998fc0573d6","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/pr/telemetry-1","sessionId":"1f63084b-3202-4338-a686-021d7c6bd211","version":"1.0.61","gitBranch":"pr/telemetry-1","message":{"id":"msg_vrtx_01RwXkJjZxaLVtMw478fQHQq","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I'll analyze the changes in that GitHub PR for you."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":1968,"cache_read_input_tokens":18991,"output_tokens":5}},"requestId":"req_vrtx_011CRbWkFBGzomqNHoSKMEtK","type":"assistant","uuid":"312a3e3a-8f55-405b-b1ed-99b4a094f41b","timestamp":"2025-07-29T15:03:18.188Z"}
{"parentUuid":"312a3e3a-8f55-405b-b1ed-99b4a094f41b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/pr/telemetry-1","sessionId":"1f63084b-3202-4338-a686-021d7c6bd211","version":"1.0.61","gitBranch":"pr/telemetry-1","message":{"id":"msg_vrtx_01RwXkJjZxaLVtMw478fQHQq","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_vrtx_01Nj8mpFTzxxJRdVQTNVUx3k","name":"mcp__firecrawl__firecrawl_scrape","input":{"url":"https://github.com/meta-llama/llama-stack/pull/2637/files","formats":["markdown"]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":1968,"cache_read_input_tokens":18991,"output_tokens":5}},"requestId":"req_vrtx_011CRbWkFBGzomqNHoSKMEtK","type":"assistant","uuid":"8136d18e-b717-4043-a8be-be7577bfaa9f","timestamp":"2025-07-29T15:03:19.015Z"}
{"parentUuid":"8136d18e-b717-4043-a8be-be7577bfaa9f","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/pr/telemetry-1","sessionId":"1f63084b-3202-4338-a686-021d7c6bd211","version":"1.0.61","gitBranch":"pr/telemetry-1","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_vrtx_01Nj8mpFTzxxJRdVQTNVUx3k","type":"tool_result","content":[{"type":"text","text":"[Skip to content](https://github.com/meta-llama/llama-stack/pull/2637/files#start-of-content)\n\n[meta-llama](https://github.com/meta-llama)/ **[llama-stack](https://github.com/meta-llama/llama-stack)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack) You must be signed in to change notification settings\n- [Fork\\\\\n1.1k](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack)\n- [Star\\\\\n7.9k](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack)\n\n\n# refactor: install external providers from module\\#2637\n\n\nNew issue\n\n\n**Have a question about this project?** Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n\n\n[Sign up for GitHub](https://github.com/signup?return_to=%2Fmeta-llama%2Fllama-stack%2Fissues%2Fnew%2Fchoose)\n\nBy clicking “Sign up for GitHub”, you agree to our [terms of service](https://docs.github.com/terms) and\n[privacy statement](https://docs.github.com/privacy). We’ll occasionally send you account related emails.\n\nAlready on GitHub?\n[Sign in](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack%2Fissues%2Fnew%2Fchoose)\nto your account\n\n\nMerged\n\n[leseb](https://github.com/leseb)\nmerged 6 commits into\n\n[meta-llama:main](https://github.com/meta-llama/llama-stack/tree/main \"meta-llama/llama-stack:main\")\n\nfrom\n\n[cdoern:external-providers-build](https://github.com/cdoern/llama-stack/tree/external-providers-build \"cdoern/llama-stack:external-providers-build\")\n\nJul 25, 2025\n\n+1,683\n\n−591\n\n\nMerged\n\n# [refactor: install external providers from module](https://github.com/meta-llama/llama-stack/pull/2637/files\\#top)\\#2637\n\nShow file tree\n\nHide file tree\n\nChanges from **all commits**\n\nCommits\n\n[Show all changes\\\\\n\\\\\n\\\\\n6 commits](https://github.com/meta-llama/llama-stack/pull/2637/files)\n\nSelect commit\nHold shift + click to select a range\n\n[`9b5e919`\\\\\n\\\\\nrefactor: Align build and run provider datatypes\\\\\n\\\\\n\\\\\ncdoern Jul 6, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/9b5e919bf3867ecabc68e1fe1a65132b5b174245) [`ff7e734`\\\\\n\\\\\nrefactor: install external provider via module\\\\\n\\\\\n\\\\\ncdoern Jul 7, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/ff7e73455d5aa2dcdb1f026c032e6efd862df3c1) [`35dc2c1`\\\\\n\\\\\nfeat: re-work distro-codegen\\\\\n\\\\\n\\\\\ncdoern Jul 7, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/35dc2c1e6d4134e247b93cec72606759949a25c0) [`edcaf7e`\\\\\n\\\\\ntest: add integration test for module\\\\\n\\\\\n\\\\\ncdoern Jul 23, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/edcaf7e8032271b0a3cfcf75270d8ff046ab7326) [`d533827`\\\\\n\\\\\ndocs: update external provider docs\\\\\n\\\\\n\\\\\ncdoern Jul 24, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/d533827287037d22eb37a26a0683c6cc91deb1e8) [`4045913`\\\\\n\\\\\ntest: add more coverage for get\\_provider\\_registry\\\\\n\\\\\n\\\\\ncdoern Jul 24, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/4045913366e07cef76b6317cab53eb688cb7dfa6)\n\n**File filter**\n\nFilter by extension\n.md\n(1)\n.py\n(17)\n.sh\n(3)\n.yaml\n(22)\n.yml\n(1)All 5 file types selected\n\n* * *\n\nOnly manifest files\n\n\n* * *\n\nViewed files\n\n\n**Conversations**\n\nFailed to load comments.  Retry\n\nLoading\n\n**Jump to**\n\nJump to file\n\n\nFailed to load files.  Retry\n\nLoading\n\n##### Diff view\n\n![Unified Diff View](https://github.githubassets.com/assets/unified-6de447b07fd7.svg)\n\nUnified\n\n\n![Split Diff View](https://github.githubassets.com/assets/split-b930d4a1df45.svg)\n\nSplit\n\n\nHide whitespace\n\nApply and reload\n\n##### Diff view\n\n![Unified Diff View](https://github.githubassets.com/assets/unified-6de447b07fd7.svg)\n\nUnified\n\n\n![Split Diff View](https://github.githubassets.com/assets/split-b930d4a1df45.svg)\n\nSplit\n\n\nHide whitespace\n\nApply and reload\n\n72 changes: 72 additions & 0 deletions\n72 [.github/workflows/test-external-provider-module.yml](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-d1e46c863387afec69b5dabdbe2cf530fc1508630ec3048afb53938545f5b225 \".github/workflows/test-external-provider-module.yml\")\n\nShow comments\n\n\n[View file](https://github.com/cdoern/llama-stack/blob/4045913366e07cef76b6317cab53eb688cb7dfa6/.github/workflows/test-external-provider-module.yml)\nEdit file\n\nDelete file\n\n\n[Open in desktop](https://desktop.github.com/)\n\n| Original file line number | Diff line number | Diff line change |\n| --- | --- | --- |\n|  |  | @@ -0,0 +1,72 @@ |\n|  |  | name: Test External Providers Installed via Module |\n|  |  |  |\n|  |  | on: |\n|  |  | push: |\n|  |  | branches: \\[ main \\] |\n|  |  | pull\\_request: |\n|  |  | branches: \\[ main \\] |\n|  |  | paths: |\n|  |  | \\- 'llama\\_stack/\\*\\*' |\n|  |  | \\- 'tests/integration/\\*\\*' |\n|  |  | \\- 'uv.lock' |\n|  |  | \\- 'pyproject.toml' |\n|  |  | \\- 'requirements.txt' |\n|  |  | \\- '.github/workflows/test-external-providers-module.yml'# This workflow |\n|  |  |  |\n|  |  | jobs: |\n|  |  | test-external-providers-from-module: |\n|  |  | runs-on: ubuntu-latest |\n|  |  | strategy: |\n|  |  | matrix: |\n|  |  | image-type: \\[venv\\] |\n|  |  | # We don't do container yet, it's tricky to install a package from the host into the |\n|  |  | # container and point 'uv pip install' to the correct path... |\n|  |  | steps: |\n|  |  | \\- name: Checkout repository |\n|  |  | uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2 |\n|  |  |  |\n|  |  | \\- name: Install dependencies |\n|  |  | uses: ./.github/actions/setup-runner |\n|  |  |  |\n|  |  | \\- name: Install Ramalama |\n|  |  | shell: bash |\n|  |  | run: \\| |\n|  |  | uv pip install ramalama |\n|  |  |  |\n|  |  | \\- name: Run Ramalama |\n|  |  | shell: bash |\n|  |  | run: \\| |\n|  |  | nohup ramalama serve llama3.2:3b-instruct-fp16 > ramalama\\_server.log 2>&1 & |\n|  |  | \\- name: Apply image type to config file |\n|  |  | run: \\| |\n|  |  | yq -i '.image\\_type = \"${{ matrix.image-type }}\"' tests/external/ramalama-stack/run.yaml |\n|  |  | cat tests/external/ramalama-stack/run.yaml |\n|  |  |  |\n|  |  | \\- name: Build distro from config file |\n|  |  | run: \\| |\n|  |  | USE\\_COPY\\_NOT\\_MOUNT=true LLAMA\\_STACK\\_DIR=. llama stack build --config tests/external/ramalama-stack/build.yaml |\n|  |  |  |\n|  |  | \\- name: Start Llama Stack server in background |\n|  |  | if: ${{ matrix.image-type }} == 'venv' |\n|  |  | env: |\n|  |  | INFERENCE\\_MODEL: \"llama3.2:3b-instruct-fp16\" |\n|  |  | run: \\| |\n|  |  | # Use the virtual environment created by the build step (name comes from build config) |\n|  |  | source ramalama-stack-test/bin/activate |\n|  |  | uv pip list |\n|  |  | nohup llama stack run tests/external/ramalama-stack/run.yaml --image-type ${{ matrix.image-type }} > server.log 2>&1 & |\n|  |  |  |\n|  |  | \\- name: Wait for Llama Stack server to be ready |\n|  |  | run: \\| |\n|  |  | for i in {1..30}; do |\n|  |  | if ! grep -q \"successfully connected to Ramalama\" server.log; then |\n|  |  | echo \"Waiting for Llama Stack server to load the provider...\" |\n|  |  | sleep 1 |\n|  |  | else |\n|  |  | echo \"Provider loaded\" |\n|  |  | exit 0 |\n|  |  | fi |\n|  |  | done |\n|  |  | echo \"Provider failed to load\" |\n|  |  | cat server.log |\n|  |  | exit 1 |\n\n71 changes: 68 additions & 3 deletions\n71 [docs/source/providers/external.md](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd \"docs/source/providers/external.md\")\n\nShow comments\n\n\n[View file](https://github.com/cdoern/llama-stack/blob/4045913366e07cef76b6317cab53eb688cb7dfa6/docs/source/providers/external.md)\nEdit file\n\nDelete file\n\n\n[Open in desktop](https://desktop.github.com/)\n\n| Original file line number | Diff line number | Diff line change |\n| --- | --- | --- |\n| [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Up | @@ -7,7 +7,17 @@ Llama Stack supports external providers that live outside of the main codebase. |\n|  |  |  |\n|  |  | \\## Configuration |\n|  |  |  |\n|  |  | To enable external providers, you need to configure the \\`external\\_providers\\_dir\\` in your Llama Stack configuration. This directory should contain your external provider specifications: |\n|  |  | To enable external providers, you need to add \\`module\\` into your build yaml, allowing Llama Stack to install the required package corresponding to the external provider. |\n|  |  |  |\n|  |  | an example entry in your build.yaml should look like: |\n|  |  |  |\n|  |  | \\`\\`\\` |\n|  |  | \\- provider\\_id: ramalama |\n|  |  | provider\\_type: remote::ramalama |\n|  |  | module: ramalama\\_stack |\n|  |  | \\`\\`\\` |\n|  |  |  |\n|  |  | Additionally you can configure the \\`external\\_providers\\_dir\\` in your Llama Stack configuration. This method is in the process of being deprecated in favor of the \\`module\\` method. If using this method, the external provider directory should contain your external provider specifications: |\n|  |  |  |\n|  |  | \\`\\`\\`yaml |\n|  |  | external\\_providers\\_dir: ~/.llama/providers.d/ |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Up | @@ -112,6 +122,31 @@ container\\_image: custom-vector-store:latest # optional |\n|  |  |  |\n|  |  | #\\# Required Implementation |\n|  |  |  |\n|  |  | #\\# All Providers |\n|  |  |  |\n|  |  | All providers must contain a \\`get\\_provider\\_spec\\` function in their \\`provider\\` module. This is a standardized structure that Llama Stack expects and is necessary for getting things such as the config class. The \\`get\\_provider\\_spec\\` method returns a structure identical to the \\`adapter\\`. An example function may look like: |\n|  |  |  |\n|  |  | \\`\\`\\`python |\n|  |  | from llama\\_stack.providers.datatypes import ( |\n|  |  | ProviderSpec, |\n|  |  | Api, |\n|  |  | AdapterSpec, |\n|  |  | remote\\_provider\\_spec, |\n|  |  | ) |\n|  |  |  |\n|  |  |  |\n|  |  | def get\\_provider\\_spec() -> ProviderSpec: |\n|  |  | return remote\\_provider\\_spec( |\n|  |  | api=Api.inference, |\n|  |  | adapter=AdapterSpec( |\n|  |  | adapter\\_type=\"ramalama\", |\n|  |  | pip\\_packages=\\[\"ramalama>=0.8.5\", \"pymilvus\"\\], |\n|  |  | config\\_class=\"ramalama\\_stack.config.RamalamaImplConfig\", |\n|  |  | module=\"ramalama\\_stack\", |\n|  |  | ), |\n|  |  | ) |\n|  |  | \\`\\`\\` |\n|  |  |  |\n|  |  | #\\## Remote Providers |\n|  |  |  |\n|  |  | Remote providers must expose a \\`get\\_adapter\\_impl()\\` function in their module that takes two arguments: |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Up | @@ -155,7 +190,7 @@ Version: 0.1.0 |\n|  |  | Location: /path/to/venv/lib/python3.10/site-packages |\n|  |  | \\`\\`\\` |\n|  |  |  |\n|  |  | #\\# Example: Custom Ollama Provider |\n|  |  | #\\# Example using \\`external\\_providers\\_dir\\`: Custom Ollama Provider |\n|  |  |  |\n|  |  | Here's a complete example of creating and using a custom Ollama provider: |\n|  |  |  |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Up | @@ -206,6 +241,35 @@ external\\_providers\\_dir: ~/.llama/providers.d/ |\n|  |  |  |\n|  |  | The provider will now be available in Llama Stack with the type \\`remote::custom\\_ollama\\`. |\n|  |  |  |\n|  |  |  |\n|  |  | #\\# Example using \\`module\\`: ramalama-stack |\n|  |  |  |\n|  |  | \\[ramalama-stack\\](https://github.com/containers/ramalama-stack) is a recognized external provider that supports installation via module. |\n|  |  |  |\n|  |  | To install Llama Stack with this external provider a user can provider the following build.yaml: |\n|  |  |  |\n|  |  | \\`\\`\\`yaml |\n|  |  | version: 2 |\n|  |  | distribution\\_spec: |\n|  |  | description: Use (an external) Ramalama server for running LLM inference |\n|  |  | container\\_image: null |\n|  |  | providers: |\n|  |  | inference: |\n|  |  | \\- provider\\_id: ramalama |\n|  |  | provider\\_type: remote::ramalama |\n|  |  | module: ramalama\\_stack==0.3.0a0 |\n|  |  | image\\_type: venv |\n|  |  | image\\_name: null |\n|  |  | external\\_providers\\_dir: null |\n|  |  | additional\\_pip\\_packages: |\n|  |  | \\- aiosqlite |\n|  |  | \\- sqlalchemy\\[asyncio\\] |\n|  |  | \\`\\`\\` |\n|  |  |  |\n|  |  | No other steps are required other than \\`llama stack build\\` and \\`llama stack run\\`. The build process will use \\`module\\` to install all of the provider dependencies, retrieve the spec, etc. |\n|  |  |  |\n|  |  | The provider will now be available in Llama Stack with the type \\`remote::ramalama\\`. |\n|  |  |  |\n|  |  | #\\# Best Practices |\n|  |  |  |\n|  |  | 1\\. \\*\\*Package Naming\\*\\*: Use the prefix \\`llama-stack-provider-\\` for your provider packages to make them easily identifiable. |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand All | @@ -229,9 +293,10 @@ information. Execute the test for the Provider type you are developing. |\n|  |  |  |\n|  |  | If your external provider isn't being loaded: |\n|  |  |  |\n|  |  | 1\\. Check that \\`module\\` points to a published pip package with a top level \\`provider\\` module including \\`get\\_provider\\_spec\\`. |\n|  |  | 1\\. Check that the \\`external\\_providers\\_dir\\` path is correct and accessible. |\n|  |  | 2\\. Verify that the YAML files are properly formatted. |\n|  |  | 3\\. Ensure all required Python packages are installed. |\n|  |  | 4\\. Check the Llama Stack server logs for any error messages - turn on debug logging to get more |\n|  |  | information using \\`LLAMA\\_STACK\\_LOGGING=all=debug\\`. |\n|  |  | 5\\. Verify that the provider package is installed in your Python environment. |\n|  |  | 5\\. Verify that the provider package is installed in your Python environment if using \\`external\\_providers\\_dir\\`. |\n\n53 changes: 26 additions & 27 deletions\n53 [llama\\_stack/cli/stack/\\_build.py](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a \"llama_stack/cli/stack/_build.py\")\n\nShow comments\n\n\n[View file](https://github.com/cdoern/llama-stack/blob/4045913366e07cef76b6317cab53eb688cb7dfa6/llama_stack/cli/stack/_build.py)\nEdit file\n\nDelete file\n\n\n[Open in desktop](https://desktop.github.com/)\n\n| Original file line number | Diff line number | Diff line change |\n| --- | --- | --- |\n| [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -94,7 +94,7 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  | ) |\n|  |  | sys.exit(1) |\n|  |  | elifargs.providers: |\n|  |  | providers\\_list: dict\\[str, str\\|list\\[str\\]\\] =dict() |\n|  |  | provider\\_list: dict\\[str, list\\[Provider\\]\\] =dict() |\n|  |  | forapi\\_providerinargs.providers.split(\",\"): |\n|  |  | if\"=\"notinapi\\_provider: |\n|  |  | cprint( |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand All | @@ -103,7 +103,7 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  | file=sys.stderr, |\n|  |  | ) |\n|  |  | sys.exit(1) |\n|  |  | api, provider=api\\_provider.split(\"=\") |\n|  |  | api, provider\\_type=api\\_provider.split(\"=\") |\n|  |  | providers\\_for\\_api=get\\_provider\\_registry().get(Api(api), None) |\n|  |  | ifproviders\\_for\\_apiisNone: |\n|  |  | cprint( |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand All | @@ -112,16 +112,14 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  | file=sys.stderr, |\n|  |  | ) |\n|  |  | sys.exit(1) |\n|  |  | ifproviderinproviders\\_for\\_api: |\n|  |  | ifapinotinproviders\\_list: |\n|  |  | providers\\_list\\[api\\] = \\[\\] |\n|  |  | \\# Use type guarding to ensure we have a list |\n|  |  | provider\\_value=providers\\_list\\[api\\] |\n|  |  | ifisinstance(provider\\_value, list): |\n|  |  | provider\\_value.append(provider) |\n|  |  | else: |\n|  |  | \\# Convert string to list and append |\n|  |  | providers\\_list\\[api\\] = \\[provider\\_value, provider\\] |\n|  |  | ifprovider\\_typeinproviders\\_for\\_api: |\n|  |  | provider=Provider( |\n|  |  | provider\\_type=provider\\_type, |\n|  |  | provider\\_id=provider\\_type.split(\"::\")\\[1\\], |\n|  |  | config={}, |\n|  |  | module=None, |\n|  |  | ) |\n|  |  | provider\\_list.setdefault(api, \\[\\]).append(provider) |\n|  |  | else: |\n|  |  | cprint( |\n|  |  | f\"{provider} is not a valid provider for the {api} API.\", |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand All | @@ -130,7 +128,7 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  | ) |\n|  |  | sys.exit(1) |\n|  |  | distribution\\_spec=DistributionSpec( |\n|  |  | providers=providers\\_list, |\n|  |  | providers=provider\\_list, |\n|  |  | description=\",\".join(args.providers), |\n|  |  | ) |\n|  |  | ifnotargs.image\\_type: |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -191,7 +189,7 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  |  |\n|  |  | cprint(\"Tip: use <TAB> to see options for the providers.\\\\n\", color=\"green\", file=sys.stderr) |\n|  |  |  |\n|  |  | providers: dict\\[str, str\\|list\\[str\\]\\] =dict() |\n|  |  | providers: dict\\[str, list\\[Provider\\]\\] =dict() |\n|  |  | forapi, providers\\_for\\_apiinget\\_provider\\_registry().items(): |\n|  |  | available\\_providers= \\[xforxinproviders\\_for\\_api.keys() ifxnotin (\"remote\", \"remote::sample\")\\] |\n|  |  | ifnotavailable\\_providers: |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -237,11 +235,13 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  |  |\n|  |  | ifargs.print\\_deps\\_only: |\n|  |  | print(f\"# Dependencies for {args.templateorargs.configorimage\\_name}\") |\n|  |  | normal\\_deps, special\\_deps=get\\_provider\\_dependencies(build\\_config) |\n|  |  | normal\\_deps, special\\_deps, external\\_provider\\_dependencies=get\\_provider\\_dependencies(build\\_config) |\n|  |  | normal\\_deps+=SERVER\\_DEPENDENCIES |\n|  |  | print(f\"uv pip install {' '.join(normal\\_deps)}\") |\n|  |  | forspecial\\_depinspecial\\_deps: |\n|  |  | print(f\"uv pip install {special\\_dep}\") |\n|  |  | forexternal\\_depinexternal\\_provider\\_dependencies: |\n|  |  | print(f\"uv pip install {external\\_dep}\") |\n|  |  | return |\n|  |  |  |\n|  |  | try: |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -304,27 +304,25 @@ def \\_generate\\_run\\_config( |\n|  |  | provider\\_registry=get\\_provider\\_registry(build\\_config) |\n|  |  | forapiinapis: |\n|  |  | run\\_config.providers\\[api\\] = \\[\\] |\n|  |  | provider\\_types=build\\_config.distribution\\_spec.providers\\[api\\] |\n|  |  | ifisinstance(provider\\_types, str): |\n|  |  | provider\\_types= \\[provider\\_types\\] |\n|  |  | providers=build\\_config.distribution\\_spec.providers\\[api\\] |\n|  |  |  |\n|  |  | fori, provider\\_typeinenumerate(provider\\_types): |\n|  |  | pid=provider\\_type.split(\"::\")\\[-1\\] |\n|  |  | forproviderinproviders: |\n|  |  | pid=provider.provider\\_id |\n|  |  |  |\n|  |  | p=provider\\_registry\\[Api(api)\\]\\[provider\\_type\\] |\n|  |  | p=provider\\_registry\\[Api(api)\\]\\[provider.provider\\_type\\] |\n|  |  | ifp.deprecation\\_error: |\n|  |  | raiseInvalidProviderError(p.deprecation\\_error) |\n|  |  |  |\n|  |  | try: |\n|  |  | config\\_type=instantiate\\_class\\_type(provider\\_registry\\[Api(api)\\]\\[provider\\_type\\].config\\_class) |\n|  |  | exceptModuleNotFoundError: |\n|  |  | config\\_type=instantiate\\_class\\_type(provider\\_registry\\[Api(api)\\]\\[provider.provider\\_type\\].config\\_class) |\n|  |  | except(ModuleNotFoundError, ValueError) asexc: |\n|  |  | \\# HACK ALERT: |\n|  |  | \\# This code executes after building is done, the import cannot work since the |\n|  |  | \\# package is either available in the venv or container - not available on the host. |\n|  |  | \\# TODO: use a \"is\\_external\" flag in ProviderSpec to check if the provider is |\n|  |  | \\# external |\n|  |  | cprint( |\n|  |  | f\"Failed to import provider {provider\\_type} for API {api} \\- assuming it's external, skipping\", |\n|  |  | f\"Failed to import provider {provider.provider\\_type} for API {api} \\- assuming it's external, skipping: {exc}\", |\n|  |  | color=\"yellow\", |\n|  |  | file=sys.stderr, |\n|  |  | ) |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand All | @@ -337,9 +335,10 @@ def \\_generate\\_run\\_config( |\n|  |  | config= {} |\n|  |  |  |\n|  |  | p\\_spec=Provider( |\n|  |  | provider\\_id=f\"{pid}-{i}\"iflen(provider\\_types) >1elsepid, |\n|  |  | provider\\_type=provider\\_type, |\n|  |  | provider\\_id=pid, |\n|  |  | provider\\_type=provider.provider\\_type, |\n|  |  | config=config, |\n|  |  | module=provider.module, |\n|  |  | ) |\n|  |  | run\\_config.providers\\[api\\].append(p\\_spec) |\n|  |  |  |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -402,7 +401,7 @@ def \\_run\\_stack\\_build\\_command\\_from\\_build\\_config( |\n|  |  | run\\_config\\_file=\\_generate\\_run\\_config(build\\_config, build\\_dir, image\\_name) |\n|  |  |  |\n|  |  | withopen(build\\_file\\_path, \"w\") asf: |\n|  |  | to\\_write=json.loads(build\\_config.model\\_dump\\_json()) |\n|  |  | to\\_write=json.loads(build\\_config.model\\_dump\\_json(exclude\\_none=True)) |\n|  |  | f.write(yaml.dump(to\\_write, sort\\_keys=False)) |\n|  |  |  |\n|  |  | \\# We first install the external APIs so that the build process can use them and discover the |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down |  |\n\nLoading\n\nToggle all file notesToggle all file annotations"}]}]},"uuid":"c1ddbd60-338c-4cf6-a921-033937998fa5","timestamp":"2025-07-29T15:03:23.203Z","toolUseResult":[{"type":"text","text":"[Skip to content](https://github.com/meta-llama/llama-stack/pull/2637/files#start-of-content)\n\n[meta-llama](https://github.com/meta-llama)/ **[llama-stack](https://github.com/meta-llama/llama-stack)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack) You must be signed in to change notification settings\n- [Fork\\\\\n1.1k](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack)\n- [Star\\\\\n7.9k](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack)\n\n\n# refactor: install external providers from module\\#2637\n\n\nNew issue\n\n\n**Have a question about this project?** Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n\n\n[Sign up for GitHub](https://github.com/signup?return_to=%2Fmeta-llama%2Fllama-stack%2Fissues%2Fnew%2Fchoose)\n\nBy clicking “Sign up for GitHub”, you agree to our [terms of service](https://docs.github.com/terms) and\n[privacy statement](https://docs.github.com/privacy). We’ll occasionally send you account related emails.\n\nAlready on GitHub?\n[Sign in](https://github.com/login?return_to=%2Fmeta-llama%2Fllama-stack%2Fissues%2Fnew%2Fchoose)\nto your account\n\n\nMerged\n\n[leseb](https://github.com/leseb)\nmerged 6 commits into\n\n[meta-llama:main](https://github.com/meta-llama/llama-stack/tree/main \"meta-llama/llama-stack:main\")\n\nfrom\n\n[cdoern:external-providers-build](https://github.com/cdoern/llama-stack/tree/external-providers-build \"cdoern/llama-stack:external-providers-build\")\n\nJul 25, 2025\n\n+1,683\n\n−591\n\n\nMerged\n\n# [refactor: install external providers from module](https://github.com/meta-llama/llama-stack/pull/2637/files\\#top)\\#2637\n\nShow file tree\n\nHide file tree\n\nChanges from **all commits**\n\nCommits\n\n[Show all changes\\\\\n\\\\\n\\\\\n6 commits](https://github.com/meta-llama/llama-stack/pull/2637/files)\n\nSelect commit\nHold shift + click to select a range\n\n[`9b5e919`\\\\\n\\\\\nrefactor: Align build and run provider datatypes\\\\\n\\\\\n\\\\\ncdoern Jul 6, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/9b5e919bf3867ecabc68e1fe1a65132b5b174245) [`ff7e734`\\\\\n\\\\\nrefactor: install external provider via module\\\\\n\\\\\n\\\\\ncdoern Jul 7, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/ff7e73455d5aa2dcdb1f026c032e6efd862df3c1) [`35dc2c1`\\\\\n\\\\\nfeat: re-work distro-codegen\\\\\n\\\\\n\\\\\ncdoern Jul 7, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/35dc2c1e6d4134e247b93cec72606759949a25c0) [`edcaf7e`\\\\\n\\\\\ntest: add integration test for module\\\\\n\\\\\n\\\\\ncdoern Jul 23, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/edcaf7e8032271b0a3cfcf75270d8ff046ab7326) [`d533827`\\\\\n\\\\\ndocs: update external provider docs\\\\\n\\\\\n\\\\\ncdoern Jul 24, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/d533827287037d22eb37a26a0683c6cc91deb1e8) [`4045913`\\\\\n\\\\\ntest: add more coverage for get\\_provider\\_registry\\\\\n\\\\\n\\\\\ncdoern Jul 24, 2025](https://github.com/meta-llama/llama-stack/pull/2637/commits/4045913366e07cef76b6317cab53eb688cb7dfa6)\n\n**File filter**\n\nFilter by extension\n.md\n(1)\n.py\n(17)\n.sh\n(3)\n.yaml\n(22)\n.yml\n(1)All 5 file types selected\n\n* * *\n\nOnly manifest files\n\n\n* * *\n\nViewed files\n\n\n**Conversations**\n\nFailed to load comments.  Retry\n\nLoading\n\n**Jump to**\n\nJump to file\n\n\nFailed to load files.  Retry\n\nLoading\n\n##### Diff view\n\n![Unified Diff View](https://github.githubassets.com/assets/unified-6de447b07fd7.svg)\n\nUnified\n\n\n![Split Diff View](https://github.githubassets.com/assets/split-b930d4a1df45.svg)\n\nSplit\n\n\nHide whitespace\n\nApply and reload\n\n##### Diff view\n\n![Unified Diff View](https://github.githubassets.com/assets/unified-6de447b07fd7.svg)\n\nUnified\n\n\n![Split Diff View](https://github.githubassets.com/assets/split-b930d4a1df45.svg)\n\nSplit\n\n\nHide whitespace\n\nApply and reload\n\n72 changes: 72 additions & 0 deletions\n72 [.github/workflows/test-external-provider-module.yml](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-d1e46c863387afec69b5dabdbe2cf530fc1508630ec3048afb53938545f5b225 \".github/workflows/test-external-provider-module.yml\")\n\nShow comments\n\n\n[View file](https://github.com/cdoern/llama-stack/blob/4045913366e07cef76b6317cab53eb688cb7dfa6/.github/workflows/test-external-provider-module.yml)\nEdit file\n\nDelete file\n\n\n[Open in desktop](https://desktop.github.com/)\n\n| Original file line number | Diff line number | Diff line change |\n| --- | --- | --- |\n|  |  | @@ -0,0 +1,72 @@ |\n|  |  | name: Test External Providers Installed via Module |\n|  |  |  |\n|  |  | on: |\n|  |  | push: |\n|  |  | branches: \\[ main \\] |\n|  |  | pull\\_request: |\n|  |  | branches: \\[ main \\] |\n|  |  | paths: |\n|  |  | \\- 'llama\\_stack/\\*\\*' |\n|  |  | \\- 'tests/integration/\\*\\*' |\n|  |  | \\- 'uv.lock' |\n|  |  | \\- 'pyproject.toml' |\n|  |  | \\- 'requirements.txt' |\n|  |  | \\- '.github/workflows/test-external-providers-module.yml'# This workflow |\n|  |  |  |\n|  |  | jobs: |\n|  |  | test-external-providers-from-module: |\n|  |  | runs-on: ubuntu-latest |\n|  |  | strategy: |\n|  |  | matrix: |\n|  |  | image-type: \\[venv\\] |\n|  |  | # We don't do container yet, it's tricky to install a package from the host into the |\n|  |  | # container and point 'uv pip install' to the correct path... |\n|  |  | steps: |\n|  |  | \\- name: Checkout repository |\n|  |  | uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2 |\n|  |  |  |\n|  |  | \\- name: Install dependencies |\n|  |  | uses: ./.github/actions/setup-runner |\n|  |  |  |\n|  |  | \\- name: Install Ramalama |\n|  |  | shell: bash |\n|  |  | run: \\| |\n|  |  | uv pip install ramalama |\n|  |  |  |\n|  |  | \\- name: Run Ramalama |\n|  |  | shell: bash |\n|  |  | run: \\| |\n|  |  | nohup ramalama serve llama3.2:3b-instruct-fp16 > ramalama\\_server.log 2>&1 & |\n|  |  | \\- name: Apply image type to config file |\n|  |  | run: \\| |\n|  |  | yq -i '.image\\_type = \"${{ matrix.image-type }}\"' tests/external/ramalama-stack/run.yaml |\n|  |  | cat tests/external/ramalama-stack/run.yaml |\n|  |  |  |\n|  |  | \\- name: Build distro from config file |\n|  |  | run: \\| |\n|  |  | USE\\_COPY\\_NOT\\_MOUNT=true LLAMA\\_STACK\\_DIR=. llama stack build --config tests/external/ramalama-stack/build.yaml |\n|  |  |  |\n|  |  | \\- name: Start Llama Stack server in background |\n|  |  | if: ${{ matrix.image-type }} == 'venv' |\n|  |  | env: |\n|  |  | INFERENCE\\_MODEL: \"llama3.2:3b-instruct-fp16\" |\n|  |  | run: \\| |\n|  |  | # Use the virtual environment created by the build step (name comes from build config) |\n|  |  | source ramalama-stack-test/bin/activate |\n|  |  | uv pip list |\n|  |  | nohup llama stack run tests/external/ramalama-stack/run.yaml --image-type ${{ matrix.image-type }} > server.log 2>&1 & |\n|  |  |  |\n|  |  | \\- name: Wait for Llama Stack server to be ready |\n|  |  | run: \\| |\n|  |  | for i in {1..30}; do |\n|  |  | if ! grep -q \"successfully connected to Ramalama\" server.log; then |\n|  |  | echo \"Waiting for Llama Stack server to load the provider...\" |\n|  |  | sleep 1 |\n|  |  | else |\n|  |  | echo \"Provider loaded\" |\n|  |  | exit 0 |\n|  |  | fi |\n|  |  | done |\n|  |  | echo \"Provider failed to load\" |\n|  |  | cat server.log |\n|  |  | exit 1 |\n\n71 changes: 68 additions & 3 deletions\n71 [docs/source/providers/external.md](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd \"docs/source/providers/external.md\")\n\nShow comments\n\n\n[View file](https://github.com/cdoern/llama-stack/blob/4045913366e07cef76b6317cab53eb688cb7dfa6/docs/source/providers/external.md)\nEdit file\n\nDelete file\n\n\n[Open in desktop](https://desktop.github.com/)\n\n| Original file line number | Diff line number | Diff line change |\n| --- | --- | --- |\n| [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Up | @@ -7,7 +7,17 @@ Llama Stack supports external providers that live outside of the main codebase. |\n|  |  |  |\n|  |  | \\## Configuration |\n|  |  |  |\n|  |  | To enable external providers, you need to configure the \\`external\\_providers\\_dir\\` in your Llama Stack configuration. This directory should contain your external provider specifications: |\n|  |  | To enable external providers, you need to add \\`module\\` into your build yaml, allowing Llama Stack to install the required package corresponding to the external provider. |\n|  |  |  |\n|  |  | an example entry in your build.yaml should look like: |\n|  |  |  |\n|  |  | \\`\\`\\` |\n|  |  | \\- provider\\_id: ramalama |\n|  |  | provider\\_type: remote::ramalama |\n|  |  | module: ramalama\\_stack |\n|  |  | \\`\\`\\` |\n|  |  |  |\n|  |  | Additionally you can configure the \\`external\\_providers\\_dir\\` in your Llama Stack configuration. This method is in the process of being deprecated in favor of the \\`module\\` method. If using this method, the external provider directory should contain your external provider specifications: |\n|  |  |  |\n|  |  | \\`\\`\\`yaml |\n|  |  | external\\_providers\\_dir: ~/.llama/providers.d/ |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Up | @@ -112,6 +122,31 @@ container\\_image: custom-vector-store:latest # optional |\n|  |  |  |\n|  |  | #\\# Required Implementation |\n|  |  |  |\n|  |  | #\\# All Providers |\n|  |  |  |\n|  |  | All providers must contain a \\`get\\_provider\\_spec\\` function in their \\`provider\\` module. This is a standardized structure that Llama Stack expects and is necessary for getting things such as the config class. The \\`get\\_provider\\_spec\\` method returns a structure identical to the \\`adapter\\`. An example function may look like: |\n|  |  |  |\n|  |  | \\`\\`\\`python |\n|  |  | from llama\\_stack.providers.datatypes import ( |\n|  |  | ProviderSpec, |\n|  |  | Api, |\n|  |  | AdapterSpec, |\n|  |  | remote\\_provider\\_spec, |\n|  |  | ) |\n|  |  |  |\n|  |  |  |\n|  |  | def get\\_provider\\_spec() -> ProviderSpec: |\n|  |  | return remote\\_provider\\_spec( |\n|  |  | api=Api.inference, |\n|  |  | adapter=AdapterSpec( |\n|  |  | adapter\\_type=\"ramalama\", |\n|  |  | pip\\_packages=\\[\"ramalama>=0.8.5\", \"pymilvus\"\\], |\n|  |  | config\\_class=\"ramalama\\_stack.config.RamalamaImplConfig\", |\n|  |  | module=\"ramalama\\_stack\", |\n|  |  | ), |\n|  |  | ) |\n|  |  | \\`\\`\\` |\n|  |  |  |\n|  |  | #\\## Remote Providers |\n|  |  |  |\n|  |  | Remote providers must expose a \\`get\\_adapter\\_impl()\\` function in their module that takes two arguments: |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Up | @@ -155,7 +190,7 @@ Version: 0.1.0 |\n|  |  | Location: /path/to/venv/lib/python3.10/site-packages |\n|  |  | \\`\\`\\` |\n|  |  |  |\n|  |  | #\\# Example: Custom Ollama Provider |\n|  |  | #\\# Example using \\`external\\_providers\\_dir\\`: Custom Ollama Provider |\n|  |  |  |\n|  |  | Here's a complete example of creating and using a custom Ollama provider: |\n|  |  |  |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand Up | @@ -206,6 +241,35 @@ external\\_providers\\_dir: ~/.llama/providers.d/ |\n|  |  |  |\n|  |  | The provider will now be available in Llama Stack with the type \\`remote::custom\\_ollama\\`. |\n|  |  |  |\n|  |  |  |\n|  |  | #\\# Example using \\`module\\`: ramalama-stack |\n|  |  |  |\n|  |  | \\[ramalama-stack\\](https://github.com/containers/ramalama-stack) is a recognized external provider that supports installation via module. |\n|  |  |  |\n|  |  | To install Llama Stack with this external provider a user can provider the following build.yaml: |\n|  |  |  |\n|  |  | \\`\\`\\`yaml |\n|  |  | version: 2 |\n|  |  | distribution\\_spec: |\n|  |  | description: Use (an external) Ramalama server for running LLM inference |\n|  |  | container\\_image: null |\n|  |  | providers: |\n|  |  | inference: |\n|  |  | \\- provider\\_id: ramalama |\n|  |  | provider\\_type: remote::ramalama |\n|  |  | module: ramalama\\_stack==0.3.0a0 |\n|  |  | image\\_type: venv |\n|  |  | image\\_name: null |\n|  |  | external\\_providers\\_dir: null |\n|  |  | additional\\_pip\\_packages: |\n|  |  | \\- aiosqlite |\n|  |  | \\- sqlalchemy\\[asyncio\\] |\n|  |  | \\`\\`\\` |\n|  |  |  |\n|  |  | No other steps are required other than \\`llama stack build\\` and \\`llama stack run\\`. The build process will use \\`module\\` to install all of the provider dependencies, retrieve the spec, etc. |\n|  |  |  |\n|  |  | The provider will now be available in Llama Stack with the type \\`remote::ramalama\\`. |\n|  |  |  |\n|  |  | #\\# Best Practices |\n|  |  |  |\n|  |  | 1\\. \\*\\*Package Naming\\*\\*: Use the prefix \\`llama-stack-provider-\\` for your provider packages to make them easily identifiable. |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-8693bedcd85ec626a99528a059be1a82ec0f3939931095a985caafe536b323bd) Expand All | @@ -229,9 +293,10 @@ information. Execute the test for the Provider type you are developing. |\n|  |  |  |\n|  |  | If your external provider isn't being loaded: |\n|  |  |  |\n|  |  | 1\\. Check that \\`module\\` points to a published pip package with a top level \\`provider\\` module including \\`get\\_provider\\_spec\\`. |\n|  |  | 1\\. Check that the \\`external\\_providers\\_dir\\` path is correct and accessible. |\n|  |  | 2\\. Verify that the YAML files are properly formatted. |\n|  |  | 3\\. Ensure all required Python packages are installed. |\n|  |  | 4\\. Check the Llama Stack server logs for any error messages - turn on debug logging to get more |\n|  |  | information using \\`LLAMA\\_STACK\\_LOGGING=all=debug\\`. |\n|  |  | 5\\. Verify that the provider package is installed in your Python environment. |\n|  |  | 5\\. Verify that the provider package is installed in your Python environment if using \\`external\\_providers\\_dir\\`. |\n\n53 changes: 26 additions & 27 deletions\n53 [llama\\_stack/cli/stack/\\_build.py](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a \"llama_stack/cli/stack/_build.py\")\n\nShow comments\n\n\n[View file](https://github.com/cdoern/llama-stack/blob/4045913366e07cef76b6317cab53eb688cb7dfa6/llama_stack/cli/stack/_build.py)\nEdit file\n\nDelete file\n\n\n[Open in desktop](https://desktop.github.com/)\n\n| Original file line number | Diff line number | Diff line change |\n| --- | --- | --- |\n| [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -94,7 +94,7 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  | ) |\n|  |  | sys.exit(1) |\n|  |  | elifargs.providers: |\n|  |  | providers\\_list: dict\\[str, str\\|list\\[str\\]\\] =dict() |\n|  |  | provider\\_list: dict\\[str, list\\[Provider\\]\\] =dict() |\n|  |  | forapi\\_providerinargs.providers.split(\",\"): |\n|  |  | if\"=\"notinapi\\_provider: |\n|  |  | cprint( |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand All | @@ -103,7 +103,7 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  | file=sys.stderr, |\n|  |  | ) |\n|  |  | sys.exit(1) |\n|  |  | api, provider=api\\_provider.split(\"=\") |\n|  |  | api, provider\\_type=api\\_provider.split(\"=\") |\n|  |  | providers\\_for\\_api=get\\_provider\\_registry().get(Api(api), None) |\n|  |  | ifproviders\\_for\\_apiisNone: |\n|  |  | cprint( |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand All | @@ -112,16 +112,14 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  | file=sys.stderr, |\n|  |  | ) |\n|  |  | sys.exit(1) |\n|  |  | ifproviderinproviders\\_for\\_api: |\n|  |  | ifapinotinproviders\\_list: |\n|  |  | providers\\_list\\[api\\] = \\[\\] |\n|  |  | \\# Use type guarding to ensure we have a list |\n|  |  | provider\\_value=providers\\_list\\[api\\] |\n|  |  | ifisinstance(provider\\_value, list): |\n|  |  | provider\\_value.append(provider) |\n|  |  | else: |\n|  |  | \\# Convert string to list and append |\n|  |  | providers\\_list\\[api\\] = \\[provider\\_value, provider\\] |\n|  |  | ifprovider\\_typeinproviders\\_for\\_api: |\n|  |  | provider=Provider( |\n|  |  | provider\\_type=provider\\_type, |\n|  |  | provider\\_id=provider\\_type.split(\"::\")\\[1\\], |\n|  |  | config={}, |\n|  |  | module=None, |\n|  |  | ) |\n|  |  | provider\\_list.setdefault(api, \\[\\]).append(provider) |\n|  |  | else: |\n|  |  | cprint( |\n|  |  | f\"{provider} is not a valid provider for the {api} API.\", |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand All | @@ -130,7 +128,7 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  | ) |\n|  |  | sys.exit(1) |\n|  |  | distribution\\_spec=DistributionSpec( |\n|  |  | providers=providers\\_list, |\n|  |  | providers=provider\\_list, |\n|  |  | description=\",\".join(args.providers), |\n|  |  | ) |\n|  |  | ifnotargs.image\\_type: |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -191,7 +189,7 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  |  |\n|  |  | cprint(\"Tip: use <TAB> to see options for the providers.\\\\n\", color=\"green\", file=sys.stderr) |\n|  |  |  |\n|  |  | providers: dict\\[str, str\\|list\\[str\\]\\] =dict() |\n|  |  | providers: dict\\[str, list\\[Provider\\]\\] =dict() |\n|  |  | forapi, providers\\_for\\_apiinget\\_provider\\_registry().items(): |\n|  |  | available\\_providers= \\[xforxinproviders\\_for\\_api.keys() ifxnotin (\"remote\", \"remote::sample\")\\] |\n|  |  | ifnotavailable\\_providers: |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -237,11 +235,13 @@ def run\\_stack\\_build\\_command(args: argparse.Namespace) -> None: |\n|  |  |  |\n|  |  | ifargs.print\\_deps\\_only: |\n|  |  | print(f\"# Dependencies for {args.templateorargs.configorimage\\_name}\") |\n|  |  | normal\\_deps, special\\_deps=get\\_provider\\_dependencies(build\\_config) |\n|  |  | normal\\_deps, special\\_deps, external\\_provider\\_dependencies=get\\_provider\\_dependencies(build\\_config) |\n|  |  | normal\\_deps+=SERVER\\_DEPENDENCIES |\n|  |  | print(f\"uv pip install {' '.join(normal\\_deps)}\") |\n|  |  | forspecial\\_depinspecial\\_deps: |\n|  |  | print(f\"uv pip install {special\\_dep}\") |\n|  |  | forexternal\\_depinexternal\\_provider\\_dependencies: |\n|  |  | print(f\"uv pip install {external\\_dep}\") |\n|  |  | return |\n|  |  |  |\n|  |  | try: |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -304,27 +304,25 @@ def \\_generate\\_run\\_config( |\n|  |  | provider\\_registry=get\\_provider\\_registry(build\\_config) |\n|  |  | forapiinapis: |\n|  |  | run\\_config.providers\\[api\\] = \\[\\] |\n|  |  | provider\\_types=build\\_config.distribution\\_spec.providers\\[api\\] |\n|  |  | ifisinstance(provider\\_types, str): |\n|  |  | provider\\_types= \\[provider\\_types\\] |\n|  |  | providers=build\\_config.distribution\\_spec.providers\\[api\\] |\n|  |  |  |\n|  |  | fori, provider\\_typeinenumerate(provider\\_types): |\n|  |  | pid=provider\\_type.split(\"::\")\\[-1\\] |\n|  |  | forproviderinproviders: |\n|  |  | pid=provider.provider\\_id |\n|  |  |  |\n|  |  | p=provider\\_registry\\[Api(api)\\]\\[provider\\_type\\] |\n|  |  | p=provider\\_registry\\[Api(api)\\]\\[provider.provider\\_type\\] |\n|  |  | ifp.deprecation\\_error: |\n|  |  | raiseInvalidProviderError(p.deprecation\\_error) |\n|  |  |  |\n|  |  | try: |\n|  |  | config\\_type=instantiate\\_class\\_type(provider\\_registry\\[Api(api)\\]\\[provider\\_type\\].config\\_class) |\n|  |  | exceptModuleNotFoundError: |\n|  |  | config\\_type=instantiate\\_class\\_type(provider\\_registry\\[Api(api)\\]\\[provider.provider\\_type\\].config\\_class) |\n|  |  | except(ModuleNotFoundError, ValueError) asexc: |\n|  |  | \\# HACK ALERT: |\n|  |  | \\# This code executes after building is done, the import cannot work since the |\n|  |  | \\# package is either available in the venv or container - not available on the host. |\n|  |  | \\# TODO: use a \"is\\_external\" flag in ProviderSpec to check if the provider is |\n|  |  | \\# external |\n|  |  | cprint( |\n|  |  | f\"Failed to import provider {provider\\_type} for API {api} \\- assuming it's external, skipping\", |\n|  |  | f\"Failed to import provider {provider.provider\\_type} for API {api} \\- assuming it's external, skipping: {exc}\", |\n|  |  | color=\"yellow\", |\n|  |  | file=sys.stderr, |\n|  |  | ) |\n| [Expand All](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand All | @@ -337,9 +335,10 @@ def \\_generate\\_run\\_config( |\n|  |  | config= {} |\n|  |  |  |\n|  |  | p\\_spec=Provider( |\n|  |  | provider\\_id=f\"{pid}-{i}\"iflen(provider\\_types) >1elsepid, |\n|  |  | provider\\_type=provider\\_type, |\n|  |  | provider\\_id=pid, |\n|  |  | provider\\_type=provider.provider\\_type, |\n|  |  | config=config, |\n|  |  | module=provider.module, |\n|  |  | ) |\n|  |  | run\\_config.providers\\[api\\].append(p\\_spec) |\n|  |  |  |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down [Expand Up](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Up | @@ -402,7 +401,7 @@ def \\_run\\_stack\\_build\\_command\\_from\\_build\\_config( |\n|  |  | run\\_config\\_file=\\_generate\\_run\\_config(build\\_config, build\\_dir, image\\_name) |\n|  |  |  |\n|  |  | withopen(build\\_file\\_path, \"w\") asf: |\n|  |  | to\\_write=json.loads(build\\_config.model\\_dump\\_json()) |\n|  |  | to\\_write=json.loads(build\\_config.model\\_dump\\_json(exclude\\_none=True)) |\n|  |  | f.write(yaml.dump(to\\_write, sort\\_keys=False)) |\n|  |  |  |\n|  |  | \\# We first install the external APIs so that the build process can use them and discover the |\n| [Expand Down](https://github.com/meta-llama/llama-stack/pull/2637/files#diff-bf6c4a51ec5c9b64db6a0bbad73bdc943c43833c07f60afc7c6348f5bf839f3a) Expand Down |  |\n\nLoading\n\nToggle all file notesToggle all file annotations"}]}
{"parentUuid":"c1ddbd60-338c-4cf6-a921-033937998fa5","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/pr/telemetry-1","sessionId":"1f63084b-3202-4338-a686-021d7c6bd211","version":"1.0.61","gitBranch":"pr/telemetry-1","message":{"id":"msg_vrtx_01GqYWgxNVfcpANJScqDvENU","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"This PR refactors how external providers are installed in Llama Stack. The key changes are:\n\n**Main Change:** External providers can now be installed via Python modules instead of only using `external_providers_dir`.\n\n**Key Components:**\n\n1. **New Configuration Method**: Providers can specify a `module` field in build.yaml to install from PyPI packages\n2. **Provider Structure Changes**: The `Provider` datatype now includes a `module` field for external package installation  \n3. **Integration Testing**: Adds CI workflow to test external provider installation using the ramalama-stack example\n4. **Documentation Updates**: Updates external provider docs with the new module-based approach\n\n**Technical Details:**\n\n- `llama_stack/cli/stack/_build.py`: Refactored to handle `Provider` objects instead of strings, added support for installing external provider dependencies via modules\n- New requirement: All external providers must implement a `get_provider_spec()` function\n- The old `external_providers_dir` method is being deprecated in favor of the module approach\n- Build process now properly installs external provider packages during the build phase\n\n**Example Usage:**\n```yaml\n- provider_id: ramalama\n  provider_type: remote::ramalama  \n  module: ramalama_stack==0.3.0a0\n```\n\nThis simplifies external provider distribution by leveraging standard Python packaging instead of requiring manual directory setup."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":9441,"cache_read_input_tokens":20959,"output_tokens":1}},"requestId":"req_vrtx_011CRbWkuQRLG83iP9Zfp5tr","type":"assistant","uuid":"ad3679c2-f937-4cc8-95e0-b67c3a989aea","timestamp":"2025-07-29T15:03:34.371Z"}
