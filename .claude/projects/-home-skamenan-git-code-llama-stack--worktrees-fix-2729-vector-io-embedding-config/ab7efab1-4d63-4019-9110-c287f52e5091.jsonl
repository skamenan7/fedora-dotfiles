{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":"If a user omits embedding_model when creating a vector store, the stack grabs “the first model in run.yaml”\n\nThanks @skamenan7 -- could you show me the place in the code where this happens?"},"uuid":"4a912626-352b-4aea-b6a6-b49367f1fc13","timestamp":"2025-07-18T20:51:26.168Z"}
{"parentUuid":"4a912626-352b-4aea-b6a6-b49367f1fc13","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01P4u9sQPt6LyWxszQtNnqZx","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I'll help you find where the code selects \"the first model in run.yaml\" when `embedding_model` is omitted for vector store creation."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":16078,"cache_read_input_tokens":0,"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CRF9QAWAYgTNQoXM2VKgn","type":"assistant","uuid":"c0aa8ab1-98b8-4fa3-b363-258469b2b03b","timestamp":"2025-07-18T20:51:28.951Z"}
{"parentUuid":"c0aa8ab1-98b8-4fa3-b363-258469b2b03b","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01P4u9sQPt6LyWxszQtNnqZx","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01RMeEsEZ3X6pUqQPFWurrAV","name":"Task","input":{"description":"Find embedding model selection logic","prompt":"Search for code that handles embedding model selection when creating vector stores, particularly where it falls back to \"the first model in run.yaml\" when embedding_model is not specified. Look for:\n\n1. Vector store creation code\n2. Embedding model selection/fallback logic\n3. Code that reads from run.yaml or similar config files\n4. Any default model selection mechanisms\n\nSearch through vector-related files, configuration handling, and model selection logic. Return the specific file paths and line numbers where this fallback behavior is implemented."}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":16078,"cache_read_input_tokens":0,"output_tokens":215,"service_tier":"standard"}},"requestId":"req_011CRF9QAWAYgTNQoXM2VKgn","type":"assistant","uuid":"576a5922-99df-4e32-bba0-72f661cf5de7","timestamp":"2025-07-18T20:51:31.697Z"}
{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":"Search for code that handles embedding model selection when creating vector stores, particularly where it falls back to \"the first model in run.yaml\" when embedding_model is not specified. Look for:\n\n1. Vector store creation code\n2. Embedding model selection/fallback logic\n3. Code that reads from run.yaml or similar config files\n4. Any default model selection mechanisms\n\nSearch through vector-related files, configuration handling, and model selection logic. Return the specific file paths and line numbers where this fallback behavior is implemented."},"uuid":"88b3d444-6f3c-4746-affc-87dd32e56805","timestamp":"2025-07-18T20:51:31.726Z"}
{"parentUuid":"88b3d444-6f3c-4746-affc-87dd32e56805","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01WS9sDk2JerNvRWzv2geKVi","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"I'll search for the embedding model selection and fallback logic in the codebase. Let me start by exploring the vector-related files and configuration handling."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":12683,"cache_read_input_tokens":0,"output_tokens":88,"service_tier":"standard"}},"requestId":"req_011CRF9QaQLsW6CEfxztwZqj","type":"assistant","uuid":"dc91b468-5a9a-453a-a3df-eea36446e89a","timestamp":"2025-07-18T20:51:34.480Z"}
{"parentUuid":"dc91b468-5a9a-453a-a3df-eea36446e89a","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01WS9sDk2JerNvRWzv2geKVi","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01AVi6FNgnM9tRNVdt1HPNDa","name":"Glob","input":{"pattern":"**/*vector*"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":12683,"cache_read_input_tokens":0,"output_tokens":88,"service_tier":"standard"}},"requestId":"req_011CRF9QaQLsW6CEfxztwZqj","type":"assistant","uuid":"17ddfd61-e743-4122-96b5-eee756a0fd19","timestamp":"2025-07-18T20:51:34.889Z"}
{"parentUuid":"17ddfd61-e743-4122-96b5-eee756a0fd19","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AVi6FNgnM9tRNVdt1HPNDa","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/ui/page/distribution/vector_dbs.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/registry/vector_io.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/vector_store.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/ui/components/vector-stores/vector-store-detail.tsx\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/utils/memory/test_vector_store.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/registry/__pycache__/vector_io.cpython-312.pyc\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_io/vector_io.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_dbs/vector_dbs.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/openai_vector_store_mixin.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/pgvector.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/docs/examples/vector_io_embedding_configuration.md\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/docs/examples/sample_vector_io_config.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_io/__pycache__/vector_io.cpython-312.pyc\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_dbs/__pycache__/vector_dbs.cpython-312.pyc\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/docs/source/providers/vector_io/remote_pgvector.md\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_openai_vector_stores.py\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/__pycache__/vector_dbs.cpython-312.pyc\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/__pycache__/vector_store.cpython-312.pyc\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/__pycache__/vector_io.cpython-312.pyc\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/__pycache__/openai_vector_store_mixin.cpython-312.pyc"}]},"uuid":"0956335f-2bef-44e7-ae5a-b988663aae7a","timestamp":"2025-07-18T20:51:34.928Z","toolUseResult":{"filenames":["/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/ui/page/distribution/vector_dbs.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/registry/vector_io.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/vector_store.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/ui/components/vector-stores/vector-store-detail.tsx","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/utils/memory/test_vector_store.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/registry/__pycache__/vector_io.cpython-312.pyc","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_io/vector_io.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_dbs/vector_dbs.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/openai_vector_store_mixin.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/pgvector.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/docs/examples/vector_io_embedding_configuration.md","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/docs/examples/sample_vector_io_config.yaml","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_io/__pycache__/vector_io.cpython-312.pyc","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_dbs/__pycache__/vector_dbs.cpython-312.pyc","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/docs/source/providers/vector_io/remote_pgvector.md","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_openai_vector_stores.py","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/__pycache__/vector_dbs.cpython-312.pyc","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/__pycache__/vector_store.cpython-312.pyc","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/__pycache__/vector_io.cpython-312.pyc","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/__pycache__/openai_vector_store_mixin.cpython-312.pyc"],"durationMs":19,"numFiles":25,"truncated":false}}
{"parentUuid":"0956335f-2bef-44e7-ae5a-b988663aae7a","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01WGRSSbbMTdrGtXqdUAkDBK","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_013uiL771QuEhg826PTHnqeA","name":"Grep","input":{"pattern":"embedding_model|first model.*run\\.yaml|fallback.*embedding","glob":"**/*.py","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1450,"cache_read_input_tokens":12683,"output_tokens":122,"service_tier":"standard"}},"requestId":"req_011CRF9QodpjS3CypddJcUzV","type":"assistant","uuid":"cf0c700d-b072-4c2b-9a3e-8acb33040e20","timestamp":"2025-07-18T20:51:39.394Z"}
{"parentUuid":"cf0c700d-b072-4c2b-9a3e-8acb33040e20","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013uiL771QuEhg826PTHnqeA","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py:208:        mock_vector_db.embedding_model = \"test-model without embeddings\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py:233:        mock_vector_db.embedding_model = \"test-model with embeddings\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py:258:        mock_vector_db.embedding_model = \"test-model with invalid embeddings\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py:296:        mock_vector_db.embedding_model = \"test-model with partial embeddings\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/routers/test_routing_tables.py:190:    await table.register_vector_db(vector_db_id=\"test-vectordb\", embedding_model=\"test-model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/routers/test_routing_tables.py:191:    await table.register_vector_db(vector_db_id=\"test-vectordb-2\", embedding_model=\"test-model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_configurable_embeddings.py:51:        embedding_model=\"all-MiniLM-L6-v2\",  # explicit even though it is default\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_configurable_embeddings.py:71:def test_vector_db_register_rejects_non_embedding_model(client_with_models):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_configurable_embeddings.py:79:            embedding_model=\"meta-llama/Llama-3.1-8B-Instruct\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:25:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:56:    assert result_vector_db.embedding_model == sample_vector_db.embedding_model\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:80:    assert result_vector_db.embedding_model == sample_vector_db.embedding_model\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:88:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:114:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:123:        embedding_model=\"different-model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:132:    assert result.embedding_model == original_vector_db.embedding_model  # Original values preserved\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:141:            embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:162:        assert stored_vector_db.embedding_model == original_vector_db.embedding_model\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:170:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:208:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:220:        '{\"type\": \"vector_db\", \"identifier\": \"invalid_cached_db\", \"embedding_model\": 12345}',  # Should be string\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/remote/test_milvus.py:142:async def test_bm25_fallback_to_simple_search(milvus_index, sample_chunks, sample_embeddings, mock_milvus_client):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:49:def test_vector_db_retrieve(client_with_empty_registry, embedding_model_id, embedding_dimension):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:54:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:62:    assert response.embedding_model == embedding_model_id\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:66:def test_vector_db_register(client_with_empty_registry, embedding_model_id, embedding_dimension):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:70:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:93:def test_insert_chunks(client_with_empty_registry, embedding_model_id, embedding_dimension, sample_chunks, test_case):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:97:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:125:def test_insert_chunks_with_precomputed_embeddings(client_with_empty_registry, embedding_model_id, embedding_dimension):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:132:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:167:    client_with_empty_registry, embedding_model_id, embedding_dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:175:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:57:        identifier=\"foo_db\", provider_id=\"test_provider\", embedding_model=\"test_model\", embedding_dimension=128\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:67:        identifier=\"foo_db\", provider_id=\"test_provider\", embedding_model=\"test_model\", embedding_dimension=128\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:80:        identifier=unique_id, provider_id=\"test_provider\", embedding_model=\"test_model\", embedding_dimension=128\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:137:        \"embedding_model\": \"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:153:        \"embedding_model\": \"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:169:        \"embedding_model\": \"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:184:        \"embedding_model\": \"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/conftest.py:154:            embedding_model=\"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/conftest.py:195:            embedding_model=\"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/conftest.py:231:            embedding_model=\"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_qdrant.py:50:    mock_vector_db.embedding_model = \"embedding_model\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_faiss.py:67:    mock_vector_db.embedding_model = \"mock_embedding_model\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:64:def test_openai_embeddings_single_string(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:66:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:71:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:77:    assert response.model == embedding_model_id\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:86:def test_openai_embeddings_multiple_strings(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:88:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:93:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:98:    assert response.model == embedding_model_id\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:109:def test_openai_embeddings_with_encoding_format_float(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:111:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:116:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:127:def test_openai_embeddings_with_dimensions(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:129:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:130:    skip_if_model_doesnt_support_variable_dimensions(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:136:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:148:def test_openai_embeddings_with_user_parameter(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:150:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:156:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:167:def test_openai_embeddings_empty_list_error(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:169:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:173:            model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:178:def test_openai_embeddings_invalid_model_error(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:180:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:189:def test_openai_embeddings_different_inputs_different_outputs(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:191:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:197:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:202:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:214:def test_openai_embeddings_with_encoding_format_base64(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:216:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:217:    skip_if_model_doesnt_support_variable_dimensions(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:223:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:247:def test_openai_embeddings_base64_batch_processing(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:249:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:254:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:261:    assert response.model == embedding_model_id\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:67:    client_with_empty_registry, sample_documents, embedding_model_id, embedding_dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:72:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:122:    client_with_empty_registry, sample_documents, embedding_model_id, embedding_dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:131:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:177:def test_rag_tool_insert_and_query(client_with_empty_registry, embedding_model_id, embedding_dimension):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:185:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:111:def test_embedding_text(llama_stack_client, embedding_model_id, contents, inference_provider_type):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:115:        model_id=embedding_model_id, contents=contents, **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:134:def test_embedding_image(llama_stack_client, embedding_model_id, contents, inference_provider_type):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:137:    if embedding_model_id not in MODELS_SUPPORTING_MEDIA:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:138:        pytest.xfail(f\"{embedding_model_id} doesn't support media\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:140:        model_id=embedding_model_id, contents=contents, **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:167:    llama_stack_client, embedding_model_id, text_truncation, contents, inference_provider_type\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:172:        model_id=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:175:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:202:    llama_stack_client, embedding_model_id, text_trunc\n\n... [105 lines truncated] ..."}]},"uuid":"ed59a5ce-eecb-467f-9ad4-bf0f98fd6b35","timestamp":"2025-07-18T20:51:39.431Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py:208:        mock_vector_db.embedding_model = \"test-model without embeddings\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py:233:        mock_vector_db.embedding_model = \"test-model with embeddings\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py:258:        mock_vector_db.embedding_model = \"test-model with invalid embeddings\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/rag/test_vector_store.py:296:        mock_vector_db.embedding_model = \"test-model with partial embeddings\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/routers/test_routing_tables.py:190:    await table.register_vector_db(vector_db_id=\"test-vectordb\", embedding_model=\"test-model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/routers/test_routing_tables.py:191:    await table.register_vector_db(vector_db_id=\"test-vectordb-2\", embedding_model=\"test-model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_configurable_embeddings.py:51:        embedding_model=\"all-MiniLM-L6-v2\",  # explicit even though it is default\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_configurable_embeddings.py:71:def test_vector_db_register_rejects_non_embedding_model(client_with_models):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_configurable_embeddings.py:79:            embedding_model=\"meta-llama/Llama-3.1-8B-Instruct\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:25:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:56:    assert result_vector_db.embedding_model == sample_vector_db.embedding_model\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:80:    assert result_vector_db.embedding_model == sample_vector_db.embedding_model\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:88:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:114:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:123:        embedding_model=\"different-model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:132:    assert result.embedding_model == original_vector_db.embedding_model  # Original values preserved\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:141:            embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:162:        assert stored_vector_db.embedding_model == original_vector_db.embedding_model\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:170:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:208:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:220:        '{\"type\": \"vector_db\", \"identifier\": \"invalid_cached_db\", \"embedding_model\": 12345}',  # Should be string\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/remote/test_milvus.py:142:async def test_bm25_fallback_to_simple_search(milvus_index, sample_chunks, sample_embeddings, mock_milvus_client):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:49:def test_vector_db_retrieve(client_with_empty_registry, embedding_model_id, embedding_dimension):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:54:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:62:    assert response.embedding_model == embedding_model_id\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:66:def test_vector_db_register(client_with_empty_registry, embedding_model_id, embedding_dimension):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:70:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:93:def test_insert_chunks(client_with_empty_registry, embedding_model_id, embedding_dimension, sample_chunks, test_case):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:97:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:125:def test_insert_chunks_with_precomputed_embeddings(client_with_empty_registry, embedding_model_id, embedding_dimension):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:132:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:167:    client_with_empty_registry, embedding_model_id, embedding_dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_vector_io.py:175:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:57:        identifier=\"foo_db\", provider_id=\"test_provider\", embedding_model=\"test_model\", embedding_dimension=128\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:67:        identifier=\"foo_db\", provider_id=\"test_provider\", embedding_model=\"test_model\", embedding_dimension=128\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:80:        identifier=unique_id, provider_id=\"test_provider\", embedding_model=\"test_model\", embedding_dimension=128\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:137:        \"embedding_model\": \"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:153:        \"embedding_model\": \"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:169:        \"embedding_model\": \"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_vector_io_openai_vector_stores.py:184:        \"embedding_model\": \"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/conftest.py:154:            embedding_model=\"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/conftest.py:195:            embedding_model=\"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/conftest.py:231:            embedding_model=\"test_model\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_qdrant.py:50:    mock_vector_db.embedding_model = \"embedding_model\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/vector_io/test_faiss.py:67:    mock_vector_db.embedding_model = \"mock_embedding_model\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:64:def test_openai_embeddings_single_string(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:66:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:71:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:77:    assert response.model == embedding_model_id\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:86:def test_openai_embeddings_multiple_strings(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:88:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:93:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:98:    assert response.model == embedding_model_id\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:109:def test_openai_embeddings_with_encoding_format_float(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:111:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:116:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:127:def test_openai_embeddings_with_dimensions(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:129:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:130:    skip_if_model_doesnt_support_variable_dimensions(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:136:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:148:def test_openai_embeddings_with_user_parameter(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:150:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:156:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:167:def test_openai_embeddings_empty_list_error(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:169:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:173:            model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:178:def test_openai_embeddings_invalid_model_error(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:180:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:189:def test_openai_embeddings_different_inputs_different_outputs(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:191:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:197:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:202:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:214:def test_openai_embeddings_with_encoding_format_base64(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:216:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:217:    skip_if_model_doesnt_support_variable_dimensions(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:223:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:247:def test_openai_embeddings_base64_batch_processing(compat_client, client_with_models, embedding_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:249:    skip_if_model_doesnt_support_openai_embeddings(client_with_models, embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:254:        model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_openai_embeddings.py:261:    assert response.model == embedding_model_id\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:67:    client_with_empty_registry, sample_documents, embedding_model_id, embedding_dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:72:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:122:    client_with_empty_registry, sample_documents, embedding_model_id, embedding_dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:131:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:177:def test_rag_tool_insert_and_query(client_with_empty_registry, embedding_model_id, embedding_dimension):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/tool_runtime/test_rag_tool.py:185:        embedding_model=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:111:def test_embedding_text(llama_stack_client, embedding_model_id, contents, inference_provider_type):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:115:        model_id=embedding_model_id, contents=contents, **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:134:def test_embedding_image(llama_stack_client, embedding_model_id, contents, inference_provider_type):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:137:    if embedding_model_id not in MODELS_SUPPORTING_MEDIA:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:138:        pytest.xfail(f\"{embedding_model_id} doesn't support media\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:140:        model_id=embedding_model_id, contents=contents, **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:167:    llama_stack_client, embedding_model_id, text_truncation, contents, inference_provider_type\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:172:        model_id=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:175:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:202:    llama_stack_client, embedding_model_id, text_truncation, contents, inference_provider_type\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:208:            model_id=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:211:            **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:215:def test_embedding_output_dimension(llama_stack_client, embedding_model_id, inference_provider_type):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:218:    if embedding_model_id not in MODELS_SUPPORTING_OUTPUT_DIMENSION:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:219:        pytest.xfail(f\"{embedding_model_id} doesn't support output_dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:221:        model_id=embedding_model_id, contents=[DUMMY_STRING], **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:224:        model_id=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:226:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:233:def test_embedding_task_type(llama_stack_client, embedding_model_id, inference_provider_type):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:236:    if embedding_model_id not in MODELS_SUPPORTING_TASK_TYPE:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:237:        pytest.xfail(f\"{embedding_model_id} doesn't support task_type\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:239:        model_id=embedding_model_id, contents=[DUMMY_STRING], task_type=\"query\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:242:        model_id=embedding_model_id, contents=[DUMMY_STRING], task_type=\"document\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:256:def test_embedding_text_truncation(llama_stack_client, embedding_model_id, text_truncation, inference_provider_type):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:260:        model_id=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:263:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:282:    llama_stack_client, embedding_model_id, text_truncation, inference_provider_type\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:288:            model_id=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:291:            **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:67:    embedding_model = ModelInput(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:102:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:124:                    embedding_model,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:54:    async def _get_first_embedding_model(self) -> tuple[str, int] | None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:61:            embedding_models = [\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:67:            if embedding_models:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:68:                dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:70:                    raise ValueError(f\"Embedding model {embedding_models[0].identifier} has no embedding dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:71:                return embedding_models[0].identifier, dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:82:        embedding_model: str,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:88:        logger.debug(f\"VectorIORouter.register_vector_db: {vector_db_id}, {embedding_model}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:91:            embedding_model,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:126:        embedding_model: str | None = None,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:133:        if embedding_model is None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:141:                        if hasattr(provider_config, \"embedding_model\") and provider_config.embedding_model:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:142:                            embedding_model = provider_config.embedding_model\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:143:                            logger.info(f\"Using provider config default embedding model: {embedding_model}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:152:            if embedding_model is None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:153:                embedding_model_info = await self._get_first_embedding_model()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:154:                if embedding_model_info is None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:156:                embedding_model, embedding_dimension = embedding_model_info\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:157:                logger.info(f\"No embedding model specified, using first available: {embedding_model}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:162:            embedding_model=embedding_model,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:174:            embedding_model=embedding_model,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:119:    embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:139:    if embedding_model_id and embedding_model_id not in model_ids:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:149:            model_id=embedding_model_id,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:169:    model_fixtures = [\"text_model_id\", \"vision_model_id\", \"embedding_model_id\", \"judge_model_id\"]\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:63:    embedding_model = ModelInput(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:86:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:80:    embedding_model = ModelInput(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:124:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/ui/page/playground/rag.py:70:                    embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:60:    embedding_model = ModelInput(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:92:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:295:    embedding_model = ModelInput(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:326:                default_models=[embedding_model] + default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/conftest.py:85:        help=\"comma-separated list of embedding models. Fixture name: embedding_model_id\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/conftest.py:145:        \"embedding_model_id\": (\"--embedding-model\", \"emb\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/openai_vector_store_mixin.py:169:        embedding_model: str | None = None,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/openai_vector_store_mixin.py:182:        if embedding_model is None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/openai_vector_store_mixin.py:193:            embedding_model=embedding_model,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/vector_store.py:196:            chunk_embedding_model=None,  # This will be set in `VectorDBWithIndex.insert_chunks`\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/vector_store.py:274:                    c.chunk_metadata.chunk_embedding_model = self.vector_db.embedding_model\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/vector_store.py:281:                self.vector_db.embedding_model,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/memory/vector_store.py:318:        embeddings_response = await self.inference_api.embeddings(self.vector_db.embedding_model, [query_string])\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/inference/embedding_mixin.py:45:        embedding_model = self._load_sentence_transformer_model(model.provider_resource_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/inference/embedding_mixin.py:46:        embeddings = embedding_model.encode(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/inference/embedding_mixin.py:66:        embedding_model = self._load_sentence_transformer_model(model_obj.provider_resource_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/inference/embedding_mixin.py:67:        embeddings = embedding_model.encode(input_list, show_progress_bar=False)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/agents/test_agents.py:467:        embedding_model=\"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:35:        embedding_model: str,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:52:        model = await self.get_object_by_identifier(\"model\", embedding_model)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:54:            raise ValueError(f\"Model {embedding_model} not found\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:56:            raise ValueError(f\"Model {embedding_model} is not an embedding model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:58:            raise ValueError(f\"Model {embedding_model} does not have an embedding dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:64:            \"embedding_model\": embedding_model,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/weaviate/config.py:24:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/weaviate/config.py:41:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_io/vector_io.py:38:    :param chunk_embedding_model: The embedding model used to create the chunk's embedding.\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_io/vector_io.py:51:    chunk_embedding_model: str | None = None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_io/vector_io.py:346:        embedding_model: str | None = None,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_io/vector_io.py:357:        :param embedding_model: The embedding model to use for this vector store.\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/sqlite_vec/config.py:20:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/sqlite_vec/config.py:38:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/milvus/config.py:23:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/milvus/config.py:41:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_dbs/vector_dbs.py:20:    embedding_model: str\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_dbs/vector_dbs.py:35:    embedding_model: str\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_dbs/vector_dbs.py:71:        embedding_model: str,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/apis/vector_dbs/vector_dbs.py:80:        :param embedding_model: The embedding model to use.\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/qdrant/qdrant.py:214:        embedding_model: str | None = None,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/qdrant/config.py:26:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/qdrant/config.py:40:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/faiss/config.py:21:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/faiss/config.py:38:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/config.py:26:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/config.py:57:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/chroma/config.py:14:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/chroma/config.py:28:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/milvus/config.py:21:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/milvus/config.py:44:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/chroma/config.py:14:    embedding_model: str | None = Field(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/chroma/config.py:28:            # \"embedding_model\": \"all-MiniLM-L6-v2\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/chroma/chroma.py:217:        embedding_model: str | None = None,","numLines":200}}
{"parentUuid":"ed59a5ce-eecb-467f-9ad4-bf0f98fd6b35","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01WAuRbqdD58NmdMxdast9JY","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me search for the specific fallback logic and vector store creation code:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":7126,"cache_read_input_tokens":14133,"output_tokens":140,"service_tier":"standard"}},"requestId":"req_011CRF9R8txB3La9mNSZQgZG","type":"assistant","uuid":"4b2ab362-754b-4f27-9daf-6d67c62d7eac","timestamp":"2025-07-18T20:51:42.465Z"}
{"parentUuid":"4b2ab362-754b-4f27-9daf-6d67c62d7eac","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01WAuRbqdD58NmdMxdast9JY","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01Fpdi9DQQPvUzzJiW7LGxfL","name":"Grep","input":{"pattern":"first.*model.*run\\.yaml|fallback.*model|default.*embedding","glob":"**/*.py","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":7126,"cache_read_input_tokens":14133,"output_tokens":140,"service_tier":"standard"}},"requestId":"req_011CRF9R8txB3La9mNSZQgZG","type":"assistant","uuid":"339a8da3-ef10-4ebe-b0f0-0fe13f113ed9","timestamp":"2025-07-18T20:51:44.221Z"}
{"parentUuid":"339a8da3-ef10-4ebe-b0f0-0fe13f113ed9","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Fpdi9DQQPvUzzJiW7LGxfL","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/verifications/conftest.py:90:            print(f\"Warning: Used fallback regex to parse case_id from nodeid {nodeid} (model unknown)\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_configurable_embeddings.py:44:    \"\"\"Register a vector DB with an explicit, non-default embedding model and verify e2e search works.\"\"\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:115:        model_id=embedding_model_id, contents=contents, **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:140:        model_id=embedding_model_id, contents=contents, **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:175:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:211:            **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:221:        model_id=embedding_model_id, contents=[DUMMY_STRING], **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:226:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:263:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:291:            **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:143:                            logger.info(f\"Using provider config default embedding model: {embedding_model}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/sqlite_vec/config.py:22:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/sqlite_vec/config.py:37:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/weaviate/config.py:26:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/weaviate/config.py:40:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:86:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/milvus/config.py:25:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/milvus/config.py:40:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:124:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/faiss/config.py:23:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/faiss/config.py:37:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:92:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/qdrant/config.py:28:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/qdrant/config.py:39:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/chroma/config.py:16:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/chroma/config.py:27:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:102:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:326:                default_models=[embedding_model] + default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/config.py:28:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/config.py:56:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/milvus/config.py:23:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/milvus/config.py:43:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/chroma/config.py:16:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/chroma/config.py:27:            # Optional: Configure default embedding model for this provider"}]},"uuid":"ba9310d4-906e-42d4-abb9-e63c937129e4","timestamp":"2025-07-18T20:51:44.284Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/verifications/conftest.py:90:            print(f\"Warning: Used fallback regex to parse case_id from nodeid {nodeid} (model unknown)\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/vector_io/test_configurable_embeddings.py:44:    \"\"\"Register a vector DB with an explicit, non-default embedding model and verify e2e search works.\"\"\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:115:        model_id=embedding_model_id, contents=contents, **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:140:        model_id=embedding_model_id, contents=contents, **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:175:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:211:            **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:221:        model_id=embedding_model_id, contents=[DUMMY_STRING], **default_task_type(embedding_model_id)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:226:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:263:        **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/inference/test_embedding.py:291:            **default_task_type(embedding_model_id),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:143:                            logger.info(f\"Using provider config default embedding model: {embedding_model}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/sqlite_vec/config.py:22:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/sqlite_vec/config.py:37:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/weaviate/config.py:26:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/weaviate/config.py:40:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:86:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/milvus/config.py:25:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/milvus/config.py:40:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:124:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/faiss/config.py:23:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/faiss/config.py:37:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:92:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/qdrant/config.py:28:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/qdrant/config.py:39:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/chroma/config.py:16:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/chroma/config.py:27:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:102:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:326:                default_models=[embedding_model] + default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/config.py:28:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/config.py:56:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/milvus/config.py:23:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/milvus/config.py:43:            # Optional: Configure default embedding model for this provider\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/chroma/config.py:16:        description=\"Optional default embedding model for this provider. If not specified, will use system default.\",\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/chroma/config.py:27:            # Optional: Configure default embedding model for this provider","numLines":34}}
{"parentUuid":"ba9310d4-906e-42d4-abb9-e63c937129e4","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01BQ3zNcM5Siqo58iBRLep2C","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Let me examine the vector_io router which seems to contain the fallback logic:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":2545,"cache_read_input_tokens":21259,"output_tokens":118,"service_tier":"standard"}},"requestId":"req_011CRF9RVnJn5S4o5WpyuW1k","type":"assistant","uuid":"a22c08bb-1d45-4d86-a389-f06dce2a19fa","timestamp":"2025-07-18T20:51:51.067Z"}
{"parentUuid":"a22c08bb-1d45-4d86-a389-f06dce2a19fa","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01BQ3zNcM5Siqo58iBRLep2C","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01T57GchWxi5aGiPLKfDWutj","name":"Read","input":{"file_path":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":2545,"cache_read_input_tokens":21259,"output_tokens":118,"service_tier":"standard"}},"requestId":"req_011CRF9RVnJn5S4o5WpyuW1k","type":"assistant","uuid":"a78115b5-1483-4646-853b-a886d7dba7fa","timestamp":"2025-07-18T20:51:52.256Z"}
{"parentUuid":"a78115b5-1483-4646-853b-a886d7dba7fa","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01T57GchWxi5aGiPLKfDWutj","type":"tool_result","content":"     1→# Copyright (c) Meta Platforms, Inc. and affiliates.\n     2→# All rights reserved.\n     3→#\n     4→# This source code is licensed under the terms described in the LICENSE file in\n     5→# the root directory of this source tree.\n     6→\n     7→import asyncio\n     8→import uuid\n     9→from typing import Any\n    10→\n    11→from llama_stack.apis.common.content_types import (\n    12→    InterleavedContent,\n    13→)\n    14→from llama_stack.apis.models import ModelType\n    15→from llama_stack.apis.vector_io import (\n    16→    Chunk,\n    17→    QueryChunksResponse,\n    18→    SearchRankingOptions,\n    19→    VectorIO,\n    20→    VectorStoreChunkingStrategy,\n    21→    VectorStoreDeleteResponse,\n    22→    VectorStoreFileContentsResponse,\n    23→    VectorStoreFileDeleteResponse,\n    24→    VectorStoreFileObject,\n    25→    VectorStoreFileStatus,\n    26→    VectorStoreListResponse,\n    27→    VectorStoreObject,\n    28→    VectorStoreSearchResponsePage,\n    29→)\n    30→from llama_stack.log import get_logger\n    31→from llama_stack.providers.datatypes import HealthResponse, HealthStatus, RoutingTable\n    32→\n    33→logger = get_logger(name=__name__, category=\"core\")\n    34→\n    35→\n    36→class VectorIORouter(VectorIO):\n    37→    \"\"\"Routes to an provider based on the vector db identifier\"\"\"\n    38→\n    39→    def __init__(\n    40→        self,\n    41→        routing_table: RoutingTable,\n    42→    ) -> None:\n    43→        logger.debug(\"Initializing VectorIORouter\")\n    44→        self.routing_table = routing_table\n    45→\n    46→    async def initialize(self) -> None:\n    47→        logger.debug(\"VectorIORouter.initialize\")\n    48→        pass\n    49→\n    50→    async def shutdown(self) -> None:\n    51→        logger.debug(\"VectorIORouter.shutdown\")\n    52→        pass\n    53→\n    54→    async def _get_first_embedding_model(self) -> tuple[str, int] | None:\n    55→        \"\"\"Get the first available embedding model identifier.\"\"\"\n    56→        try:\n    57→            # Get all models from the routing table\n    58→            all_models = await self.routing_table.get_all_with_type(\"model\")\n    59→\n    60→            # Filter for embedding models\n    61→            embedding_models = [\n    62→                model\n    63→                for model in all_models\n    64→                if hasattr(model, \"model_type\") and model.model_type == ModelType.embedding\n    65→            ]\n    66→\n    67→            if embedding_models:\n    68→                dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n    69→                if dimension is None:\n    70→                    raise ValueError(f\"Embedding model {embedding_models[0].identifier} has no embedding dimension\")\n    71→                return embedding_models[0].identifier, dimension\n    72→            else:\n    73→                logger.warning(\"No embedding models found in the routing table\")\n    74→                return None\n    75→        except Exception as e:\n    76→            logger.error(f\"Error getting embedding models: {e}\")\n    77→            return None\n    78→\n    79→    async def register_vector_db(\n    80→        self,\n    81→        vector_db_id: str,\n    82→        embedding_model: str,\n    83→        embedding_dimension: int | None = 384,\n    84→        provider_id: str | None = None,\n    85→        vector_db_name: str | None = None,\n    86→        provider_vector_db_id: str | None = None,\n    87→    ) -> None:\n    88→        logger.debug(f\"VectorIORouter.register_vector_db: {vector_db_id}, {embedding_model}\")\n    89→        await self.routing_table.register_vector_db(\n    90→            vector_db_id,\n    91→            embedding_model,\n    92→            embedding_dimension,\n    93→            provider_id,\n    94→            vector_db_name,\n    95→            provider_vector_db_id,\n    96→        )\n    97→\n    98→    async def insert_chunks(\n    99→        self,\n   100→        vector_db_id: str,\n   101→        chunks: list[Chunk],\n   102→        ttl_seconds: int | None = None,\n   103→    ) -> None:\n   104→        logger.debug(\n   105→            f\"VectorIORouter.insert_chunks: {vector_db_id}, {len(chunks)} chunks, ttl_seconds={ttl_seconds}, chunk_ids={[chunk.metadata['document_id'] for chunk in chunks[:3]]}{' and more...' if len(chunks) > 3 else ''}\",\n   106→        )\n   107→        return await self.routing_table.get_provider_impl(vector_db_id).insert_chunks(vector_db_id, chunks, ttl_seconds)\n   108→\n   109→    async def query_chunks(\n   110→        self,\n   111→        vector_db_id: str,\n   112→        query: InterleavedContent,\n   113→        params: dict[str, Any] | None = None,\n   114→    ) -> QueryChunksResponse:\n   115→        logger.debug(f\"VectorIORouter.query_chunks: {vector_db_id}\")\n   116→        return await self.routing_table.get_provider_impl(vector_db_id).query_chunks(vector_db_id, query, params)\n   117→\n   118→    # OpenAI Vector Stores API endpoints\n   119→    async def openai_create_vector_store(\n   120→        self,\n   121→        name: str,\n   122→        file_ids: list[str] | None = None,\n   123→        expires_after: dict[str, Any] | None = None,\n   124→        chunking_strategy: dict[str, Any] | None = None,\n   125→        metadata: dict[str, Any] | None = None,\n   126→        embedding_model: str | None = None,\n   127→        embedding_dimension: int | None = None,\n   128→        provider_id: str | None = None,\n   129→    ) -> VectorStoreObject:\n   130→        logger.debug(f\"VectorIORouter.openai_create_vector_store: name={name}, provider_id={provider_id}\")\n   131→\n   132→        # If no embedding model is provided, try provider defaults then fallback\n   133→        if embedding_model is None:\n   134→            # Try to get provider-specific embedding model configuration\n   135→            if provider_id:\n   136→                try:\n   137→                    provider_impl = self.routing_table.get_provider_impl(provider_id)\n   138→                    provider_config = getattr(provider_impl, \"config\", None)\n   139→\n   140→                    if provider_config:\n   141→                        if hasattr(provider_config, \"embedding_model\") and provider_config.embedding_model:\n   142→                            embedding_model = provider_config.embedding_model\n   143→                            logger.info(f\"Using provider config default embedding model: {embedding_model}\")\n   144→\n   145→                        if hasattr(provider_config, \"embedding_dimension\") and provider_config.embedding_dimension:\n   146→                            embedding_dimension = provider_config.embedding_dimension\n   147→                            logger.info(f\"Using provider config embedding dimension: {embedding_dimension}\")\n   148→                except Exception as e:\n   149→                    logger.debug(f\"Could not get provider config for {provider_id}: {e}\")\n   150→\n   151→            # If still no embedding model, use system fallback\n   152→            if embedding_model is None:\n   153→                embedding_model_info = await self._get_first_embedding_model()\n   154→                if embedding_model_info is None:\n   155→                    raise ValueError(\"No embedding model provided and no embedding models available in the system\")\n   156→                embedding_model, embedding_dimension = embedding_model_info\n   157→                logger.info(f\"No embedding model specified, using first available: {embedding_model}\")\n   158→\n   159→        vector_db_id = f\"vs_{uuid.uuid4()}\"\n   160→        registered_vector_db = await self.routing_table.register_vector_db(\n   161→            vector_db_id=vector_db_id,\n   162→            embedding_model=embedding_model,\n   163→            embedding_dimension=embedding_dimension,\n   164→            provider_id=provider_id,\n   165→            provider_vector_db_id=vector_db_id,\n   166→            vector_db_name=name,\n   167→        )\n   168→        return await self.routing_table.get_provider_impl(registered_vector_db.identifier).openai_create_vector_store(\n   169→            name=name,\n   170→            file_ids=file_ids,\n   171→            expires_after=expires_after,\n   172→            chunking_strategy=chunking_strategy,\n   173→            metadata=metadata,\n   174→            embedding_model=embedding_model,\n   175→            embedding_dimension=embedding_dimension,\n   176→            provider_id=registered_vector_db.provider_id,\n   177→            provider_vector_db_id=registered_vector_db.provider_resource_id,\n   178→        )\n   179→\n   180→    async def openai_list_vector_stores(\n   181→        self,\n   182→        limit: int | None = 20,\n   183→        order: str | None = \"desc\",\n   184→        after: str | None = None,\n   185→        before: str | None = None,\n   186→    ) -> VectorStoreListResponse:\n   187→        logger.debug(f\"VectorIORouter.openai_list_vector_stores: limit={limit}\")\n   188→        # Route to default provider for now - could aggregate from all providers in the future\n   189→        # call retrieve on each vector dbs to get list of vector stores\n   190→        vector_dbs = await self.routing_table.get_all_with_type(\"vector_db\")\n   191→        all_stores = []\n   192→        for vector_db in vector_dbs:\n   193→            try:\n   194→                vector_store = await self.routing_table.get_provider_impl(\n   195→                    vector_db.identifier\n   196→                ).openai_retrieve_vector_store(vector_db.identifier)\n   197→                all_stores.append(vector_store)\n   198→            except Exception as e:\n   199→                logger.error(f\"Error retrieving vector store {vector_db.identifier}: {e}\")\n   200→                continue\n   201→\n   202→        # Sort by created_at\n   203→        reverse_order = order == \"desc\"\n   204→        all_stores.sort(key=lambda x: x.created_at, reverse=reverse_order)\n   205→\n   206→        # Apply cursor-based pagination\n   207→        if after:\n   208→            after_index = next((i for i, store in enumerate(all_stores) if store.id == after), -1)\n   209→            if after_index >= 0:\n   210→                all_stores = all_stores[after_index + 1 :]\n   211→\n   212→        if before:\n   213→            before_index = next((i for i, store in enumerate(all_stores) if store.id == before), len(all_stores))\n   214→            all_stores = all_stores[:before_index]\n   215→\n   216→        # Apply limit\n   217→        limited_stores = all_stores[:limit]\n   218→\n   219→        # Determine pagination info\n   220→        has_more = len(all_stores) > limit\n   221→        first_id = limited_stores[0].id if limited_stores else None\n   222→        last_id = limited_stores[-1].id if limited_stores else None\n   223→\n   224→        return VectorStoreListResponse(\n   225→            data=limited_stores,\n   226→            has_more=has_more,\n   227→            first_id=first_id,\n   228→            last_id=last_id,\n   229→        )\n   230→\n   231→    async def openai_retrieve_vector_store(\n   232→        self,\n   233→        vector_store_id: str,\n   234→    ) -> VectorStoreObject:\n   235→        logger.debug(f\"VectorIORouter.openai_retrieve_vector_store: {vector_store_id}\")\n   236→        # Route based on vector store ID\n   237→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   238→        return await provider.openai_retrieve_vector_store(vector_store_id)\n   239→\n   240→    async def openai_update_vector_store(\n   241→        self,\n   242→        vector_store_id: str,\n   243→        name: str | None = None,\n   244→        expires_after: dict[str, Any] | None = None,\n   245→        metadata: dict[str, Any] | None = None,\n   246→    ) -> VectorStoreObject:\n   247→        logger.debug(f\"VectorIORouter.openai_update_vector_store: {vector_store_id}\")\n   248→        # Route based on vector store ID\n   249→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   250→        return await provider.openai_update_vector_store(\n   251→            vector_store_id=vector_store_id,\n   252→            name=name,\n   253→            expires_after=expires_after,\n   254→            metadata=metadata,\n   255→        )\n   256→\n   257→    async def openai_delete_vector_store(\n   258→        self,\n   259→        vector_store_id: str,\n   260→    ) -> VectorStoreDeleteResponse:\n   261→        logger.debug(f\"VectorIORouter.openai_delete_vector_store: {vector_store_id}\")\n   262→        # Route based on vector store ID\n   263→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   264→        result = await provider.openai_delete_vector_store(vector_store_id)\n   265→        # drop from registry\n   266→        await self.routing_table.unregister_vector_db(vector_store_id)\n   267→        return result\n   268→\n   269→    async def openai_search_vector_store(\n   270→        self,\n   271→        vector_store_id: str,\n   272→        query: str | list[str],\n   273→        filters: dict[str, Any] | None = None,\n   274→        max_num_results: int | None = 10,\n   275→        ranking_options: SearchRankingOptions | None = None,\n   276→        rewrite_query: bool | None = False,\n   277→        search_mode: str | None = \"vector\",\n   278→    ) -> VectorStoreSearchResponsePage:\n   279→        logger.debug(f\"VectorIORouter.openai_search_vector_store: {vector_store_id}\")\n   280→        # Route based on vector store ID\n   281→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   282→        return await provider.openai_search_vector_store(\n   283→            vector_store_id=vector_store_id,\n   284→            query=query,\n   285→            filters=filters,\n   286→            max_num_results=max_num_results,\n   287→            ranking_options=ranking_options,\n   288→            rewrite_query=rewrite_query,\n   289→            search_mode=search_mode,\n   290→        )\n   291→\n   292→    async def openai_attach_file_to_vector_store(\n   293→        self,\n   294→        vector_store_id: str,\n   295→        file_id: str,\n   296→        attributes: dict[str, Any] | None = None,\n   297→        chunking_strategy: VectorStoreChunkingStrategy | None = None,\n   298→    ) -> VectorStoreFileObject:\n   299→        logger.debug(f\"VectorIORouter.openai_attach_file_to_vector_store: {vector_store_id}, {file_id}\")\n   300→        # Route based on vector store ID\n   301→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   302→        return await provider.openai_attach_file_to_vector_store(\n   303→            vector_store_id=vector_store_id,\n   304→            file_id=file_id,\n   305→            attributes=attributes,\n   306→            chunking_strategy=chunking_strategy,\n   307→        )\n   308→\n   309→    async def openai_list_files_in_vector_store(\n   310→        self,\n   311→        vector_store_id: str,\n   312→        limit: int | None = 20,\n   313→        order: str | None = \"desc\",\n   314→        after: str | None = None,\n   315→        before: str | None = None,\n   316→        filter: VectorStoreFileStatus | None = None,\n   317→    ) -> list[VectorStoreFileObject]:\n   318→        logger.debug(f\"VectorIORouter.openai_list_files_in_vector_store: {vector_store_id}\")\n   319→        # Route based on vector store ID\n   320→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   321→        return await provider.openai_list_files_in_vector_store(\n   322→            vector_store_id=vector_store_id,\n   323→            limit=limit,\n   324→            order=order,\n   325→            after=after,\n   326→            before=before,\n   327→            filter=filter,\n   328→        )\n   329→\n   330→    async def openai_retrieve_vector_store_file(\n   331→        self,\n   332→        vector_store_id: str,\n   333→        file_id: str,\n   334→    ) -> VectorStoreFileObject:\n   335→        logger.debug(f\"VectorIORouter.openai_retrieve_vector_store_file: {vector_store_id}, {file_id}\")\n   336→        # Route based on vector store ID\n   337→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   338→        return await provider.openai_retrieve_vector_store_file(\n   339→            vector_store_id=vector_store_id,\n   340→            file_id=file_id,\n   341→        )\n   342→\n   343→    async def openai_retrieve_vector_store_file_contents(\n   344→        self,\n   345→        vector_store_id: str,\n   346→        file_id: str,\n   347→    ) -> VectorStoreFileContentsResponse:\n   348→        logger.debug(f\"VectorIORouter.openai_retrieve_vector_store_file_contents: {vector_store_id}, {file_id}\")\n   349→        # Route based on vector store ID\n   350→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   351→        return await provider.openai_retrieve_vector_store_file_contents(\n   352→            vector_store_id=vector_store_id,\n   353→            file_id=file_id,\n   354→        )\n   355→\n   356→    async def openai_update_vector_store_file(\n   357→        self,\n   358→        vector_store_id: str,\n   359→        file_id: str,\n   360→        attributes: dict[str, Any],\n   361→    ) -> VectorStoreFileObject:\n   362→        logger.debug(f\"VectorIORouter.openai_update_vector_store_file: {vector_store_id}, {file_id}\")\n   363→        # Route based on vector store ID\n   364→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   365→        return await provider.openai_update_vector_store_file(\n   366→            vector_store_id=vector_store_id,\n   367→            file_id=file_id,\n   368→            attributes=attributes,\n   369→        )\n   370→\n   371→    async def openai_delete_vector_store_file(\n   372→        self,\n   373→        vector_store_id: str,\n   374→        file_id: str,\n   375→    ) -> VectorStoreFileDeleteResponse:\n   376→        logger.debug(f\"VectorIORouter.openai_delete_vector_store_file: {vector_store_id}, {file_id}\")\n   377→        # Route based on vector store ID\n   378→        provider = self.routing_table.get_provider_impl(vector_store_id)\n   379→        return await provider.openai_delete_vector_store_file(\n   380→            vector_store_id=vector_store_id,\n   381→            file_id=file_id,\n   382→        )\n   383→\n   384→    async def health(self) -> dict[str, HealthResponse]:\n   385→        health_statuses = {}\n   386→        timeout = 1  # increasing the timeout to 1 second for health checks\n   387→        for provider_id, impl in self.routing_table.impls_by_provider_id.items():\n   388→            try:\n   389→                # check if the provider has a health method\n   390→                if not hasattr(impl, \"health\"):\n   391→                    continue\n   392→                health = await asyncio.wait_for(impl.health(), timeout=timeout)\n   393→                health_statuses[provider_id] = health\n   394→            except TimeoutError:\n   395→                health_statuses[provider_id] = HealthResponse(\n   396→                    status=HealthStatus.ERROR,\n   397→                    message=f\"Health check timed out after {timeout} seconds\",\n   398→                )\n   399→            except NotImplementedError:\n   400→                health_statuses[provider_id] = HealthResponse(status=HealthStatus.NOT_IMPLEMENTED)\n   401→            except Exception as e:\n   402→                health_statuses[provider_id] = HealthResponse(\n   403→                    status=HealthStatus.ERROR, message=f\"Health check failed: {str(e)}\"\n   404→                )\n   405→        return health_statuses\n   406→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"095f9240-14ca-4c9f-8c4c-7d14e0fcd93f","timestamp":"2025-07-18T20:51:52.291Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py","content":"# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the terms described in the LICENSE file in\n# the root directory of this source tree.\n\nimport asyncio\nimport uuid\nfrom typing import Any\n\nfrom llama_stack.apis.common.content_types import (\n    InterleavedContent,\n)\nfrom llama_stack.apis.models import ModelType\nfrom llama_stack.apis.vector_io import (\n    Chunk,\n    QueryChunksResponse,\n    SearchRankingOptions,\n    VectorIO,\n    VectorStoreChunkingStrategy,\n    VectorStoreDeleteResponse,\n    VectorStoreFileContentsResponse,\n    VectorStoreFileDeleteResponse,\n    VectorStoreFileObject,\n    VectorStoreFileStatus,\n    VectorStoreListResponse,\n    VectorStoreObject,\n    VectorStoreSearchResponsePage,\n)\nfrom llama_stack.log import get_logger\nfrom llama_stack.providers.datatypes import HealthResponse, HealthStatus, RoutingTable\n\nlogger = get_logger(name=__name__, category=\"core\")\n\n\nclass VectorIORouter(VectorIO):\n    \"\"\"Routes to an provider based on the vector db identifier\"\"\"\n\n    def __init__(\n        self,\n        routing_table: RoutingTable,\n    ) -> None:\n        logger.debug(\"Initializing VectorIORouter\")\n        self.routing_table = routing_table\n\n    async def initialize(self) -> None:\n        logger.debug(\"VectorIORouter.initialize\")\n        pass\n\n    async def shutdown(self) -> None:\n        logger.debug(\"VectorIORouter.shutdown\")\n        pass\n\n    async def _get_first_embedding_model(self) -> tuple[str, int] | None:\n        \"\"\"Get the first available embedding model identifier.\"\"\"\n        try:\n            # Get all models from the routing table\n            all_models = await self.routing_table.get_all_with_type(\"model\")\n\n            # Filter for embedding models\n            embedding_models = [\n                model\n                for model in all_models\n                if hasattr(model, \"model_type\") and model.model_type == ModelType.embedding\n            ]\n\n            if embedding_models:\n                dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n                if dimension is None:\n                    raise ValueError(f\"Embedding model {embedding_models[0].identifier} has no embedding dimension\")\n                return embedding_models[0].identifier, dimension\n            else:\n                logger.warning(\"No embedding models found in the routing table\")\n                return None\n        except Exception as e:\n            logger.error(f\"Error getting embedding models: {e}\")\n            return None\n\n    async def register_vector_db(\n        self,\n        vector_db_id: str,\n        embedding_model: str,\n        embedding_dimension: int | None = 384,\n        provider_id: str | None = None,\n        vector_db_name: str | None = None,\n        provider_vector_db_id: str | None = None,\n    ) -> None:\n        logger.debug(f\"VectorIORouter.register_vector_db: {vector_db_id}, {embedding_model}\")\n        await self.routing_table.register_vector_db(\n            vector_db_id,\n            embedding_model,\n            embedding_dimension,\n            provider_id,\n            vector_db_name,\n            provider_vector_db_id,\n        )\n\n    async def insert_chunks(\n        self,\n        vector_db_id: str,\n        chunks: list[Chunk],\n        ttl_seconds: int | None = None,\n    ) -> None:\n        logger.debug(\n            f\"VectorIORouter.insert_chunks: {vector_db_id}, {len(chunks)} chunks, ttl_seconds={ttl_seconds}, chunk_ids={[chunk.metadata['document_id'] for chunk in chunks[:3]]}{' and more...' if len(chunks) > 3 else ''}\",\n        )\n        return await self.routing_table.get_provider_impl(vector_db_id).insert_chunks(vector_db_id, chunks, ttl_seconds)\n\n    async def query_chunks(\n        self,\n        vector_db_id: str,\n        query: InterleavedContent,\n        params: dict[str, Any] | None = None,\n    ) -> QueryChunksResponse:\n        logger.debug(f\"VectorIORouter.query_chunks: {vector_db_id}\")\n        return await self.routing_table.get_provider_impl(vector_db_id).query_chunks(vector_db_id, query, params)\n\n    # OpenAI Vector Stores API endpoints\n    async def openai_create_vector_store(\n        self,\n        name: str,\n        file_ids: list[str] | None = None,\n        expires_after: dict[str, Any] | None = None,\n        chunking_strategy: dict[str, Any] | None = None,\n        metadata: dict[str, Any] | None = None,\n        embedding_model: str | None = None,\n        embedding_dimension: int | None = None,\n        provider_id: str | None = None,\n    ) -> VectorStoreObject:\n        logger.debug(f\"VectorIORouter.openai_create_vector_store: name={name}, provider_id={provider_id}\")\n\n        # If no embedding model is provided, try provider defaults then fallback\n        if embedding_model is None:\n            # Try to get provider-specific embedding model configuration\n            if provider_id:\n                try:\n                    provider_impl = self.routing_table.get_provider_impl(provider_id)\n                    provider_config = getattr(provider_impl, \"config\", None)\n\n                    if provider_config:\n                        if hasattr(provider_config, \"embedding_model\") and provider_config.embedding_model:\n                            embedding_model = provider_config.embedding_model\n                            logger.info(f\"Using provider config default embedding model: {embedding_model}\")\n\n                        if hasattr(provider_config, \"embedding_dimension\") and provider_config.embedding_dimension:\n                            embedding_dimension = provider_config.embedding_dimension\n                            logger.info(f\"Using provider config embedding dimension: {embedding_dimension}\")\n                except Exception as e:\n                    logger.debug(f\"Could not get provider config for {provider_id}: {e}\")\n\n            # If still no embedding model, use system fallback\n            if embedding_model is None:\n                embedding_model_info = await self._get_first_embedding_model()\n                if embedding_model_info is None:\n                    raise ValueError(\"No embedding model provided and no embedding models available in the system\")\n                embedding_model, embedding_dimension = embedding_model_info\n                logger.info(f\"No embedding model specified, using first available: {embedding_model}\")\n\n        vector_db_id = f\"vs_{uuid.uuid4()}\"\n        registered_vector_db = await self.routing_table.register_vector_db(\n            vector_db_id=vector_db_id,\n            embedding_model=embedding_model,\n            embedding_dimension=embedding_dimension,\n            provider_id=provider_id,\n            provider_vector_db_id=vector_db_id,\n            vector_db_name=name,\n        )\n        return await self.routing_table.get_provider_impl(registered_vector_db.identifier).openai_create_vector_store(\n            name=name,\n            file_ids=file_ids,\n            expires_after=expires_after,\n            chunking_strategy=chunking_strategy,\n            metadata=metadata,\n            embedding_model=embedding_model,\n            embedding_dimension=embedding_dimension,\n            provider_id=registered_vector_db.provider_id,\n            provider_vector_db_id=registered_vector_db.provider_resource_id,\n        )\n\n    async def openai_list_vector_stores(\n        self,\n        limit: int | None = 20,\n        order: str | None = \"desc\",\n        after: str | None = None,\n        before: str | None = None,\n    ) -> VectorStoreListResponse:\n        logger.debug(f\"VectorIORouter.openai_list_vector_stores: limit={limit}\")\n        # Route to default provider for now - could aggregate from all providers in the future\n        # call retrieve on each vector dbs to get list of vector stores\n        vector_dbs = await self.routing_table.get_all_with_type(\"vector_db\")\n        all_stores = []\n        for vector_db in vector_dbs:\n            try:\n                vector_store = await self.routing_table.get_provider_impl(\n                    vector_db.identifier\n                ).openai_retrieve_vector_store(vector_db.identifier)\n                all_stores.append(vector_store)\n            except Exception as e:\n                logger.error(f\"Error retrieving vector store {vector_db.identifier}: {e}\")\n                continue\n\n        # Sort by created_at\n        reverse_order = order == \"desc\"\n        all_stores.sort(key=lambda x: x.created_at, reverse=reverse_order)\n\n        # Apply cursor-based pagination\n        if after:\n            after_index = next((i for i, store in enumerate(all_stores) if store.id == after), -1)\n            if after_index >= 0:\n                all_stores = all_stores[after_index + 1 :]\n\n        if before:\n            before_index = next((i for i, store in enumerate(all_stores) if store.id == before), len(all_stores))\n            all_stores = all_stores[:before_index]\n\n        # Apply limit\n        limited_stores = all_stores[:limit]\n\n        # Determine pagination info\n        has_more = len(all_stores) > limit\n        first_id = limited_stores[0].id if limited_stores else None\n        last_id = limited_stores[-1].id if limited_stores else None\n\n        return VectorStoreListResponse(\n            data=limited_stores,\n            has_more=has_more,\n            first_id=first_id,\n            last_id=last_id,\n        )\n\n    async def openai_retrieve_vector_store(\n        self,\n        vector_store_id: str,\n    ) -> VectorStoreObject:\n        logger.debug(f\"VectorIORouter.openai_retrieve_vector_store: {vector_store_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_retrieve_vector_store(vector_store_id)\n\n    async def openai_update_vector_store(\n        self,\n        vector_store_id: str,\n        name: str | None = None,\n        expires_after: dict[str, Any] | None = None,\n        metadata: dict[str, Any] | None = None,\n    ) -> VectorStoreObject:\n        logger.debug(f\"VectorIORouter.openai_update_vector_store: {vector_store_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_update_vector_store(\n            vector_store_id=vector_store_id,\n            name=name,\n            expires_after=expires_after,\n            metadata=metadata,\n        )\n\n    async def openai_delete_vector_store(\n        self,\n        vector_store_id: str,\n    ) -> VectorStoreDeleteResponse:\n        logger.debug(f\"VectorIORouter.openai_delete_vector_store: {vector_store_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        result = await provider.openai_delete_vector_store(vector_store_id)\n        # drop from registry\n        await self.routing_table.unregister_vector_db(vector_store_id)\n        return result\n\n    async def openai_search_vector_store(\n        self,\n        vector_store_id: str,\n        query: str | list[str],\n        filters: dict[str, Any] | None = None,\n        max_num_results: int | None = 10,\n        ranking_options: SearchRankingOptions | None = None,\n        rewrite_query: bool | None = False,\n        search_mode: str | None = \"vector\",\n    ) -> VectorStoreSearchResponsePage:\n        logger.debug(f\"VectorIORouter.openai_search_vector_store: {vector_store_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_search_vector_store(\n            vector_store_id=vector_store_id,\n            query=query,\n            filters=filters,\n            max_num_results=max_num_results,\n            ranking_options=ranking_options,\n            rewrite_query=rewrite_query,\n            search_mode=search_mode,\n        )\n\n    async def openai_attach_file_to_vector_store(\n        self,\n        vector_store_id: str,\n        file_id: str,\n        attributes: dict[str, Any] | None = None,\n        chunking_strategy: VectorStoreChunkingStrategy | None = None,\n    ) -> VectorStoreFileObject:\n        logger.debug(f\"VectorIORouter.openai_attach_file_to_vector_store: {vector_store_id}, {file_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_attach_file_to_vector_store(\n            vector_store_id=vector_store_id,\n            file_id=file_id,\n            attributes=attributes,\n            chunking_strategy=chunking_strategy,\n        )\n\n    async def openai_list_files_in_vector_store(\n        self,\n        vector_store_id: str,\n        limit: int | None = 20,\n        order: str | None = \"desc\",\n        after: str | None = None,\n        before: str | None = None,\n        filter: VectorStoreFileStatus | None = None,\n    ) -> list[VectorStoreFileObject]:\n        logger.debug(f\"VectorIORouter.openai_list_files_in_vector_store: {vector_store_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_list_files_in_vector_store(\n            vector_store_id=vector_store_id,\n            limit=limit,\n            order=order,\n            after=after,\n            before=before,\n            filter=filter,\n        )\n\n    async def openai_retrieve_vector_store_file(\n        self,\n        vector_store_id: str,\n        file_id: str,\n    ) -> VectorStoreFileObject:\n        logger.debug(f\"VectorIORouter.openai_retrieve_vector_store_file: {vector_store_id}, {file_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_retrieve_vector_store_file(\n            vector_store_id=vector_store_id,\n            file_id=file_id,\n        )\n\n    async def openai_retrieve_vector_store_file_contents(\n        self,\n        vector_store_id: str,\n        file_id: str,\n    ) -> VectorStoreFileContentsResponse:\n        logger.debug(f\"VectorIORouter.openai_retrieve_vector_store_file_contents: {vector_store_id}, {file_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_retrieve_vector_store_file_contents(\n            vector_store_id=vector_store_id,\n            file_id=file_id,\n        )\n\n    async def openai_update_vector_store_file(\n        self,\n        vector_store_id: str,\n        file_id: str,\n        attributes: dict[str, Any],\n    ) -> VectorStoreFileObject:\n        logger.debug(f\"VectorIORouter.openai_update_vector_store_file: {vector_store_id}, {file_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_update_vector_store_file(\n            vector_store_id=vector_store_id,\n            file_id=file_id,\n            attributes=attributes,\n        )\n\n    async def openai_delete_vector_store_file(\n        self,\n        vector_store_id: str,\n        file_id: str,\n    ) -> VectorStoreFileDeleteResponse:\n        logger.debug(f\"VectorIORouter.openai_delete_vector_store_file: {vector_store_id}, {file_id}\")\n        # Route based on vector store ID\n        provider = self.routing_table.get_provider_impl(vector_store_id)\n        return await provider.openai_delete_vector_store_file(\n            vector_store_id=vector_store_id,\n            file_id=file_id,\n        )\n\n    async def health(self) -> dict[str, HealthResponse]:\n        health_statuses = {}\n        timeout = 1  # increasing the timeout to 1 second for health checks\n        for provider_id, impl in self.routing_table.impls_by_provider_id.items():\n            try:\n                # check if the provider has a health method\n                if not hasattr(impl, \"health\"):\n                    continue\n                health = await asyncio.wait_for(impl.health(), timeout=timeout)\n                health_statuses[provider_id] = health\n            except TimeoutError:\n                health_statuses[provider_id] = HealthResponse(\n                    status=HealthStatus.ERROR,\n                    message=f\"Health check timed out after {timeout} seconds\",\n                )\n            except NotImplementedError:\n                health_statuses[provider_id] = HealthResponse(status=HealthStatus.NOT_IMPLEMENTED)\n            except Exception as e:\n                health_statuses[provider_id] = HealthResponse(\n                    status=HealthStatus.ERROR, message=f\"Health check failed: {str(e)}\"\n                )\n        return health_statuses\n","numLines":406,"startLine":1,"totalLines":406}}}
{"parentUuid":"095f9240-14ca-4c9f-8c4c-7d14e0fcd93f","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01NeozYUATHZ9rz3FrFtnfci","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Perfect! I found the main fallback logic. Now let me also check the routing table code to understand how the \"first model\" is selected:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":6064,"cache_read_input_tokens":23804,"output_tokens":155,"service_tier":"standard"}},"requestId":"req_011CRF9S5x8DoEqFhAxBhJyh","type":"assistant","uuid":"a2ea7411-b21b-4d99-bf0a-532726e47ec8","timestamp":"2025-07-18T20:51:56.371Z"}
{"parentUuid":"a2ea7411-b21b-4d99-bf0a-532726e47ec8","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01NeozYUATHZ9rz3FrFtnfci","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_016uxLNmL6mYJSMTG7XkgNei","name":"Grep","input":{"pattern":"get_first_embedding_model|get_all_with_type.*model","glob":"**/*.py","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":6064,"cache_read_input_tokens":23804,"output_tokens":155,"service_tier":"standard"}},"requestId":"req_011CRF9S5x8DoEqFhAxBhJyh","type":"assistant","uuid":"92256bde-2df2-4772-84a0-b841d0ceaafd","timestamp":"2025-07-18T20:51:57.795Z"}
{"parentUuid":"92256bde-2df2-4772-84a0-b841d0ceaafd","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016uxLNmL6mYJSMTG7XkgNei","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:23:        return ListModelsResponse(data=await self.get_all_with_type(\"model\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:26:        models = await self.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:54:    async def _get_first_embedding_model(self) -> tuple[str, int] | None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:58:            all_models = await self.routing_table.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:153:                embedding_model_info = await self._get_first_embedding_model()"}]},"uuid":"342626f6-02fe-4551-b49b-d5cb74b554a2","timestamp":"2025-07-18T20:51:57.845Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:23:        return ListModelsResponse(data=await self.get_all_with_type(\"model\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:26:        models = await self.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:54:    async def _get_first_embedding_model(self) -> tuple[str, int] | None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:58:            all_models = await self.routing_table.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:153:                embedding_model_info = await self._get_first_embedding_model()","numLines":5}}
{"parentUuid":"342626f6-02fe-4551-b49b-d5cb74b554a2","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01P3aQPRuwqjtRArJrjTRqR7","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me check the routing table implementation to understand how models are stored and retrieved:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":525,"cache_read_input_tokens":29868,"output_tokens":75,"service_tier":"standard"}},"requestId":"req_011CRF9SVf9dyxrzDVdDRYJq","type":"assistant","uuid":"e02416fc-9cc9-40ff-bf08-6341fc77da4e","timestamp":"2025-07-18T20:52:00.297Z"}
{"parentUuid":"e02416fc-9cc9-40ff-bf08-6341fc77da4e","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01P3aQPRuwqjtRArJrjTRqR7","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01WVeNP2XXZNGGbcJmSwvhMG","name":"Glob","input":{"pattern":"**/routing_table*"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":525,"cache_read_input_tokens":29868,"output_tokens":75,"service_tier":"standard"}},"requestId":"req_011CRF9SVf9dyxrzDVdDRYJq","type":"assistant","uuid":"9206e9b5-c5ab-4149-aa60-6b940e23838c","timestamp":"2025-07-18T20:52:00.770Z"}
{"parentUuid":"9206e9b5-c5ab-4149-aa60-6b940e23838c","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WVeNP2XXZNGGbcJmSwvhMG","type":"tool_result","content":"No files found"}]},"uuid":"dac01939-5da2-4023-9d69-69ec3698c065","timestamp":"2025-07-18T20:52:00.800Z","toolUseResult":{"filenames":[],"durationMs":12,"numFiles":0,"truncated":false}}
{"parentUuid":"dac01939-5da2-4023-9d69-69ec3698c065","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_015s51QENKvnWscCFTEHyWWC","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01PfaqQuwZhC1wcJkrxfEu7r","name":"Glob","input":{"pattern":"**/*routing*table*"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":90,"cache_read_input_tokens":30393,"output_tokens":57,"service_tier":"standard"}},"requestId":"req_011CRF9SiixfvwJnfqFWXwCe","type":"assistant","uuid":"3e5cf159-a052-4ae4-8f37-364b3a3440dd","timestamp":"2025-07-18T20:52:04.193Z"}
{"parentUuid":"3e5cf159-a052-4ae4-8f37-364b3a3440dd","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PfaqQuwZhC1wcJkrxfEu7r","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/routers/test_routing_tables.py"}]},"uuid":"ff7601fd-ade1-4cf8-bcc8-7f17a2750bfd","timestamp":"2025-07-18T20:52:04.247Z","toolUseResult":{"filenames":["/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/routers/test_routing_tables.py"],"durationMs":10,"numFiles":1,"truncated":false}}
{"parentUuid":"ff7601fd-ade1-4cf8-bcc8-7f17a2750bfd","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01Ai75GYiMAePmbxxdiE7Qgp","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01K62ws2Mm9D5TtXpRdgPuL5","name":"Grep","input":{"pattern":"class.*RoutingTable|get_all_with_type","glob":"**/*.py","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":117,"cache_read_input_tokens":30483,"output_tokens":121,"service_tier":"standard"}},"requestId":"req_011CRF9SyLnJz1HPik6wrqRZ","type":"assistant","uuid":"6eda4194-3dac-4845-af62-beed43ec0a26","timestamp":"2025-07-18T20:52:07.320Z"}
{"parentUuid":"6eda4194-3dac-4845-af62-beed43ec0a26","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01K62ws2Mm9D5TtXpRdgPuL5","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/datatypes.py:115:class RoutingTable(Protocol):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/datatypes.py:123:class RoutingTableProviderSpec(ProviderSpec):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:22:class VectorDBsRoutingTable(CommonRoutingTableImpl, VectorDBs):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:24:        return ListVectorDBsResponse(data=await self.get_all_with_type(\"vector_db\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/toolgroups.py:28:class ToolGroupsRoutingTable(CommonRoutingTableImpl, ToolGroups):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/toolgroups.py:51:            toolgroups = await self.get_all_with_type(\"tool_group\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/toolgroups.py:85:        return ListToolGroupsResponse(data=await self.get_all_with_type(\"tool_group\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py:71:class CommonRoutingTableImpl(RoutingTable):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py:212:    async def get_all_with_type(self, type: str) -> list[RoutableObjectWithProvider]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/shields.py:21:class ShieldsRoutingTable(CommonRoutingTableImpl, Shields):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/shields.py:23:        return ListShieldsResponse(data=await self.get_all_with_type(ResourceType.shield.value))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/benchmarks.py:20:class BenchmarksRoutingTable(CommonRoutingTableImpl, Benchmarks):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/benchmarks.py:22:        return ListBenchmarksResponse(data=await self.get_all_with_type(\"benchmark\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/scoring_functions.py:25:class ScoringFunctionsRoutingTable(CommonRoutingTableImpl, ScoringFunctions):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/scoring_functions.py:27:        return ListScoringFunctionsResponse(data=await self.get_all_with_type(ResourceType.scoring_function.value))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/datasets.py:31:class DatasetsRoutingTable(CommonRoutingTableImpl, Datasets):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/datasets.py:33:        return ListDatasetsResponse(data=await self.get_all_with_type(ResourceType.dataset.value))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:21:class ModelsRoutingTable(CommonRoutingTableImpl, Models):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:23:        return ListModelsResponse(data=await self.get_all_with_type(\"model\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:26:        models = await self.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:58:            all_models = await self.routing_table.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:190:        vector_dbs = await self.routing_table.get_all_with_type(\"vector_db\")"}]},"uuid":"a93ccd7f-83fd-4fc3-875f-76b7615400dd","timestamp":"2025-07-18T20:52:07.369Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/datatypes.py:115:class RoutingTable(Protocol):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/datatypes.py:123:class RoutingTableProviderSpec(ProviderSpec):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:22:class VectorDBsRoutingTable(CommonRoutingTableImpl, VectorDBs):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/vector_dbs.py:24:        return ListVectorDBsResponse(data=await self.get_all_with_type(\"vector_db\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/toolgroups.py:28:class ToolGroupsRoutingTable(CommonRoutingTableImpl, ToolGroups):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/toolgroups.py:51:            toolgroups = await self.get_all_with_type(\"tool_group\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/toolgroups.py:85:        return ListToolGroupsResponse(data=await self.get_all_with_type(\"tool_group\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py:71:class CommonRoutingTableImpl(RoutingTable):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py:212:    async def get_all_with_type(self, type: str) -> list[RoutableObjectWithProvider]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/shields.py:21:class ShieldsRoutingTable(CommonRoutingTableImpl, Shields):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/shields.py:23:        return ListShieldsResponse(data=await self.get_all_with_type(ResourceType.shield.value))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/benchmarks.py:20:class BenchmarksRoutingTable(CommonRoutingTableImpl, Benchmarks):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/benchmarks.py:22:        return ListBenchmarksResponse(data=await self.get_all_with_type(\"benchmark\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/scoring_functions.py:25:class ScoringFunctionsRoutingTable(CommonRoutingTableImpl, ScoringFunctions):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/scoring_functions.py:27:        return ListScoringFunctionsResponse(data=await self.get_all_with_type(ResourceType.scoring_function.value))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/datasets.py:31:class DatasetsRoutingTable(CommonRoutingTableImpl, Datasets):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/datasets.py:33:        return ListDatasetsResponse(data=await self.get_all_with_type(ResourceType.dataset.value))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:21:class ModelsRoutingTable(CommonRoutingTableImpl, Models):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:23:        return ListModelsResponse(data=await self.get_all_with_type(\"model\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/models.py:26:        models = await self.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:58:            all_models = await self.routing_table.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:190:        vector_dbs = await self.routing_table.get_all_with_type(\"vector_db\")","numLines":22}}
{"parentUuid":"a93ccd7f-83fd-4fc3-875f-76b7615400dd","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01XaDGJF91MgZdhKGxoPwbnZ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Let me examine the common routing table implementation:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1728,"cache_read_input_tokens":30600,"output_tokens":143,"service_tier":"standard"}},"requestId":"req_011CRF9TCN7vgAya2dLDVmUx","type":"assistant","uuid":"39d10080-e3e9-4149-8695-aeda16bb37b2","timestamp":"2025-07-18T20:52:10.732Z"}
{"parentUuid":"39d10080-e3e9-4149-8695-aeda16bb37b2","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01XaDGJF91MgZdhKGxoPwbnZ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01WGdM5GGaiwZvsKgPyNaFVP","name":"Read","input":{"file_path":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py","offset":200,"limit":30}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":1728,"cache_read_input_tokens":30600,"output_tokens":143,"service_tier":"standard"}},"requestId":"req_011CRF9TCN7vgAya2dLDVmUx","type":"assistant","uuid":"c7b035e2-f5b6-4fa0-9021-7a6d66930180","timestamp":"2025-07-18T20:52:12.192Z"}
{"parentUuid":"c7b035e2-f5b6-4fa0-9021-7a6d66930180","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WGdM5GGaiwZvsKgPyNaFVP","type":"tool_result","content":"   200→            logger.info(f\"Setting owner for {obj.type} '{obj.identifier}' to {obj.owner.principal}\")\n   201→\n   202→        registered_obj = await register_object_with_provider(obj, p)\n   203→        # TODO: This needs to be fixed for all APIs once they return the registered object\n   204→        if obj.type == ResourceType.model.value:\n   205→            await self.dist_registry.register(registered_obj)\n   206→            return registered_obj\n   207→\n   208→        else:\n   209→            await self.dist_registry.register(obj)\n   210→            return obj\n   211→\n   212→    async def get_all_with_type(self, type: str) -> list[RoutableObjectWithProvider]:\n   213→        objs = await self.dist_registry.get_all()\n   214→        filtered_objs = [obj for obj in objs if obj.type == type]\n   215→\n   216→        # Apply attribute-based access control filtering\n   217→        if filtered_objs:\n   218→            filtered_objs = [\n   219→                obj for obj in filtered_objs if is_action_allowed(self.policy, \"read\", obj, get_authenticated_user())\n   220→            ]\n   221→\n   222→        return filtered_objs\n   223→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"39e988b7-5e70-4759-b7ec-a1a7271606e0","timestamp":"2025-07-18T20:52:12.237Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py","content":"            logger.info(f\"Setting owner for {obj.type} '{obj.identifier}' to {obj.owner.principal}\")\n\n        registered_obj = await register_object_with_provider(obj, p)\n        # TODO: This needs to be fixed for all APIs once they return the registered object\n        if obj.type == ResourceType.model.value:\n            await self.dist_registry.register(registered_obj)\n            return registered_obj\n\n        else:\n            await self.dist_registry.register(obj)\n            return obj\n\n    async def get_all_with_type(self, type: str) -> list[RoutableObjectWithProvider]:\n        objs = await self.dist_registry.get_all()\n        filtered_objs = [obj for obj in objs if obj.type == type]\n\n        # Apply attribute-based access control filtering\n        if filtered_objs:\n            filtered_objs = [\n                obj for obj in filtered_objs if is_action_allowed(self.policy, \"read\", obj, get_authenticated_user())\n            ]\n\n        return filtered_objs\n","numLines":24,"startLine":200,"totalLines":223}}}
{"parentUuid":"39e988b7-5e70-4759-b7ec-a1a7271606e0","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01GtrK84VyxA6UwBk4KFgWoe","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me search for where models are loaded from run.yaml or similar configuration files:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":556,"cache_read_input_tokens":32328,"output_tokens":137,"service_tier":"standard"}},"requestId":"req_011CRF9TZJhLhGVDF66VL7KD","type":"assistant","uuid":"8ec821d5-ca91-4e32-bdf3-714553646834","timestamp":"2025-07-18T20:52:15.452Z"}
{"parentUuid":"8ec821d5-ca91-4e32-bdf3-714553646834","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01GtrK84VyxA6UwBk4KFgWoe","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_011s3ADEWsHT4TcnA835f4rS","name":"Grep","input":{"pattern":"run\\.yaml|run_config|default_models","glob":"**/*.py","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":556,"cache_read_input_tokens":32328,"output_tokens":137,"service_tier":"standard"}},"requestId":"req_011CRF9TZJhLhGVDF66VL7KD","type":"assistant","uuid":"ee05c25f-d26c-423f-ac4b-915425311c39","timestamp":"2025-07-18T20:52:16.886Z"}
{"parentUuid":"ee05c25f-d26c-423f-ac4b-915425311c39","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011s3ADEWsHT4TcnA835f4rS","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/scripts/run_client_sdk_tests.py:41:            template_configs = list(d.rglob(\"run.yaml\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/scripts/provider_codegen.py:127:            \"sample_config\": getattr(config_class, \"sample_run_config\", None),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/server/test_resolver.py:50:    def sample_run_config(cls, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/server/test_resolver.py:81:    run_config = StackRunConfig(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/server/test_resolver.py:88:                    config=SampleConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/server/test_resolver.py:104:    impls = await resolve_impls(run_config, provider_registry, dist_registry, policy={})\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:130:    default_models: list[ModelInput] | None = None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:138:    def run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:169:                if hasattr(config_class, \"sample_run_config\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:170:                    config = config_class.sample_run_config(__distro_dir__=f\"~/.llama/distributions/{name}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:193:            or SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:198:            or SqliteSqlStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:202:            \"models\": [m.model_dump(exclude_none=True) for m in (self.default_models or [])],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:226:    run_configs: dict[str, RunConfigSettings]\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:230:    run_config_env_vars: dict[str, tuple[str, str]] | None = None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:242:        for run_config in self.run_configs.values():\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:243:            run_config_ = run_config.run_config(self.name, self.providers, self.container_image)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:249:            if run_config_.get(\"inference_store\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:250:                additional_pip_packages.extend(get_sql_pip_packages(run_config_[\"inference_store\"]))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:252:            if run_config_.get(\"metadata_store\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:253:                additional_pip_packages.extend(get_kv_pip_packages(run_config_[\"metadata_store\"]))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:294:        default_models = []\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:305:                    default_models.append(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:317:            run_config_env_vars=self.run_config_env_vars,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:318:            default_models=default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:342:        for yaml_pth, settings in self.run_configs.items():\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:343:            run_config = settings.run_config(self.name, self.providers, self.container_image)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:346:                    {k: v for k, v in run_config.items() if v is not None},\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:93:async def register_resources(run_config: StackRunConfig, impls: dict[Api, Any]):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:95:        objects = getattr(run_config, rsrc)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:293:def add_internal_implementations(impls: dict[Api, Any], run_config: StackRunConfig) -> None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:298:        run_config: Stack run configuration\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:301:        DistributionInspectConfig(run_config=run_config),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:307:        ProviderImplConfig(run_config=run_config),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:316:    run_config: StackRunConfig, provider_registry: ProviderRegistry | None = None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:318:    dist_registry, _ = await create_dist_registry(run_config.metadata_store, run_config.image_name)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:319:    policy = run_config.server.auth.access_policy if run_config.server.auth else []\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:321:        run_config, provider_registry or get_provider_registry(run_config), dist_registry, policy\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:325:    add_internal_implementations(impls, run_config)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:327:    await register_resources(run_config, impls)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:331:def get_stack_run_config_from_template(template: str) -> StackRunConfig:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:332:    template_path = importlib.resources.files(\"llama_stack\") / f\"templates/{template}/run.yaml\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:337:        run_config = yaml.safe_load(path.open())\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:339:    return StackRunConfig(**replace_env_vars(run_config))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:342:def run_config_from_adhoc_config_spec(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:373:        # call method \"sample_run_config\" on the provider spec config class\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:375:        provider_config = replace_env_vars(provider_config_type.sample_run_config(__distro_dir__=distro_dir))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:40:        config=WatsonXConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:46:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:72:    default_models, _ = get_model_registry(available_models)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:81:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:82:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:86:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:90:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:119:    run_config: StackRunConfig,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:134:        run_config, provider_registry, routing_table_apis, router_apis\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:137:    apis_to_serve = run_config.apis or set(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:143:    sorted_providers = sort_providers_by_deps(providers_with_specs, run_config)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:145:    return await instantiate_providers(sorted_providers, router_apis, dist_registry, run_config, policy)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:190:    run_config: StackRunConfig, provider_registry: ProviderRegistry, routing_table_apis: set[Api], router_apis: set[Api]\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:195:    for api_str, providers in run_config.providers.items():\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:234:    providers_with_specs: dict[str, dict[str, ProviderWithSpec]], run_config: StackRunConfig\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:251:    run_config: StackRunConfig,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:271:        impl = await instantiate_provider(provider, deps, inner_impls, dist_registry, run_config, policy)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:321:    run_config: StackRunConfig,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:342:        args = [provider_spec.api, deps[provider_spec.routing_table_api], deps, run_config, policy]\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:43:        config=VLLMConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:48:        config=FaissVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:53:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:86:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:87:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:92:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:96:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/library_client.py:43:    get_stack_run_config_from_template,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/library_client.py:207:            config = get_stack_run_config_from_template(config_path_or_template_name)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:146:    if hasattr(config_class, \"sample_run_config\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:147:        config: dict[str, Any] = config_class.sample_run_config()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:223:            config=FaissVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:228:            config=SQLiteVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:233:            config=MilvusVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:238:            config=ChromaVectorIOConfig.sample_run_config(url=\"${env.CHROMADB_URL:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:243:            config=PGVectorVectorIOConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:273:        config=LocalfsFilesImplConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:278:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:283:        config=HuggingFacePostTrainingConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:304:    default_models, ids_conflict_in_models = get_model_registry(available_models)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:318:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:319:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:326:                default_models=[embedding_model] + default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:332:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/inspect.py:24:    run_config: StackRunConfig\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/inspect.py:42:        run_config: StackRunConfig = self.config.run_config\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/inspect.py:60:                providers = run_config.providers.get(api.value, [])\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:31:            config=VLLMInferenceAdapterConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:55:            config=ChromaVectorIOConfig.sample_run_config(url=\"${env.CHROMADB_URL:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:69:    default_models = [\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:78:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:88:    postgres_config = PostgresSqlStoreConfig.sample_run_config()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:97:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:98:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:124:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:127:                metadata_store=PostgresKVStoreConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:131:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:35:        config=NVIDIAConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:40:        config=NVIDIASafetyConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:45:        config=NvidiaDatasetIOConfig.sample_run_co\n\n... [177 lines truncated] ..."}]},"uuid":"23440ad4-325c-4759-9484-153fe7544d38","timestamp":"2025-07-18T20:52:16.920Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/scripts/run_client_sdk_tests.py:41:            template_configs = list(d.rglob(\"run.yaml\"))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/scripts/provider_codegen.py:127:            \"sample_config\": getattr(config_class, \"sample_run_config\", None),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/server/test_resolver.py:50:    def sample_run_config(cls, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/server/test_resolver.py:81:    run_config = StackRunConfig(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/server/test_resolver.py:88:                    config=SampleConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/server/test_resolver.py:104:    impls = await resolve_impls(run_config, provider_registry, dist_registry, policy={})\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:130:    default_models: list[ModelInput] | None = None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:138:    def run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:169:                if hasattr(config_class, \"sample_run_config\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:170:                    config = config_class.sample_run_config(__distro_dir__=f\"~/.llama/distributions/{name}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:193:            or SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:198:            or SqliteSqlStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:202:            \"models\": [m.model_dump(exclude_none=True) for m in (self.default_models or [])],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:226:    run_configs: dict[str, RunConfigSettings]\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:230:    run_config_env_vars: dict[str, tuple[str, str]] | None = None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:242:        for run_config in self.run_configs.values():\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:243:            run_config_ = run_config.run_config(self.name, self.providers, self.container_image)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:249:            if run_config_.get(\"inference_store\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:250:                additional_pip_packages.extend(get_sql_pip_packages(run_config_[\"inference_store\"]))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:252:            if run_config_.get(\"metadata_store\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:253:                additional_pip_packages.extend(get_kv_pip_packages(run_config_[\"metadata_store\"]))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:294:        default_models = []\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:305:                    default_models.append(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:317:            run_config_env_vars=self.run_config_env_vars,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:318:            default_models=default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:342:        for yaml_pth, settings in self.run_configs.items():\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:343:            run_config = settings.run_config(self.name, self.providers, self.container_image)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/template.py:346:                    {k: v for k, v in run_config.items() if v is not None},\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:93:async def register_resources(run_config: StackRunConfig, impls: dict[Api, Any]):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:95:        objects = getattr(run_config, rsrc)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:293:def add_internal_implementations(impls: dict[Api, Any], run_config: StackRunConfig) -> None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:298:        run_config: Stack run configuration\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:301:        DistributionInspectConfig(run_config=run_config),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:307:        ProviderImplConfig(run_config=run_config),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:316:    run_config: StackRunConfig, provider_registry: ProviderRegistry | None = None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:318:    dist_registry, _ = await create_dist_registry(run_config.metadata_store, run_config.image_name)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:319:    policy = run_config.server.auth.access_policy if run_config.server.auth else []\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:321:        run_config, provider_registry or get_provider_registry(run_config), dist_registry, policy\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:325:    add_internal_implementations(impls, run_config)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:327:    await register_resources(run_config, impls)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:331:def get_stack_run_config_from_template(template: str) -> StackRunConfig:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:332:    template_path = importlib.resources.files(\"llama_stack\") / f\"templates/{template}/run.yaml\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:337:        run_config = yaml.safe_load(path.open())\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:339:    return StackRunConfig(**replace_env_vars(run_config))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:342:def run_config_from_adhoc_config_spec(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:373:        # call method \"sample_run_config\" on the provider spec config class\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py:375:        provider_config = replace_env_vars(provider_config_type.sample_run_config(__distro_dir__=distro_dir))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:40:        config=WatsonXConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:46:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:72:    default_models, _ = get_model_registry(available_models)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:81:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:82:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:86:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/watsonx.py:90:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:119:    run_config: StackRunConfig,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:134:        run_config, provider_registry, routing_table_apis, router_apis\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:137:    apis_to_serve = run_config.apis or set(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:143:    sorted_providers = sort_providers_by_deps(providers_with_specs, run_config)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:145:    return await instantiate_providers(sorted_providers, router_apis, dist_registry, run_config, policy)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:190:    run_config: StackRunConfig, provider_registry: ProviderRegistry, routing_table_apis: set[Api], router_apis: set[Api]\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:195:    for api_str, providers in run_config.providers.items():\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:234:    providers_with_specs: dict[str, dict[str, ProviderWithSpec]], run_config: StackRunConfig\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:251:    run_config: StackRunConfig,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:271:        impl = await instantiate_provider(provider, deps, inner_impls, dist_registry, run_config, policy)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:321:    run_config: StackRunConfig,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/resolver.py:342:        args = [provider_spec.api, deps[provider_spec.routing_table_api], deps, run_config, policy]\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:43:        config=VLLMConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:48:        config=FaissVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:53:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:86:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:87:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:92:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/vllm.py:96:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/library_client.py:43:    get_stack_run_config_from_template,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/library_client.py:207:            config = get_stack_run_config_from_template(config_path_or_template_name)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:146:    if hasattr(config_class, \"sample_run_config\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:147:        config: dict[str, Any] = config_class.sample_run_config()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:223:            config=FaissVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:228:            config=SQLiteVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:233:            config=MilvusVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:238:            config=ChromaVectorIOConfig.sample_run_config(url=\"${env.CHROMADB_URL:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:243:            config=PGVectorVectorIOConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:273:        config=LocalfsFilesImplConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:278:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:283:        config=HuggingFacePostTrainingConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:304:    default_models, ids_conflict_in_models = get_model_registry(available_models)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:318:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:319:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:326:                default_models=[embedding_model] + default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/starter.py:332:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/inspect.py:24:    run_config: StackRunConfig\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/inspect.py:42:        run_config: StackRunConfig = self.config.run_config\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/inspect.py:60:                providers = run_config.providers.get(api.value, [])\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:31:            config=VLLMInferenceAdapterConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:55:            config=ChromaVectorIOConfig.sample_run_config(url=\"${env.CHROMADB_URL:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:69:    default_models = [\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:78:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:88:    postgres_config = PostgresSqlStoreConfig.sample_run_config()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:97:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:98:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:124:                default_models=default_models + [embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:127:                metadata_store=PostgresKVStoreConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/postgres_demo.py:131:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:35:        config=NVIDIAConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:40:        config=NVIDIASafetyConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:45:        config=NvidiaDatasetIOConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:50:        config=NVIDIAEvalConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:71:    default_models, _ = get_model_registry(available_models)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:80:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:81:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:87:                default_models=default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:98:                default_models=[inference_model, safety_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/nvidia.py:103:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/test_distribution.py:26:    def sample_run_config(cls, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/test_distribution.py:57:                    config=SampleConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/test_distribution.py:179:                        config=SampleConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:49:            OpenAIConfig.sample_run_config(api_key=\"${env.OPENAI_API_KEY:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:59:            AnthropicConfig.sample_run_config(api_key=\"${env.ANTHROPIC_API_KEY:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:69:            GeminiConfig.sample_run_config(api_key=\"${env.GEMINI_API_KEY:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:74:            GroqConfig.sample_run_config(api_key=\"${env.GROQ_API_KEY:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:79:            TogetherImplConfig.sample_run_config(api_key=\"${env.TOGETHER_API_KEY:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:120:            config=SQLiteVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:125:            config=ChromaVectorIOConfig.sample_run_config(url=\"${env.CHROMADB_URL:=}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:130:            config=PGVectorVectorIOConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:151:    default_models = models + [\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:263:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:264:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:269:                default_models=default_models,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/open_benchmark.py:276:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/build.py:102:    run_config: str | None = None,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/build.py:121:        if run_config is not None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/build.py:122:            args.append(run_config)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/test_build_path.py:19:    def spy_build_image(cfg, build_file_path, image_name, template_or_config, run_config=None):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/test_build_path.py:21:        called_with[\"run_config\"] = run_config\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/distribution/test_build_path.py:40:    assert called_with[\"run_config\"] is None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/sqlstore/sqlstore.py:49:    def sample_run_config(cls, __distro_dir__: str, db_name: str = \"sqlstore.db\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/sqlstore/sqlstore.py:77:    def sample_run_config(cls, **kwargs):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:47:        config=MetaReferenceInferenceConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:55:        config=SentenceTransformersInferenceConfig.sample_run_config(),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:60:        config=FaissVectorIOConfig.sample_run_config(f\"~/.llama/distributions/{name}\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:96:        run_configs={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:97:            \"run.yaml\": RunConfigSettings(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:102:                default_models=[inference_model, embedding_model],\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:113:                            config=MetaReferenceInferenceConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:121:                default_models=[\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/meta_reference.py:130:        run_config_env_vars={\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:23:from llama_stack.distribution.stack import run_config_from_adhoc_config_spec\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:237:        run_config = run_config_from_adhoc_config_spec(config)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:238:        run_config_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".yaml\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:239:        with open(run_config_file.name, \"w\") as f:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:240:            yaml.dump(run_config.model_dump(), f)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/fixtures/common.py:241:        config = run_config_file.name\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/conftest.py:69:            (b) a path to a run.yaml file, or\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/test_configs.py:40:        assert hasattr(config_type, \"sample_run_config\"), f\"{config_class_name} does not have sample_run_config method\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/test_configs.py:42:        sample_config = config_type.sample_run_config(__distro_dir__=\"foobarbaz\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/providers/test_configs.py:43:        assert isinstance(sample_config, dict), f\"{config_class_name}.sample_run_config() did not return a dict\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/run.py:99:            config_file = Path(REPO_ROOT) / \"llama_stack\" / \"templates\" / args.config / \"run.yaml\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/run.py:105:            config_file = Path(DISTRIBS_BASE_DIR / f\"llamastack-{args.config}\" / f\"{args.config}-run.yaml\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/run.py:109:                f\"File {str(config_file)} does not exist.\\n\\nPlease run `llama stack build` to generate (and optionally edit) a run.yaml file\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/agents/test_persistence.py:61:    run_config = agents_stack.run_config\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/agents/test_persistence.py:62:    provider_config = run_config.providers[\"agents\"][0].config\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/agents/test_persistence.py:103:    provider_config = agents_stack.run_config.providers[\"agents\"][0].config\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/server/server.py:420:        config_file = Path(REPO_ROOT) / \"llama_stack\" / \"templates\" / args.template / \"run.yaml\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/providers.py:23:    run_config: StackRunConfig\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/providers.py:45:        run_config = self.config.run_config\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/providers.py:46:        safe_config = StackRunConfig(**redact_sensitive_fields(run_config.model_dump()))\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/list_stacks.py:53:            run_config = \"Yes\" if (path / f\"{name}-run.yaml\").exists() else \"No\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/list_stacks.py:54:            row.extend([build_config, run_config])\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:247:        run_config = _run_stack_build_command_from_build_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:266:    if run_config is None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:275:        config_dict = yaml.safe_load(run_config.read_text())\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:280:        run_args.extend([str(os.getenv(\"LLAMA_STACK_PORT\", 8321)), \"--config\", run_config])\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:284:def _generate_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:290:    Generate a run.yaml template file for user to edit from a build.yaml file\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:293:    run_config = StackRunConfig(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:305:        run_config.providers[api] = []\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:333:            if config_type is not None and hasattr(config_type, \"sample_run_config\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:334:                config = config_type.sample_run_config(__distro_dir__=f\"~/.llama/distributions/{image_name}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:343:            run_config.providers[api].append(p_spec)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:345:    run_config_file = build_dir / f\"{image_name}-run.yaml\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:347:    with open(run_config_file, \"w\") as f:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:348:        to_write = json.loads(run_config.model_dump_json())\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:353:    # For non-container builds, the run.yaml is generated at the very end of the build process so it\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:356:        cprint(f\"You can now run your stack with `llama stack run {run_config_file}`\", color=\"green\", file=sys.stderr)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:357:    return run_config_file\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:396:    run_config_file = None\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:397:    # Generate the run.yaml so it can be included in the container image with the proper entrypoint\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:400:        cprint(\"Generating run.yaml file\", color=\"yellow\", file=sys.stderr)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:401:        run_config_file = _generate_run_config(build_config, build_dir, image_name)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:412:        run_config=run_config_file.as_posix() if run_config_file else None,\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:418:        # copy run.yaml from template to build_dir instead of generating it again\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:419:        template_path = importlib.resources.files(\"llama_stack\") / f\"templates/{template_name}/run.yaml\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:420:        run_config_file = build_dir / f\"{template_name}-run.yaml\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:423:            shutil.copy(path, run_config_file)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:426:        cprint(f\"You can find the newly-built template here: {run_config_file}\", color=\"blue\", file=sys.stderr)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:429:            + colored(f\"llama stack run {run_config_file} --image-type {build_config.image_type}\", \"blue\"),\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/cli/stack/_build.py:435:        return _generate_run_config(build_config, build_dir, image_name)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/__init__.py:50:    api: Api, routing_table: RoutingTable, deps: dict[str, Any], run_config: StackRunConfig, policy: list[AccessRule]\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/__init__.py:80:    if api == Api.inference and run_config.inference_store:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/__init__.py:81:        inference_store = InferenceStore(run_config.inference_store, policy)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/log.py:74:    Helper function to parse a yaml logging configuration found in the run.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/log.py:77:        yaml_config (Logging): the logger config object found in the run.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/weaviate/config.py:34:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/weaviate/config.py:36:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/qdrant/config.py:36:    def sample_run_config(cls, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/bedrock/config.py:58:    def sample_run_config(cls, **kwargs):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/config.py:36:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/pgvector/config.py:52:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/chroma/config.py:24:    def sample_run_config(cls, url: str = \"${env.CHROMADB_URL}\", **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/datasetio/localfs/config.py:20:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/datasetio/localfs/config.py:22:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/milvus/config.py:35:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/vector_io/milvus/config.py:39:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/kvstore/config.py:44:    def sample_run_config(cls):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/kvstore/config.py:64:    def sample_run_config(cls, __distro_dir__: str, db_name: str = \"kvstore.db\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/kvstore/config.py:81:    def sample_run_config(cls, table_name: str = \"llamastack_kvstore\", **kwargs):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/utils/kvstore/config.py:127:    def sample_run_config(cls, collection_name: str = \"llamastack_kvstore\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/tool_runtime/wolfram_alpha/config.py:18:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/agents/meta_reference/config.py:21:    def sample_run_config(cls, __distro_dir__: str) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/agents/meta_reference/config.py:23:            \"persistence_store\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/agents/meta_reference/config.py:27:            \"responses_store\": SqliteSqlStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/tool_runtime/tavily_search/config.py:23:    def sample_run_config(cls, __distro_dir__: str) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/sqlite_vec/config.py:30:    def sample_run_config(cls, __distro_dir__: str) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/sqlite_vec/config.py:33:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/tool_runtime/model_context_protocol/config.py:19:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/files/localfs/config.py:24:    def sample_run_config(cls, __distro_dir__: str) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/files/localfs/config.py:27:            \"metadata_store\": SqliteSqlStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/safety/prompt_guard/config.py:29:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/inference/vllm/config.py:45:    def sample_run_config(cls, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/safety/code_scanner/config.py:14:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/safety/llama_guard/config.py:16:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/tool_runtime/brave_search/config.py:23:    def sample_run_config(cls, __distro_dir__: str) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/qdrant/config.py:20:    def sample_run_config(cls, __distro_dir__: str) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/scoring/llm_as_judge/config.py:13:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/inference/sentence_transformers/config.py:14:    def sample_run_config(cls, **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/eval/meta_reference/config.py:20:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/eval/meta_reference/config.py:22:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/safety/sambanova/config.py:33:    def sample_run_config(cls, api_key: str = \"${env.SAMBANOVA_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/post_training/torchtune/config.py:17:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/scoring/basic/config.py:13:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/safety/nvidia/config.py:36:    def sample_run_config(cls, **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/milvus/config.py:33:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/milvus/config.py:36:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/inference/meta_reference/config.py:19:    # models in the resouce list in the run.yaml config will be registered automatically\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/inference/meta_reference/config.py:49:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/tool_runtime/bing_search/config.py:19:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/scoring/braintrust/config.py:18:    def sample_run_config(cls, **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/telemetry/meta_reference/config.py:49:    def sample_run_config(cls, __distro_dir__: str, db_name: str = \"trace_store.db\") -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/tool_runtime/rag/config.py:14:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/faiss/config.py:31:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/faiss/config.py:33:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/post_training/nvidia/config.py:56:    def sample_run_config(cls, **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/post_training/huggingface/config.py:71:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/vector_io/chroma/config.py:24:    def sample_run_config(cls, db_path: str = \"${env.CHROMADB_PATH}\", **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/eval/nvidia/config.py:26:    def sample_run_config(cls, **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/datasetio/huggingface/config.py:20:    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/datasetio/huggingface/config.py:22:            \"kvstore\": SqliteKVStoreConfig.sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/datasetio/nvidia/config.py:55:    def sample_run_config(cls, **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/anthropic/config.py:29:    def sample_run_config(cls, api_key: str = \"${env.ANTHROPIC_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/databricks/config.py:26:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/cerebras_openai_compat/config.py:34:    def sample_run_config(cls, api_key: str = \"${env.CEREBRAS_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/cerebras/config.py:29:    def sample_run_config(cls, api_key: str = \"${env.CEREBRAS_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/fireworks_openai_compat/config.py:34:    def sample_run_config(cls, api_key: str = \"${env.FIREWORKS_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/gemini/config.py:29:    def sample_run_config(cls, api_key: str = \"${env.GEMINI_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/groq/config.py:35:    def sample_run_config(cls, api_key: str = \"${env.GROQ_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/openai/config.py:29:    def sample_run_config(cls, api_key: str = \"${env.OPENAI_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/watsonx/config.py:41:    def sample_run_config(cls, **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/together/config.py:26:    def sample_run_config(cls, **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/fireworks/config.py:26:    def sample_run_config(cls, api_key: str = \"${env.FIREWORKS_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/ollama/config.py:18:    def sample_run_config(cls, url: str = \"${env.OLLAMA_URL:=http://localhost:11434}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/sambanova_openai_compat/config.py:34:    def sample_run_config(cls, api_key: str = \"${env.SAMBANOVA_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/llama_openai_compat/config.py:34:    def sample_run_config(cls, api_key: str = \"${env.LLAMA_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/together_openai_compat/config.py:34:    def sample_run_config(cls, api_key: str = \"${env.TOGETHER_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/groq_openai_compat/config.py:34:    def sample_run_config(cls, api_key: str = \"${env.GROQ_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/vllm/config.py:47:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/sambanova/config.py:33:    def sample_run_config(cls, api_key: str = \"${env.SAMBANOVA_API_KEY}\", **kwargs) -> dict[str, Any]:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/nvidia/config.py:56:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/passthrough/config.py:27:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/tgi/config.py:20:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/tgi/config.py:41:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/tgi/config.py:64:    def sample_run_config(\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/runpod/config.py:26:    def sample_run_config(cls, **kwargs: Any) -> dict[str, Any]:","numLines":282}}
{"parentUuid":"23440ad4-325c-4759-9484-153fe7544d38","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01LghJFVatjmPMSkkVWxs8tN","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Let me look at the stack.py file to understand how resources are registered from the run config:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":7263,"cache_read_input_tokens":32884,"output_tokens":150,"service_tier":"standard"}},"requestId":"req_011CRF9TunFHSDpSwp1URo8G","type":"assistant","uuid":"a98838b3-d84f-456d-ad82-e3b9c6818b8b","timestamp":"2025-07-18T20:52:19.822Z"}
{"parentUuid":"a98838b3-d84f-456d-ad82-e3b9c6818b8b","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01LghJFVatjmPMSkkVWxs8tN","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01GqniyYMbxxQ6UZCHi3U8mp","name":"Read","input":{"file_path":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py","offset":90,"limit":30}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":7263,"cache_read_input_tokens":32884,"output_tokens":150,"service_tier":"standard"}},"requestId":"req_011CRF9TunFHSDpSwp1URo8G","type":"assistant","uuid":"3a99071b-3e56-42e7-92bf-bbb447182302","timestamp":"2025-07-18T20:52:21.110Z"}
{"parentUuid":"3a99071b-3e56-42e7-92bf-bbb447182302","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GqniyYMbxxQ6UZCHi3U8mp","type":"tool_result","content":"    90→]\n    91→\n    92→\n    93→async def register_resources(run_config: StackRunConfig, impls: dict[Api, Any]):\n    94→    for rsrc, api, register_method, list_method in RESOURCES:\n    95→        objects = getattr(run_config, rsrc)\n    96→        if api not in impls:\n    97→            continue\n    98→\n    99→        method = getattr(impls[api], register_method)\n   100→        for obj in objects:\n   101→            logger.debug(f\"registering {rsrc.capitalize()} {obj} for provider {obj.provider_id}\")\n   102→            # Do not register models on disabled providers\n   103→            if hasattr(obj, \"provider_id\") and obj.provider_id is not None and obj.provider_id == \"__disabled__\":\n   104→                logger.debug(f\"Skipping {rsrc.capitalize()} registration for disabled provider.\")\n   105→                continue\n   106→            # In complex templates, like our starter template, we may have dynamic model ids\n   107→            # given by environment variables. This allows those environment variables to have\n   108→            # a default value of __disabled__ to skip registration of the model if not set.\n   109→            if (\n   110→                hasattr(obj, \"provider_model_id\")\n   111→                and obj.provider_model_id is not None\n   112→                and \"__disabled__\" in obj.provider_model_id\n   113→            ):\n   114→                logger.debug(f\"Skipping {rsrc.capitalize()} registration for disabled model.\")\n   115→                continue\n   116→\n   117→            if hasattr(obj, \"shield_id\") and obj.shield_id is not None and obj.shield_id == \"__disabled__\":\n   118→                logger.debug(f\"Skipping {rsrc.capitalize()} registration for disabled shield.\")\n   119→                continue\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"736f5629-4f26-4017-8c39-d86397210ac8","timestamp":"2025-07-18T20:52:21.142Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py","content":"]\n\n\nasync def register_resources(run_config: StackRunConfig, impls: dict[Api, Any]):\n    for rsrc, api, register_method, list_method in RESOURCES:\n        objects = getattr(run_config, rsrc)\n        if api not in impls:\n            continue\n\n        method = getattr(impls[api], register_method)\n        for obj in objects:\n            logger.debug(f\"registering {rsrc.capitalize()} {obj} for provider {obj.provider_id}\")\n            # Do not register models on disabled providers\n            if hasattr(obj, \"provider_id\") and obj.provider_id is not None and obj.provider_id == \"__disabled__\":\n                logger.debug(f\"Skipping {rsrc.capitalize()} registration for disabled provider.\")\n                continue\n            # In complex templates, like our starter template, we may have dynamic model ids\n            # given by environment variables. This allows those environment variables to have\n            # a default value of __disabled__ to skip registration of the model if not set.\n            if (\n                hasattr(obj, \"provider_model_id\")\n                and obj.provider_model_id is not None\n                and \"__disabled__\" in obj.provider_model_id\n            ):\n                logger.debug(f\"Skipping {rsrc.capitalize()} registration for disabled model.\")\n                continue\n\n            if hasattr(obj, \"shield_id\") and obj.shield_id is not None and obj.shield_id == \"__disabled__\":\n                logger.debug(f\"Skipping {rsrc.capitalize()} registration for disabled shield.\")\n                continue","numLines":30,"startLine":90,"totalLines":390}}}
{"parentUuid":"736f5629-4f26-4017-8c39-d86397210ac8","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01MJmEZEeieNWWqm6yJfVSsU","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me search for how the \"first model\" is selected specifically from the models registered in the routing table:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":702,"cache_read_input_tokens":40147,"output_tokens":147,"service_tier":"standard"}},"requestId":"req_011CRF9UDJCrWQ9xkGuu7666","type":"assistant","uuid":"19a8b2a4-919c-42b0-96a5-be7d99492a59","timestamp":"2025-07-18T20:52:24.766Z"}
{"parentUuid":"19a8b2a4-919c-42b0-96a5-be7d99492a59","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01MJmEZEeieNWWqm6yJfVSsU","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_019stYMcJvWzX3qMSpFXK8p4","name":"Grep","input":{"pattern":"embedding_models\\[0\\]|first.*embedding|order.*model","glob":"**/*.py","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":702,"cache_read_input_tokens":40147,"output_tokens":147,"service_tier":"standard"}},"requestId":"req_011CRF9UDJCrWQ9xkGuu7666","type":"assistant","uuid":"a0618c15-7c75-4ddd-a30d-0862fc73f274","timestamp":"2025-07-18T20:52:26.290Z"}
{"parentUuid":"a0618c15-7c75-4ddd-a30d-0862fc73f274","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019stYMcJvWzX3qMSpFXK8p4","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/agents/test_openai_responses.py:141:def test_list_response_input_items_with_limit_and_order(openai_client, client_with_models, text_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:54:    async def _get_first_embedding_model(self) -> tuple[str, int] | None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:55:        \"\"\"Get the first available embedding model identifier.\"\"\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:68:                dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:70:                    raise ValueError(f\"Embedding model {embedding_models[0].identifier} has no embedding dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:71:                return embedding_models[0].identifier, dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:153:                embedding_model_info = await self._get_first_embedding_model()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:157:                logger.info(f\"No embedding model specified, using first available: {embedding_model}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/scoring/basic/utils/bfcl/checker.py:925:        return parallel_function_checker_no_order(func_description, model_output, possible_answer, language, model_name)"}]},"uuid":"a6a8bc9f-30ea-438d-95ba-0993d842bb55","timestamp":"2025-07-18T20:52:26.359Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/integration/agents/test_openai_responses.py:141:def test_list_response_input_items_with_limit_and_order(openai_client, client_with_models, text_model_id):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:54:    async def _get_first_embedding_model(self) -> tuple[str, int] | None:\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:55:        \"\"\"Get the first available embedding model identifier.\"\"\"\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:68:                dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:70:                    raise ValueError(f\"Embedding model {embedding_models[0].identifier} has no embedding dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:71:                return embedding_models[0].identifier, dimension\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:153:                embedding_model_info = await self._get_first_embedding_model()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:157:                logger.info(f\"No embedding model specified, using first available: {embedding_model}\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/inline/scoring/basic/utils/bfcl/checker.py:925:        return parallel_function_checker_no_order(func_description, model_output, possible_answer, language, model_name)","numLines":9}}
{"parentUuid":"a6a8bc9f-30ea-438d-95ba-0993d842bb55","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01NaN8NAepCVvz5YJ3xGzS6U","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me check how models are loaded from the templates and their order in run.yaml files:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":825,"cache_read_input_tokens":40849,"output_tokens":80,"service_tier":"standard"}},"requestId":"req_011CRF9Ubd8XQbnsgCjBpef5","type":"assistant","uuid":"130c59f6-48a2-4c72-858f-4abbadfe0ea8","timestamp":"2025-07-18T20:52:29.270Z"}
{"parentUuid":"130c59f6-48a2-4c72-858f-4abbadfe0ea8","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01NaN8NAepCVvz5YJ3xGzS6U","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_011dwhNmr4PMWhujmTvYPKTS","name":"Glob","input":{"pattern":"**/templates/**/run.yaml"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":825,"cache_read_input_tokens":40849,"output_tokens":80,"service_tier":"standard"}},"requestId":"req_011CRF9Ubd8XQbnsgCjBpef5","type":"assistant","uuid":"bb4c7d22-4bbb-4360-9db8-8cd0b3a68219","timestamp":"2025-07-18T20:52:30.056Z"}
{"parentUuid":"bb4c7d22-4bbb-4360-9db8-8cd0b3a68219","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011dwhNmr4PMWhujmTvYPKTS","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/run.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/run.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/run.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/run.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/run.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/run.yaml\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/run.yaml"}]},"uuid":"e95af6f0-8d40-472b-beed-8e7e6924f43a","timestamp":"2025-07-18T20:52:30.085Z","toolUseResult":{"filenames":["/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/meta-reference-gpu/run.yaml","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/nvidia/run.yaml","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/postgres-demo/run.yaml","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/vllm-gpu/run.yaml","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/watsonx/run.yaml","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/open-benchmark/run.yaml","/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/run.yaml"],"durationMs":14,"numFiles":7,"truncated":false}}
{"parentUuid":"e95af6f0-8d40-472b-beed-8e7e6924f43a","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01CHqzg3ixTa5qNFAbfdtSjJ","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01YPAwYNvdVTkgxPvTfQGsU8","name":"Read","input":{"file_path":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/run.yaml"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":496,"cache_read_input_tokens":41674,"output_tokens":97,"service_tier":"standard"}},"requestId":"req_011CRF9Ut2cF5vNzb1xWzzJ1","type":"assistant","uuid":"c052f886-24b2-4523-a0d2-259429737e87","timestamp":"2025-07-18T20:52:34.226Z"}
{"parentUuid":"c052f886-24b2-4523-a0d2-259429737e87","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01YPAwYNvdVTkgxPvTfQGsU8","type":"tool_result","content":"     1→version: 2\n     2→image_name: starter\n     3→apis:\n     4→- agents\n     5→- datasetio\n     6→- eval\n     7→- files\n     8→- inference\n     9→- post_training\n    10→- safety\n    11→- scoring\n    12→- telemetry\n    13→- tool_runtime\n    14→- vector_io\n    15→providers:\n    16→  inference:\n    17→  - provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n    18→    provider_type: remote::cerebras\n    19→    config:\n    20→      base_url: https://api.cerebras.ai\n    21→      api_key: ${env.CEREBRAS_API_KEY}\n    22→  - provider_id: ${env.ENABLE_OLLAMA:=__disabled__}\n    23→    provider_type: remote::ollama\n    24→    config:\n    25→      url: ${env.OLLAMA_URL:=http://localhost:11434}\n    26→  - provider_id: ${env.ENABLE_VLLM:=__disabled__}\n    27→    provider_type: remote::vllm\n    28→    config:\n    29→      url: ${env.VLLM_URL}\n    30→      max_tokens: ${env.VLLM_MAX_TOKENS:=4096}\n    31→      api_token: ${env.VLLM_API_TOKEN:=fake}\n    32→      tls_verify: ${env.VLLM_TLS_VERIFY:=true}\n    33→  - provider_id: ${env.ENABLE_TGI:=__disabled__}\n    34→    provider_type: remote::tgi\n    35→    config:\n    36→      url: ${env.TGI_URL}\n    37→  - provider_id: ${env.ENABLE_HF_SERVERLESS:=__disabled__}\n    38→    provider_type: remote::hf::serverless\n    39→    config:\n    40→      huggingface_repo: ${env.INFERENCE_MODEL}\n    41→      api_token: ${env.HF_API_TOKEN}\n    42→  - provider_id: ${env.ENABLE_HF_ENDPOINT:=__disabled__}\n    43→    provider_type: remote::hf::endpoint\n    44→    config:\n    45→      endpoint_name: ${env.INFERENCE_ENDPOINT_NAME}\n    46→      api_token: ${env.HF_API_TOKEN}\n    47→  - provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n    48→    provider_type: remote::fireworks\n    49→    config:\n    50→      url: https://api.fireworks.ai/inference/v1\n    51→      api_key: ${env.FIREWORKS_API_KEY}\n    52→  - provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n    53→    provider_type: remote::together\n    54→    config:\n    55→      url: https://api.together.xyz/v1\n    56→      api_key: ${env.TOGETHER_API_KEY}\n    57→  - provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n    58→    provider_type: remote::bedrock\n    59→    config: {}\n    60→  - provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n    61→    provider_type: remote::databricks\n    62→    config:\n    63→      url: ${env.DATABRICKS_URL}\n    64→      api_token: ${env.DATABRICKS_API_TOKEN}\n    65→  - provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n    66→    provider_type: remote::nvidia\n    67→    config:\n    68→      url: ${env.NVIDIA_BASE_URL:=https://integrate.api.nvidia.com}\n    69→      api_key: ${env.NVIDIA_API_KEY:=}\n    70→      append_api_version: ${env.NVIDIA_APPEND_API_VERSION:=True}\n    71→  - provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n    72→    provider_type: remote::runpod\n    73→    config:\n    74→      url: ${env.RUNPOD_URL:=}\n    75→      api_token: ${env.RUNPOD_API_TOKEN}\n    76→  - provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n    77→    provider_type: remote::openai\n    78→    config:\n    79→      api_key: ${env.OPENAI_API_KEY}\n    80→  - provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n    81→    provider_type: remote::anthropic\n    82→    config:\n    83→      api_key: ${env.ANTHROPIC_API_KEY}\n    84→  - provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n    85→    provider_type: remote::gemini\n    86→    config:\n    87→      api_key: ${env.GEMINI_API_KEY}\n    88→  - provider_id: ${env.ENABLE_GROQ:=__disabled__}\n    89→    provider_type: remote::groq\n    90→    config:\n    91→      url: https://api.groq.com\n    92→      api_key: ${env.GROQ_API_KEY}\n    93→  - provider_id: ${env.ENABLE_FIREWORKS_OPENAI_COMPAT:=__disabled__}\n    94→    provider_type: remote::fireworks-openai-compat\n    95→    config:\n    96→      openai_compat_api_base: https://api.fireworks.ai/inference/v1\n    97→      api_key: ${env.FIREWORKS_API_KEY}\n    98→  - provider_id: ${env.ENABLE_LLAMA_OPENAI_COMPAT:=__disabled__}\n    99→    provider_type: remote::llama-openai-compat\n   100→    config:\n   101→      openai_compat_api_base: https://api.llama.com/compat/v1/\n   102→      api_key: ${env.LLAMA_API_KEY}\n   103→  - provider_id: ${env.ENABLE_TOGETHER_OPENAI_COMPAT:=__disabled__}\n   104→    provider_type: remote::together-openai-compat\n   105→    config:\n   106→      openai_compat_api_base: https://api.together.xyz/v1\n   107→      api_key: ${env.TOGETHER_API_KEY}\n   108→  - provider_id: ${env.ENABLE_GROQ_OPENAI_COMPAT:=__disabled__}\n   109→    provider_type: remote::groq-openai-compat\n   110→    config:\n   111→      openai_compat_api_base: https://api.groq.com/openai/v1\n   112→      api_key: ${env.GROQ_API_KEY}\n   113→  - provider_id: ${env.ENABLE_SAMBANOVA_OPENAI_COMPAT:=__disabled__}\n   114→    provider_type: remote::sambanova-openai-compat\n   115→    config:\n   116→      openai_compat_api_base: https://api.sambanova.ai/v1\n   117→      api_key: ${env.SAMBANOVA_API_KEY}\n   118→  - provider_id: ${env.ENABLE_CEREBRAS_OPENAI_COMPAT:=__disabled__}\n   119→    provider_type: remote::cerebras-openai-compat\n   120→    config:\n   121→      openai_compat_api_base: https://api.cerebras.ai/v1\n   122→      api_key: ${env.CEREBRAS_API_KEY}\n   123→  - provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n   124→    provider_type: remote::sambanova\n   125→    config:\n   126→      url: https://api.sambanova.ai/v1\n   127→      api_key: ${env.SAMBANOVA_API_KEY}\n   128→  - provider_id: ${env.ENABLE_PASSTHROUGH:=__disabled__}\n   129→    provider_type: remote::passthrough\n   130→    config:\n   131→      url: ${env.PASSTHROUGH_URL}\n   132→      api_key: ${env.PASSTHROUGH_API_KEY}\n   133→  - provider_id: ${env.ENABLE_SENTENCE_TRANSFORMERS:=sentence-transformers}\n   134→    provider_type: inline::sentence-transformers\n   135→    config: {}\n   136→  vector_io:\n   137→  - provider_id: ${env.ENABLE_FAISS:=faiss}\n   138→    provider_type: inline::faiss\n   139→    config:\n   140→      kvstore:\n   141→        type: sqlite\n   142→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/faiss_store.db\n   143→  - provider_id: ${env.ENABLE_SQLITE_VEC:=__disabled__}\n   144→    provider_type: inline::sqlite-vec\n   145→    config:\n   146→      db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/sqlite_vec.db\n   147→      kvstore:\n   148→        type: sqlite\n   149→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/sqlite_vec_registry.db\n   150→  - provider_id: ${env.ENABLE_MILVUS:=__disabled__}\n   151→    provider_type: inline::milvus\n   152→    config:\n   153→      db_path: ${env.MILVUS_DB_PATH:=~/.llama/distributions/starter}/milvus.db\n   154→      kvstore:\n   155→        type: sqlite\n   156→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/milvus_registry.db\n   157→  - provider_id: ${env.ENABLE_CHROMADB:=__disabled__}\n   158→    provider_type: remote::chromadb\n   159→    config:\n   160→      url: ${env.CHROMADB_URL:=}\n   161→  - provider_id: ${env.ENABLE_PGVECTOR:=__disabled__}\n   162→    provider_type: remote::pgvector\n   163→    config:\n   164→      host: ${env.PGVECTOR_HOST:=localhost}\n   165→      port: ${env.PGVECTOR_PORT:=5432}\n   166→      db: ${env.PGVECTOR_DB:=}\n   167→      user: ${env.PGVECTOR_USER:=}\n   168→      password: ${env.PGVECTOR_PASSWORD:=}\n   169→      kvstore:\n   170→        type: sqlite\n   171→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/pgvector_registry.db\n   172→  files:\n   173→  - provider_id: meta-reference-files\n   174→    provider_type: inline::localfs\n   175→    config:\n   176→      storage_dir: ${env.FILES_STORAGE_DIR:=~/.llama/distributions/starter/files}\n   177→      metadata_store:\n   178→        type: sqlite\n   179→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/files_metadata.db\n   180→  safety:\n   181→  - provider_id: llama-guard\n   182→    provider_type: inline::llama-guard\n   183→    config:\n   184→      excluded_categories: []\n   185→  agents:\n   186→  - provider_id: meta-reference\n   187→    provider_type: inline::meta-reference\n   188→    config:\n   189→      persistence_store:\n   190→        type: sqlite\n   191→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db\n   192→      responses_store:\n   193→        type: sqlite\n   194→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db\n   195→  telemetry:\n   196→  - provider_id: meta-reference\n   197→    provider_type: inline::meta-reference\n   198→    config:\n   199→      service_name: \"${env.OTEL_SERVICE_NAME:=\\u200B}\"\n   200→      sinks: ${env.TELEMETRY_SINKS:=console,sqlite}\n   201→      sqlite_db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/trace_store.db\n   202→      otel_exporter_otlp_endpoint: ${env.OTEL_EXPORTER_OTLP_ENDPOINT:=}\n   203→  post_training:\n   204→  - provider_id: huggingface\n   205→    provider_type: inline::huggingface\n   206→    config:\n   207→      checkpoint_format: huggingface\n   208→      distributed_backend: null\n   209→      device: cpu\n   210→  eval:\n   211→  - provider_id: meta-reference\n   212→    provider_type: inline::meta-reference\n   213→    config:\n   214→      kvstore:\n   215→        type: sqlite\n   216→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/meta_reference_eval.db\n   217→  datasetio:\n   218→  - provider_id: huggingface\n   219→    provider_type: remote::huggingface\n   220→    config:\n   221→      kvstore:\n   222→        type: sqlite\n   223→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/huggingface_datasetio.db\n   224→  - provider_id: localfs\n   225→    provider_type: inline::localfs\n   226→    config:\n   227→      kvstore:\n   228→        type: sqlite\n   229→        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/localfs_datasetio.db\n   230→  scoring:\n   231→  - provider_id: basic\n   232→    provider_type: inline::basic\n   233→    config: {}\n   234→  - provider_id: llm-as-judge\n   235→    provider_type: inline::llm-as-judge\n   236→    config: {}\n   237→  - provider_id: braintrust\n   238→    provider_type: inline::braintrust\n   239→    config:\n   240→      openai_api_key: ${env.OPENAI_API_KEY:=}\n   241→  tool_runtime:\n   242→  - provider_id: brave-search\n   243→    provider_type: remote::brave-search\n   244→    config:\n   245→      api_key: ${env.BRAVE_SEARCH_API_KEY:=}\n   246→      max_results: 3\n   247→  - provider_id: tavily-search\n   248→    provider_type: remote::tavily-search\n   249→    config:\n   250→      api_key: ${env.TAVILY_SEARCH_API_KEY:=}\n   251→      max_results: 3\n   252→  - provider_id: rag-runtime\n   253→    provider_type: inline::rag-runtime\n   254→    config: {}\n   255→  - provider_id: model-context-protocol\n   256→    provider_type: remote::model-context-protocol\n   257→    config: {}\n   258→metadata_store:\n   259→  type: sqlite\n   260→  db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/registry.db\n   261→inference_store:\n   262→  type: sqlite\n   263→  db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/inference_store.db\n   264→models:\n   265→- metadata:\n   266→    embedding_dimension: 384\n   267→  model_id: all-MiniLM-L6-v2\n   268→  provider_id: ${env.ENABLE_SENTENCE_TRANSFORMERS:=sentence-transformers}\n   269→  model_type: embedding\n   270→- metadata: {}\n   271→  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/llama3.1-8b\n   272→  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n   273→  provider_model_id: llama3.1-8b\n   274→  model_type: llm\n   275→- metadata: {}\n   276→  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n   277→  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n   278→  provider_model_id: llama3.1-8b\n   279→  model_type: llm\n   280→- metadata: {}\n   281→  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/llama-3.3-70b\n   282→  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n   283→  provider_model_id: llama-3.3-70b\n   284→  model_type: llm\n   285→- metadata: {}\n   286→  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n   287→  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n   288→  provider_model_id: llama-3.3-70b\n   289→  model_type: llm\n   290→- metadata: {}\n   291→  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/llama-4-scout-17b-16e-instruct\n   292→  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n   293→  provider_model_id: llama-4-scout-17b-16e-instruct\n   294→  model_type: llm\n   295→- metadata: {}\n   296→  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n   297→  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n   298→  provider_model_id: llama-4-scout-17b-16e-instruct\n   299→  model_type: llm\n   300→- metadata: {}\n   301→  model_id: ${env.ENABLE_OLLAMA:=__disabled__}/${env.OLLAMA_INFERENCE_MODEL:=__disabled__}\n   302→  provider_id: ${env.ENABLE_OLLAMA:=__disabled__}\n   303→  provider_model_id: ${env.OLLAMA_INFERENCE_MODEL:=__disabled__}\n   304→  model_type: llm\n   305→- metadata: {}\n   306→  model_id: ${env.ENABLE_OLLAMA:=__disabled__}/${env.SAFETY_MODEL:=__disabled__}\n   307→  provider_id: ${env.ENABLE_OLLAMA:=__disabled__}\n   308→  provider_model_id: ${env.SAFETY_MODEL:=__disabled__}\n   309→  model_type: llm\n   310→- metadata:\n   311→    embedding_dimension: ${env.OLLAMA_EMBEDDING_DIMENSION:=384}\n   312→  model_id: ${env.ENABLE_OLLAMA:=__disabled__}/${env.OLLAMA_EMBEDDING_MODEL:=__disabled__}\n   313→  provider_id: ${env.ENABLE_OLLAMA:=__disabled__}\n   314→  provider_model_id: ${env.OLLAMA_EMBEDDING_MODEL:=__disabled__}\n   315→  model_type: embedding\n   316→- metadata: {}\n   317→  model_id: ${env.ENABLE_VLLM:=__disabled__}/${env.VLLM_INFERENCE_MODEL:=__disabled__}\n   318→  provider_id: ${env.ENABLE_VLLM:=__disabled__}\n   319→  provider_model_id: ${env.VLLM_INFERENCE_MODEL:=__disabled__}\n   320→  model_type: llm\n   321→- metadata: {}\n   322→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p1-8b-instruct\n   323→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   324→  provider_model_id: accounts/fireworks/models/llama-v3p1-8b-instruct\n   325→  model_type: llm\n   326→- metadata: {}\n   327→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n   328→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   329→  provider_model_id: accounts/fireworks/models/llama-v3p1-8b-instruct\n   330→  model_type: llm\n   331→- metadata: {}\n   332→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p1-70b-instruct\n   333→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   334→  provider_model_id: accounts/fireworks/models/llama-v3p1-70b-instruct\n   335→  model_type: llm\n   336→- metadata: {}\n   337→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n   338→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   339→  provider_model_id: accounts/fireworks/models/llama-v3p1-70b-instruct\n   340→  model_type: llm\n   341→- metadata: {}\n   342→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p1-405b-instruct\n   343→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   344→  provider_model_id: accounts/fireworks/models/llama-v3p1-405b-instruct\n   345→  model_type: llm\n   346→- metadata: {}\n   347→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n   348→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   349→  provider_model_id: accounts/fireworks/models/llama-v3p1-405b-instruct\n   350→  model_type: llm\n   351→- metadata: {}\n   352→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p2-3b-instruct\n   353→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   354→  provider_model_id: accounts/fireworks/models/llama-v3p2-3b-instruct\n   355→  model_type: llm\n   356→- metadata: {}\n   357→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n   358→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   359→  provider_model_id: accounts/fireworks/models/llama-v3p2-3b-instruct\n   360→  model_type: llm\n   361→- metadata: {}\n   362→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n   363→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   364→  provider_model_id: accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n   365→  model_type: llm\n   366→- metadata: {}\n   367→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct\n   368→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   369→  provider_model_id: accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n   370→  model_type: llm\n   371→- metadata: {}\n   372→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n   373→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   374→  provider_model_id: accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n   375→  model_type: llm\n   376→- metadata: {}\n   377→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct\n   378→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   379→  provider_model_id: accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n   380→  model_type: llm\n   381→- metadata: {}\n   382→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p3-70b-instruct\n   383→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   384→  provider_model_id: accounts/fireworks/models/llama-v3p3-70b-instruct\n   385→  model_type: llm\n   386→- metadata: {}\n   387→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n   388→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   389→  provider_model_id: accounts/fireworks/models/llama-v3p3-70b-instruct\n   390→  model_type: llm\n   391→- metadata: {}\n   392→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama4-scout-instruct-basic\n   393→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   394→  provider_model_id: accounts/fireworks/models/llama4-scout-instruct-basic\n   395→  model_type: llm\n   396→- metadata: {}\n   397→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n   398→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   399→  provider_model_id: accounts/fireworks/models/llama4-scout-instruct-basic\n   400→  model_type: llm\n   401→- metadata: {}\n   402→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama4-maverick-instruct-basic\n   403→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   404→  provider_model_id: accounts/fireworks/models/llama4-maverick-instruct-basic\n   405→  model_type: llm\n   406→- metadata: {}\n   407→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n   408→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   409→  provider_model_id: accounts/fireworks/models/llama4-maverick-instruct-basic\n   410→  model_type: llm\n   411→- metadata:\n   412→    embedding_dimension: 768\n   413→    context_length: 8192\n   414→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/nomic-ai/nomic-embed-text-v1.5\n   415→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   416→  provider_model_id: nomic-ai/nomic-embed-text-v1.5\n   417→  model_type: embedding\n   418→- metadata: {}\n   419→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-guard-3-8b\n   420→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   421→  provider_model_id: accounts/fireworks/models/llama-guard-3-8b\n   422→  model_type: llm\n   423→- metadata: {}\n   424→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-Guard-3-8B\n   425→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   426→  provider_model_id: accounts/fireworks/models/llama-guard-3-8b\n   427→  model_type: llm\n   428→- metadata: {}\n   429→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-guard-3-11b-vision\n   430→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   431→  provider_model_id: accounts/fireworks/models/llama-guard-3-11b-vision\n   432→  model_type: llm\n   433→- metadata: {}\n   434→  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-Guard-3-11B-Vision\n   435→  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n   436→  provider_model_id: accounts/fireworks/models/llama-guard-3-11b-vision\n   437→  model_type: llm\n   438→- metadata: {}\n   439→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n   440→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   441→  provider_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n   442→  model_type: llm\n   443→- metadata: {}\n   444→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n   445→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   446→  provider_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n   447→  model_type: llm\n   448→- metadata: {}\n   449→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n   450→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   451→  provider_model_id: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n   452→  model_type: llm\n   453→- metadata: {}\n   454→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n   455→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   456→  provider_model_id: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n   457→  model_type: llm\n   458→- metadata: {}\n   459→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n   460→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   461→  provider_model_id: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n   462→  model_type: llm\n   463→- metadata: {}\n   464→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n   465→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   466→  provider_model_id: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n   467→  model_type: llm\n   468→- metadata: {}\n   469→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct-Turbo\n   470→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   471→  provider_model_id: meta-llama/Llama-3.2-3B-Instruct-Turbo\n   472→  model_type: llm\n   473→- metadata: {}\n   474→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n   475→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   476→  provider_model_id: meta-llama/Llama-3.2-3B-Instruct-Turbo\n   477→  model_type: llm\n   478→- metadata: {}\n   479→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\n   480→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   481→  provider_model_id: meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\n   482→  model_type: llm\n   483→- metadata: {}\n   484→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct\n   485→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   486→  provider_model_id: meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\n   487→  model_type: llm\n   488→- metadata: {}\n   489→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\n   490→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   491→  provider_model_id: meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\n   492→  model_type: llm\n   493→- metadata: {}\n   494→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct\n   495→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   496→  provider_model_id: meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\n   497→  model_type: llm\n   498→- metadata: {}\n   499→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct-Turbo\n   500→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   501→  provider_model_id: meta-llama/Llama-3.3-70B-Instruct-Turbo\n   502→  model_type: llm\n   503→- metadata: {}\n   504→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n   505→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   506→  provider_model_id: meta-llama/Llama-3.3-70B-Instruct-Turbo\n   507→  model_type: llm\n   508→- metadata:\n   509→    embedding_dimension: 768\n   510→    context_length: 8192\n   511→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/togethercomputer/m2-bert-80M-8k-retrieval\n   512→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   513→  provider_model_id: togethercomputer/m2-bert-80M-8k-retrieval\n   514→  model_type: embedding\n   515→- metadata:\n   516→    embedding_dimension: 768\n   517→    context_length: 32768\n   518→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/togethercomputer/m2-bert-80M-32k-retrieval\n   519→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   520→  provider_model_id: togethercomputer/m2-bert-80M-32k-retrieval\n   521→  model_type: embedding\n   522→- metadata: {}\n   523→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n   524→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   525→  provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct\n   526→  model_type: llm\n   527→- metadata: {}\n   528→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n   529→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   530→  provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct\n   531→  model_type: llm\n   532→- metadata: {}\n   533→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/together/meta-llama/Llama-4-Scout-17B-16E-Instruct\n   534→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   535→  provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct\n   536→  model_type: llm\n   537→- metadata: {}\n   538→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n   539→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   540→  provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n   541→  model_type: llm\n   542→- metadata: {}\n   543→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n   544→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   545→  provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n   546→  model_type: llm\n   547→- metadata: {}\n   548→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n   549→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   550→  provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n   551→  model_type: llm\n   552→- metadata: {}\n   553→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-Guard-3-8B\n   554→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   555→  provider_model_id: meta-llama/Llama-Guard-3-8B\n   556→  model_type: llm\n   557→- metadata: {}\n   558→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-Guard-3-8B\n   559→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   560→  provider_model_id: meta-llama/Llama-Guard-3-8B\n   561→  model_type: llm\n   562→- metadata: {}\n   563→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-Guard-3-11B-Vision-Turbo\n   564→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   565→  provider_model_id: meta-llama/Llama-Guard-3-11B-Vision-Turbo\n   566→  model_type: llm\n   567→- metadata: {}\n   568→  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-Guard-3-11B-Vision\n   569→  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n   570→  provider_model_id: meta-llama/Llama-Guard-3-11B-Vision-Turbo\n   571→  model_type: llm\n   572→- metadata: {}\n   573→  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta.llama3-1-8b-instruct-v1:0\n   574→  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n   575→  provider_model_id: meta.llama3-1-8b-instruct-v1:0\n   576→  model_type: llm\n   577→- metadata: {}\n   578→  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n   579→  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n   580→  provider_model_id: meta.llama3-1-8b-instruct-v1:0\n   581→  model_type: llm\n   582→- metadata: {}\n   583→  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta.llama3-1-70b-instruct-v1:0\n   584→  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n   585→  provider_model_id: meta.llama3-1-70b-instruct-v1:0\n   586→  model_type: llm\n   587→- metadata: {}\n   588→  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n   589→  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n   590→  provider_model_id: meta.llama3-1-70b-instruct-v1:0\n   591→  model_type: llm\n   592→- metadata: {}\n   593→  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta.llama3-1-405b-instruct-v1:0\n   594→  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n   595→  provider_model_id: meta.llama3-1-405b-instruct-v1:0\n   596→  model_type: llm\n   597→- metadata: {}\n   598→  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n   599→  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n   600→  provider_model_id: meta.llama3-1-405b-instruct-v1:0\n   601→  model_type: llm\n   602→- metadata: {}\n   603→  model_id: ${env.ENABLE_DATABRICKS:=__disabled__}/databricks-meta-llama-3-1-70b-instruct\n   604→  provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n   605→  provider_model_id: databricks-meta-llama-3-1-70b-instruct\n   606→  model_type: llm\n   607→- metadata: {}\n   608→  model_id: ${env.ENABLE_DATABRICKS:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n   609→  provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n   610→  provider_model_id: databricks-meta-llama-3-1-70b-instruct\n   611→  model_type: llm\n   612→- metadata: {}\n   613→  model_id: ${env.ENABLE_DATABRICKS:=__disabled__}/databricks-meta-llama-3-1-405b-instruct\n   614→  provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n   615→  provider_model_id: databricks-meta-llama-3-1-405b-instruct\n   616→  model_type: llm\n   617→- metadata: {}\n   618→  model_id: ${env.ENABLE_DATABRICKS:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n   619→  provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n   620→  provider_model_id: databricks-meta-llama-3-1-405b-instruct\n   621→  model_type: llm\n   622→- metadata: {}\n   623→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama3-8b-instruct\n   624→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   625→  provider_model_id: meta/llama3-8b-instruct\n   626→  model_type: llm\n   627→- metadata: {}\n   628→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3-8B-Instruct\n   629→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   630→  provider_model_id: meta/llama3-8b-instruct\n   631→  model_type: llm\n   632→- metadata: {}\n   633→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama3-70b-instruct\n   634→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   635→  provider_model_id: meta/llama3-70b-instruct\n   636→  model_type: llm\n   637→- metadata: {}\n   638→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3-70B-Instruct\n   639→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   640→  provider_model_id: meta/llama3-70b-instruct\n   641→  model_type: llm\n   642→- metadata: {}\n   643→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.1-8b-instruct\n   644→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   645→  provider_model_id: meta/llama-3.1-8b-instruct\n   646→  model_type: llm\n   647→- metadata: {}\n   648→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n   649→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   650→  provider_model_id: meta/llama-3.1-8b-instruct\n   651→  model_type: llm\n   652→- metadata: {}\n   653→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.1-70b-instruct\n   654→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   655→  provider_model_id: meta/llama-3.1-70b-instruct\n   656→  model_type: llm\n   657→- metadata: {}\n   658→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n   659→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   660→  provider_model_id: meta/llama-3.1-70b-instruct\n   661→  model_type: llm\n   662→- metadata: {}\n   663→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.1-405b-instruct\n   664→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   665→  provider_model_id: meta/llama-3.1-405b-instruct\n   666→  model_type: llm\n   667→- metadata: {}\n   668→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n   669→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   670→  provider_model_id: meta/llama-3.1-405b-instruct\n   671→  model_type: llm\n   672→- metadata: {}\n   673→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.2-1b-instruct\n   674→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   675→  provider_model_id: meta/llama-3.2-1b-instruct\n   676→  model_type: llm\n   677→- metadata: {}\n   678→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.2-1B-Instruct\n   679→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   680→  provider_model_id: meta/llama-3.2-1b-instruct\n   681→  model_type: llm\n   682→- metadata: {}\n   683→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.2-3b-instruct\n   684→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   685→  provider_model_id: meta/llama-3.2-3b-instruct\n   686→  model_type: llm\n   687→- metadata: {}\n   688→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n   689→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   690→  provider_model_id: meta/llama-3.2-3b-instruct\n   691→  model_type: llm\n   692→- metadata: {}\n   693→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.2-11b-vision-instruct\n   694→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   695→  provider_model_id: meta/llama-3.2-11b-vision-instruct\n   696→  model_type: llm\n   697→- metadata: {}\n   698→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct\n   699→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   700→  provider_model_id: meta/llama-3.2-11b-vision-instruct\n   701→  model_type: llm\n   702→- metadata: {}\n   703→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.2-90b-vision-instruct\n   704→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   705→  provider_model_id: meta/llama-3.2-90b-vision-instruct\n   706→  model_type: llm\n   707→- metadata: {}\n   708→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct\n   709→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   710→  provider_model_id: meta/llama-3.2-90b-vision-instruct\n   711→  model_type: llm\n   712→- metadata: {}\n   713→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.3-70b-instruct\n   714→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   715→  provider_model_id: meta/llama-3.3-70b-instruct\n   716→  model_type: llm\n   717→- metadata: {}\n   718→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n   719→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   720→  provider_model_id: meta/llama-3.3-70b-instruct\n   721→  model_type: llm\n   722→- metadata:\n   723→    embedding_dimension: 2048\n   724→    context_length: 8192\n   725→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/nvidia/llama-3.2-nv-embedqa-1b-v2\n   726→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   727→  provider_model_id: nvidia/llama-3.2-nv-embedqa-1b-v2\n   728→  model_type: embedding\n   729→- metadata:\n   730→    embedding_dimension: 1024\n   731→    context_length: 512\n   732→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/nvidia/nv-embedqa-e5-v5\n   733→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   734→  provider_model_id: nvidia/nv-embedqa-e5-v5\n   735→  model_type: embedding\n   736→- metadata:\n   737→    embedding_dimension: 4096\n   738→    context_length: 512\n   739→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/nvidia/nv-embedqa-mistral-7b-v2\n   740→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   741→  provider_model_id: nvidia/nv-embedqa-mistral-7b-v2\n   742→  model_type: embedding\n   743→- metadata:\n   744→    embedding_dimension: 1024\n   745→    context_length: 512\n   746→  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/snowflake/arctic-embed-l\n   747→  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n   748→  provider_model_id: snowflake/arctic-embed-l\n   749→  model_type: embedding\n   750→- metadata: {}\n   751→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-8B\n   752→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   753→  provider_model_id: Llama3.1-8B\n   754→  model_type: llm\n   755→- metadata: {}\n   756→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-70B\n   757→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   758→  provider_model_id: Llama3.1-70B\n   759→  model_type: llm\n   760→- metadata: {}\n   761→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B:bf16-mp8\n   762→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   763→  provider_model_id: Llama3.1-405B:bf16-mp8\n   764→  model_type: llm\n   765→- metadata: {}\n   766→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B\n   767→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   768→  provider_model_id: Llama3.1-405B\n   769→  model_type: llm\n   770→- metadata: {}\n   771→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B:bf16-mp16\n   772→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   773→  provider_model_id: Llama3.1-405B:bf16-mp16\n   774→  model_type: llm\n   775→- metadata: {}\n   776→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-8B-Instruct\n   777→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   778→  provider_model_id: Llama3.1-8B-Instruct\n   779→  model_type: llm\n   780→- metadata: {}\n   781→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-70B-Instruct\n   782→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   783→  provider_model_id: Llama3.1-70B-Instruct\n   784→  model_type: llm\n   785→- metadata: {}\n   786→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B-Instruct:bf16-mp8\n   787→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   788→  provider_model_id: Llama3.1-405B-Instruct:bf16-mp8\n   789→  model_type: llm\n   790→- metadata: {}\n   791→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B-Instruct\n   792→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   793→  provider_model_id: Llama3.1-405B-Instruct\n   794→  model_type: llm\n   795→- metadata: {}\n   796→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B-Instruct:bf16-mp16\n   797→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   798→  provider_model_id: Llama3.1-405B-Instruct:bf16-mp16\n   799→  model_type: llm\n   800→- metadata: {}\n   801→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.2-1B\n   802→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   803→  provider_model_id: Llama3.2-1B\n   804→  model_type: llm\n   805→- metadata: {}\n   806→  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.2-3B\n   807→  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n   808→  provider_model_id: Llama3.2-3B\n   809→  model_type: llm\n   810→- metadata: {}\n   811→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/gpt-4o\n   812→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   813→  provider_model_id: openai/gpt-4o\n   814→  model_type: llm\n   815→- metadata: {}\n   816→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/gpt-4o-mini\n   817→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   818→  provider_model_id: openai/gpt-4o-mini\n   819→  model_type: llm\n   820→- metadata: {}\n   821→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/chatgpt-4o-latest\n   822→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   823→  provider_model_id: openai/chatgpt-4o-latest\n   824→  model_type: llm\n   825→- metadata: {}\n   826→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-3.5-turbo-0125\n   827→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   828→  provider_model_id: gpt-3.5-turbo-0125\n   829→  model_type: llm\n   830→- metadata: {}\n   831→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-3.5-turbo\n   832→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   833→  provider_model_id: gpt-3.5-turbo\n   834→  model_type: llm\n   835→- metadata: {}\n   836→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-3.5-turbo-instruct\n   837→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   838→  provider_model_id: gpt-3.5-turbo-instruct\n   839→  model_type: llm\n   840→- metadata: {}\n   841→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4\n   842→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   843→  provider_model_id: gpt-4\n   844→  model_type: llm\n   845→- metadata: {}\n   846→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4-turbo\n   847→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   848→  provider_model_id: gpt-4-turbo\n   849→  model_type: llm\n   850→- metadata: {}\n   851→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4o\n   852→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   853→  provider_model_id: gpt-4o\n   854→  model_type: llm\n   855→- metadata: {}\n   856→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4o-2024-08-06\n   857→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   858→  provider_model_id: gpt-4o-2024-08-06\n   859→  model_type: llm\n   860→- metadata: {}\n   861→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4o-mini\n   862→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   863→  provider_model_id: gpt-4o-mini\n   864→  model_type: llm\n   865→- metadata: {}\n   866→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4o-audio-preview\n   867→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   868→  provider_model_id: gpt-4o-audio-preview\n   869→  model_type: llm\n   870→- metadata: {}\n   871→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/chatgpt-4o-latest\n   872→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   873→  provider_model_id: chatgpt-4o-latest\n   874→  model_type: llm\n   875→- metadata: {}\n   876→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/o1\n   877→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   878→  provider_model_id: o1\n   879→  model_type: llm\n   880→- metadata: {}\n   881→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/o1-mini\n   882→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   883→  provider_model_id: o1-mini\n   884→  model_type: llm\n   885→- metadata: {}\n   886→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/o3-mini\n   887→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   888→  provider_model_id: o3-mini\n   889→  model_type: llm\n   890→- metadata: {}\n   891→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/o4-mini\n   892→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   893→  provider_model_id: o4-mini\n   894→  model_type: llm\n   895→- metadata:\n   896→    embedding_dimension: 1536\n   897→    context_length: 8192\n   898→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/text-embedding-3-small\n   899→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   900→  provider_model_id: openai/text-embedding-3-small\n   901→  model_type: embedding\n   902→- metadata:\n   903→    embedding_dimension: 3072\n   904→    context_length: 8192\n   905→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/text-embedding-3-large\n   906→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   907→  provider_model_id: openai/text-embedding-3-large\n   908→  model_type: embedding\n   909→- metadata:\n   910→    embedding_dimension: 1536\n   911→    context_length: 8192\n   912→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/text-embedding-3-small\n   913→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   914→  provider_model_id: text-embedding-3-small\n   915→  model_type: embedding\n   916→- metadata:\n   917→    embedding_dimension: 3072\n   918→    context_length: 8192\n   919→  model_id: ${env.ENABLE_OPENAI:=__disabled__}/text-embedding-3-large\n   920→  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n   921→  provider_model_id: text-embedding-3-large\n   922→  model_type: embedding\n   923→- metadata: {}\n   924→  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/claude-3-5-sonnet-latest\n   925→  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n   926→  provider_model_id: anthropic/claude-3-5-sonnet-latest\n   927→  model_type: llm\n   928→- metadata: {}\n   929→  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/claude-3-7-sonnet-latest\n   930→  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n   931→  provider_model_id: anthropic/claude-3-7-sonnet-latest\n   932→  model_type: llm\n   933→- metadata: {}\n   934→  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/claude-3-5-haiku-latest\n   935→  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n   936→  provider_model_id: anthropic/claude-3-5-haiku-latest\n   937→  model_type: llm\n   938→- metadata:\n   939→    embedding_dimension: 1024\n   940→    context_length: 32000\n   941→  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/voyage-3\n   942→  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n   943→  provider_model_id: anthropic/voyage-3\n   944→  model_type: embedding\n   945→- metadata:\n   946→    embedding_dimension: 512\n   947→    context_length: 32000\n   948→  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/voyage-3-lite\n   949→  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n   950→  provider_model_id: anthropic/voyage-3-lite\n   951→  model_type: embedding\n   952→- metadata:\n   953→    embedding_dimension: 1024\n   954→    context_length: 32000\n   955→  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/voyage-code-3\n   956→  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n   957→  provider_model_id: anthropic/voyage-code-3\n   958→  model_type: embedding\n   959→- metadata: {}\n   960→  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-1.5-flash\n   961→  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n   962→  provider_model_id: gemini/gemini-1.5-flash\n   963→  model_type: llm\n   964→- metadata: {}\n   965→  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-1.5-pro\n   966→  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n   967→  provider_model_id: gemini/gemini-1.5-pro\n   968→  model_type: llm\n   969→- metadata: {}\n   970→  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-2.0-flash\n   971→  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n   972→  provider_model_id: gemini/gemini-2.0-flash\n   973→  model_type: llm\n   974→- metadata: {}\n   975→  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-2.5-flash\n   976→  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n   977→  provider_model_id: gemini/gemini-2.5-flash\n   978→  model_type: llm\n   979→- metadata: {}\n   980→  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-2.5-pro\n   981→  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n   982→  provider_model_id: gemini/gemini-2.5-pro\n   983→  model_type: llm\n   984→- metadata:\n   985→    embedding_dimension: 768\n   986→    context_length: 2048\n   987→  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/text-embedding-004\n   988→  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n   989→  provider_model_id: gemini/text-embedding-004\n   990→  model_type: embedding\n   991→- metadata: {}\n   992→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama3-8b-8192\n   993→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n   994→  provider_model_id: groq/llama3-8b-8192\n   995→  model_type: llm\n   996→- metadata: {}\n   997→  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n   998→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n   999→  provider_model_id: groq/llama3-8b-8192\n  1000→  model_type: llm\n  1001→- metadata: {}\n  1002→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-3.1-8b-instant\n  1003→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1004→  provider_model_id: groq/llama-3.1-8b-instant\n  1005→  model_type: llm\n  1006→- metadata: {}\n  1007→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama3-70b-8192\n  1008→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1009→  provider_model_id: groq/llama3-70b-8192\n  1010→  model_type: llm\n  1011→- metadata: {}\n  1012→  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-3-70B-Instruct\n  1013→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1014→  provider_model_id: groq/llama3-70b-8192\n  1015→  model_type: llm\n  1016→- metadata: {}\n  1017→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-3.3-70b-versatile\n  1018→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1019→  provider_model_id: groq/llama-3.3-70b-versatile\n  1020→  model_type: llm\n  1021→- metadata: {}\n  1022→  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n  1023→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1024→  provider_model_id: groq/llama-3.3-70b-versatile\n  1025→  model_type: llm\n  1026→- metadata: {}\n  1027→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-3.2-3b-preview\n  1028→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1029→  provider_model_id: groq/llama-3.2-3b-preview\n  1030→  model_type: llm\n  1031→- metadata: {}\n  1032→  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n  1033→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1034→  provider_model_id: groq/llama-3.2-3b-preview\n  1035→  model_type: llm\n  1036→- metadata: {}\n  1037→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-4-scout-17b-16e-instruct\n  1038→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1039→  provider_model_id: groq/llama-4-scout-17b-16e-instruct\n  1040→  model_type: llm\n  1041→- metadata: {}\n  1042→  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  1043→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1044→  provider_model_id: groq/llama-4-scout-17b-16e-instruct\n  1045→  model_type: llm\n  1046→- metadata: {}\n  1047→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/meta-llama/llama-4-scout-17b-16e-instruct\n  1048→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1049→  provider_model_id: groq/meta-llama/llama-4-scout-17b-16e-instruct\n  1050→  model_type: llm\n  1051→- metadata: {}\n  1052→  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  1053→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1054→  provider_model_id: groq/meta-llama/llama-4-scout-17b-16e-instruct\n  1055→  model_type: llm\n  1056→- metadata: {}\n  1057→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-4-maverick-17b-128e-instruct\n  1058→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1059→  provider_model_id: groq/llama-4-maverick-17b-128e-instruct\n  1060→  model_type: llm\n  1061→- metadata: {}\n  1062→  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n  1063→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1064→  provider_model_id: groq/llama-4-maverick-17b-128e-instruct\n  1065→  model_type: llm\n  1066→- metadata: {}\n  1067→  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/meta-llama/llama-4-maverick-17b-128e-instruct\n  1068→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1069→  provider_model_id: groq/meta-llama/llama-4-maverick-17b-128e-instruct\n  1070→  model_type: llm\n  1071→- metadata: {}\n  1072→  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n  1073→  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  1074→  provider_model_id: groq/meta-llama/llama-4-maverick-17b-128e-instruct\n  1075→  model_type: llm\n  1076→- metadata: {}\n  1077→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.1-8B-Instruct\n  1078→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1079→  provider_model_id: sambanova/Meta-Llama-3.1-8B-Instruct\n  1080→  model_type: llm\n  1081→- metadata: {}\n  1082→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n  1083→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1084→  provider_model_id: sambanova/Meta-Llama-3.1-8B-Instruct\n  1085→  model_type: llm\n  1086→- metadata: {}\n  1087→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.1-405B-Instruct\n  1088→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1089→  provider_model_id: sambanova/Meta-Llama-3.1-405B-Instruct\n  1090→  model_type: llm\n  1091→- metadata: {}\n  1092→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n  1093→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1094→  provider_model_id: sambanova/Meta-Llama-3.1-405B-Instruct\n  1095→  model_type: llm\n  1096→- metadata: {}\n  1097→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.2-1B-Instruct\n  1098→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1099→  provider_model_id: sambanova/Meta-Llama-3.2-1B-Instruct\n  1100→  model_type: llm\n  1101→- metadata: {}\n  1102→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.2-1B-Instruct\n  1103→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1104→  provider_model_id: sambanova/Meta-Llama-3.2-1B-Instruct\n  1105→  model_type: llm\n  1106→- metadata: {}\n  1107→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.2-3B-Instruct\n  1108→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1109→  provider_model_id: sambanova/Meta-Llama-3.2-3B-Instruct\n  1110→  model_type: llm\n  1111→- metadata: {}\n  1112→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n  1113→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1114→  provider_model_id: sambanova/Meta-Llama-3.2-3B-Instruct\n  1115→  model_type: llm\n  1116→- metadata: {}\n  1117→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.3-70B-Instruct\n  1118→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1119→  provider_model_id: sambanova/Meta-Llama-3.3-70B-Instruct\n  1120→  model_type: llm\n  1121→- metadata: {}\n  1122→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n  1123→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1124→  provider_model_id: sambanova/Meta-Llama-3.3-70B-Instruct\n  1125→  model_type: llm\n  1126→- metadata: {}\n  1127→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Llama-3.2-11B-Vision-Instruct\n  1128→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1129→  provider_model_id: sambanova/Llama-3.2-11B-Vision-Instruct\n  1130→  model_type: llm\n  1131→- metadata: {}\n  1132→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct\n  1133→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1134→  provider_model_id: sambanova/Llama-3.2-11B-Vision-Instruct\n  1135→  model_type: llm\n  1136→- metadata: {}\n  1137→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Llama-3.2-90B-Vision-Instruct\n  1138→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1139→  provider_model_id: sambanova/Llama-3.2-90B-Vision-Instruct\n  1140→  model_type: llm\n  1141→- metadata: {}\n  1142→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct\n  1143→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1144→  provider_model_id: sambanova/Llama-3.2-90B-Vision-Instruct\n  1145→  model_type: llm\n  1146→- metadata: {}\n  1147→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Llama-4-Scout-17B-16E-Instruct\n  1148→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1149→  provider_model_id: sambanova/Llama-4-Scout-17B-16E-Instruct\n  1150→  model_type: llm\n  1151→- metadata: {}\n  1152→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  1153→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1154→  provider_model_id: sambanova/Llama-4-Scout-17B-16E-Instruct\n  1155→  model_type: llm\n  1156→- metadata: {}\n  1157→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Llama-4-Maverick-17B-128E-Instruct\n  1158→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1159→  provider_model_id: sambanova/Llama-4-Maverick-17B-128E-Instruct\n  1160→  model_type: llm\n  1161→- metadata: {}\n  1162→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n  1163→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1164→  provider_model_id: sambanova/Llama-4-Maverick-17B-128E-Instruct\n  1165→  model_type: llm\n  1166→- metadata: {}\n  1167→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-Guard-3-8B\n  1168→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1169→  provider_model_id: sambanova/Meta-Llama-Guard-3-8B\n  1170→  model_type: llm\n  1171→- metadata: {}\n  1172→  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-Guard-3-8B\n  1173→  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  1174→  provider_model_id: sambanova/Meta-Llama-Guard-3-8B\n  1175→  model_type: llm\n  1176→shields:\n  1177→- shield_id: ${env.SAFETY_MODEL:=__disabled__}\n  1178→  provider_shield_id: ${env.ENABLE_OLLAMA:=__disabled__}/${env.SAFETY_MODEL:=__disabled__}\n  1179→vector_dbs: []\n  1180→datasets: []\n  1181→scoring_fns: []\n  1182→benchmarks: []\n  1183→tool_groups:\n  1184→- toolgroup_id: builtin::websearch\n  1185→  provider_id: tavily-search\n  1186→- toolgroup_id: builtin::rag\n  1187→  provider_id: rag-runtime\n  1188→server:\n  1189→  port: 8321\n  1190→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"1b9a499f-1324-4d6b-84d3-67e0a84e1324","timestamp":"2025-07-18T20:52:34.501Z","toolUseResult":{"type":"text","file":{"filePath":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/run.yaml","content":"version: 2\nimage_name: starter\napis:\n- agents\n- datasetio\n- eval\n- files\n- inference\n- post_training\n- safety\n- scoring\n- telemetry\n- tool_runtime\n- vector_io\nproviders:\n  inference:\n  - provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n    provider_type: remote::cerebras\n    config:\n      base_url: https://api.cerebras.ai\n      api_key: ${env.CEREBRAS_API_KEY}\n  - provider_id: ${env.ENABLE_OLLAMA:=__disabled__}\n    provider_type: remote::ollama\n    config:\n      url: ${env.OLLAMA_URL:=http://localhost:11434}\n  - provider_id: ${env.ENABLE_VLLM:=__disabled__}\n    provider_type: remote::vllm\n    config:\n      url: ${env.VLLM_URL}\n      max_tokens: ${env.VLLM_MAX_TOKENS:=4096}\n      api_token: ${env.VLLM_API_TOKEN:=fake}\n      tls_verify: ${env.VLLM_TLS_VERIFY:=true}\n  - provider_id: ${env.ENABLE_TGI:=__disabled__}\n    provider_type: remote::tgi\n    config:\n      url: ${env.TGI_URL}\n  - provider_id: ${env.ENABLE_HF_SERVERLESS:=__disabled__}\n    provider_type: remote::hf::serverless\n    config:\n      huggingface_repo: ${env.INFERENCE_MODEL}\n      api_token: ${env.HF_API_TOKEN}\n  - provider_id: ${env.ENABLE_HF_ENDPOINT:=__disabled__}\n    provider_type: remote::hf::endpoint\n    config:\n      endpoint_name: ${env.INFERENCE_ENDPOINT_NAME}\n      api_token: ${env.HF_API_TOKEN}\n  - provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n    provider_type: remote::fireworks\n    config:\n      url: https://api.fireworks.ai/inference/v1\n      api_key: ${env.FIREWORKS_API_KEY}\n  - provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n    provider_type: remote::together\n    config:\n      url: https://api.together.xyz/v1\n      api_key: ${env.TOGETHER_API_KEY}\n  - provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n    provider_type: remote::bedrock\n    config: {}\n  - provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n    provider_type: remote::databricks\n    config:\n      url: ${env.DATABRICKS_URL}\n      api_token: ${env.DATABRICKS_API_TOKEN}\n  - provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n    provider_type: remote::nvidia\n    config:\n      url: ${env.NVIDIA_BASE_URL:=https://integrate.api.nvidia.com}\n      api_key: ${env.NVIDIA_API_KEY:=}\n      append_api_version: ${env.NVIDIA_APPEND_API_VERSION:=True}\n  - provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n    provider_type: remote::runpod\n    config:\n      url: ${env.RUNPOD_URL:=}\n      api_token: ${env.RUNPOD_API_TOKEN}\n  - provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n    provider_type: remote::openai\n    config:\n      api_key: ${env.OPENAI_API_KEY}\n  - provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n    provider_type: remote::anthropic\n    config:\n      api_key: ${env.ANTHROPIC_API_KEY}\n  - provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n    provider_type: remote::gemini\n    config:\n      api_key: ${env.GEMINI_API_KEY}\n  - provider_id: ${env.ENABLE_GROQ:=__disabled__}\n    provider_type: remote::groq\n    config:\n      url: https://api.groq.com\n      api_key: ${env.GROQ_API_KEY}\n  - provider_id: ${env.ENABLE_FIREWORKS_OPENAI_COMPAT:=__disabled__}\n    provider_type: remote::fireworks-openai-compat\n    config:\n      openai_compat_api_base: https://api.fireworks.ai/inference/v1\n      api_key: ${env.FIREWORKS_API_KEY}\n  - provider_id: ${env.ENABLE_LLAMA_OPENAI_COMPAT:=__disabled__}\n    provider_type: remote::llama-openai-compat\n    config:\n      openai_compat_api_base: https://api.llama.com/compat/v1/\n      api_key: ${env.LLAMA_API_KEY}\n  - provider_id: ${env.ENABLE_TOGETHER_OPENAI_COMPAT:=__disabled__}\n    provider_type: remote::together-openai-compat\n    config:\n      openai_compat_api_base: https://api.together.xyz/v1\n      api_key: ${env.TOGETHER_API_KEY}\n  - provider_id: ${env.ENABLE_GROQ_OPENAI_COMPAT:=__disabled__}\n    provider_type: remote::groq-openai-compat\n    config:\n      openai_compat_api_base: https://api.groq.com/openai/v1\n      api_key: ${env.GROQ_API_KEY}\n  - provider_id: ${env.ENABLE_SAMBANOVA_OPENAI_COMPAT:=__disabled__}\n    provider_type: remote::sambanova-openai-compat\n    config:\n      openai_compat_api_base: https://api.sambanova.ai/v1\n      api_key: ${env.SAMBANOVA_API_KEY}\n  - provider_id: ${env.ENABLE_CEREBRAS_OPENAI_COMPAT:=__disabled__}\n    provider_type: remote::cerebras-openai-compat\n    config:\n      openai_compat_api_base: https://api.cerebras.ai/v1\n      api_key: ${env.CEREBRAS_API_KEY}\n  - provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n    provider_type: remote::sambanova\n    config:\n      url: https://api.sambanova.ai/v1\n      api_key: ${env.SAMBANOVA_API_KEY}\n  - provider_id: ${env.ENABLE_PASSTHROUGH:=__disabled__}\n    provider_type: remote::passthrough\n    config:\n      url: ${env.PASSTHROUGH_URL}\n      api_key: ${env.PASSTHROUGH_API_KEY}\n  - provider_id: ${env.ENABLE_SENTENCE_TRANSFORMERS:=sentence-transformers}\n    provider_type: inline::sentence-transformers\n    config: {}\n  vector_io:\n  - provider_id: ${env.ENABLE_FAISS:=faiss}\n    provider_type: inline::faiss\n    config:\n      kvstore:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/faiss_store.db\n  - provider_id: ${env.ENABLE_SQLITE_VEC:=__disabled__}\n    provider_type: inline::sqlite-vec\n    config:\n      db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/sqlite_vec.db\n      kvstore:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/sqlite_vec_registry.db\n  - provider_id: ${env.ENABLE_MILVUS:=__disabled__}\n    provider_type: inline::milvus\n    config:\n      db_path: ${env.MILVUS_DB_PATH:=~/.llama/distributions/starter}/milvus.db\n      kvstore:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/milvus_registry.db\n  - provider_id: ${env.ENABLE_CHROMADB:=__disabled__}\n    provider_type: remote::chromadb\n    config:\n      url: ${env.CHROMADB_URL:=}\n  - provider_id: ${env.ENABLE_PGVECTOR:=__disabled__}\n    provider_type: remote::pgvector\n    config:\n      host: ${env.PGVECTOR_HOST:=localhost}\n      port: ${env.PGVECTOR_PORT:=5432}\n      db: ${env.PGVECTOR_DB:=}\n      user: ${env.PGVECTOR_USER:=}\n      password: ${env.PGVECTOR_PASSWORD:=}\n      kvstore:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/pgvector_registry.db\n  files:\n  - provider_id: meta-reference-files\n    provider_type: inline::localfs\n    config:\n      storage_dir: ${env.FILES_STORAGE_DIR:=~/.llama/distributions/starter/files}\n      metadata_store:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/files_metadata.db\n  safety:\n  - provider_id: llama-guard\n    provider_type: inline::llama-guard\n    config:\n      excluded_categories: []\n  agents:\n  - provider_id: meta-reference\n    provider_type: inline::meta-reference\n    config:\n      persistence_store:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db\n      responses_store:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db\n  telemetry:\n  - provider_id: meta-reference\n    provider_type: inline::meta-reference\n    config:\n      service_name: \"${env.OTEL_SERVICE_NAME:=\\u200B}\"\n      sinks: ${env.TELEMETRY_SINKS:=console,sqlite}\n      sqlite_db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/trace_store.db\n      otel_exporter_otlp_endpoint: ${env.OTEL_EXPORTER_OTLP_ENDPOINT:=}\n  post_training:\n  - provider_id: huggingface\n    provider_type: inline::huggingface\n    config:\n      checkpoint_format: huggingface\n      distributed_backend: null\n      device: cpu\n  eval:\n  - provider_id: meta-reference\n    provider_type: inline::meta-reference\n    config:\n      kvstore:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/meta_reference_eval.db\n  datasetio:\n  - provider_id: huggingface\n    provider_type: remote::huggingface\n    config:\n      kvstore:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/huggingface_datasetio.db\n  - provider_id: localfs\n    provider_type: inline::localfs\n    config:\n      kvstore:\n        type: sqlite\n        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/localfs_datasetio.db\n  scoring:\n  - provider_id: basic\n    provider_type: inline::basic\n    config: {}\n  - provider_id: llm-as-judge\n    provider_type: inline::llm-as-judge\n    config: {}\n  - provider_id: braintrust\n    provider_type: inline::braintrust\n    config:\n      openai_api_key: ${env.OPENAI_API_KEY:=}\n  tool_runtime:\n  - provider_id: brave-search\n    provider_type: remote::brave-search\n    config:\n      api_key: ${env.BRAVE_SEARCH_API_KEY:=}\n      max_results: 3\n  - provider_id: tavily-search\n    provider_type: remote::tavily-search\n    config:\n      api_key: ${env.TAVILY_SEARCH_API_KEY:=}\n      max_results: 3\n  - provider_id: rag-runtime\n    provider_type: inline::rag-runtime\n    config: {}\n  - provider_id: model-context-protocol\n    provider_type: remote::model-context-protocol\n    config: {}\nmetadata_store:\n  type: sqlite\n  db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/registry.db\ninference_store:\n  type: sqlite\n  db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/inference_store.db\nmodels:\n- metadata:\n    embedding_dimension: 384\n  model_id: all-MiniLM-L6-v2\n  provider_id: ${env.ENABLE_SENTENCE_TRANSFORMERS:=sentence-transformers}\n  model_type: embedding\n- metadata: {}\n  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/llama3.1-8b\n  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n  provider_model_id: llama3.1-8b\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n  provider_model_id: llama3.1-8b\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/llama-3.3-70b\n  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n  provider_model_id: llama-3.3-70b\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n  provider_model_id: llama-3.3-70b\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/llama-4-scout-17b-16e-instruct\n  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n  provider_model_id: llama-4-scout-17b-16e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_CEREBRAS:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_CEREBRAS:=__disabled__}\n  provider_model_id: llama-4-scout-17b-16e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OLLAMA:=__disabled__}/${env.OLLAMA_INFERENCE_MODEL:=__disabled__}\n  provider_id: ${env.ENABLE_OLLAMA:=__disabled__}\n  provider_model_id: ${env.OLLAMA_INFERENCE_MODEL:=__disabled__}\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OLLAMA:=__disabled__}/${env.SAFETY_MODEL:=__disabled__}\n  provider_id: ${env.ENABLE_OLLAMA:=__disabled__}\n  provider_model_id: ${env.SAFETY_MODEL:=__disabled__}\n  model_type: llm\n- metadata:\n    embedding_dimension: ${env.OLLAMA_EMBEDDING_DIMENSION:=384}\n  model_id: ${env.ENABLE_OLLAMA:=__disabled__}/${env.OLLAMA_EMBEDDING_MODEL:=__disabled__}\n  provider_id: ${env.ENABLE_OLLAMA:=__disabled__}\n  provider_model_id: ${env.OLLAMA_EMBEDDING_MODEL:=__disabled__}\n  model_type: embedding\n- metadata: {}\n  model_id: ${env.ENABLE_VLLM:=__disabled__}/${env.VLLM_INFERENCE_MODEL:=__disabled__}\n  provider_id: ${env.ENABLE_VLLM:=__disabled__}\n  provider_model_id: ${env.VLLM_INFERENCE_MODEL:=__disabled__}\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p1-8b-instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p1-8b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p1-8b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p1-70b-instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p1-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p1-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p1-405b-instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p1-405b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p1-405b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p2-3b-instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p2-3b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p2-3b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-v3p3-70b-instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p3-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-v3p3-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama4-scout-instruct-basic\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama4-scout-instruct-basic\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama4-scout-instruct-basic\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama4-maverick-instruct-basic\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama4-maverick-instruct-basic\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama4-maverick-instruct-basic\n  model_type: llm\n- metadata:\n    embedding_dimension: 768\n    context_length: 8192\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/nomic-ai/nomic-embed-text-v1.5\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: nomic-ai/nomic-embed-text-v1.5\n  model_type: embedding\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-guard-3-8b\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-guard-3-8b\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-Guard-3-8B\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-guard-3-8b\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/accounts/fireworks/models/llama-guard-3-11b-vision\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-guard-3-11b-vision\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_FIREWORKS:=__disabled__}/meta-llama/Llama-Guard-3-11B-Vision\n  provider_id: ${env.ENABLE_FIREWORKS:=__disabled__}\n  provider_model_id: accounts/fireworks/models/llama-guard-3-11b-vision\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct-Turbo\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-3.2-3B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-3.2-3B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct-Turbo\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-3.3-70B-Instruct-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-3.3-70B-Instruct-Turbo\n  model_type: llm\n- metadata:\n    embedding_dimension: 768\n    context_length: 8192\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/togethercomputer/m2-bert-80M-8k-retrieval\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: togethercomputer/m2-bert-80M-8k-retrieval\n  model_type: embedding\n- metadata:\n    embedding_dimension: 768\n    context_length: 32768\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/togethercomputer/m2-bert-80M-32k-retrieval\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: togethercomputer/m2-bert-80M-32k-retrieval\n  model_type: embedding\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/together/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-Guard-3-8B\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-Guard-3-8B\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-Guard-3-8B\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-Guard-3-8B\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-Guard-3-11B-Vision-Turbo\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-Guard-3-11B-Vision-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_TOGETHER:=__disabled__}/meta-llama/Llama-Guard-3-11B-Vision\n  provider_id: ${env.ENABLE_TOGETHER:=__disabled__}\n  provider_model_id: meta-llama/Llama-Guard-3-11B-Vision-Turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta.llama3-1-8b-instruct-v1:0\n  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n  provider_model_id: meta.llama3-1-8b-instruct-v1:0\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n  provider_model_id: meta.llama3-1-8b-instruct-v1:0\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta.llama3-1-70b-instruct-v1:0\n  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n  provider_model_id: meta.llama3-1-70b-instruct-v1:0\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n  provider_model_id: meta.llama3-1-70b-instruct-v1:0\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta.llama3-1-405b-instruct-v1:0\n  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n  provider_model_id: meta.llama3-1-405b-instruct-v1:0\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_BEDROCK:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n  provider_id: ${env.ENABLE_BEDROCK:=__disabled__}\n  provider_model_id: meta.llama3-1-405b-instruct-v1:0\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_DATABRICKS:=__disabled__}/databricks-meta-llama-3-1-70b-instruct\n  provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n  provider_model_id: databricks-meta-llama-3-1-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_DATABRICKS:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n  provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n  provider_model_id: databricks-meta-llama-3-1-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_DATABRICKS:=__disabled__}/databricks-meta-llama-3-1-405b-instruct\n  provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n  provider_model_id: databricks-meta-llama-3-1-405b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_DATABRICKS:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n  provider_id: ${env.ENABLE_DATABRICKS:=__disabled__}\n  provider_model_id: databricks-meta-llama-3-1-405b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama3-8b-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama3-8b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3-8B-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama3-8b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama3-70b-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama3-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3-70B-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama3-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.1-8b-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.1-8b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.1-8b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.1-70b-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.1-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.1-70B-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.1-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.1-405b-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.1-405b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.1-405b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.2-1b-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.2-1b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.2-1B-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.2-1b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.2-3b-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.2-3b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.2-3b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.2-11b-vision-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.2-11b-vision-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.2-11b-vision-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.2-90b-vision-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.2-90b-vision-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.2-90b-vision-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta/llama-3.3-70b-instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.3-70b-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: meta/llama-3.3-70b-instruct\n  model_type: llm\n- metadata:\n    embedding_dimension: 2048\n    context_length: 8192\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/nvidia/llama-3.2-nv-embedqa-1b-v2\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: nvidia/llama-3.2-nv-embedqa-1b-v2\n  model_type: embedding\n- metadata:\n    embedding_dimension: 1024\n    context_length: 512\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/nvidia/nv-embedqa-e5-v5\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: nvidia/nv-embedqa-e5-v5\n  model_type: embedding\n- metadata:\n    embedding_dimension: 4096\n    context_length: 512\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/nvidia/nv-embedqa-mistral-7b-v2\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: nvidia/nv-embedqa-mistral-7b-v2\n  model_type: embedding\n- metadata:\n    embedding_dimension: 1024\n    context_length: 512\n  model_id: ${env.ENABLE_NVIDIA:=__disabled__}/snowflake/arctic-embed-l\n  provider_id: ${env.ENABLE_NVIDIA:=__disabled__}\n  provider_model_id: snowflake/arctic-embed-l\n  model_type: embedding\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-8B\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-8B\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-70B\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-70B\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B:bf16-mp8\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-405B:bf16-mp8\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-405B\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B:bf16-mp16\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-405B:bf16-mp16\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-8B-Instruct\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-8B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-70B-Instruct\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-70B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B-Instruct:bf16-mp8\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-405B-Instruct:bf16-mp8\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B-Instruct\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-405B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.1-405B-Instruct:bf16-mp16\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.1-405B-Instruct:bf16-mp16\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.2-1B\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.2-1B\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_RUNPOD:=__disabled__}/Llama3.2-3B\n  provider_id: ${env.ENABLE_RUNPOD:=__disabled__}\n  provider_model_id: Llama3.2-3B\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/gpt-4o\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: openai/gpt-4o\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/gpt-4o-mini\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: openai/gpt-4o-mini\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/chatgpt-4o-latest\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: openai/chatgpt-4o-latest\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-3.5-turbo-0125\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-3.5-turbo-0125\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-3.5-turbo\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-3.5-turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-3.5-turbo-instruct\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-3.5-turbo-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-4\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4-turbo\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-4-turbo\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4o\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-4o\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4o-2024-08-06\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-4o-2024-08-06\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4o-mini\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-4o-mini\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/gpt-4o-audio-preview\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: gpt-4o-audio-preview\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/chatgpt-4o-latest\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: chatgpt-4o-latest\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/o1\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: o1\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/o1-mini\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: o1-mini\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/o3-mini\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: o3-mini\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/o4-mini\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: o4-mini\n  model_type: llm\n- metadata:\n    embedding_dimension: 1536\n    context_length: 8192\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/text-embedding-3-small\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: openai/text-embedding-3-small\n  model_type: embedding\n- metadata:\n    embedding_dimension: 3072\n    context_length: 8192\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/openai/text-embedding-3-large\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: openai/text-embedding-3-large\n  model_type: embedding\n- metadata:\n    embedding_dimension: 1536\n    context_length: 8192\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/text-embedding-3-small\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: text-embedding-3-small\n  model_type: embedding\n- metadata:\n    embedding_dimension: 3072\n    context_length: 8192\n  model_id: ${env.ENABLE_OPENAI:=__disabled__}/text-embedding-3-large\n  provider_id: ${env.ENABLE_OPENAI:=__disabled__}\n  provider_model_id: text-embedding-3-large\n  model_type: embedding\n- metadata: {}\n  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/claude-3-5-sonnet-latest\n  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n  provider_model_id: anthropic/claude-3-5-sonnet-latest\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/claude-3-7-sonnet-latest\n  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n  provider_model_id: anthropic/claude-3-7-sonnet-latest\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/claude-3-5-haiku-latest\n  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n  provider_model_id: anthropic/claude-3-5-haiku-latest\n  model_type: llm\n- metadata:\n    embedding_dimension: 1024\n    context_length: 32000\n  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/voyage-3\n  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n  provider_model_id: anthropic/voyage-3\n  model_type: embedding\n- metadata:\n    embedding_dimension: 512\n    context_length: 32000\n  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/voyage-3-lite\n  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n  provider_model_id: anthropic/voyage-3-lite\n  model_type: embedding\n- metadata:\n    embedding_dimension: 1024\n    context_length: 32000\n  model_id: ${env.ENABLE_ANTHROPIC:=__disabled__}/anthropic/voyage-code-3\n  provider_id: ${env.ENABLE_ANTHROPIC:=__disabled__}\n  provider_model_id: anthropic/voyage-code-3\n  model_type: embedding\n- metadata: {}\n  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-1.5-flash\n  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n  provider_model_id: gemini/gemini-1.5-flash\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-1.5-pro\n  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n  provider_model_id: gemini/gemini-1.5-pro\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-2.0-flash\n  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n  provider_model_id: gemini/gemini-2.0-flash\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-2.5-flash\n  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n  provider_model_id: gemini/gemini-2.5-flash\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/gemini-2.5-pro\n  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n  provider_model_id: gemini/gemini-2.5-pro\n  model_type: llm\n- metadata:\n    embedding_dimension: 768\n    context_length: 2048\n  model_id: ${env.ENABLE_GEMINI:=__disabled__}/gemini/text-embedding-004\n  provider_id: ${env.ENABLE_GEMINI:=__disabled__}\n  provider_model_id: gemini/text-embedding-004\n  model_type: embedding\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama3-8b-8192\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama3-8b-8192\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama3-8b-8192\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-3.1-8b-instant\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-3.1-8b-instant\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama3-70b-8192\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama3-70b-8192\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-3-70B-Instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama3-70b-8192\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-3.3-70b-versatile\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-3.3-70b-versatile\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-3.3-70b-versatile\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-3.2-3b-preview\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-3.2-3b-preview\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-3.2-3b-preview\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-4-scout-17b-16e-instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-4-scout-17b-16e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-4-scout-17b-16e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/meta-llama/llama-4-scout-17b-16e-instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/meta-llama/llama-4-scout-17b-16e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/meta-llama/llama-4-scout-17b-16e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/llama-4-maverick-17b-128e-instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-4-maverick-17b-128e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/llama-4-maverick-17b-128e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/groq/meta-llama/llama-4-maverick-17b-128e-instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/meta-llama/llama-4-maverick-17b-128e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_GROQ:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n  provider_id: ${env.ENABLE_GROQ:=__disabled__}\n  provider_model_id: groq/meta-llama/llama-4-maverick-17b-128e-instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.1-8B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.1-8B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.1-8B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.1-8B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.1-405B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.1-405B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.1-405B-Instruct-FP8\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.1-405B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.2-1B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.2-1B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.2-1B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.2-1B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.2-3B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.2-3B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.2-3B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.2-3B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-3.3-70B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.3-70B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.3-70B-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-3.3-70B-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Llama-3.2-11B-Vision-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Llama-3.2-11B-Vision-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.2-11B-Vision-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Llama-3.2-11B-Vision-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Llama-3.2-90B-Vision-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Llama-3.2-90B-Vision-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-3.2-90B-Vision-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Llama-3.2-90B-Vision-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Llama-4-Scout-17B-16E-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-4-Scout-17B-16E-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Llama-4-Scout-17B-16E-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Llama-4-Maverick-17B-128E-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Llama-4-Maverick-17B-128E-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-4-Maverick-17B-128E-Instruct\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Llama-4-Maverick-17B-128E-Instruct\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/sambanova/Meta-Llama-Guard-3-8B\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-Guard-3-8B\n  model_type: llm\n- metadata: {}\n  model_id: ${env.ENABLE_SAMBANOVA:=__disabled__}/meta-llama/Llama-Guard-3-8B\n  provider_id: ${env.ENABLE_SAMBANOVA:=__disabled__}\n  provider_model_id: sambanova/Meta-Llama-Guard-3-8B\n  model_type: llm\nshields:\n- shield_id: ${env.SAFETY_MODEL:=__disabled__}\n  provider_shield_id: ${env.ENABLE_OLLAMA:=__disabled__}/${env.SAFETY_MODEL:=__disabled__}\nvector_dbs: []\ndatasets: []\nscoring_fns: []\nbenchmarks: []\ntool_groups:\n- toolgroup_id: builtin::websearch\n  provider_id: tavily-search\n- toolgroup_id: builtin::rag\n  provider_id: rag-runtime\nserver:\n  port: 8321\n","numLines":1190,"startLine":1,"totalLines":1190}}}
{"parentUuid":"1b9a499f-1324-4d6b-84d3-67e0a84e1324","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_013rrUM1xLEymGTMK7RdNw8f","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Perfect! Now I have found the complete picture of how the embedding model fallback works. Let me examine the key pieces of code to understand the exact behavior:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":24466,"cache_read_input_tokens":42170,"output_tokens":165,"service_tier":"standard"}},"requestId":"req_011CRF9VCfXgykh1AGCaKELV","type":"assistant","uuid":"a230e73c-ca71-4712-ab15-4e28ebd917e0","timestamp":"2025-07-18T20:52:38.197Z"}
{"parentUuid":"a230e73c-ca71-4712-ab15-4e28ebd917e0","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_013rrUM1xLEymGTMK7RdNw8f","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01DGA6ptSWNCt7nZQQV3bwPu","name":"Grep","input":{"pattern":"await self\\.routing_table\\.get_all_with_type|\\.metadata\\.get\\(.*embedding_dimension","glob":"**/*.py","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":24466,"cache_read_input_tokens":42170,"output_tokens":165,"service_tier":"standard"}},"requestId":"req_011CRF9VCfXgykh1AGCaKELV","type":"assistant","uuid":"7b1b3822-19ef-4e0e-b96d-f550d376ce19","timestamp":"2025-07-18T20:52:40.001Z"}
{"parentUuid":"7b1b3822-19ef-4e0e-b96d-f550d376ce19","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DGA6ptSWNCt7nZQQV3bwPu","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:58:            all_models = await self.routing_table.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:68:                dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:190:        vector_dbs = await self.routing_table.get_all_with_type(\"vector_db\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/fireworks/fireworks.py:274:        if model.metadata.get(\"embedding_dimension\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/fireworks/fireworks.py:275:            kwargs[\"dimensions\"] = model.metadata.get(\"embedding_dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/vllm/vllm.py:524:        assert model.metadata.get(\"embedding_dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/vllm/vllm.py:525:        kwargs[\"dimensions\"] = model.metadata.get(\"embedding_dimension\")"}]},"uuid":"d009f519-37c1-4569-89ca-69cc5ac7495a","timestamp":"2025-07-18T20:52:40.048Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:58:            all_models = await self.routing_table.get_all_with_type(\"model\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:68:                dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py:190:        vector_dbs = await self.routing_table.get_all_with_type(\"vector_db\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/fireworks/fireworks.py:274:        if model.metadata.get(\"embedding_dimension\"):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/fireworks/fireworks.py:275:            kwargs[\"dimensions\"] = model.metadata.get(\"embedding_dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/vllm/vllm.py:524:        assert model.metadata.get(\"embedding_dimension\")\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/providers/remote/inference/vllm/vllm.py:525:        kwargs[\"dimensions\"] = model.metadata.get(\"embedding_dimension\")","numLines":7}}
{"parentUuid":"d009f519-37c1-4569-89ca-69cc5ac7495a","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01LBBtnm3wubhRVg8mB2uRqn","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Now let me search for any distribution registry implementation to understand how models are stored and retrieved:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":677,"cache_read_input_tokens":66636,"output_tokens":139,"service_tier":"standard"}},"requestId":"req_011CRF9VcBPiTxijE2Lf4S3L","type":"assistant","uuid":"ec134953-0a76-443c-9dcc-512d13e0932f","timestamp":"2025-07-18T20:52:42.909Z"}
{"parentUuid":"ec134953-0a76-443c-9dcc-512d13e0932f","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01LBBtnm3wubhRVg8mB2uRqn","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"tool_use","id":"toolu_01J6R9NDVRbGhpKcv8wod4hP","name":"Grep","input":{"pattern":"get_all.*registry|dist_registry\\.get_all","glob":"**/*.py","output_mode":"content","-n":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":677,"cache_read_input_tokens":66636,"output_tokens":139,"service_tier":"standard"}},"requestId":"req_011CRF9VcBPiTxijE2Lf4S3L","type":"assistant","uuid":"b8044fe1-c0b4-413e-a2ab-f10dd9c71b62","timestamp":"2025-07-18T20:52:44.071Z"}
{"parentUuid":"b8044fe1-c0b4-413e-a2ab-f10dd9c71b62","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01J6R9NDVRbGhpKcv8wod4hP","type":"tool_result","content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry_acl.py:64:    all_models = await cached_disk_dist_registry.get_all()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry_acl.py:80:    all_models = await cached_disk_dist_registry.get_all()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:135:async def test_get_all_objects(cached_disk_dist_registry):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:154:    all_results = await cached_disk_dist_registry.get_all()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py:213:        objs = await self.dist_registry.get_all()"}]},"uuid":"f335e934-c289-4bc8-9ea5-8511c8371607","timestamp":"2025-07-18T20:52:44.126Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry_acl.py:64:    all_models = await cached_disk_dist_registry.get_all()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry_acl.py:80:    all_models = await cached_disk_dist_registry.get_all()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:135:async def test_get_all_objects(cached_disk_dist_registry):\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/tests/unit/registry/test_registry.py:154:    all_results = await cached_disk_dist_registry.get_all()\n/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py:213:        objs = await self.dist_registry.get_all()","numLines":5}}
{"parentUuid":"f335e934-c289-4bc8-9ea5-8511c8371607","isSidechain":true,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01MHNG2WUVc3tjTzqRHMDcdz","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Based on my comprehensive search and analysis of the codebase, I found the exact implementation of the embedding model fallback behavior. Here's what I discovered:\n\n## Summary of Embedding Model Selection and Fallback Logic\n\nThe embedding model selection when creating vector stores without an explicit `embedding_model` parameter follows this specific fallback hierarchy:\n\n### 1. **Main Fallback Logic Location**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py`\n\n**Key Method:** `openai_create_vector_store()` (lines 119-178)\n\n### 2. **Fallback Hierarchy**\n\nThe fallback logic follows this order when `embedding_model` is not specified:\n\n1. **Provider-specific defaults** (lines 133-149): If a `provider_id` is specified, the router attempts to get the provider's configuration and use its default embedding model\n2. **System-wide \"first available\" model** (lines 151-157): If no provider default exists, it uses `_get_first_embedding_model()` method\n\n### 3. **The \"First Model\" Selection Logic**\n**Method:** `_get_first_embedding_model()` (lines 54-77)\n\n```python\nasync def _get_first_embedding_model(self) -> tuple[str, int] | None:\n    \"\"\"Get the first available embedding model identifier.\"\"\"\n    try:\n        # Get all models from the routing table\n        all_models = await self.routing_table.get_all_with_type(\"model\")\n        \n        # Filter for embedding models\n        embedding_models = [\n            model\n            for model in all_models\n            if hasattr(model, \"model_type\") and model.model_type == ModelType.embedding\n        ]\n        \n        if embedding_models:\n            dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n            if dimension is None:\n                raise ValueError(f\"Embedding model {embedding_models[0].identifier} has no embedding dimension\")\n            return embedding_models[0].identifier, dimension\n```\n\n### 4. **Model Order Source: run.yaml Files**\nThe \"first model\" comes from the order of models as they are defined in the `run.yaml` configuration files in templates. Looking at the starter template:\n\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/run.yaml`\n\n**Lines 264-315:** The models section shows that the **first embedding model** defined is:\n```yaml\n- metadata:\n    embedding_dimension: 384\n  model_id: all-MiniLM-L6-v2\n  provider_id: ${env.ENABLE_SENTENCE_TRANSFORMERS:=sentence-transformers}\n  model_type: embedding\n```\n\n### 5. **How Models are Loaded**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py`\n\n**Method:** `register_resources()` (lines 93-119)\nModels are registered in the order they appear in the `run.yaml` file's `models` section.\n\n### 6. **Routing Table Implementation**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py`\n\n**Method:** `get_all_with_type()` (lines 212-222)\nThis method retrieves all models from the distribution registry and filters by type.\n\n## Key Findings:\n\n1. **The fallback to \"first model in run.yaml\"** occurs in `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py` at **lines 151-157**\n\n2. **The selection logic** is in the `_get_first_embedding_model()` method at **lines 54-77** of the same file\n\n3. **The model ordering** comes from the sequence defined in template `run.yaml` files, with `all-MiniLM-L6-v2` being the first embedding model in the starter template\n\n4. **Models are registered** in order via `register_resources()` in `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py` at **lines 93-119**\n\nThe system takes the **first** embedding model (by array index `[0]`) from the list of embedding models returned by the routing table, which preserves the original order from the run.yaml configuration."}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":485,"cache_read_input_tokens":67313,"output_tokens":1177,"service_tier":"standard"}},"requestId":"req_011CRF9Vudd96kCXjHrc4xQs","type":"assistant","uuid":"3f1eef8a-c539-4666-8ddb-5cb6f59f0205","timestamp":"2025-07-18T20:53:05.223Z"}
{"parentUuid":"576a5922-99df-4e32-bba0-72f661cf5de7","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RMeEsEZ3X6pUqQPFWurrAV","type":"tool_result","content":[{"type":"text","text":"Based on my comprehensive search and analysis of the codebase, I found the exact implementation of the embedding model fallback behavior. Here's what I discovered:\n\n## Summary of Embedding Model Selection and Fallback Logic\n\nThe embedding model selection when creating vector stores without an explicit `embedding_model` parameter follows this specific fallback hierarchy:\n\n### 1. **Main Fallback Logic Location**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py`\n\n**Key Method:** `openai_create_vector_store()` (lines 119-178)\n\n### 2. **Fallback Hierarchy**\n\nThe fallback logic follows this order when `embedding_model` is not specified:\n\n1. **Provider-specific defaults** (lines 133-149): If a `provider_id` is specified, the router attempts to get the provider's configuration and use its default embedding model\n2. **System-wide \"first available\" model** (lines 151-157): If no provider default exists, it uses `_get_first_embedding_model()` method\n\n### 3. **The \"First Model\" Selection Logic**\n**Method:** `_get_first_embedding_model()` (lines 54-77)\n\n```python\nasync def _get_first_embedding_model(self) -> tuple[str, int] | None:\n    \"\"\"Get the first available embedding model identifier.\"\"\"\n    try:\n        # Get all models from the routing table\n        all_models = await self.routing_table.get_all_with_type(\"model\")\n        \n        # Filter for embedding models\n        embedding_models = [\n            model\n            for model in all_models\n            if hasattr(model, \"model_type\") and model.model_type == ModelType.embedding\n        ]\n        \n        if embedding_models:\n            dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n            if dimension is None:\n                raise ValueError(f\"Embedding model {embedding_models[0].identifier} has no embedding dimension\")\n            return embedding_models[0].identifier, dimension\n```\n\n### 4. **Model Order Source: run.yaml Files**\nThe \"first model\" comes from the order of models as they are defined in the `run.yaml` configuration files in templates. Looking at the starter template:\n\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/run.yaml`\n\n**Lines 264-315:** The models section shows that the **first embedding model** defined is:\n```yaml\n- metadata:\n    embedding_dimension: 384\n  model_id: all-MiniLM-L6-v2\n  provider_id: ${env.ENABLE_SENTENCE_TRANSFORMERS:=sentence-transformers}\n  model_type: embedding\n```\n\n### 5. **How Models are Loaded**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py`\n\n**Method:** `register_resources()` (lines 93-119)\nModels are registered in the order they appear in the `run.yaml` file's `models` section.\n\n### 6. **Routing Table Implementation**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py`\n\n**Method:** `get_all_with_type()` (lines 212-222)\nThis method retrieves all models from the distribution registry and filters by type.\n\n## Key Findings:\n\n1. **The fallback to \"first model in run.yaml\"** occurs in `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py` at **lines 151-157**\n\n2. **The selection logic** is in the `_get_first_embedding_model()` method at **lines 54-77** of the same file\n\n3. **The model ordering** comes from the sequence defined in template `run.yaml` files, with `all-MiniLM-L6-v2` being the first embedding model in the starter template\n\n4. **Models are registered** in order via `register_resources()` in `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py` at **lines 93-119**\n\nThe system takes the **first** embedding model (by array index `[0]`) from the list of embedding models returned by the routing table, which preserves the original order from the run.yaml configuration."}]}]},"uuid":"8b29288b-44ae-40e0-b589-75ca2bd17a69","timestamp":"2025-07-18T20:53:05.266Z","toolUseResult":{"content":[{"type":"text","text":"Based on my comprehensive search and analysis of the codebase, I found the exact implementation of the embedding model fallback behavior. Here's what I discovered:\n\n## Summary of Embedding Model Selection and Fallback Logic\n\nThe embedding model selection when creating vector stores without an explicit `embedding_model` parameter follows this specific fallback hierarchy:\n\n### 1. **Main Fallback Logic Location**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py`\n\n**Key Method:** `openai_create_vector_store()` (lines 119-178)\n\n### 2. **Fallback Hierarchy**\n\nThe fallback logic follows this order when `embedding_model` is not specified:\n\n1. **Provider-specific defaults** (lines 133-149): If a `provider_id` is specified, the router attempts to get the provider's configuration and use its default embedding model\n2. **System-wide \"first available\" model** (lines 151-157): If no provider default exists, it uses `_get_first_embedding_model()` method\n\n### 3. **The \"First Model\" Selection Logic**\n**Method:** `_get_first_embedding_model()` (lines 54-77)\n\n```python\nasync def _get_first_embedding_model(self) -> tuple[str, int] | None:\n    \"\"\"Get the first available embedding model identifier.\"\"\"\n    try:\n        # Get all models from the routing table\n        all_models = await self.routing_table.get_all_with_type(\"model\")\n        \n        # Filter for embedding models\n        embedding_models = [\n            model\n            for model in all_models\n            if hasattr(model, \"model_type\") and model.model_type == ModelType.embedding\n        ]\n        \n        if embedding_models:\n            dimension = embedding_models[0].metadata.get(\"embedding_dimension\", None)\n            if dimension is None:\n                raise ValueError(f\"Embedding model {embedding_models[0].identifier} has no embedding dimension\")\n            return embedding_models[0].identifier, dimension\n```\n\n### 4. **Model Order Source: run.yaml Files**\nThe \"first model\" comes from the order of models as they are defined in the `run.yaml` configuration files in templates. Looking at the starter template:\n\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/templates/starter/run.yaml`\n\n**Lines 264-315:** The models section shows that the **first embedding model** defined is:\n```yaml\n- metadata:\n    embedding_dimension: 384\n  model_id: all-MiniLM-L6-v2\n  provider_id: ${env.ENABLE_SENTENCE_TRANSFORMERS:=sentence-transformers}\n  model_type: embedding\n```\n\n### 5. **How Models are Loaded**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py`\n\n**Method:** `register_resources()` (lines 93-119)\nModels are registered in the order they appear in the `run.yaml` file's `models` section.\n\n### 6. **Routing Table Implementation**\n**File:** `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routing_tables/common.py`\n\n**Method:** `get_all_with_type()` (lines 212-222)\nThis method retrieves all models from the distribution registry and filters by type.\n\n## Key Findings:\n\n1. **The fallback to \"first model in run.yaml\"** occurs in `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/routers/vector_io.py` at **lines 151-157**\n\n2. **The selection logic** is in the `_get_first_embedding_model()` method at **lines 54-77** of the same file\n\n3. **The model ordering** comes from the sequence defined in template `run.yaml` files, with `all-MiniLM-L6-v2` being the first embedding model in the starter template\n\n4. **Models are registered** in order via `register_resources()` in `/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config/llama_stack/distribution/stack.py` at **lines 93-119**\n\nThe system takes the **first** embedding model (by array index `[0]`) from the list of embedding models returned by the routing table, which preserves the original order from the run.yaml configuration."}],"totalDurationMs":93541,"totalTokens":68980,"totalToolUseCount":16,"usage":{"input_tokens":5,"cache_creation_input_tokens":485,"cache_read_input_tokens":67313,"output_tokens":1177,"service_tier":"standard"},"wasInterrupted":false}}
{"parentUuid":"8b29288b-44ae-40e0-b589-75ca2bd17a69","isSidechain":false,"userType":"external","cwd":"/home/skamenan/git/code/llama-stack/.worktrees/fix-2729-vector-io-embedding-config","sessionId":"ab7efab1-4d63-4019-9110-c287f52e5091","version":"1.0.55","gitBranch":"feat/2729-configurable-embeddings-v2","message":{"id":"msg_01Cv4Y42TyDANoQqG9WK7DM9","type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"The code that selects \"the first model in run.yaml\" when `embedding_model` is omitted is in:\n\n**`llama_stack/distribution/routers/vector_io.py:151-157`** and **`llama_stack/distribution/routers/vector_io.py:54-77`**\n\nThe fallback logic in `openai_create_vector_store()` calls `_get_first_embedding_model()` which gets all embedding models from the routing table and returns `embedding_models[0]` - the first one in the list, which preserves the order from `run.yaml`."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":1398,"cache_read_input_tokens":16078,"output_tokens":144,"service_tier":"standard"}},"requestId":"req_011CRF9XU2ZB71LKaoQB3j7q","type":"assistant","uuid":"ff884afb-9d7c-4021-ab77-e68ebd93eeb2","timestamp":"2025-07-18T20:53:09.719Z"}
